---
r-fit-text: true
bibliography: ../references.bib
format: 
  clean-revealjs: default
  cleanpdf-typst: default
---

# The General Linear Model as a Foundation {background-color="#1E3D59"}

```{r}
#| echo: false
#| message: false
#| warning: false
library(tidyverse)
library(haven)
library(knitr)
library(broom)
library(ggplot2)
library(gtsummary)
library(cowplot)
library(gridExtra)
library(hrbrthemes)
library(patchwork) # For combining plots

# Set common options
knitr::opts_chunk$set(dev = "ragg_png")
pdf.options(encoding = "CP1250")
```

## Real-World Applications of Statistical Tests

**Key Applications:**

- **QC Testing**: Pharmaceutical quality control
- **A/B Testing**: Digital marketing optimization
- **Agricultural Research**: Crop yield optimization
- **Retail Strategy**: Store location and pricing
- **Workforce Planning**: Staff scheduling
- **Property Valuation**: Real estate pricing

**The Real Question**: Can we unify these seemingly different techniques?

## Applications in Business and Research {.smaller}

:::::: columns
:::: {.column width="33%"}
**One-sample t-test**

![QC Testing](https://images.unsplash.com/photo-1587854692152-cbe660dbde88?w=500&auto=format&fit=crop&q=60&ixlib=rb-4.0.3)

Quality Control: Testing medication tablets against 500mg standard
::::

:::: {.column width="33%"}
**Independent t-test**

![A/B Testing](https://images.unsplash.com/photo-1551288049-bebda4e38f71?w=500&auto=format&fit=crop&q=60&ixlib=rb-4.0.3)

A/B Testing: Comparing website conversion rates between designs
::::

:::: {.column width="33%"}
**ANOVA**

![Fertilizer Testing](https://images.unsplash.com/photo-1592982537447-7440770cbfc9?w=500&auto=format&fit=crop&q=60&ixlib=rb-4.0.3)

Agricultural: Comparing yields across multiple fertilizer types
::::
::::::

::: notes
One-sample t-test applications:
- Quality Control: Testing if medication tablets contain exactly 500mg of active ingredient
- Variables: Measured amounts in sample tablets vs. labeled 500mg
- Why appropriate: Need to test against a specific fixed value, not compare groups
- Impact: FDA compliance, preventing recalls and ensuring patient safety

Independent t-test applications:
- A/B Testing: Comparing website conversion rates between designs
- Variables: Conversion rate (%) for visitors shown design A vs. B
- Why appropriate: Two separate groups with continuous outcome
- Impact: Implementing design with higher conversion rate can increase revenue by millions

ANOVA applications:
- Agricultural Research: Comparing crop yields across fertilizer types
- Variables: Yield (bushels/acre) for four different fertilizers
- Why appropriate: Comparing means across >2 groups
- Impact: Selecting highest-yielding fertilizer can increase annual revenue by thousands per acre

These are concrete examples that students can relate to, showing the practical importance of these statistical methods.
:::

## From Isolated Tests to Unified Framework

:::::: columns
:::: {.column width="50%"}
```{r, echo=FALSE, fig.height=4, fig.width=4.5}
# Create a visualization of the traditional vs. unified approach
tests <- c("t-tests", "ANOVA", "Correlation", "Regression")
x_pos <- c(1, 2, 3, 4)
y_pos1 <- c(1, 1, 1, 1)
y_pos2 <- c(3, 3, 3, 3)

# Create a data frame for plotting
plot_data <- data.frame(
  x = c(x_pos, x_pos),
  y = c(y_pos1, y_pos2),
  test = c(tests, tests),
  approach = c(rep("Traditional", 4), rep("Unified", 4))
)

# Create connections for unified approach
connections <- data.frame(
  x = c(1, 2, 3, 1, 2, 3),
  y = c(3, 3, 3, 3, 3, 3),
  xend = c(2, 3, 4, 3, 4, 4),
  yend = c(3, 3, 3, 3, 3, 3)
)

ggplot() +
  # Traditional approach: isolated points
  geom_point(
    data = subset(plot_data, approach == "Traditional"),
    aes(x = x, y = y), size = 8, color = "darkblue"
  ) +
  # Unified approach: connected points
  geom_point(
    data = subset(plot_data, approach == "Unified"),
    aes(x = x, y = y), size = 8, color = "darkred"
  ) +
  # Add connections for unified approach
  geom_segment(
    data = connections,
    aes(x = x, y = y, xend = xend, yend = yend),
    color = "darkred", size = 1
  ) +
  # Add test labels
  geom_text(
    data = plot_data,
    aes(x = x, y = y, label = test),
    color = "white", size = 3
  ) +
  # Add approach labels
  annotate("text",
    x = 2.5, y = 1.5, label = "Traditional: Isolated Tests",
    size = 4, color = "darkblue"
  ) +
  annotate("text",
    x = 2.5, y = 3.5, label = "Unified: Connected Framework",
    size = 4, color = "darkred"
  ) +
  # Theming
  theme_void() +
  theme(plot.background = element_rect(fill = "white"))
```
::::

:::: {.column width="50%"}
**Traditional Approach:**

- Different formulas for each test
- Separate assumptions to memorize
- Disconnected interpretation methods
- No clear pathway between methods

**Unified GLM Approach:**
- One underlying framework
- Common set of assumptions
- Consistent interpretation
- Clear relationships between tests
- Greater flexibility for complex questions
::::
::::::

**Today's Goal:** See how seemingly different methods are variations of a single powerful framework that can answer complex, real-world questions.

::: notes
The traditional approach to teaching statistics presents each test as a separate technique with its own formulas, assumptions, and applications. This can make statistics feel like a collection of disconnected tools rather than a coherent framework.

In contrast, the unified GLM approach reveals that many common statistical tests are actually special cases of the same underlying model. This perspective has several advantages:
- It reduces the amount of information students need to memorize
- It clarifies the connections between different statistical procedures
- It provides a more coherent framework for understanding statistics
- It makes it easier to extend to more complex situations

Our goal today is to show how t-tests, ANOVA, correlation, and regression can all be understood as variations of the general linear model, providing a more unified and powerful approach to statistical analysis.
:::

## The Beauty of Unified Statistical Thinking

::: {.nonincremental}
Adapted from:

- [*Statistical Thinking*](https://statsthinking21.github.io/statsthinking21-core-site/), Chapter 10-11. Russell A. Poldrack (2019).
- [*Common statistical tests are linear models*](https://lindeloev.github.io/tests-as-linear/). Jonas Kristoffer LindelÃ¸v (2019).
:::

```{r echo=FALSE, fig.height=4}
# Create family tree diagram to visualize relationships
set.seed(42)
# Create a simple network visualization using ggplot
nodes <- tibble(
  id = c(1, 2, 3, 4, 5, 6, 7, 8),
  label = c(
    "General Linear Model", "Regression", "ANOVA", "t-tests",
    "Multiple Regression", "Simple Regression", "One-way ANOVA",
    "Independent t-test"
  ),
  x = c(5, 3, 7, 5, 2, 4, 7, 5),
  y = c(5, 4, 4, 3, 3, 3, 3, 2)
)

edges <- tibble(
  from = c(1, 1, 1, 2, 2, 3, 4),
  to = c(2, 3, 4, 5, 6, 7, 8),
  x = c(nodes$x[1], nodes$x[1], nodes$x[1], nodes$x[2], nodes$x[2], nodes$x[3], nodes$x[4]),
  y = c(nodes$y[1], nodes$y[1], nodes$y[1], nodes$y[2], nodes$y[2], nodes$y[3], nodes$y[4]),
  xend = c(nodes$x[2], nodes$x[3], nodes$x[4], nodes$x[5], nodes$x[6], nodes$x[7], nodes$x[8]),
  yend = c(nodes$y[2], nodes$y[3], nodes$y[4], nodes$y[5], nodes$y[6], nodes$y[7], nodes$y[8])
)

ggplot() +
  # Add edges first
  geom_segment(
    data = edges,
    aes(x = x, y = y, xend = xend, yend = yend),
    color = "gray50", size = 1, lineend = "round"
  ) +
  # Add nodes
  geom_point(
    data = nodes,
    aes(x = x, y = y),
    size = 15,
    color = ifelse(nodes$id == 1, "darkred", "steelblue")
  ) +
  # Add labels
  geom_text(
    data = nodes,
    aes(x = x, y = y, label = label),
    color = "white", size = 3, fontface = "bold"
  ) +
  # Set theme
  theme_void() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 14),
    plot.margin = margin(20, 20, 20, 20)
  ) +
  coord_fixed()
```

::: notes
In traditional statistics education, students often learn about different statistical tests as if they were distinct techniques with different formulas, assumptions, and applications. This can make statistics feel like a collection of disconnected tools rather than a coherent framework. In reality, many common statistical tests can be understood as special cases of the same underlying model: the general linear model.

The diagram shows how the General Linear Model serves as the unifying framework, with various statistical tests branching from it. This hierarchy helps students visualize how seemingly different tests are actually related.
:::

## The General Linear Model Framework

:::::: columns
:::: {.column width="60%"}
The general linear model can be expressed as:

$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_n x_n + \varepsilon$$

Where:
- $y$ is the outcome variable
- $\beta_0$ is the intercept
- $\beta_1, \beta_2, ..., \beta_n$ are the coefficients
- $x_1, x_2, ..., x_n$ are the predictor variables
- $\varepsilon$ is the error term (normally distributed with mean 0)
::::

:::: {.column width="40%"}
```{r echo=FALSE, fig.height=3.5}
# Create a simple visual representation of the linear model formula
library(ggplot2)

# Create some simulated data
set.seed(123)
x <- seq(1, 10, length.out = 20)
y <- 2 + 0.5 * x + rnorm(20, 0, 1)
df <- data.frame(x = x, y = y)

# Plot
ggplot(df, aes(x, y)) +
  geom_point(size = 2, color = "steelblue") +
  geom_smooth(method = "lm", se = FALSE, color = "darkred") +
  annotate("text", x = 3, y = 7.5, label = expression(beta[0]), color = "darkred", size = 5) +
  annotate("segment",
    x = 2, xend = 2, y = 2, yend = 4,
    arrow = arrow(length = unit(0.3, "cm")), color = "darkred"
  ) +
  annotate("text", x = 8, y = 7, label = expression(beta[1]), color = "darkred", size = 5) +
  annotate("segment",
    x = 8, xend = 9, y = 6.5, yend = 6.5,
    arrow = arrow(length = unit(0.3, "cm")), color = "darkred"
  ) +
  annotate("segment",
    x = 9, xend = 9, y = 6.5, yend = 7.5,
    arrow = arrow(length = unit(0.3, "cm")), color = "darkred"
  ) +
  theme_minimal() +
  labs(
    title = "Visual Representation",
    x = "Predictor (x)",
    y = "Outcome (y)"
  )
```
::::
::::::

Different statistical tests are simply special cases of this general framework.

::: notes
The general linear model is a statistical framework that encompasses many common statistical tests. At its core, it models the relationship between a dependent variable (y) and one or more independent variables (x). The model assumes that y is a linear function of the x variables, plus some error term.

This equation looks like a multiple regression equation - and that's because regression is indeed one case of the general linear model. But so are t-tests, ANOVA, and many other statistical procedures.

The visual representation shows:
- Î²â (beta zero) is the intercept - the value of y when all predictors are zero
- Î²â (beta one) is the slope - the change in y for a one-unit increase in x
- The dots represent actual data points
- The line represents the model's predictions
- The error term (Îµ) accounts for the deviation of points from the line
:::

## Building from Simple Cases: One-sample t-test {.smaller}

:::::: columns
:::: {.column width="55%"}
The one-sample t-test can be represented as:

$$y = \beta_0 + \varepsilon$$

Here, $\beta_0$ is the population mean Î¼, and we test the null hypothesis that $\beta_0 = \mu_0$ (some specified value).

```{r}
# Create example data
set.seed(123)
y <- rnorm(20, mean = 5, sd = 2)
```

```{r}
#| echo: true
# Traditional t-test
t_test_result <- t.test(y, mu = 0)

# Same test as linear model
lm_result <- lm(y ~ 1)
```
::::

:::: {.column width="45%"}
```{r echo=FALSE, fig.height=3.5}
# Visualize one-sample t-test as intercept-only model
set.seed(123)
sample_data <- data.frame(
  x = rep(1, 20), # Dummy x-variable
  y = y
)

# Plot the data with mean line
ggplot(sample_data, aes(x = x, y = y)) +
  # Add jittered points for visibility
  geom_jitter(width = 0.2, height = 0, alpha = 0.7, color = "steelblue", size = 2) +
  # Add mean line (intercept)
  geom_hline(yintercept = mean(y), color = "darkred", size = 1) +
  # Add confidence interval
  geom_ribbon(
    aes(
      ymin = mean(y) - 2 * sd(y) / sqrt(length(y)),
      ymax = mean(y) + 2 * sd(y) / sqrt(length(y))
    ),
    alpha = 0.2, fill = "darkred"
  ) +
  # Add annotation for intercept
  annotate("text",
    x = 1.3, y = mean(y) + 0.5,
    label = expression(beta[0] ~ "(mean)"), color = "darkred", size = 4
  ) +
  # Theme and labels
  theme_minimal() +
  labs(
    title = "One-sample t-test as Linear Model",
    subtitle = "Testing if mean (Î²â) equals hypothesized value",
    x = "",
    y = "Value"
  ) +
  scale_x_continuous(breaks = NULL) +
  coord_cartesian(xlim = c(0.5, 1.5))
```

**T-test and Linear Model Equivalence:**

```{r}
#| echo: true
# Compare results
data.frame(
  Method = c("t-test", "lm"),
  Mean = c(t_test_result$estimate, coef(lm_result)[1]),
  t_value = c(t_test_result$statistic, summary(lm_result)$coefficients[1, 3])
)
```
::::
::::::

::: notes
Let's start with the simplest case: the one-sample t-test. This test is used when we want to compare a sample mean to a known value. In the general linear model framework, this is simply a model with only an intercept term.

The intercept in this model represents the mean of the variable y. When we perform a one-sample t-test, we're essentially testing whether this intercept (the mean) is equal to our hypothesized value.

The t-statistic from the t-test is exactly the same as the t-statistic for the intercept in the linear model. This demonstrates that the one-sample t-test is just a special case of the linear model where we're only estimating and testing the intercept.

In the visualization:
- Each blue dot represents a data point in our sample
- The horizontal red line represents the mean (Î²â)
- The shaded area represents the confidence interval around the mean
- We're testing whether this mean equals some hypothesized value (e.g., zero)
:::

## Independent t-test as Linear Model

:::::: columns
:::: {.column width="45%"}
The independent t-test can be represented as:

$$y = \beta_0 + \beta_1 x_1 + \varepsilon$$

Where $x_1$ is a dummy variable (0/1) for group membership.

```{r}
# Example data for two groups
set.seed(123)
group <- factor(rep(c("A", "B"), each = 10))
y_grouped <- c(
  rnorm(10, mean = 5, sd = 2),
  rnorm(10, mean = 7, sd = 2)
)
data <- data.frame(y = y_grouped, group = group)

# Run both tests
t_test_grouped <- t.test(y ~ group, data = data, var.equal = TRUE)
lm_grouped <- lm(y ~ group, data = data)
```

- $\beta_0$ = mean of reference group A
- $\beta_1$ = difference between groups (B - A)
::::

:::: {.column width="55%"}
```{r echo=FALSE, fig.height=4}
# Create a visualization of the independent t-test as linear model
ggplot(data, aes(x = group, y = y, color = group)) +
  # Add jittered points
  geom_jitter(width = 0.2, alpha = 0.7, size = 2) +
  # Add group means
  stat_summary(fun = mean, geom = "point", size = 4, shape = 18) +
  # Add lines for means
  stat_summary(
    fun = mean, geom = "errorbar",
    aes(ymax = ..y.., ymin = ..y..), width = 0.4
  ) +
  # Add intercept annotation
  annotate("text",
    x = 1, y = mean(data$y[data$group == "A"]) - 0.5,
    label = expression(beta[0] ~ "(Group A mean)"), color = "red", size = 3.5
  ) +
  # Add difference annotation with arrow
  annotate("segment",
    x = 1.2, xend = 1.8,
    y = mean(data$y[data$group == "A"]) + 0.5,
    yend = mean(data$y[data$group == "A"]) + 0.5,
    arrow = arrow(length = unit(0.3, "cm")), color = "blue"
  ) +
  annotate("text",
    x = 1.5, y = mean(data$y[data$group == "A"]) + 1,
    label = expression(beta[1] ~ "(difference)"), color = "blue", size = 3.5
  ) +
  # Theme and labels
  theme_minimal() +
  labs(
    title = "Independent t-test as Linear Model",
    subtitle = "The coefficient (Î²â) tests the difference between groups",
    x = "Group",
    y = "Value"
  ) +
  theme(legend.position = "none")
```

**Equivalence of Results:**
```{r echo=TRUE}
# Compare t-statistic for group difference
c(
  t_test = t_test_grouped$statistic,
  lm_t = summary(lm_grouped)$coefficients[2, 3]
)
```
::::
::::::

::: notes
Moving to the independent samples t-test, we're now comparing means between two groups. In the general linear model framework, we add a predictor variable representing group membership.

This predictor is a dummy variable: it's 0 for one group and 1 for the other. The intercept (Î²â) now represents the mean of the reference group (the one coded as 0), and the coefficient Î²â represents the difference in means between the two groups.

The t-statistic for testing whether Î²â equals zero is exactly the same as the t-statistic from the independent samples t-test. This tests whether the difference between group means is zero.

In the visualization:
- Group A's mean is represented by Î²â (the intercept)
- The difference between groups (B - A) is represented by Î²â
- The t-test is testing whether this difference (Î²â) is significantly different from zero
- The exact same t-value is produced by both the traditional t-test and the linear model

This shows how the independent t-test is just a special case of the linear model with a binary predictor variable.
:::

## Multiple Regression: The Full Linear Model

:::::: columns
:::: {.column width="45%"}
Adding multiple predictors extends the model:

$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \varepsilon$$

```{r}
# Example data with continuous predictors
set.seed(456)
x1 <- rnorm(20, mean = 50, sd = 10)
x2 <- rnorm(20, mean = 100, sd = 15)
y_multi <- 10 + 0.5 * x1 + 0.3 * x2 + rnorm(20, 0, 5)
multi_data <- data.frame(y = y_multi, x1 = x1, x2 = x2)

# Multiple regression model
multi_model <- lm(y ~ x1 + x2, data = multi_data)
```

**Interpretation:**

- $\beta_0$: Expected y when all predictors = 0
- $\beta_1$: Effect of x1, holding x2 constant
- $\beta_2$: Effect of x2, holding x1 constant
::::

:::: {.column width="55%"}
```{r echo=FALSE, fig.height=4.5, fig.width=5.5}
# Create a 3D visualization of multiple regression
# First, create a grid of values for x1 and x2
grid_size <- 10
x1_range <- seq(min(x1), max(x1), length.out = grid_size)
x2_range <- seq(min(x2), max(x2), length.out = grid_size)
grid <- expand.grid(x1 = x1_range, x2 = x2_range)

# Predict y values for the grid
grid$y_pred <- predict(multi_model, newdata = grid)

# Convert to matrix format for persp()
z_matrix <- matrix(grid$y_pred, nrow = grid_size)

# Create prediction surface
library(plotly)

plot_ly() |>
  add_markers(
    data = multi_data, x = ~x1, y = ~x2, z = ~y,
    marker = list(color = "blue", size = 6, opacity = 0.8),
    name = "Data points"
  ) |>
  add_surface(
    x = x1_range, y = x2_range, z = z_matrix,
    opacity = 0.7, colorscale = "Reds", name = "Regression plane"
  ) |>
  layout(
    title = "Multiple Regression as a Plane in 3D Space",
    scene = list(
      xaxis = list(title = "Predictor x1"),
      yaxis = list(title = "Predictor x2"),
      zaxis = list(title = "Outcome y")
    )
  )
```

**Model Coefficients:**
```{r}
# Display coefficients
tidy(multi_model) |>
  select(term, estimate, p.value) |>
  knitr::kable(digits = 3)
```
::::
::::::

::: notes
When we add more predictors to our model, we get multiple regression. Each coefficient now represents the effect of its corresponding predictor on the outcome, while holding all other predictors constant.

The interpretation of these coefficients follows the same pattern as before: the intercept is the expected value of y when all predictors are zero, and each coefficient represents the expected change in y for a one-unit increase in the corresponding predictor, while holding all other predictors constant.

The t-statistics for each coefficient test whether that predictor has a significant effect on the outcome, controlling for all other predictors in the model.

The 3D visualization shows:
- The blue dots are our actual data points in 3D space (x1, x2, y)
- The red plane is the predicted relationship from our multiple regression model
- The plane's height at any point (x1, x2) represents the predicted value of y
- The plane's slope in the x1 direction represents Î²â
- The plane's slope in the x2 direction represents Î²â
- The plane's height when both x1 and x2 are zero represents Î²â (the intercept)

This shows how multiple regression extends our 2D line to a multidimensional plane or hyperplane.
:::

## Real-world Example: HR Analytics

:::::: columns
:::: {.column width="50%"}
Let's apply the GLM approach to a real dataset from an insurance company HR department.

```{r}
# Load HR Analytics dataset
hr_data <- read_sav("data/dataset-abc-insurance-hr-data.sav") |>
  janitor::clean_names() |>
  mutate(gender = as_factor(gender))
```

**Key Variables:**
- `salarygrade`: Salary level (outcome)
- `tenure`: Years of experience
- `evaluation`: Performance rating
- `gender`: Employee gender
- `job_satisfaction`: Employee satisfaction
- `job_role`: Department/role

**Research Question:** What factors predict salary in this organization?
::::

:::: {.column width="50%"}
```{r echo=FALSE, fig.height=4.2}
# Create a visual summary of key variables
p1 <- ggplot(hr_data, aes(x = tenure)) +
  geom_histogram(fill = "steelblue", bins = 10) +
  theme_minimal() +
  labs(title = "Years of Experience")

p2 <- ggplot(hr_data, aes(x = evaluation)) +
  geom_histogram(fill = "darkred", bins = 10) +
  theme_minimal() +
  labs(title = "Performance Ratings")

p3 <- ggplot(hr_data, aes(x = gender, fill = gender)) +
  geom_bar() +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(title = "Gender Distribution")

p4 <- ggplot(hr_data, aes(y = salarygrade)) +
  geom_boxplot(fill = "darkgreen") +
  theme_minimal() +
  labs(title = "Salary Distribution", y = "")

(p1 + p2) / (p3 + p4)
```
::::
::::::

::: notes
Now let's apply these concepts to a real-world dataset. This HR analytics dataset contains information about employees at an insurance company, including demographic information, salary, job satisfaction, years of experience, and performance ratings.

The visual summary shows the distributions of our key variables. We see that:
- Years of experience has a roughly normal distribution with most employees having 5-15 years
- Performance ratings are also roughly normally distributed
- There's a gender imbalance with more males than females
- Salary shows a wide range with some outliers at the high end

We'll use this dataset to build multiple regression models predicting salary based on various employee characteristics.
:::

## Multiple Regression with HR Data {.smaller}

:::::: columns
:::: {.column width="45%"}
Let's predict salary based on years of experience, performance rating, and gender:

```{r}
# Create HR linear model
hr_model <- lm(salarygrade ~ tenure + evaluation + gender,
  data = hr_data
)

# Model summary
model_summary <- tidy(hr_model) |>
  mutate(
    term = case_when(
      term == "(Intercept)" ~ "Intercept",
      term == "tenure" ~ "Years Experience",
      term == "evaluation" ~ "Performance Rating",
      term == "genderMale" ~ "Gender (Male)",
      TRUE ~ term
    )
  )
```

**Coefficients:**
```{r}
model_summary |>
  select(term, estimate, p.value) |>
  kable(digits = 2)
```

**Model Fit:** $R^2 = `r round(glance(hr_model)$r.squared, 2)`$  
**Interpretation:** The model explains `r round(glance(hr_model)$r.squared*100)`% of salary variance.
::::

:::: {.column width="55%"}
```{r echo=FALSE, fig.height=4.5}
# Create a coefficient plot
model_summary |>
  filter(term != "Intercept") |>
  mutate(term = factor(term, levels = rev(c("Years Experience", "Performance Rating", "Gender (Male)")))) |>
  ggplot(aes(x = estimate, y = term, color = p.value < 0.05)) +
  geom_point(size = 3) +
  geom_errorbarh(
    aes(
      xmin = estimate - 1.96 * std.error,
      xmax = estimate + 1.96 * std.error
    ),
    height = 0.2
  ) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  scale_color_manual(
    values = c("gray50", "darkred"),
    labels = c("Non-significant", "Significant (p<0.05)")
  ) +
  labs(
    title = "Effect of Predictors on Salary",
    subtitle = "Points show estimates with 95% confidence intervals",
    x = "Effect on Salary ($1000s)",
    y = "",
    color = "Significance"
  ) +
  theme_minimal()
```

**Key Findings:**

1. **Gender Gap:** Male employees earn ~$7,700 more on average, holding other factors constant
   
2. **Experience:** Each additional year of experience adds ~$1,200 to salary

3. **Performance:** Each additional point in performance rating adds ~$4,700 to salary

All these effects are statistically significant (p < 0.05).
::::
::::::

::: notes
In this multiple regression model, we're predicting salary based on years of experience, performance rating, and gender. The coefficients tell us:

- For each additional year of experience, salary increases by about $1,169, holding other factors constant
- For each additional point in performance rating, salary increases by about $4,743, holding other factors constant
- Male employees earn about $7,722 more than female employees with the same experience and performance rating

The R-squared value of 0.55 tells us that about 55% of the variance in salary is explained by these three predictors combined.

The coefficient plot provides a visual representation of these effects and their confidence intervals. It makes it easier to see which predictors have the largest effects and which are statistically significant (those where the confidence interval doesn't cross zero).

This analysis might prompt further investigation into potential gender-based pay disparities in this organization.
:::

## Visualizing the Gender Effect

```{r echo=FALSE, fig.height=4.5}
# Create a more informative visualization of gender differences
ggplot(hr_data, aes(x = tenure, y = salarygrade, color = gender)) +
  # Add points with transparency
  geom_point(alpha = 0.4) +
  # Add regression lines by gender
  geom_smooth(method = "lm", formula = y ~ x, se = FALSE, linewidth = 1) +
  # Add explicit labels for the gender gap
  annotate("segment",
    x = 15, xend = 15,
    y = predict(hr_model, newdata = data.frame(tenure = 15, evaluation = mean(hr_data$evaluation), gender = "Female")),
    yend = predict(hr_model, newdata = data.frame(tenure = 15, evaluation = mean(hr_data$evaluation), gender = "Male")),
    arrow = arrow(length = unit(0.2, "cm"), ends = "both"),
    color = "black", linewidth = 0.8
  ) +
  annotate("text",
    x = 16,
    y = mean(c(
      predict(hr_model, newdata = data.frame(tenure = 15, evaluation = mean(hr_data$evaluation), gender = "Female")),
      predict(hr_model, newdata = data.frame(tenure = 15, evaluation = mean(hr_data$evaluation), gender = "Male"))
    )),
    label = "Gender Gap\n$7,722", hjust = 0
  ) +
  # Theme and labels
  theme_minimal() +
  scale_color_brewer(palette = "Set1", labels = c("Female", "Male")) +
  labs(
    title = "Gender Pay Gap Visualization",
    subtitle = "Male employees earn more at the same experience level",
    x = "Years of Experience",
    y = "Salary ($1000s)",
    color = "Gender"
  )
```

::: notes
This visualization shows the relationship between years of experience and salary, separated by gender. The parallel lines represent our model's assumption that the effect of years of experience on salary is the same for both genders - the only difference is in the intercept (the starting point).

The gap between the lines represents the gender effect we saw in our model. Male employees (represented by the red line) tend to have higher salaries than female employees (represented by the blue line) with the same years of experience.

We've explicitly labeled the gender gap ($7,722) to make the effect size clear. This represents how much more, on average, male employees earn compared to female employees with the same experience and performance rating.

This illustrates how categorical variables work in the general linear model - they shift the intercept (or baseline) for different groups but don't change the slope of the relationship (assuming we don't include an interaction term).

In a real-world analysis, this finding would likely prompt further investigation into whether this gap represents a pay equity issue that needs to be addressed.
:::
