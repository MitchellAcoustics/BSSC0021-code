---
r-fit-text: true
bibliography: ../references.bib
---

# ANOVA and Extended Applications of Linear Models {background-color="#1E3D59"}

```{r}
#| echo: false
#| message: false
#| warning: false
library(tidyverse)
library(haven)
library(knitr)
library(broom)
library(ggplot2)
library(gtsummary)
library(cowplot)
library(gridExtra)
library(hrbrthemes)
library(effectsize)
library(emmeans)
library(patchwork)

# Load the fuel consumption dataset
load("data/dataset-canada-fuel-2015-subset1.Rdata")
fuel_data <- data

# Set common options
knitr::opts_chunk$set(
  dev = "ragg_png",
  message = FALSE,
  warning = FALSE,
  fig.width = 7,
  fig.height = 5
)
pdf.options(encoding = "CP1250")

# For reproducible examples
set.seed(1234)
```

## Extending the General Linear Model Framework

Having established the basic GLM framework and seen how t-tests and multiple regression are special cases, we'll now explore:

1. **Analysis of Variance (ANOVA)** as an extension of the linear model
2. **Correlation techniques** expressed as linear models
3. **Non-parametric tests** as transformations of parametric tests
4. **Practical code implementations** showing the equivalences

These applications further demonstrate the power of the unified statistical framework.

::: notes
Now that we've established how the General Linear Model provides a unified framework for basic statistical tests like t-tests and simple regression, we'll extend our exploration to more complex applications. 

In this section, we'll see how ANOVA, which is traditionally taught as a separate technique, is actually just another manifestation of the linear model. We'll also explore correlation methods, non-parametric alternatives, and practical code implementations.

By continuing to build on the GLM framework, we reinforce the idea that these seemingly different statistical procedures are variations on the same underlying theme. This approach not only simplifies learning but also enables more flexible and sophisticated statistical modeling.
:::

## Unifying Statistical Tests: Summary Table

```{r echo=FALSE}
tests_table <- tibble(
    Test = c("Correlation", "One-sample t-test", "Independent t-test", "Paired t-test", "One-way ANOVA", "Two-way ANOVA", "Multiple regression"),
    `Linear Model Formula` = c("y ~ x", "y ~ 1", "y ~ group", "diff ~ 1", "y ~ group", "y ~ factorA * factorB", "y ~ x1 + x2 + ..."),
    `What's being tested` = c("Slope coefficient", "Intercept", "Group coefficient", "Intercept of differences", "Group coefficients", "Main effects & interaction", "Multiple coefficients")
)

knitr::kable(tests_table)
```

**Key insights from this table:**

- Each common test has a corresponding linear model formulation
- Many tests share the same model structure but test different coefficients
- Understanding the pattern makes it easier to apply the right test for your research question

::: notes
This table summarizes how different common tests map to linear model formulations. For each test, we can identify what linear model would be equivalent and which coefficient(s) we're testing.

Notice that the difference between tests often comes down to:

1. What variables we include in the model
2. Which coefficient(s) we're interested in testing
3. How we interpret the results

This unified framework helps students see that they're not learning completely different procedures for each test, but rather applying the same underlying model in different contexts.

The table serves as a reference guide that students can use when deciding which statistical approach to use for their research questions. It emphasizes that the choice of test is about identifying the appropriate model structure for the research question, rather than selecting from an unrelated menu of statistical options.
:::

## ANOVA: Comparing Multiple Groups

ANOVA (Analysis of Variance) is traditionally taught as a distinct statistical test for comparing means across multiple groups.

::: incremental
- Null hypothesis: All group means are equal ($\mu_1 = \mu_2 = ... = \mu_k$)
- Alternative hypothesis: At least one group mean differs from the others
- Test statistic: F-ratio (ratio of between-group to within-group variance)
:::

::: notes
ANOVA is traditionally taught as a distinct test from regression, with its own set of formulas and concepts like "sums of squares" and "F-ratios." However, ANOVA is actually just another manifestation of the general linear model.

The key insight is that when we compare means across groups, we're essentially predicting an outcome (y) based on group membership (a categorical variable). This can be seamlessly represented within the linear model framework.
:::

## Fuel Consumption Dataset

Let's use a real dataset on fuel consumption in Canada to demonstrate ANOVA as a linear model.

```{r}
# View the structure of the fuel consumption dataset
fuel_data |>
  select(make, model, class, enginesize, cylinders468, fueluseboth) |>
  head(5) |>
  kable()
```

::: notes
This dataset contains information about vehicles sold in Canada, including their fuel consumption (measured in liters per 100 kilometers), engine characteristics, and vehicle class.

We'll use this data to compare average fuel consumption across different vehicle classes, first using traditional ANOVA and then showing the equivalent linear model approach.
:::

## One-way ANOVA: Traditional Approach

Let's compare fuel consumption across vehicle classes:

```{r}
# Run traditional ANOVA
anova_result <- aov(fueluseboth ~ class, data = fuel_data)
summary(anova_result)
```

The significant p-value (< 0.05) indicates that average fuel consumption differs significantly across vehicle classes.

::: notes
The traditional ANOVA output shows us the familiar ANOVA table with sums of squares, degrees of freedom, mean squares, and the F-statistic. The very small p-value tells us that there are significant differences in fuel consumption between vehicle classes.

But how does this relate to the linear model? Let's see.
:::

## One-way ANOVA as Linear Model

The same analysis using the linear model approach:

```{r}
# Run equivalent linear model
lm_result <- lm(fueluseboth ~ class, data = fuel_data)
anova(lm_result)
```

Notice the identical F-value and p-value as the traditional ANOVA!

::: notes
When we run the same analysis using lm() instead of aov(), and then use anova() on the result, we get the exact same F-value and p-value as the traditional ANOVA. That's because they're mathematically equivalent - ANOVA is just a linear model with categorical predictors.

In this linear model, we're predicting fuel consumption based on vehicle class. The model creates dummy variables for each vehicle class (except one, which serves as the reference group).
:::

## Understanding the Linear Model Coefficients

```{r}
# View coefficients from the linear model
tidy(lm_result) |>
  kable(digits = 3)
```

Interpretation:
- The intercept (9.171) is the mean fuel consumption for the reference class (COMPACT)
- Each coefficient represents the difference between that class and the reference class
- E.g., FULL-SIZE vehicles consume 3.514 L/100km more fuel than COMPACT vehicles, on average

::: notes
Looking at the coefficients from the linear model gives us more detailed information than the ANOVA table alone. The intercept represents the mean fuel consumption for the reference group (in this case, COMPACT vehicles).

Each other coefficient represents the difference in mean fuel consumption between that vehicle class and the reference class. For example, the coefficient for classFULL-SIZE is 3.514, which means that, on average, full-size vehicles consume 3.514 liters per 100km more fuel than compact vehicles.

This is a much more detailed result than the overall ANOVA, which only tells us that there are differences somewhere. The linear model pinpoints exactly where those differences are.
:::

## Visualizing the ANOVA Results

```{r}
# Create a visualization of group means
ggplot(fuel_data, aes(x = class, y = fueluseboth)) +
  geom_boxplot(fill = "lightblue") +
  geom_point(position = position_jitter(width = 0.2), alpha = 0.3) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(
    x = "Vehicle Class", y = "Fuel Consumption (L/100km)",
    title = "Fuel Consumption by Vehicle Class"
  )
```

::: notes
This visualization helps us see the differences in fuel consumption across vehicle classes. We can visually confirm that larger vehicle classes like full-size, SUV, and pickup trucks tend to have higher fuel consumption than compact and subcompact vehicles.

The boxplots show the median (middle line), quartiles (box), and range (whiskers) of fuel consumption for each class, while the individual points represent actual vehicles in the dataset.
:::

## Two-way ANOVA: Adding Another Factor

Let's extend our model to include the number of cylinders468:

```{r}
# Create a simplified cylinder factor
fuel_data <- fuel_data |>
  mutate(cyl_factor = factor(case_when(
    cylinders468 <= 4 ~ "4 or fewer",
    cylinders468 == 6 ~ "6",
    cylinders468 >= 8 ~ "8 or more"
  )))

# Run two-way ANOVA
two_way_model <- lm(fueluseboth ~ class + cyl_factor, data = fuel_data)
anova(two_way_model)
```

Both vehicle class and number of cylinders468 significantly affect fuel consumption.

::: notes
Here we've extended our model to include two factors: vehicle class and number of cylinders468. This is called a two-way ANOVA in traditional statistics.

The ANOVA table shows that both factors have significant effects on fuel consumption. In other words, fuel consumption varies significantly based on both vehicle class and number of cylinders468.

But this model only looks at the main effects - it doesn't consider interactions between the factors.
:::

## Adding Interaction Effects

In the linear model framework, interactions are easy to add:

```{r}
# Run two-way ANOVA with interaction
interaction_model <- lm(fueluseboth ~ class * cyl_factor, data = fuel_data)
anova(interaction_model)
```

The interaction term tests whether the effect of one factor depends on the level of the other factor.

::: notes
An interaction effect occurs when the effect of one factor depends on the level of another factor. For example, the difference in fuel consumption between 4-cylinder and 8-cylinder engines might be larger for SUVs than for compact cars.

In the linear model, we can easily test for interactions by using the * operator instead of +. This adds both main effects and their interaction.

The ANOVA table shows a significant interaction effect, indicating that the effect of cylinders468 on fuel consumption differs across vehicle classes (or equivalently, the effect of vehicle class differs depending on the number of cylinders468).
:::

## Visualizing the Interaction

```{r}
# Create an interaction plot
ggplot(fuel_data, aes(x = class, y = fueluseboth, fill = cyl_factor)) +
  stat_summary(fun = mean, geom = "bar", position = "dodge") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(
    x = "Vehicle Class", y = "Average Fuel Consumption (L/100km)",
    fill = "Cylinders",
    title = "Interaction between Vehicle Class and Engine Cylinders"
  )
```

::: notes
This bar chart helps visualize the interaction effect. Each group of bars represents a vehicle class, and the different colored bars within each group represent different cylinder categories.

If there were no interaction, the pattern of differences between cylinder categories would be consistent across all vehicle classes. The fact that the pattern varies - for example, the difference between 4-cylinder and 8-cylinder engines seems larger for some vehicle classes than others - illustrates the interaction effect.

This is a powerful aspect of the general linear model: it allows us to model and interpret complex relationships between variables, including interactions.
:::

## ANCOVA: Mixing Categorical and Continuous Predictors

ANCOVA (Analysis of Covariance) combines ANOVA with regression by including both categorical and continuous predictors:

```{r}
# Run ANCOVA
ancova_model <- lm(fueluseboth ~ class + enginesize, data = fuel_data)
summary(ancova_model)
```

This model predicts fuel consumption based on both vehicle class (categorical) and engine size (continuous).

::: notes
ANCOVA is traditionally taught as yet another distinct technique, but in the general linear model framework, it's simply a model that includes both categorical and continuous predictors.

In this model, we're predicting fuel consumption based on vehicle class and engine size. The coefficients for vehicle class represent the differences between classes after controlling for engine size. The coefficient for engine size represents the effect of engine size on fuel consumption, controlling for vehicle class.

This is another example of how the general linear model provides a unified framework for various statistical techniques.
:::

## Effect Sizes: Understanding Practical Significance

Statistical significance (p-values) tells us if effects are likely real, but effect sizes tell us if they're practically important:

```{r}
# Calculate effect sizes
eta_squared(anova_result)
```

Interpretation:

- η² = proportion of variance explained by each factor
- Vehicle class explains about 43% of the variance in fuel consumption
- Values of 0.01, 0.06, and 0.14 are considered small, medium, and large effects

::: notes
While p-values tell us whether an effect is statistically significant (unlikely to be due to chance), effect sizes tell us about the practical significance or magnitude of the effect.

For ANOVA, a common effect size is eta-squared (η²), which represents the proportion of variance explained by each factor. Values around 0.01 are considered small, 0.06 medium, and 0.14 large.

The eta-squared value of 0.43 for vehicle class indicates that about 43% of the variance in fuel consumption is explained by vehicle class, which is a very large effect.

Effect sizes are important because with large enough sample sizes, even tiny, practically meaningless effects can become statistically significant.
:::

## Post-hoc Tests: Which Groups Differ?

When ANOVA finds significant differences, post-hoc tests help identify which specific groups differ:

```{r}
# Calculate estimated marginal means
emm <- emmeans(lm_result, ~class)

# Pairwise comparisons with Tukey adjustment
pairs(emm) |>
  as_tibble() |>
  filter(p.value < 0.05) |>
  arrange(p.value) |>
  head(5) |>
  kable(digits = 3)
```

::: notes
When ANOVA indicates significant differences between groups, we often want to know which specific groups differ from each other. Post-hoc tests help answer this question.

Here we're using estimated marginal means and pairwise comparisons with Tukey's adjustment for multiple comparisons. The results show the estimated difference between each pair of vehicle classes, along with confidence intervals and adjusted p-values.

The table shows the 5 most significant pairwise differences. For example, fuel consumption differs significantly between SUV-UTILITY and COMPACT-SUV vehicle classes.

This is another example of how the linear model framework provides a comprehensive approach to statistical analysis, from overall tests to detailed comparisons.
:::

## Pearson and Spearman Correlation as Linear Models

Model: $y = \beta_0 + \beta_1 x \quad$ where $\mathcal{H}_0: \beta_1 = 0$

This is simply a linear regression with one predictor. When we test whether the correlation is significant, we're testing whether the slope ($\beta_1$) differs from zero.

:::::: columns
:::: {.column width="50%"}
```{r}
# Create example data
set.seed(42)
x <- rnorm(50)
y <- 0.6 * x + rnorm(50, 0, 0.8)
data <- data.frame(x = x, y = y)

# Traditional correlation
cor_result <- cor.test(data$x, data$y)
cor_result$estimate
cor_result$p.value
```
::::

:::: {.column width="50%"}
```{r}
# As linear model (standardized variables)
lm_cor <- lm(scale(y) ~ scale(x), data = data)
coef(lm_cor)[2] # slope = correlation coefficient
summary(lm_cor)$coefficients[2, "Pr(>|t|)"] # p-value
```

When we standardize both variables (giving them mean=0 and sd=1), the slope coefficient equals the correlation coefficient!
::::
::::::

::: notes
Here we demonstrate that Pearson's correlation is equivalent to the standardized regression coefficient in a simple linear regression model.

The mathematical model is exactly the same as simple linear regression: y = β₀ + β₁x + ε. The null hypothesis being tested is that β₁ = 0, which means there is no linear relationship between the variables.

When we standardize both x and y (to have mean=0 and sd=1), the slope coefficient in a linear regression equals the correlation coefficient r. This makes intuitive sense because standardization puts both variables on the same scale, making their relationship directly comparable.

The t-test on this coefficient tests exactly the same hypothesis as the correlation test: is there a linear relationship between the variables? The p-values are identical between the two approaches.
:::

## Pearson vs. Spearman Correlation

Spearman correlation is Pearson correlation on rank-transformed variables:

```{r}
# Pearson correlation on original data
cor(x, y, method = "pearson")

# Spearman correlation = Pearson on ranks
cor(x, y, method = "spearman")

# Same as Pearson correlation on ranked variables
cor(rank(x), rank(y), method = "pearson")

# As linear model with ranks
lm_spearman <- lm(rank(y) ~ rank(x))
summary(lm_spearman)$coefficients[2, "Estimate"]
```

The "non-parametric" Spearman correlation is simply the "parametric" Pearson correlation applied to ranked data!

::: notes
Spearman's rank correlation is a brilliant example of how a "non-parametric" test is simply a parametric test applied to transformed data.

Instead of correlating the original values, Spearman correlation first converts all values to their ranks (1st, 2nd, 3rd, etc.) and then applies the Pearson correlation formula to these ranks.

This transformation accomplishes two things:

1. It makes the test robust to outliers, since extreme values just become the highest or lowest rank
2. It allows the test to detect monotonic but non-linear relationships, since ranking linearizes any monotonic relationship

By understanding Spearman correlation as "Pearson on ranks," we demystify non-parametric statistics. Many so-called non-parametric tests are simply parametric tests applied to transformed data, making them more accessible conceptually.
:::

## Correlation Visualized

```{r}
p1 <- ggplot(data, aes(x = x, y = y)) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE, color = "blue") +
    labs(title = "Pearson: Original Values") +
    theme_minimal()

rank_data <- data.frame(x_rank = rank(x), y_rank = rank(y))
p2 <- ggplot(rank_data, aes(x = x_rank, y = y_rank)) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE, color = "red") +
    labs(title = "Spearman: Ranked Values") +
    theme_minimal()

p1 + p2
```

Left: Pearson correlation fits a line to the original data points  
Right: Spearman correlation fits a line to the ranked data points

::: notes
This visualization helps us understand the relationship between Pearson and Spearman correlation.

The left panel shows the original data with the regression line (Pearson's r). The right panel shows the same data after converting to ranks, with its regression line (Spearman's rho).

Notice how the ranked data (right panel) tends to form a more linear pattern. This is because ranking removes the influence of outliers and transforms any monotonic relationship into a linear one.

Another key insight: the slope of the line through the ranked data is the Spearman correlation coefficient, just as the slope of the line through the standardized original data is the Pearson correlation coefficient.
:::

## Non-parametric Tests: Just Ranked Versions of Parametric Tests

For many common "non-parametric" tests, we can simplify by thinking of them as the parametric equivalent applied to ranks:

```{r echo=FALSE}
nonparam_table <- tibble(
    `Parametric Test` = c("Pearson correlation", "One-sample t-test", "Independent t-test", "Paired t-test", "One-way ANOVA"),
    `Non-parametric Equivalent` = c("Spearman correlation", "Wilcoxon signed-rank test", "Mann-Whitney U test", "Wilcoxon matched pairs", "Kruskal-Wallis test"),
    `Transformation` = c("Rank both variables", "Signed rank of values", "Rank all values", "Signed rank of differences", "Rank all values")
)

knitr::kable(nonparam_table)
```

This unified perspective demystifies "non-parametric" statistics:

- They're not completely different tests but transformations of familiar ones
- Ranking reduces the influence of outliers and nonlinearity
- They're not "assumption-free" but rather make different assumptions
- Understanding them as ranked versions of parametric tests makes them easier to grasp

::: notes
This table summarizes one of the key insights from our exploration: many "non-parametric" tests can be understood as simple transformations of familiar parametric tests.

For each common parametric test, there's a corresponding "non-parametric" version that's essentially the same test applied to ranked data:

1. Spearman correlation is Pearson correlation on ranked variables
2. Wilcoxon signed-rank test is a one-sample t-test on signed ranks
3. Mann-Whitney U test is an independent t-test on ranks
4. Wilcoxon matched pairs test is a paired t-test on signed rank differences
5. Kruskal-Wallis test is a one-way ANOVA on ranks

This perspective offers several benefits:
- It demystifies "non-parametric" statistics, making them more accessible
- It shows how ranking can make tests more robust to outliers and non-normality
- It clarifies that "non-parametric" tests aren't assumption-free, but make different assumptions
- It reduces the number of distinct procedures students need to learn
:::

## Integrated Example: HR Analytics with ANOVA

Let's return to our HR dataset and use ANOVA to compare job satisfaction across job roles:

```{r}
# Load HR Analytics dataset if not already loaded
if (!exists("hr_data")) {
  hr_data <- read_sav("data/dataset-abc-insurance-hr-data.sav") |> janitor::clean_names()
}

# Run ANOVA for job satisfaction by department
hr_anova <- lm(job_satisfaction ~ job_role, data = hr_data)
summary(hr_anova)
```

::: notes
Now let's apply what we've learned to our HR analytics dataset. Here we're comparing job satisfaction across different job roles using a linear model (which is equivalent to ANOVA).

The results show the mean job satisfaction for the reference role (the intercept) and the differences between each other role and the reference role Some departments appear to have significantly higher or lower job satisfaction than others.

This is a practical application of ANOVA as a linear model in a human resources context.
:::

## Visualizing HR Department Differences

```{r}
# Create a visualization of satisfaction by department
ggplot(hr_data, aes(
  x = reorder(job_role, job_satisfaction, FUN = mean),
  y = job_satisfaction
)) +
  geom_boxplot(fill = "lightgreen") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(
    x = "Role", y = "Job Satisfaction (1-5 scale)",
    title = "Job Satisfaction by Role"
  )
```

::: notes
This visualization helps us see the differences in job satisfaction across departments. The departments are ordered by their mean job satisfaction, with departments having higher average satisfaction appearing towards the right.

We can see variations in both the central tendency (median, indicated by the line in the middle of each box) and the spread of job satisfaction scores within each department.

This kind of analysis could help HR identify departments that might need intervention to improve employee satisfaction, or departments with particularly high satisfaction that might serve as models for others.
:::

## Combining ANOVA and Regression

We can build more complex models that include:
- Multiple categorical predictors (multi-way ANOVA)
- Continuous predictors alongside categorical ones (ANCOVA)
- Interaction terms between predictors

```{r}
# Build a complex model
complex_model <- lm(job_satisfaction ~ job_role + gender + evaluation +
  job_role:evaluation, data = hr_data)

# View model summary
anova(complex_model) |>
  as_tibble() |>
  kable(digits = 3)
```

::: notes
Here we've built a more complex model that includes multiple predictors: department (categorical), gender (categorical), and performance rating (continuous), as well as an interaction between department and performance rating.

This model tests whether job satisfaction varies by department, gender, and performance rating, and whether the relationship between performance rating and job satisfaction differs across departments.

The ANOVA table shows which effects are statistically significant. This demonstrates how the general linear model framework allows us to build and test complex models that would be difficult to conceptualize using traditional statistical procedures taught in isolation.
:::

## Beyond The Basics: Generalized Linear Models

Linear models can be extended to handle other types of outcomes:

$$g(E[Y]) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ...$$

Where $g()$ is a link function:

| Model Type | Outcome | Link Function | Example |
|------------|---------|---------------|---------|
| Linear Model | Continuous | Identity | Linear regression |
| Logistic Model | Binary | Logit | Binary classification |
| Poisson Model | Count | Log | Event frequency |

The general linear model framework extends naturally to handle many different types of outcome variables, not just continuous ones.

::: notes
While we've focused on the general linear model (GLM) for continuous outcomes, the framework extends naturally to other types of outcomes through Generalized Linear Models (GLMs).

The key innovation in GLMs is the addition of a link function, which transforms the expected value of the outcome. The linear combination of predictors (β₀ + β₁x₁ + β₂x₂ + ...) then predicts this transformed value rather than the raw outcome.

Different types of outcomes call for different link functions:

- For continuous outcomes, we use the identity link (no transformation), giving us the standard linear model
- For binary outcomes (0/1), we use the logit link, giving us logistic regression
- For count data, we use the log link, giving us Poisson regression

Other common GLMs include:

- Probit regression (using the probit link for binary outcomes)
- Negative binomial regression (an alternative to Poisson for overdispersed count data)
- Gamma regression (for positive continuous data with variance proportional to the square of the mean)

This extension to GLMs shows how the same core concepts we've explored apply across a wide range of statistical models.
:::

## Practical Code Cheat Sheet

```{r echo=TRUE, eval=FALSE}
# CORRELATION
cor.test(x, y)                     # Pearson correlation
lm(scale(y) ~ scale(x))            # Same as Pearson
cor.test(x, y, method="spearman")  # Spearman correlation
lm(rank(y) ~ rank(x))              # Approximates Spearman

# ONE SAMPLE TESTS
t.test(y, mu=0)                    # One-sample t-test
lm(y ~ 1)                          # Same as one-sample t-test
wilcox.test(y, mu=0)               # Wilcoxon signed-rank
lm(signed_rank(y) ~ 1)             # Approximates Wilcoxon

# TWO SAMPLE TESTS
t.test(y ~ group)                  # Independent t-test
lm(y ~ group)                      # Same as independent t-test
t.test(post, pre, paired=TRUE)     # Paired t-test
lm(post - pre ~ 1)                 # Same as paired t-test
wilcox.test(y ~ group)             # Mann-Whitney U
lm(rank(y) ~ group)                # Approximates Mann-Whitney

# ANOVA & REGRESSION
aov(y ~ group)                     # One-way ANOVA
lm(y ~ group)                      # Same as one-way ANOVA
aov(y ~ factorA * factorB)         # Two-way ANOVA  
lm(y ~ factorA * factorB)          # Same as two-way ANOVA
lm(y ~ group + covariate)          # ANCOVA
lm(y ~ x1 + x2 + x3)               # Multiple regression
```

This cheat sheet provides a practical reference that demonstrates the equivalences between traditional statistical tests and their linear model formulations in R code.

::: notes
This code cheat sheet provides a quick reference for the equivalences we've explored between traditional statistical tests and their linear model formulations in R.

The cheat sheet is organized by test type:
- Correlation tests (Pearson and Spearman)
- One-sample tests (t-test and Wilcoxon signed-rank)
- Two-sample tests (independent t-test, paired t-test, Mann-Whitney U)
- ANOVA and regression models (one-way ANOVA, two-way ANOVA, ANCOVA, multiple regression)

For each traditional test (e.g., t.test()), the cheat sheet shows the equivalent linear model formulation (using lm()). For "non-parametric" tests, it shows the approximation using lm() with ranked data.

Students can use this as a reference when transitioning from thinking about statistics as a collection of separate tests to understanding them as variations of the unified linear model framework.
:::

## The Power of the Unified Approach

Benefits of viewing statistical tests as linear models:

::: incremental
1. **Conceptual simplicity**: Learn one framework instead of many isolated techniques
2. **Flexibility**: Easily combine and extend models to suit your research questions
3. **Interpretability**: Consistent approach to understanding and communicating results
4. **Practicality**: Simplifies implementation in statistical software
5. **Extensibility**: Natural pathway to more advanced methods (mixed effects, generalized linear models)
:::

::: notes
The unified linear model approach offers several benefits over the traditional approach of teaching statistical tests as separate, unrelated techniques.

First, it's conceptually simpler. Instead of learning different formulas and procedures for t-tests, ANOVA, regression, etc., you learn one framework that encompasses all of these.

Second, it's more flexible. You can easily combine different types of predictors and test complex hypotheses within the same framework.

Third, it provides a consistent approach to interpretation. The coefficients in a linear model always have the same basic interpretation, regardless of whether the model is implementing a t-test, ANOVA, or regression.

Fourth, it's practical. In R and many other statistical software packages, the linear model (lm() function in R) is the workhorse for a wide range of analyses.

Finally, it provides a natural pathway to more advanced methods like mixed-effects models and generalized linear models, which extend the linear model framework to handle more complex data structures and non-normal distributions.
:::

## Practical Implications of the GLM Framework

:::::: columns
:::: {.column width="60%"}
Now that we've explored various extensions of the GLM framework, let's consider the practical implications:

1. **Data analysis workflow becomes more coherent**:
   - Start with your research question
   - Identify outcome and predictor variables
   - Select appropriate version of the GLM
   - Interpret coefficients in a consistent way

2. **Model extensions become more accessible**:
   - Add interactions to test context-dependent effects
   - Include covariates to control for confounders
   - Transform variables to meet assumptions
   - Move to GLMs for non-normal outcomes
::::

:::: {.column width="40%"}
![](https://images.unsplash.com/photo-1561557944-6e7860d1a7eb?q=80&w=1887&auto=format&fit=crop)
::::
::::::

::: notes
These practical implications highlight how the general linear model framework changes not just how we understand statistics conceptually, but also how we approach data analysis in practice.

When facing a new analytical problem, thinking in terms of the linear model helps clarify the essential components: the outcome variable, the predictor variables, and the relationships we're interested in testing. This approach guides the entire analysis process.

The unified framework also makes it much easier to extend and adapt models to fit different research scenarios. For instance:
- Adding interaction terms allows us to test whether the effect of one variable depends on the level of another
- Including covariates helps control for potential confounding variables
- Variable transformations (like ranking for non-parametric tests) can help meet model assumptions
- Extending to generalized linear models allows us to analyze non-normal outcomes like binary or count data

This flexibility is a major advantage over the traditional "cookbook" approach to statistics, where each test is treated as a separate entity with its own rules and applications.

By focusing on the underlying linear model framework, we're equipping students with a more powerful and adaptable analytical toolkit.
:::

## Concluding Thoughts

- Statistical tests are not isolated tools but connected members of the same family
- The general linear model provides a unified framework for understanding these connections
- This perspective simplifies learning, application, and interpretation of statistics
- When facing a new analytical problem, think in terms of the linear model: what is my outcome? What are my predictors? What relationships am I testing?

::: notes
In conclusion, the general linear model provides a powerful, unified framework for statistical analysis. By understanding that many common statistical tests are special cases of the linear model, we gain a deeper and more coherent understanding of statistics.

Rather than memorizing different formulas and procedures for different tests, we can focus on understanding the core principles of the linear model and how to apply them to different research questions.

When approaching a new analytical problem, thinking in terms of the linear model helps clarify the essential components: the outcome variable, the predictor variables, and the relationships we're interested in testing.

This approach not only simplifies learning and application but also enables us to build more sophisticated models that better capture the complexity of real-world phenomena.
:::
