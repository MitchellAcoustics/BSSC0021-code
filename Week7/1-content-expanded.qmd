---
r-fit-text: true
bibliography: ../references.bib
---

# The General Linear Model: Multiple Variables {background-color="#1E3D59"}

```{r}
#| echo: false
#| message: false
#| warning: false
library(tidyverse)
library(haven)
library(knitr)
library(broom)
library(ggplot2)
library(gtsummary)
library(cowplot)
library(gridExtra)
library(hrbrthemes)
library(patchwork) # For combining plots
library(latex2exp)
library(webshot2)

# Set common options
knitr::opts_chunk$set(dev = "ragg_png")
pdf.options(encoding = "CP1250")
```

## From Simple to Multiple Regression

Before wrapping up our discussion of statistical tests, let's first build up our understanding of regression from simple to multiple predictor variables. Once we've introduced these concepts, we'll apply them to an HR dataset to answer these questions:

1.  Is the average tenure at our company different from the industry standard? (One-sample t-test)
2.  Is there a gender difference in the intention to quit? (Independent t-test)
3.  Does the intention to quit differ across job roles? (ANOVA)
4.  What factors predict the intention to quit? (Multiple regression)

## Understanding the Building Blocks {.smaller}

::::: columns
::: {.column width="50%"}
The General Linear Model has two key components:

1.  **Variables**:
    -   **Outcome (y)**: What we're trying to understand
    -   **Predictors (x)**: Factors that might explain the outcome
2.  **Parameters**:
    -   **Intercept (β₀)**: Base value when predictors are 0
    -   **Coefficients (β₁, β₂, etc.)**: Effects of predictors
    -   **Error (ε)**: What the model doesn't explain
:::

::: {.column width="50%"}
```{r, echo=FALSE, fig.height=5, fig.width=6}
# Create example data
set.seed(123)
x <- seq(1, 10, length.out = 20)
y <- 2 + 0.5 * x + rnorm(20, 0, 1)
df <- data.frame(x = x, y = y)

# Fit a model to get the intercept and slope
model <- lm(y ~ x, data = df)
intercept <- coef(model)[1]
slope <- coef(model)[2]

# Plot
ggplot(df, aes(x, y)) +
  # Add data points
  geom_point(size = 3, color = "steelblue") +

  # Add the regression line
  geom_abline(intercept = intercept, slope = slope, color = "darkred", size = 1.5) +

  # Add error bars for a few points
  geom_segment(
    data = df[c(5, 10, 15), ],
    aes(x = x, y = y, xend = x, yend = predict(model)[c(5, 10, 15)]),
    color = "gray50", linetype = "dashed"
  ) +

  # Add annotations
  annotate("text",
    x = 2, y = 2, label = expression(beta[0] ~ "(intercept)"),
    color = "darkred", size = 5, hjust = 0
  ) +
  annotate("text",
    x = 8, y = 6.5, label = expression(beta[1] ~ "(slope)"),
    color = "darkred", size = 5
  ) +
  annotate("text",
    x = 9.5, y = 3.5, label = expression(epsilon ~ "(error)"),
    color = "gray50", size = 5
  ) +

  # Add an arrow for the slope
  annotate("segment",
    x = 7, xend = 9, y = 6, yend = 7,
    arrow = arrow(length = unit(0.3, "cm")), color = "darkred"
  ) +

  # Format the plot
  theme_minimal() +
  labs(
    title = "Components of the General Linear Model",
    x = "Predictor (x)",
    y = "Outcome (y)"
  )
```
:::
:::::

::: notes
To understand the General Linear Model, we need to break it down into its building blocks.

First, we have two types of variables:

1.  The outcome variable (y): This is what we're trying to understand, explain, or predict. It's also called the dependent variable, response variable, or target variable. Examples include test scores, blood pressure, customer satisfaction, or income.
2.  Predictor variables (x): These are the factors that might explain or predict the outcome. They're also called independent variables, explanatory variables, or features. Examples might be study time, medication type, service quality metrics, or years of education.

Next, we have parameters that describe the relationship between these variables:

3.  The intercept (β₀): This is the baseline value of y when all predictors are zero. It's the starting point of our model.
4.  Coefficients (β₁, β₂, etc.): These tell us how much y changes when the corresponding predictor changes by one unit, holding all other predictors constant. The coefficients quantify the effects of our predictors.
5.  Error term (ε): This represents what our model doesn't explain - the deviation between our model's predictions and the actual data. A good model minimizes this error.

The visualization shows these components: 

- Blue dots represent the data points (observations) 
- The red line is our model, with the intercept (β₀) as the starting point and the slope (β₁) showing the effect of the predictor 
- The dashed gray lines show the error (ε) for some points - the difference between what the model predicts and the actual values

Understanding these components gives us the foundation to see how different statistical tests are variations of the same underlying model.
:::

## Simple Linear Regression: One Predictor

In simple linear regression, we have one outcome variable and one predictor:

::::: columns
::: {.column width="50%"}
**Key components:**

-   $y$ is the outcome we want to predict
-   $\beta_0$ is the intercept (value of y when x = 0)
-   $\beta_1$ is the slope (effect of the predictor)
-   $x_1$ is the predictor variable
-   $\varepsilon$ is the error term

**Example:** Predicting salary based on years of experience
:::

::: {.column width="50%"}
$$y = \beta_0 + \beta_1 x_1 + \varepsilon$$

```{r echo=FALSE, fig.height=4, fig.width=5}
# Create data for simple regression
set.seed(123)
experience <- runif(40, 1, 20)
salary <- 30000 + 2500 * experience + rnorm(40, 0, 10000)
simple_data <- data.frame(experience = experience, salary = salary)

# Fit model
simple_model <- lm(salary ~ experience, data = simple_data)

# Create plot
ggplot(simple_data, aes(x = experience, y = salary)) +
  geom_point(color = "steelblue", alpha = 0.7) +
  geom_smooth(method = "lm", color = "darkred") +
  theme_minimal() +
  labs(
    title = "Simple Linear Regression",
    x = "Years of Experience",
    y = "Salary ($)",
    subtitle = paste("Slope (β₁):", round(coef(simple_model)[2]))
  ) +
  annotate("text",
    x = 5, y = 30000,
    label = paste("β₀ =", round(coef(simple_model)[1])),
    color = "darkred", size = 4
  )
```
:::
:::::

::: notes
Simple linear regression is where most students begin their regression journey. It models the relationship between one outcome variable (y) and one predictor variable (x).

The model estimates two key parameters: 

- The intercept (β₀) represents the predicted value of y when x equals zero 
- The slope (β₁) represents how much y changes when x increases by one unit

In our example, we're predicting salary based on years of experience: 
- Each additional year of experience is associated with approximately \$2,500 more in salary 
- The intercept suggests that someone with zero experience would have a salary around \$30,000

The blue dots represent individual data points, while the red line shows our model's prediction. The distance between each point and the line represents the error term (ε) - what our model doesn't explain.

Simple linear regression provides a foundation, but in real-world situations, outcomes are typically influenced by multiple factors. That's where multiple regression comes in.
:::

## Multiple Regression: Adding More Predictors

What if multiple factors affect our outcome? Multiple regression extends the model:

$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \varepsilon$$

**Key advantages:**

-   Models real-world complexity
-   Accounts for multiple influences
-   Controls for confounding variables
-   Improves prediction accuracy
-   Allows comparing relative importance of predictors

---

:::{.columns}
::: {.column width=30%}
**Example:** Predicting salary based on years of experience AND performance rating
:::

::: {.column width=70%}
```{r echo=FALSE, fig.height=7, fig.width=7}
# Create data for multiple regression
set.seed(456)
performance <- runif(40, 2, 5)
salary_multi <- 20000 + 2000 * experience + 8000 * performance + rnorm(40, 0, 8000)
multi_data <- data.frame(
  experience = experience,
  performance = performance,
  salary = salary_multi
)

# Fit model
multi_model <- lm(salary ~ experience + performance, data = multi_data)

# Create 3D scatterplot
library(plotly)

plot_ly(multi_data,
  x = ~experience,
  y = ~performance,
  z = ~salary,
  marker = list(size = 5, color = "steelblue", opacity = 0.8)
) |>
  add_markers() |>
  add_surface(
    x = seq(min(multi_data$experience), max(multi_data$experience), length.out = 10),
    y = seq(min(multi_data$performance), max(multi_data$performance), length.out = 10),
    z = outer(
      seq(min(multi_data$experience), max(multi_data$experience), length.out = 10),
      seq(min(multi_data$performance), max(multi_data$performance), length.out = 10),
      function(exp, perf) coef(multi_model)[1] + coef(multi_model)[2] * exp + coef(multi_model)[3] * perf
    ),
    opacity = 0.7,
    colorscale = "Reds"
  ) |>
  layout(
    title = "Multiple Regression",
    scene = list(
      xaxis = list(title = "Experience"),
      yaxis = list(title = "Performance"),
      zaxis = list(title = "Salary")
    )
  )
```

:::
:::

::: notes
Multiple regression extends our model by adding more predictor variables. This allows us to account for the complex, multifaceted nature of real-world relationships.
:::

## Multivariate Regression {.smaller}

Now our model includes: 

- The intercept (β₀): The predicted value of y when all predictors are zero 
- Multiple slope coefficients (β₁, β₂, etc.): Each representing the effect of its corresponding predictor when all other predictors are held constant

This "holding other variables constant" is a crucial concept. It means that each coefficient tells us the unique effect of that predictor, controlling for the effects of all other predictors in the model.

In our example, we're now predicting salary based on both years of experience and performance rating: 

- Each additional year of experience is associated with about \$1,865 more in salary, holding performance constant 
- Each additional point in performance rating is associated with about \$9,890 more in salary, holding experience constant

---

Multiple regression provides several advantages: 

1. It models the complexity of real-world situations where outcomes are influenced by multiple factors 
2. It allows us to control for confounding variables 
3. It often provides more accurate predictions than simple regression 
4. It helps us understand the relative importance of different predictors

This approach can be extended to include any number of predictors, creating a multidimensional hyperplane that we can't easily visualize but that follows the same principles.

## Extending to Many Predictors

The model can be extended to include any number of predictors:

$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + ... + \beta_n x_n + \varepsilon$$

::: columns
::: {.column width=50%}
```{r}
# Example with multiple predictors using HR data
hr_data <- read_sav("data/dataset-abc-insurance-hr-data.sav") |>
  janitor::clean_names() |>
  mutate(gender = factor(gender, levels = 1:2, labels = c("Female", "Male")))
```

```{r echo=TRUE}
#| code-fold: false
# Build model with multiple predictors
full_model <- lm(
  salarygrade ~ gender + tenure +
    evaluation + age + job_satisfaction,
  data = hr_data
)
```
:::

::: {.column width=50%}

```{r echo=FALSE}
# Display coefficient summary
summary(full_model)$coefficients |>
  as.data.frame() |>
  rownames_to_column(var = "Predictor") |>
  select(Predictor, Estimate, `Pr(>|t|)`) |>
  knitr::kable(
    digits = 3,
    col.names = c("Predictor", "Effect on Salary", "p-value")
  )
```

:::
:::

::: notes
We can continue extending our multiple regression model to include any number of predictors. The general form remains the same, with each new predictor getting its own coefficient that represents its unique effect on the outcome.

In this example, we're using real HR data to predict salary grade based on multiple factors: 

- Gender (categorical: male/female) 
- Tenure (years of experience) 
- Evaluation (performance rating) 
- Age (in years) 
- Job satisfaction (rating scale)

The model output shows: 

1. Each predictor's coefficient (effect on salary) 
2. The statistical significance of each effect (p-value)

The interpretation of each coefficient is: 

- Gender: Being male is associated with a 0.35 point higher salary grade, holding all else constant 
- Tenure: Each additional year of experience is associated with a 0.1 point increase in salary grade 
- Evaluation: Each additional point in performance rating is associated with a 0.02 point increase in salary grade(not statistically significant)
- Age: Each additional year of age is associated with a 0.02 point change in salary grade (effectively zero). Remember, this is the effect, when tenure is held constant!
- Job satisfaction: Each additional point in job satisfaction is associated with a 0.18 point increase in salary grade

From these results, we can see that gender, tenure, and evaluation ratings have the strongest effects on salary, while age appears to have no meaningful impact.

This approach allows us to model complex real-world situations where many factors simultaneously influence an outcome. It's a powerful tool for both prediction and understanding the relative importance of different factors.

The multiple regression model we've just explored is actually the general form of the General Linear Model (GLM), which we'll see can represent many different statistical tests.
:::

