---
r-fit-text: true
bibliography: ../references.bib
format: 
  clean-revealjs: default
  cleanpdf-typst: default
---

# The General Linear Model: Unifying Statistical Tests {background-color="#1E3D59"}

```{r}
#| echo: false
#| message: false
#| warning: false
library(tidyverse)
library(haven)
library(knitr)
library(broom)
library(ggplot2)
library(gtsummary)
library(cowplot)
library(gridExtra)
library(hrbrthemes)
library(patchwork) # For combining plots

# Set common options
knitr::opts_chunk$set(dev = "ragg_png")
pdf.options(encoding = "CP1250")
```

## The Statistical Test Dilemma

In a typical statistics course, you are likely to learn many different tests:

-   **t-tests** (one-sample, independent, paired)
-   **ANOVA, Analysis of Variance** (one-way, two-way)
-   **Correlation** (Pearson, Spearman)
-   **Regression** (simple, multiple)
-   **Chi-square tests**
-   **Non-parametric alternatives**

With so many tests, it can feel overwhelming to remember which one to use when!

::: notes
When students learn statistics, they're often taught different statistical tests as separate, unrelated procedures:

1.  Want to compare one sample to a known value? Use a one-sample t-test.
2.  Comparing two groups? That's an independent t-test.
3.  Comparing multiple groups? Now you need ANOVA.
4.  Looking at relationships between continuous variables? Time for correlation or regression.

This approach creates several problems:

First, it encourages memorization rather than understanding. Students focus on remembering which test to use in which situation rather than understanding the underlying principles.

Second, it obscures the connections between different tests, making statistics seem more complex and fragmented than it really is.

Third, it can lead to confusion about which test to choose, especially in situations that don't neatly fit the examples covered in class.

Finally, it makes it harder to transition to more advanced statistical methods because each new technique seems like a completely new concept to learn.

Today, we'll explore a different approach: understanding common statistical tests as variations of the same underlying framework - the General Linear Model. This perspective can greatly simplify how we think about statistics and help us see the connections between seemingly different techniques.
:::

## The Statistical Test Dilemma

![Example decision tree, or flowchart, for selecting an appropriate statistical procedure. @McElreath2020Statistical](StatRethinking-test-tree.png){fig-align="center" width="100%"}

## Challenging the Traditional Approach

::::: columns
::: {.column width="40%"}
**Traditional approach:**

-   Each test is taught as a separate technique
-   Different formulas to memorize
-   Different assumptions to check
-   Different procedures to follow
-   No clear connections between tests

**Result:** Statistics feels like a collection of disconnected tools rather than a coherent framework.
:::

::: {.column width="60%"}
```{r, echo=FALSE, fig.height=5, fig.width=7}
# Create a visualization showing traditional vs. unified approach
# Create data for the plot
traditional <- data.frame(
  x = c(1, 2, 3, 4, 5),
  y = rep(1, 5),
  labels = c("t-test", "ANOVA", "Correlation", "Regression", "Chi-square")
)

unified <- data.frame(
  x = c(1, 2, 3, 4, 5),
  y = rep(2, 5),
  labels = c("t-test", "ANOVA", "Correlation", "Regression", "Chi-square")
)

center <- data.frame(
  x = 3,
  y = 3,
  labels = "General\nLinear\nModel"
)

# Create the plot
ggplot() +
  # Traditional approach - isolated islands
  geom_point(data = traditional, aes(x = x, y = y), size = 22, color = "steelblue", alpha = 0.7) +
  geom_text(data = traditional, aes(x = x, y = y, label = labels), size = 3.5) +

  # Unified approach - connected to central framework
  geom_point(data = unified, aes(x = x, y = y), size = 22, color = "steelblue", alpha = 0.7) +
  geom_text(data = unified, aes(x = x, y = y, label = labels), size = 3.5) +

  # Connections to central framework
  geom_segment(
    aes(
      x = unified$x, y = unified$y + 0.3,
      xend = center$x, yend = center$y - 0.3
    ),
    arrow = arrow(length = unit(0.2, "cm")), size = 0.7
  ) +

  # Central framework
  geom_point(data = center, aes(x = x, y = y), size = 28, color = "darkred", alpha = 0.7) +
  geom_text(data = center, aes(x = x, y = y, label = labels), size = 4, color = "white") +

  # Labels for the approaches
  annotate("text",
    x = 0.5, y = 1, label = "Traditional Approach:",
    hjust = 0, size = 5, fontface = "bold"
  ) +
  annotate("text",
    x = 0.5, y = 2, label = "Unified Approach:",
    hjust = 0, size = 5, fontface = "bold"
  ) +

  # Formatting
  theme_void() +
  annotate("text",
    x = 3, y = 0.3,
    label = "Each test exists as a separate island with its own rules and procedures",
    size = 4
  ) +
  annotate("text",
    x = 3, y = 3.8,
    label = "All tests are connected through a common underlying framework",
    size = 4
  ) +
  coord_cartesian(xlim = c(0, 6), ylim = c(0, 4))
```
:::
:::::

::: notes
The traditional approach to teaching statistics typically presents each test as a separate entity with its own formulas, assumptions, and procedures. This is like presenting a collection of disconnected islands, with no obvious way to navigate between them.

In this traditional approach:

-   Students learn the one-sample t-test, then move on to the independent t-test, then ANOVA, and so on
-   Each test seems to have its own set of rules and formulas to memorize
-   There's little emphasis on how these tests relate to each other
-   The focus is often on "which test to use when" rather than understanding the underlying principles

This approach has several drawbacks:

1.  It emphasizes memorization over conceptual understanding
2.  It makes statistics seem more complex than it really is
3.  It doesn't prepare students well for situations that don't fit neatly into the categories they've learned
4.  It can make more advanced statistical methods seem disconnected from basic techniques

In contrast, a unified approach connects all these seemingly different tests through a common framework - the General Linear Model. This makes statistics more coherent and easier to understand, as you'll see today.
:::

## A Different Perspective: Everything is Connected

The **General Linear Model** provides a unified framework for statistical analysis.

Under this framework:

-   **t-tests** are special cases of regression
-   **Correlation** is related to regression
-   **Non-parametric tests (e.g. Spearman correlation)** are transformations of parametric tests
-   **ANOVA** is a special case of regression

This means there's less to learn and more to understand!

```{r, include=FALSE, eval=FALSE, echo=FALSE, fig.height=5, fig.width=6}
# Create an image showing tests as special cases
library(DiagrammeR)
library(htmltools)

graph <- grViz("
  digraph GLM {
    # Node attributes
    node [shape = rectangle, style = filled, fillcolor = steelblue, fontcolor = white,
          width = 2, height = 0.8, fontname = 'Helvetica', fontsize = 12];

    # Edge attributes
    edge [color = gray30, arrowsize = 0.8];

    # Nodes
    GLM [label = 'General Linear Model', fillcolor = darkred, fontsize = 14];
    reg [label = 'Regression'];
    anova [label = 'ANOVA'];
    ttest [label = 't-tests'];
    corr [label = 'Correlation'];

    # Sub-nodes
    simple [label = 'Simple Regression'];
    multiple [label = 'Multiple Regression'];
    oneway [label = 'One-way ANOVA'];
    twoway [label = 'Two-way ANOVA'];
    onesample [label = 'One-sample t-test'];
    independent [label = 'Independent t-test'];
    paired [label = 'Paired t-test'];
    pearson [label = 'Pearson Correlation'];
    spearman [label = 'Spearman Correlation'];

    # Edges
    GLM -> {reg anova ttest corr};
    reg -> {simple multiple};
    anova -> {oneway twoway};
    ttest -> {onesample independent paired};
    corr -> {pearson spearman};
  }
")

# Convert to HTML and display
html_print(graph)
```

::: notes
Now, let's explore a different perspective: the General Linear Model (GLM) as a unifying framework for statistical analysis.

The key insight is that many common statistical tests are actually special cases of the same underlying model. Instead of viewing t-tests, ANOVA, correlation, and regression as completely different techniques, we can understand them as variations of the general linear model.

For example:

-   A t-test is just a regression model with a categorical predictor that has two levels

-   ANOVA is a regression model with a categorical predictor that has more than two levels

-   Simple regression is, well, regression with one continuous predictor

-   Multiple regression extends this to multiple predictors

This unified perspective has several advantages:

1.  It reduces the conceptual load - instead of learning many different techniques, you learn one framework with variations
2.  It highlights the connections between different statistical approaches
3.  It makes the transition to more advanced methods more intuitive
4.  It focuses on understanding rather than memorizing formulas and procedures

The hierarchical diagram shows how different statistical tests are related through the general linear model. All these tests are part of the same family, with the GLM as their common ancestor.

This perspective was eloquently described by Jonas Kristoffer Lindeløv in his blog post "Common statistical tests are linear models" and is increasingly being adopted in modern statistics education.
:::

## Understanding the Building Blocks

::::: columns
::: {.column width="50%"}
The General Linear Model has two key components:

1.  **Variables**:
    -   **Outcome (y)**: What we're trying to understand
    -   **Predictors (x)**: Factors that might explain the outcome
2.  **Parameters**:
    -   **Intercept (β₀)**: Base value when predictors are 0
    -   **Coefficients (β₁, β₂, etc.)**: Effects of predictors
    -   **Error (ε)**: What the model doesn't explain
:::

::: {.column width="50%"}
```{r, echo=FALSE, fig.height=5, fig.width=6}
# Create example data
set.seed(123)
x <- seq(1, 10, length.out = 20)
y <- 2 + 0.5 * x + rnorm(20, 0, 1)
df <- data.frame(x = x, y = y)

# Fit a model to get the intercept and slope
model <- lm(y ~ x, data = df)
intercept <- coef(model)[1]
slope <- coef(model)[2]

# Plot
ggplot(df, aes(x, y)) +
  # Add data points
  geom_point(size = 3, color = "steelblue") +

  # Add the regression line
  geom_abline(intercept = intercept, slope = slope, color = "darkred", size = 1.5) +

  # Add error bars for a few points
  geom_segment(
    data = df[c(5, 10, 15), ],
    aes(x = x, y = y, xend = x, yend = predict(model)[c(5, 10, 15)]),
    color = "gray50", linetype = "dashed"
  ) +

  # Add annotations
  annotate("text",
    x = 2, y = 2, label = expression(beta[0] ~ "(intercept)"),
    color = "darkred", size = 5, hjust = 0
  ) +
  annotate("text",
    x = 8, y = 6.5, label = expression(beta[1] ~ "(slope)"),
    color = "darkred", size = 5
  ) +
  annotate("text",
    x = 9.5, y = 3.5, label = expression(epsilon ~ "(error)"),
    color = "gray50", size = 5
  ) +

  # Add an arrow for the slope
  annotate("segment",
    x = 7, xend = 9, y = 6, yend = 7,
    arrow = arrow(length = unit(0.3, "cm")), color = "darkred"
  ) +

  # Format the plot
  theme_minimal() +
  labs(
    title = "Components of the General Linear Model",
    x = "Predictor (x)",
    y = "Outcome (y)"
  )
```
:::
:::::

::: notes
To understand the General Linear Model, we need to break it down into its building blocks.

First, we have two types of variables:

1.  The outcome variable (y): This is what we're trying to understand, explain, or predict. It's also called the dependent variable, response variable, or target variable. Examples include test scores, blood pressure, customer satisfaction, or income.

2.  Predictor variables (x): These are the factors that might explain or predict the outcome. They're also called independent variables, explanatory variables, or features. Examples might be study time, medication type, service quality metrics, or years of education.

Next, we have parameters that describe the relationship between these variables:

3.  The intercept (β₀): This is the baseline value of y when all predictors are zero. It's the starting point of our model.

4.  Coefficients (β₁, β₂, etc.): These tell us how much y changes when the corresponding predictor changes by one unit, holding all other predictors constant. The coefficients quantify the effects of our predictors.

5.  Error term (ε): This represents what our model doesn't explain - the deviation between our model's predictions and the actual data. A good model minimizes this error.

The visualization shows these components: - Blue dots represent the data points (observations) - The red line is our model, with the intercept (β₀) as the starting point and the slope (β₁) showing the effect of the predictor - The dashed gray lines show the error (ε) for some points - the difference between what the model predicts and the actual values

Understanding these components gives us the foundation to see how different statistical tests are variations of the same underlying model.
:::

## The General Linear Model: The Basic Formula

The general linear model can be written as:

$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_n x_n + \varepsilon$$

Where:

-   $y$ is the outcome we want to understand

-   $\beta_0$ is the intercept (value of y when all predictors are 0)

-   $\beta_1, \beta_2, etc.$ are coefficients that tell us the effect of each predictor

-   $x_1, x_2, etc.$ are the predictor variables

-   $\varepsilon$ is the error term (what our model doesn't explain)

This single formula is the foundation for most statistical tests!

::: notes
The general linear model is expressed mathematically with this formula:

y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ + ε

This may look like a multiple regression equation - and that's exactly right. Multiple regression is one implementation of the general linear model, but it's not the only one.

Let's break down the components:

-   y is our outcome variable - what we're trying to understand or predict
-   β₀ is the intercept - the value of y when all predictors are zero
-   β₁, β₂, etc. are the coefficients that tell us the effect of each predictor
-   x₁, x₂, etc. are our predictor variables
-   ε is the error term - what our model doesn't explain

The beauty of this formula is its flexibility. By making small adjustments to it, we can represent a wide range of statistical tests:

-   In a one-sample t-test, we have no predictors, just an intercept to test
-   In an independent t-test, we have one binary predictor
-   In ANOVA, we have categorical predictors with multiple levels
-   In correlation and regression, we have continuous predictors

All of these tests are just special cases of the same underlying model. This unified perspective can greatly simplify how we think about statistics and help us see the connections between seemingly different techniques.

This is why modern statistics education is increasingly moving toward teaching the general linear model as a foundation, with specific tests introduced as special cases of this framework.
:::

## Example 1: One-Sample t-test as a Linear Model

::::: columns
::: {.column width="50%"}
**One-sample t-test**: Tests if a sample mean differs from a known value.

**As a linear model**: $$y = \beta_0 + \varepsilon$$

Where:

-   $\beta_0$ is the sample mean

-   The test examines whether $\beta_0 = \mu_0$ (the hypothesized value)

**Example**: Testing if average student test scores (70) differ from the expected value (65)
:::

::: {.column width="50%"}
```{r, echo=FALSE, fig.height=5, fig.width=6}
# Create simulated test score data
set.seed(42)
test_scores <- rnorm(30, mean = 70, sd = 10)
test_data <- data.frame(
  index = rep(1, length(test_scores)),
  score = test_scores
)

# Calculate the mean and test value
mean_score <- mean(test_scores)
test_value <- 65

# Plot
ggplot(test_data, aes(x = index, y = score)) +
  # Add jittered points
  geom_jitter(width = 0.2, alpha = 0.6, color = "steelblue", size = 3) +
  # Add actual mean line
  geom_hline(yintercept = mean_score, color = "darkred", size = 1.5) +
  # Add test value line
  geom_hline(yintercept = test_value, color = "darkgreen", linetype = "dashed", size = 1.5) +
  # Add annotations
  annotate("text",
    x = 1.3, y = mean_score + 3,
    label = paste("Sample Mean =", round(mean_score, 1), "(β₀)"),
    color = "darkred", size = 5
  ) +
  annotate("text",
    x = 1.3, y = test_value - 3,
    label = paste("Test Value =", test_value),
    color = "darkgreen", size = 5
  ) +
  # Format
  theme_minimal() +
  theme(
    axis.text.x = element_blank(),
    axis.title.x = element_blank()
  ) +
  labs(
    title = "One-Sample t-test as Linear Model",
    subtitle = "Testing if mean differs from a reference value",
    y = "Test Score"
  )
```
:::
:::::

::: notes
Let's start with one of the simplest statistical tests: the one-sample t-test.

A one-sample t-test compares a sample mean to a known value. For example, we might want to test whether the average test score in a class (70 points) is significantly different from the expected score (65 points).

In the general linear model framework, this test is incredibly simple. Our model becomes:

y = β₀ + ε

Here, β₀ is the intercept, which represents the mean of our sample. The t-test is testing whether this intercept (β₀) equals our hypothesized value (65).

The visualization shows: - Blue dots: individual test scores (our data points) - Red line: the sample mean (β₀ in our model) at approximately 70 - Green dashed line: the test value of 65

The one-sample t-test is asking: "Is the difference between the red line (our sample mean) and the green line (our test value) statistically significant, or could it be due to random chance?"

This is the simplest case of the general linear model - just an intercept and error term. There are no predictor variables (x terms) in the equation.

In R, we can perform this test using either the traditional t.test() function or the linear model approach with lm():

``` r
# Traditional approach
t.test(test_scores, mu = 65)

# Linear model approach
lm(test_scores ~ 1)  # The '1' gives us just an intercept
```

Both approaches will give us identical t-statistics and p-values, showing that they're mathematically equivalent.
:::

## Example 2: Independent t-test as a Linear Model

::::: columns
::: {.column width="50%"}
**Independent t-test**: Compares means between two groups.

**As a linear model**: $$y = \beta_0 + \beta_1 x_1 + \varepsilon$$

Where:

-   $x_1$ is a binary (0/1) indicator for group membership

-   $\beta_0$ is the mean for group 0 (reference group)

-   $\beta_1$ is the difference between groups

-   We test whether $\beta_1 = 0$ (no difference)
:::

::: {.column width="50%"}
**Example:** Comparing male vs. female test scores

```{r, echo=FALSE, fig.height=5, fig.width=6}
# Create data for independent t-test example
set.seed(123)
group_data <- data.frame(
  group = factor(rep(c("Group A", "Group B"), each = 15)),
  score = c(rnorm(15, mean = 65, sd = 8), rnorm(15, mean = 75, sd = 8))
)

# Calculate group means
mean_A <- mean(group_data$score[group_data$group == "Group A"])
mean_B <- mean(group_data$score[group_data$group == "Group B"])
diff <- mean_B - mean_A

# Plot
ggplot(group_data, aes(x = group, y = score, color = group)) +
  # Add jittered points
  geom_jitter(width = 0.2, alpha = 0.7, size = 3) +
  # Add group means
  stat_summary(fun = mean, geom = "point", size = 5, shape = 18) +
  stat_summary(
    fun = mean, geom = "errorbar",
    aes(ymax = after_stat(y), ymin = after_stat(y)), width = 0.3, linewidth = 1.5
  ) +
  # Add annotations
  annotate("text",
    x = 1, y = mean_A - 5,
    label = expression(beta[0] ~ "(Group A mean)"), color = "#F8766D", size = 5
  ) +
  annotate("segment",
    x = 1.1, xend = 1.9,
    y = mean_A + 5,
    yend = mean_A + 5,
    arrow = arrow(length = unit(0.3, "cm")), color = "#00BFC4"
  ) +
  annotate("text",
    x = 1.5, y = mean_A + 8,
    label = expression(beta[1] ~ "(difference)"), color = "#00BFC4", size = 5
  ) +
  # Format
  theme_minimal() +
  scale_color_brewer(palette = "Set1") +
  theme(legend.position = "none") +
  labs(
    title = "Independent t-test as Linear Model",
    subtitle = paste("The coefficient (β₁) tests the difference between groups:", round(diff, 1)),
    x = "Group",
    y = "Score"
  )
```
:::
:::::

::: notes
Now let's look at how the independent t-test fits into the general linear model framework.

An independent t-test compares means between two groups, such as test scores between male and female students. In the traditional approach, we calculate the means of each group, their difference, and determine if this difference is statistically significant.

In the general linear model framework, this becomes:

y = β₀ + β₁x₁ + ε

Where: - x₁ is a binary variable indicating group membership (0 for Group A, 1 for Group B) - β₀ is the intercept, which represents the mean of Group A (the reference group) - β₁ is the coefficient for the group difference, which represents how much higher or lower Group B's mean is compared to Group A's - The t-test for β₁ tests whether this difference is significantly different from zero

This approach uses what's called "dummy coding" or "indicator variables." Group membership is coded as 0 or 1, and the model estimates the effect of being in Group B compared to Group A.

In the visualization: - Colored dots: individual scores for each group - Horizontal lines: group means - β₀ (the intercept): Group A's mean - β₁ (the coefficient): the difference between Group B and Group A (about 10 points in this example)

The t-test for the coefficient β₁ is exactly the same as the traditional independent t-test. They are mathematically equivalent.

In R, we can perform this test using either approach:

``` r
# Traditional approach
t.test(score ~ group, data = group_data, var.equal = TRUE)

# Linear model approach
lm(score ~ group, data = group_data)
```

Both will give identical t-statistics and p-values for the group difference.
:::

## Dummy Coding: How Categorical Variables Work in Linear Models

::::: columns
::: {.column width="50%"}
**Dummy coding** transforms categorical variables into a format linear models can use:

1.  Choose a reference group (usually the first category)
2.  Create 0/1 indicator variables for other groups
3.  The model estimates:
    -   $\beta_0$ = mean of reference group
    -   $\beta_1, \beta_2, etc.$ = differences from reference

This allows us to include categorical predictors in our linear models, extending beyond just continuous variables.
:::

::: {.column width="50%"}
```{r, echo=FALSE, fig.height=5, fig.width=6}
# Create data for dummy coding visualization
dummy_data <- data.frame(
  Category = factor(rep(c("A", "B", "C"), each = 10)),
  Value = c(rnorm(10, 10, 2), rnorm(10, 15, 2), rnorm(10, 12, 2))
)

# Create dummy variables
dummy_data$B_dummy <- ifelse(dummy_data$Category == "B", 1, 0)
dummy_data$C_dummy <- ifelse(dummy_data$Category == "C", 1, 0)

# Calculate means
means <- tapply(dummy_data$Value, dummy_data$Category, mean)

# Create a visualization of dummy coding
ggplot(dummy_data, aes(x = Category, y = Value, color = Category)) +
  # Add jittered points
  geom_jitter(width = 0.2, alpha = 0.7, size = 3) +
  # Add means
  stat_summary(fun = mean, geom = "point", size = 5, shape = 18) +
  stat_summary(
    fun = mean, geom = "errorbar",
    aes(ymax = after_stat(y), ymin = after_stat(y)), width = 0.3, linewidth = 1.5
  ) +
  # Add annotations
  annotate("text",
    x = 1, y = means["A"] - 2,
    label = expression(beta[0] ~ "(reference group mean)"), color = "#F8766D", size = 4.5
  ) +
  annotate("text",
    x = 2, y = means["B"] + 2,
    label = expression(beta[1] ~ "(difference from reference)"), color = "#00BA38", size = 4.5
  ) +
  annotate("text",
    x = 3, y = means["C"] + 2,
    label = expression(beta[2] ~ "(difference from reference)"), color = "#619CFF", size = 4.5
  ) +
  # Format
  theme_minimal() +
  theme(legend.position = "none") +
  labs(
    title = "Dummy Coding of Categorical Variables",
    subtitle = "Creating numeric indicators for linear models",
    x = "Category",
    y = "Value"
  )
```
:::
:::::

::: notes
Dummy coding is a key concept that allows us to include categorical variables in our linear models. It's worth understanding this in detail since it's central to how tests like the independent t-test and ANOVA work within the linear model framework.

Here's how dummy coding works:

1.  First, we choose one category as the reference group (typically the first category alphabetically or numerically)
2.  For each of the other categories, we create a binary indicator variable (0 or 1)
3.  The reference group gets zeros for all these indicator variables

For example, with three categories A, B, and C:

-   Category A is our reference group

-   For Category B, we create a variable B_dummy (1 if in category B, 0 otherwise)

-   For Category C, we create a variable C_dummy (1 if in category C, 0 otherwise)

In the resulting model:

-   β₀ (the intercept) represents the mean of the reference group (A)

-   β₁ represents the difference between category B and the reference

-   β₂ represents the difference between category C and the reference

This approach allows us to include categorical variables with any number of levels in our linear models. With k categories, we'll have k-1 dummy variables (one serves as the reference).

In the visualization:

-   Each color represents a different category

-   The dots are individual data points

-   The horizontal lines are the group means

-   β₀ is the mean of the reference group (A)

-   β₁ and β₂ are the differences between the other groups and the reference

Statistical software like R automatically does this dummy coding when you include a categorical variable in a model. When you run `lm(y ~ category)`, R creates these dummy variables behind the scenes.

This is why the independent t-test can be represented as a linear model with a binary predictor, and why ANOVA can be represented as a linear model with multiple dummy-coded predictors.
:::

## Example 3: ANOVA as a Linear Model

::::: columns
::: {.column width="45%"}
**ANOVA**: Compares means across multiple groups.

**As a linear model**: $$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_k x_k + \varepsilon$$

Where:

-   $x_1, x_2, etc.$ are dummy variables for group membership

-   $\beta_0$ is the mean for the reference group

-   $\beta_1, \beta_2, etc.$ are differences from reference group

-   We test whether any group differences exist

**Example**: Comparing test scores across different teaching methods
:::

::: {.column width="55%"}
```{r, echo=FALSE, fig.height=5, fig.width=6}
# Create data for ANOVA example
set.seed(42)
anova_data <- data.frame(
  group = factor(rep(c("Method A", "Method B", "Method C", "Method D"), each = 10)),
  score = c(
    rnorm(10, mean = 65, sd = 8),
    rnorm(10, mean = 70, sd = 8),
    rnorm(10, mean = 75, sd = 8),
    rnorm(10, mean = 72, sd = 8)
  )
)

# Calculate means for annotations
means <- tapply(anova_data$score, anova_data$group, mean)

# Plot
ggplot(anova_data, aes(x = group, y = score, fill = group)) +
  # Add boxplots
  geom_boxplot(alpha = 0.7) +
  # Add jittered points
  geom_jitter(width = 0.2, alpha = 0.6, color = "darkblue") +
  # Add annotations
  annotate("text",
    x = 1, y = max(anova_data$score) + 5,
    label = expression(beta[0] ~ "(reference group mean)"), color = "black", size = 4
  ) +
  annotate("text",
    x = 2, y = max(anova_data$score) + 5,
    label = expression(beta[1] ~ "(difference)"), color = "black", size = 4
  ) +
  annotate("text",
    x = 3, y = max(anova_data$score) + 5,
    label = expression(beta[2] ~ "(difference)"), color = "black", size = 4
  ) +
  annotate("text",
    x = 4, y = max(anova_data$score) + 5,
    label = expression(beta[3] ~ "(difference)"), color = "black", size = 4
  ) +
  # Add arrows pointing to means
  annotate("segment",
    x = 1, xend = 1,
    y = max(anova_data$score) + 4, yend = means["Method A"] + 5,
    arrow = arrow(length = unit(0.2, "cm"))
  ) +
  annotate("segment",
    x = 2, xend = 2,
    y = max(anova_data$score) + 4, yend = means["Method B"] + 5,
    arrow = arrow(length = unit(0.2, "cm"))
  ) +
  annotate("segment",
    x = 3, xend = 3,
    y = max(anova_data$score) + 4, yend = means["Method C"] + 5,
    arrow = arrow(length = unit(0.2, "cm"))
  ) +
  annotate("segment",
    x = 4, xend = 4,
    y = max(anova_data$score) + 4, yend = means["Method D"] + 5,
    arrow = arrow(length = unit(0.2, "cm"))
  ) +
  # Format
  theme_minimal() +
  theme(legend.position = "none") +
  scale_fill_brewer(palette = "Pastel1") +
  labs(
    title = "ANOVA as Linear Model",
    subtitle = "Comparing means across multiple groups",
    x = "Teaching Method",
    y = "Test Score"
  ) +
  coord_cartesian(ylim = c(min(anova_data$score) - 5, max(anova_data$score) + 10))
```
:::
:::::

::: notes
Now let's examine how Analysis of Variance (ANOVA) fits into the general linear model framework.

ANOVA is traditionally used to compare means across three or more groups. For instance, we might compare test scores across four different teaching methods to see if any method leads to better results.

In the general linear model framework, a one-way ANOVA is formulated as:

y = β₀ + β₁x₁ + β₂x₂ + ... + βₖxₖ + ε

Where: - x₁, x₂, etc. are dummy variables for group membership (using the dummy coding we just discussed) - β₀ is the intercept, representing the mean of the reference group (Method A in our example) - β₁, β₂, etc. represent the differences between each other group and the reference group - The overall F-test tests whether any of these differences are significantly different from zero

This is a direct extension of what we saw with the independent t-test. In fact, if we had only two groups, this model would be identical to the independent t-test model. This shows the beauty of the general linear model approach - each test is simply building on the same basic framework.

In the visualization: - The boxplots show the distribution of scores for each teaching method - The blue dots represent individual student scores - β₀ represents the mean score for Method A (the reference group) - β₁, β₂, and β₃ represent the differences between Methods B, C, D and Method A - The overall ANOVA tests whether there are any significant differences among the groups

In R, we can perform this analysis using either approach:

``` r
# Traditional approach
aov(score ~ group, data = anova_data)

# Linear model approach
lm(score ~ group, data = anova_data)
```

The F-statistic and p-value from both approaches will be identical, confirming that ANOVA is just a special case of the general linear model.

One advantage of the linear model approach is that it gives us not just the overall test of differences (like ANOVA) but also the specific estimates of each group difference, which can be very informative.
:::

## Example 4: Multiple Regression as a Linear Model

::::: columns
::: {.column width="45%"}
**Multiple Regression**: Predicts an outcome based on multiple predictors.

**As a linear model**: $$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_n x_n + \varepsilon$$

Where: - $x_1, x_2, etc.$ are continuous (or categorical) predictors - $\beta_0$ is the intercept - $\beta_1, \beta_2, etc.$ are the effects of each predictor - We test whether each $\beta_i ≠ 0$

**Example**: Predicting test scores based on study hours, previous grades, and teaching method
:::

::: {.column width="55%"}
```{r, echo=FALSE, fig.height=5, fig.width=6}
# Create data for regression example
set.seed(42)
study_hours <- runif(50, 1, 10)
previous_grades <- rnorm(50, mean = 70, sd = 10)
test_score <- 40 + 3 * study_hours + 0.3 * previous_grades + rnorm(50, 0, 5)
regression_data <- data.frame(
  study_hours = study_hours,
  previous_grades = previous_grades,
  test_score = test_score
)

# Create 3D scatter plot with plotly
library(plotly)

plot_ly(regression_data,
  x = ~study_hours, y = ~previous_grades, z = ~test_score,
  marker = list(size = 5, color = "blue", opacity = 0.6)
) %>%
  add_markers() %>%
  layout(
    title = "Multiple Regression Model",
    scene = list(
      xaxis = list(title = "Study Hours"),
      yaxis = list(title = "Previous Grades"),
      zaxis = list(title = "Test Score")
    )
  )
```
:::
:::::

::: notes
Finally, let's look at multiple regression within the general linear model framework.

Multiple regression predicts an outcome based on two or more predictors. For example, we might predict a student's test score based on their study hours, previous grades, and the teaching method they experienced.

The general linear model for multiple regression is:

y = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ + ε

Where: - x₁, x₂, etc. are our predictor variables (can be continuous or categorical) - β₀ is the intercept, representing the expected value of y when all predictors are zero - β₁, β₂, etc. are the coefficients that tell us the effect of each predictor on the outcome - We test whether each coefficient is significantly different from zero

This should look familiar - it's the same general form we've been using all along! In fact, this is the full general linear model that we started with. All the other tests we've discussed are just special cases of this model:

-   One-sample t-test: y = β₀ + ε
-   Independent t-test: y = β₀ + β₁x₁ + ε (where x₁ is a binary group indicator)
-   ANOVA: y = β₀ + β₁x₁ + β₂x₂ + ... + ε (where x₁, x₂, etc. are dummy-coded group indicators)
-   Multiple regression: y = β₀ + β₁x₁ + β₂x₂ + ... + ε (where x₁, x₂, etc. can be any mix of continuous or categorical predictors)

The 3D visualization shows how multiple regression works with two continuous predictors: - Each blue dot represents a student's data (study hours, previous grades, and test score) - The model creates a "plane" in this 3D space that best fits the data points - The plane's position at y-axis=0 represents β₀ (the intercept) - The plane's slope in the x₁ direction represents β₁ (effect of study hours) - The plane's slope in the x₂ direction represents β₂ (effect of previous grades)

With more than two predictors, the model creates a "hyperplane" in higher-dimensional space, which we can't visualize directly but follows the same principles.

In R, this is implemented simply as:

``` r
lm(test_score ~ study_hours + previous_grades, data = regression_data)
```

This unified framework makes it easy to build models that mix continuous and categorical predictors, allowing for flexible and powerful statistical analyses.
:::

## A Unified Approach to Statistical Tests

```{r echo=FALSE}
tests_table <- tibble(
  Test = c("One-sample t-test", "Independent t-test", "One-way ANOVA", "Multiple regression"),
  `Linear Model` = c("y ~ 1", "y ~ group", "y ~ group", "y ~ x1 + x2 + ..."),
  `What's being tested` = c("Is the intercept equal to a specific value?", "Is there a difference between groups?", "Are there differences between any groups?", "Do the predictors affect the outcome?")
)

knitr::kable(tests_table)
```

**Key Insight**: All these tests are variations of the same underlying model - they just differ in what predictors are included and what questions are being asked about the relationships.

::: notes
This table summarizes the unified approach we've been discussing. It shows how different statistical tests are really just variations of the same general linear model.

For the one-sample t-test: - Linear model: y \~ 1 (just an intercept) - We're testing whether the intercept equals a specific value

For the independent t-test: - Linear model: y \~ group (a categorical predictor with two levels) - We're testing whether there's a difference between groups

For one-way ANOVA: - Linear model: y \~ group (a categorical predictor with multiple levels) - We're testing whether there are differences between any groups

For multiple regression: - Linear model: y \~ x1 + x2 + ... (multiple predictors) - We're testing whether the predictors affect the outcome

The key insight here is that despite their different names and applications, these tests all use the same underlying model - the general linear model. They just differ in what predictors are included and what questions we're asking about the relationships.

This unified approach has several advantages: 1. It reduces the number of distinct concepts you need to learn 2. It helps you see the connections between different statistical techniques 3. It makes it easier to transition to more complex models 4. It focuses on understanding rather than memorization

In statistical software like R, this unified approach is reflected in how these tests are implemented. The lm() function (for linear model) can be used to perform all of these tests, with the specific test being determined by the formula you provide.

This perspective transforms statistics from a collection of seemingly unrelated tests into a coherent framework for understanding relationships in data.
:::

## Practical Applications: HR Analytics

Let's apply the general linear model to a real HR dataset to answer these questions:

1.  Is the average salary at our company different from the industry standard? (One-sample t-test)
2.  Is there a gender difference in salaries? (Independent t-test)
3.  Do salaries differ across job roles? (ANOVA)
4.  What factors predict salary? (Multiple regression)

All using the same unified framework!

```{r}
#| echo: false
#| message: false
# Load the HR dataset
hr_data <- read_sav("data/dataset-abc-insurance-hr-data.sav") %>%
  janitor::clean_names() %>%
  mutate(
    gender = factor(gender, levels = 1:2, labels = c("Female", "Male")),
    job_role = factor(job_role)
  )
```

::: notes
Now that we've explored the theory behind the general linear model, let's apply this unified framework to a real-world example using an HR analytics dataset.

Our dataset contains information about employees at an insurance company, including demographic information, job roles, salaries, and performance ratings. We'll use this data to answer four different questions, each corresponding to a different "traditional" statistical test:

1.  Is the average salary at our company different from the industry standard? This is traditionally a one-sample t-test.

2.  Is there a gender difference in salaries? This is traditionally an independent t-test.

3.  Do salaries differ across different job roles? This is traditionally a one-way ANOVA.

4.  What factors predict salary? This is traditionally a multiple regression.

By answering all these questions within the general linear model framework, we'll demonstrate how this unified approach simplifies our analysis while providing consistent and interpretable results.

This practical application will show how the theoretical concepts we've discussed translate into real-world data analysis, and how the different "tests" emerge naturally from the same underlying model.
:::

## Question 1: Is the average salary different from the standard?

**Question**: Is the average salary grade at our company (30.3) different from the industry standard (30)?

**Linear Model**: $\text{salary} = \beta_0 + \varepsilon$

```{r}
#| echo: true
# Traditional one-sample t-test
t.test(hr_data$salarygrade, mu = 30)

# Same test as linear model
summary(lm(salarygrade ~ 1, data = hr_data))
```

::: notes
Let's start by addressing our first question: Is the average salary grade at our company different from the industry standard of 30?

In the traditional approach, we would use a one-sample t-test for this question. In the general linear model framework, this is an intercept-only model:

salary = β₀ + ε

We're testing whether β₀ (the average salary grade) equals 30.

First, we run a traditional t-test using the t.test() function. The results show that the average salary grade is 30.3, and the p-value is 0.022, indicating that our company's average is significantly different from 30 at the conventional alpha level of 0.05.

Next, we run the same test as a linear model using lm(). The intercept is 30.3 (the same as before), and the t-value and p-value are also identical to those from the t-test.

This demonstrates that the one-sample t-test is just a special case of the general linear model - specifically, it's testing whether the intercept equals a particular value.

The advantage of understanding this equivalence is that it provides a unified framework for thinking about statistical tests. Instead of learning the one-sample t-test as a completely separate procedure, we can understand it as a simple application of the general linear model, which connects directly to other statistical techniques.
:::

## Question 2: Is there a gender difference in salaries?

**Question**: Is there a gender difference in salary grades?

**Linear Model**: $\text{salary} = \beta_0 + \beta_1 \text{gender} + \varepsilon$

```{r}
#| echo: true
# Traditional independent t-test
t.test(salarygrade ~ gender, data = hr_data, var.equal = TRUE)

# Same test as linear model
summary(lm(salarygrade ~ gender, data = hr_data))
```

::: notes
Now let's address our second question: Is there a gender difference in salary grades?

In the traditional approach, we would use an independent t-test for this question. In the general linear model framework, this is:

salary = β₀ + β₁×gender + ε

where gender is coded as 0 for females and 1 for males.

First, we run a traditional independent t-test using the t.test() function. The results show that males have a higher average salary grade (33.2) compared to females (27.3), and this difference is statistically significant (p \< 0.001).

Next, we run the same test as a linear model using lm(). Here: - The intercept (β₀) is 27.3, which is the average salary grade for females (the reference group) - The coefficient for genderMale (β₁) is 5.9, which is the difference between male and female salaries - The t-value and p-value for this coefficient are identical to those from the independent t-test

This shows that the independent t-test is just a linear model with a binary predictor. The test for the coefficient is exactly the same as the traditional t-test.

The advantage of the linear model approach is that it gives us not just the test of difference but also the estimate of how large that difference is (5.9 salary grade points), which is directly interpretable.

Understanding this equivalence helps us see how the independent t-test connects to other statistical techniques within the general linear model framework.
:::

## Question 3: Do salaries differ across job roles?

**Question**: Do salary grades differ across job roles?

**Linear Model**: $\text{salary} = \beta_0 + \beta_1 \text{role}_1 + \beta_2 \text{role}_2 + ... + \varepsilon$

```{r}
#| echo: true
# Traditional ANOVA
summary(aov(salarygrade ~ job_role, data = hr_data))

# Same test as linear model
anova(lm(salarygrade ~ job_role, data = hr_data))
```

::: notes
Next, let's examine our third question: Do salary grades differ across different job roles?

In the traditional approach, we would use a one-way ANOVA for this question. In the general linear model framework, this is:

salary = β₀ + β₁×role₁ + β₂×role₂ + ... + ε

where each role variable is a dummy indicator for a particular job role.

First, we run a traditional ANOVA using the aov() function. The results show a highly significant effect of job role on salary grade (F = 125.9, p \< 0.001).

Then, we run the same test as a linear model using lm() and obtain the ANOVA table using the anova() function. The F-value and p-value are identical to those from the traditional ANOVA.

This demonstrates that one-way ANOVA is just a linear model with a categorical predictor that has multiple levels. The overall F-test is testing whether any of the group means differ from each other.

The advantage of the linear model approach is that we can easily extract the specific differences between job roles (not shown in this output but available through the coefficients of the model), which tells us not just that there are differences, but exactly what those differences are.

Understanding this equivalence helps us see how ANOVA is connected to other statistical techniques within the general linear model framework, and provides a more complete understanding of our data.
:::

## Question 4: What factors predict salary?

**Question**: What factors predict salary grades?

**Linear Model**: $\text{salary} = \beta_0 + \beta_1 \text{gender} + \beta_2 \text{experience} + \beta_3 \text{performance} + \varepsilon$

```{r}
#| echo: true
# Multiple regression model
salary_model <- lm(salarygrade ~ gender + tenure + evaluation,
  data = hr_data
)
summary(salary_model)
```

::: notes
Finally, let's address our fourth question: What factors predict salary grades?

Here, we're building a multiple regression model that includes several predictors: gender, years of experience (tenure), and performance rating (evaluation).

In the general linear model framework, this is:

salary = β₀ + β₁×gender + β₂×experience + β₃×performance + ε

This is a direct extension of the models we've been working with, just with more predictors.

The results show: - The intercept (β₀) is 19.85, representing the expected salary grade for a female employee with no experience and no performance rating - Being male (β₁) is associated with a 6.07 point increase in salary grade, holding other factors constant - Each additional year of experience (β₂) is associated with a 1.37 point increase in salary grade - Each additional point in performance rating (β₃) is associated with a 2.05 point increase in salary grade - All of these effects are statistically significant (p \< 0.001) - The model explains about 50% of the variance in salary grades (R² = 0.503)

This model allows us to understand the relative importance of different factors in predicting salary. Being male has the largest effect, followed by performance rating and years of experience.

The beauty of the general linear model approach is that we can easily add or remove predictors, combine categorical and continuous variables, and interpret the results in a consistent way.

These four analyses - traditionally taught as entirely separate techniques - are all special cases of the same general linear model. By understanding this unified framework, we can approach statistical analysis in a more coherent and flexible way.
:::

## Visualizing Multiple Regression Results

```{r}
#| echo: false
# Create visualizations of the regression relationships
p1 <- ggplot(hr_data, aes(x = tenure, y = salarygrade, color = gender)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal() +
  labs(
    title = "Experience and Salary by Gender",
    x = "Years of Experience",
    y = "Salary Grade"
  ) +
  scale_color_manual(values = c("Female" = "#FF9999", "Male" = "#6699CC"))

p2 <- ggplot(hr_data, aes(x = evaluation, y = salarygrade, color = gender)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "lm", se = FALSE) +
  theme_minimal() +
  labs(
    title = "Performance and Salary by Gender",
    x = "Performance Rating",
    y = "Salary Grade"
  ) +
  scale_color_manual(values = c("Female" = "#FF9999", "Male" = "#6699CC"))

p1 + p2
```

::: notes
These visualizations help us better understand the relationships in our multiple regression model.

The left panel shows the relationship between years of experience and salary grade, with gender indicated by color. We can observe several patterns:

1.  There's a positive relationship between experience and salary for both genders - employees with more experience tend to have higher salaries
2.  The lines are roughly parallel, suggesting that the effect of experience on salary is similar for both genders
3.  There's a clear gender gap - the blue line (males) is consistently above the red line (females), indicating that males tend to have higher salaries at the same level of experience

The right panel shows the relationship between performance rating and salary grade. Again, we see:

1.  A positive relationship - employees with higher performance ratings tend to have higher salaries
2.  Parallel lines, suggesting similar effects of performance on salary for both genders
3.  The same gender gap is visible here

These visualizations complement our regression results. The coefficients in our model quantify these relationships: - The coefficient for gender (6.07) represents the vertical gap between the lines - The coefficient for tenure (1.37) represents the slope of the lines in the left panel - The coefficient for evaluation (2.05) represents the slope of the lines in the right panel

The power of the general linear model is that it can capture all these relationships simultaneously in a single model, allowing us to understand how multiple factors jointly affect our outcome of interest.
:::

## Combining Different Types of Predictors

::::: columns
::: {.column width="50%"}
The general linear model can easily combine:

-   **Categorical predictors** (like gender, job role)
-   **Continuous predictors** (like age, experience)
-   **Interaction terms** (when effects depend on each other)

This flexibility allows us to model complex relationships using the same unified framework.

For example, ANCOVA combines ANOVA (categorical predictors) with regression (continuous predictors).
:::

::: {.column width="50%"}
```{r, echo=FALSE, fig.height=5, fig.width=6}
# Create data for ANCOVA visualization
set.seed(123)
ancova_data <- data.frame(
  group = rep(c("Group A", "Group B", "Group C"), each = 30),
  x = runif(90, 0, 10),
  y = numeric(90)
)

# Generate y values with different intercepts but same slope
ancova_data$y[ancova_data$group == "Group A"] <-
  5 + 2 * ancova_data$x[ancova_data$group == "Group A"] + rnorm(30, 0, 2)
ancova_data$y[ancova_data$group == "Group B"] <-
  10 + 2 * ancova_data$x[ancova_data$group == "Group B"] + rnorm(30, 0, 2)
ancova_data$y[ancova_data$group == "Group C"] <-
  7 + 2 * ancova_data$x[ancova_data$group == "Group C"] + rnorm(30, 0, 2)

# Plot
ggplot(ancova_data, aes(x = x, y = y, color = group)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = FALSE, linewidth = 1.5) +
  theme_minimal() +
  labs(
    title = "ANCOVA: Combining Categorical and Continuous",
    subtitle = "Different intercepts (group effects) with same slope (x effect)",
    x = "Continuous Predictor",
    y = "Outcome"
  ) +
  scale_color_brewer(palette = "Set1")
```
:::
:::::

::: notes
A major advantage of the general linear model framework is its flexibility to combine different types of predictors in the same model:

1.  Categorical predictors (like gender, job role, or treatment group) are included through dummy coding, as we've seen
2.  Continuous predictors (like age, experience, or test scores) are included directly
3.  Interaction terms can be added to model situations where the effect of one predictor depends on the level of another

This flexibility allows us to build models that more accurately reflect the complexity of real-world relationships.

The visualization shows an example of combining categorical and continuous predictors in an Analysis of Covariance (ANCOVA) model. Here:

-   The three colored lines represent three different groups (categorical predictor)
-   The x-axis represents a continuous predictor
-   Each line has its own intercept (representing the group effect)
-   The lines have the same slope (representing the effect of the continuous predictor)

In this ANCOVA model: - The categorical predictor tells us that the groups have different baseline levels (Group B \> Group C \> Group A) - The continuous predictor tells us that as x increases, y increases at the same rate for all groups - The parallel lines indicate no interaction between the categorical and continuous predictors

If we wanted to allow for different slopes across groups, we could add an interaction term to our model.

The general linear model makes it easy to construct and interpret such complex models by following the same principles we've applied to simpler cases.
:::

## Why This Unified Approach Matters

::::: columns
::: {.column width="50%"}
Understanding statistics through the general linear model has multiple benefits:

1.  **Simpler conceptual framework** - one model instead of many separate techniques

2.  **Easier interpretation** - consistent approach across analyses

3.  **Greater flexibility** - easily add or modify predictors as needed

4.  **Clearer pathway to advanced methods** - natural extension to more complex models

5.  **Focus on relationships** rather than "which test to use"
:::

::: {.column width="50%"}
```{r, echo=FALSE, fig.height=5, fig.width=6}
# Create a visualization showing benefits of unified approach
benefit_data <- data.frame(
  Benefit = c(
    "Conceptual Simplicity", "Consistent Interpretation",
    "Greater Flexibility", "Path to Advanced Methods",
    "Focus on Relationships"
  ),
  Score = c(90, 85, 95, 80, 92),
  Color = c("#1b9e77", "#d95f02", "#7570b3", "#e7298a", "#66a61e")
)

# Create horizontal bar chart
ggplot(benefit_data, aes(x = reorder(Benefit, Score), y = Score, fill = Benefit)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  scale_fill_manual(values = benefit_data$Color) +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(
    title = "Benefits of the Unified Approach",
    subtitle = "Why the general linear model framework matters",
    x = "",
    y = "Relative Benefit"
  ) +
  geom_text(aes(label = paste0(Score, "%")), hjust = -0.2, color = "black") +
  scale_y_continuous(limits = c(0, 100))
```
:::
:::::

::: notes
Why does this unified perspective matter? There are several practical benefits:

1.  Simpler conceptual framework: Instead of learning many different statistical techniques with different formulas and assumptions, you can understand them all as variations of the same underlying model. This reduces cognitive load and makes statistics more accessible.

2.  Consistent interpretation: When all tests follow the same framework, interpretation becomes more consistent. Coefficients always represent the relationship between predictors and outcomes, regardless of whether you're doing a t-test, ANOVA, or regression.

3.  Greater flexibility: Once you understand the general linear model, you can easily combine different types of predictors (categorical and continuous) in the same model, allowing for more nuanced analyses that better reflect the complexity of real-world relationships.

4.  Clearer pathway to advanced methods: The general linear model is the foundation for more advanced statistical techniques like mixed-effects models, generalized linear models, and many others. Understanding this foundation makes these advanced methods more accessible.

5.  Focus on relationships: Instead of starting with "Which test should I use?", you can focus on "What relationships am I interested in?" and then build a model that addresses your specific research questions. This shifts the emphasis from procedure to substance.

This approach won't just help you with this course - it provides a foundation for understanding statistics that will serve you throughout your academic and professional career.

As you continue to develop your statistical skills, thinking in terms of the general linear model will help you make more informed choices about how to analyze your data and interpret your results.
:::

## Summing Up: The Unified View of Statistical Tests

::: incremental
-   Many common statistical tests are special cases of the general linear model
-   The differences lie in the types of predictors and specific hypotheses
-   This unified framework simplifies learning and application
-   It provides a foundation for understanding more advanced methods
-   Focus on modelling relationships, not selecting the "right" test
:::

::: notes
To summarize what we've covered today:

1.  Many common statistical tests - including t-tests, ANOVA, and regression - are special cases of the general linear model.

2.  The differences between these tests lie in the types of predictors they use (none, binary, categorical with multiple levels, or continuous) and the specific hypotheses they test.

3.  This unified framework simplifies learning and application of statistics by reducing the number of distinct concepts you need to understand.

4.  It provides a solid foundation for understanding more advanced statistical methods, which are often extensions of the general linear model.

5.  This approach encourages you to focus on the relationships you want to investigate and the questions you want to answer, rather than worrying about which test to select.

By understanding this unified framework, you've gained a powerful tool for data analysis that will serve you well in this course and beyond.

In our upcoming exercise, you'll have the opportunity to apply these concepts to real data, further solidifying your understanding of the general linear model as a unifying framework for statistical analysis.
:::

## Further Resources

If you'd like to explore the general linear model further:

-   "Common statistical tests are linear models" by Jonas Kristoffer Lindeløv\
    <https://lindeloev.github.io/tests-as-linear/>

-   *Statistical Thinking for the 21st Century* by Russell A. Poldrack (2019)\
    <https://statsthinking21.github.io/statsthinking21-core-site/>

-   Our practical exercise will help you apply these concepts to real data

::: notes
If you're interested in exploring the general linear model further, here are some excellent resources:

"Common statistical tests are linear models" by Jonas Kristoffer Lindeløv is a comprehensive online resource that goes into detail about how different statistical tests can be expressed as linear models, with code examples in R.

"Statistical Thinking for the 21st Century" by Russell A. Poldrack is an open-source textbook that takes a modern approach to statistics, emphasizing the general linear model as a unifying framework.

And of course, our practical exercise will give you hands-on experience applying these concepts to real data, which is the best way to solidify your understanding.

The shift toward understanding statistics through the general linear model is gaining momentum in statistics education. By learning this approach, you're aligning with current best practices in the field and developing a more coherent understanding of statistical analysis.

Remember that the goal isn't just to pass a statistics course but to develop a way of thinking about data that will help you answer meaningful questions throughout your academic and professional career.
:::
