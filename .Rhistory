geom_vline(xintercept = t_res$statistic, color='red', size=1) +
xlab('Number of Heads') +
ylab('Frequency') +
ggtitle("Distribution of Heads in 100,000 Simulated Trials")
xpos <- seq(-5, 5, by=0.01)
degree <- 280
ypos <- dt(xpos, df = degree)
ggplot() +
xlim(-8, 8) +
geom_function(aes(colour="t, df=280"), fun = dt, args = list(df = 280)) +
geom_vline(xintercept = t_res$statistic, color='red', size=1) +
xlab('Number of Heads') +
ylab('Frequency') +
ggtitle("Distribution of Heads in 100,000 Simulated Trials")
xpos <- seq(-5, 5, by=0.01)
degree <- 280
ypos <- dt(xpos, df = degree)
ggplot() +
xlim(-8, 8) +
geom_function(aes(colour="t, df=280"), fun = dt, args = list(df = 280)) +
geom_vline(xintercept = t_res$statistic, color='black', size=1) +
xlab('Number of Heads') +
ylab('Frequency') +
ggtitle("Distribution of Heads in 100,000 Simulated Trials")
xpos <- seq(-5, 5, by=0.01)
degree <- 280
ypos <- dt(xpos, df = degree)
ggplot() +
xlim(-8, 8) +
geom_function(aes(colour="t, df=280"), fun = dt, args = list(df = 280)) +
geom_vline(xintercept = t_res$statistic, color='black', size=1) +
xlab(paste0('Test statistic under the null hypothesis ', TeX("$P(t | H_0)$")) +
ylab('Frequency') +
ggtitle("Student's T distribution")
xpos <- seq(-5, 5, by=0.01)
degree <- 280
ypos <- dt(xpos, df = degree)
ggplot() +
xlim(-8, 8) +
geom_function(aes(colour="t, df=280"), fun = dt, args = list(df = 280)) +
geom_vline(xintercept = t_res$statistic, color='black', size=1) +
xlab(paste0('Test statistic under the null hypothesis ', TeX("$P(t | H_0)$"))) +
ylab('Frequency') +
ggtitle("Student's T distribution")
xpos <- seq(-5, 5, by=0.01)
degree <- 280
ypos <- dt(xpos, df = degree)
ggplot() +
xlim(-8, 8) +
geom_function(aes(colour="t, df=280"), fun = dt, args = list(df = 280)) +
geom_vline(xintercept = t_res$statistic, color='black', size=1) +
xlab('Test statistic under the null hypothesis (P(t | H0))') +
ylab('Frequency') +
ggtitle("Student's T distribution")
ai_jobs |>
ggplot(mapping = aes(x = salary_usd, colour = job_title, label = job_title)) +
geom_textdensity() +
labs(
title = "Distribution of Salaries for AI Jobs",
x = "Salary (USD)",
y = "Frequency"
) +
theme_minimal() +
guides(color = "none")
ai_jobs |>
ggplot(mapping = aes(x = salary_usd, colour = automation_risk, label = automation_risk)) +
geom_boxplot() +
labs(
title = "Distribution of Salaries for AI Jobs",
x = "Salary (USD)",
y = "Frequency"
) +
theme_minimal()
NHANES_adult |>
gt_preview() |>
tab_header(title = "NHANES Dataset")
names(NHANES_adult)
NHANES_adult |>
select(c('SurveyYr', 'Gender', 'Age', 'Education', 'Weight', 'Height'))
gt_preview() |>
tab_header(title = "NHANES Dataset")
NHANES_adult |>
select(c('SurveyYr', 'Gender', 'Age', 'Education', 'Weight', 'Height')) |>
gt_preview() |>
tab_header(title = "NHANES Dataset")
NHANES_adult |>
select(c('SurveyYr', 'Gender', 'Age', 'Race1', 'Education', 'Weight', 'Height', ')) |>
gt_preview() |>
tab_header(title = "NHANES Dataset")
NHANES_adult |>
select(c('SurveyYr', 'Gender', 'Age', 'Race1', 'Education', 'Weight', 'Height')) |>
gt_preview() |>
tab_header(title = "NHANES Dataset")
NHANES_adult |>
select(c('SurveyYr', 'Gender', 'Age', 'Race1', 'Education', 'Weight', 'Height', 'Pulse', 'Diabetes')) |>
gt_preview() |>
tab_header(title = "NHANES Dataset")
library(tidyverse)
data <- tibble(
"1" = c(1, 2, 3),
"2" = c(4, 5, 6),
"3" = c(7, 8, 9)
)
data
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
data
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
data
source("~/.active-rstudio-document", echo=TRUE)
data
source("~/.active-rstudio-document", echo=TRUE)
data
source("~/.active-rstudio-document", echo=TRUE)
data
source("~/.active-rstudio-document", echo=TRUE)
data
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
sample_means
sample_means[1]
sample_means[[1]]
sample_means[1,]
values(sample_means)
source("~/.active-rstudio-document", echo=TRUE)
source("~/.active-rstudio-document", echo=TRUE)
sample_means[[1]
sample_means[[1]]
source("~/.active-rstudio-document", echo=TRUE)
data.frame(data)
t(data.frame(data))
data.frame(sample_1, sample_2, sample_3)
?data.frame
?tibble
source("~/.active-rstudio-document", echo=TRUE)
ample_2, sample_3) |>
data <- data.frame(sample_1, sample_2, sample_3) |>
t() |> # Transpose the table
as_tibble()
data
id = 1:3,
data <- tibble(
id = 1:3,
sample_1 = sample_1,
sample_2 = sample_2,
sample_3 = sample_3
) |>
pivot_longer(cols = -id, names_to = "sample", values_to = "value")
data <- tibble(sample_1, sample_2, sample_3) |>
rownames_to_column(var = "id")
data
data <- tibble(sample_1, sample_2, sample_3) |>
rownames_to_column(var = "id") |>
gather(key = "sample", value = "value", -id)
data # Look at the table
data <- tibble(sample_1, sample_2, sample_3) |>
gather(key = "sample", value = "value", -id)
library(tidyverse)
# Input the values of the samples
sample_1 <- c(1, 2, 3, 4, 5)
sample_2 <- c(6, 7, 8, 9, 10)
sample_3 <- c(11, 12, 13, 14, 15)
# ... and so on
#
# Create a table with the samples and an id column
data <- tibble(sample_1, sample_2, sample_3) |>
gather(key = "sample", value = "value")
data # Look at the table
source("~/.active-rstudio-document", echo=TRUE)
sample_means <- data |>
group_by(sample) |>
summarise(mean = mean(value))
sample_means # Look at the mean
ggplot(sample_means, mapping = aes(x = mean)) +
geom_histogram() +
theme_minimal()
#| echo: false
#| message: false
#| warning: false
library(tidyverse)
library(NHANES)
library(cowplot)
library(ggplot2)
library(gganimate)
library(hrbrthemes)
library(gridExtra)
library(httr)
library(lubridate)
library(gtExtras)
library(gtsummary)
library(labelled)
library(geomtextpath)
library(infer)
library(broom)
library(knitr)
knitr::opts_chunk$set(dev = "ragg_png")
extrafont::loadfonts(quiet = TRUE)
pdf.options(encoding = "CP1250")
set_null_device("png")
# create a NHANES dataset without duplicated IDs
NHANES <- NHANES %>%
distinct(ID, .keep_all = TRUE)
# create a dataset of only adults
NHANES_adult <- NHANES %>%
filter(!is.na(Height), !is.na(Weight), Age >= 18)
# Get the project management dataset
# project_data_httr <- GET("https://www.kaggle.com/api/v1/datasets/download/mayarmohamedswilam/project-management")
# temp <- tempfile()
# download.file(project_data_httr$url, temp)
# project_data <- read_csv(unz(temp, "Project Management Dataset (1).csv"))
# unlink(temp)
# project_data <- project_data |>
#   janitor::clean_names() |>
#   mutate(
#     project_type = factor(project_type),
#     region = factor(region),
#     department = factor(department),
#     complexity = factor(complexity, levels=c('Low', 'Medium', 'High'), ordered=TRUE),
#     status = factor(status),
#     completion_percent = as.numeric(sub("%", "", completion_percent))/100,
#     start_date = mdy(start_date),
#     end_date = mdy(end_date)
#   )
ai_jobs_url <- GET("https://www.kaggle.com/api/v1/datasets/download/uom190346a/ai-powered-job-market-insights")
temp <- tempfile()
download.file(ai_jobs_url$url, temp)
ai_jobs <- read_csv(unz(temp, "ai_job_market_insights.csv"))
unlink(temp)
ai_jobs <- ai_jobs |>
janitor::clean_names() |>
mutate(salary_usd = case_when(
job_title == "HR Manager" ~ salary_usd - 15000,
job_title == "Marketing Specialist" ~ salary_usd - 20000,
job_title == "Software Engineer" ~ salary_usd - 10000,
job_title == "UX Designer" ~ salary_usd - 25000,
job_title == "Sales Manager" ~ salary_usd - 10000,
job_title == "AI Researcher" ~ salary_usd + 30000,
job_title == "Data Scientist" ~ salary_usd + 10000,
.default = salary_usd
)) |>
rowwise() |>
mutate(automation_risk = case_when(
job_title == "HR Manager" ~ sample(c("Low", "Medium", "High"), 1, replace = TRUE, prob = c(0.2, 0.4, 0.6)),
job_title == "Marketing Specialist" ~ sample(c("Low", "Medium", "High"), 1, replace = TRUE, prob = c(0, 0.1, 0.9)),
job_title == "Software Engineer" ~ sample(c("Low", "Medium", "High"), 1, replace = TRUE, prob = c(0.1, 0.1, 0.8)),
job_title == "UX Designer" ~ sample(c("Low", "Medium", "High"), 1, replace = TRUE, prob = c(0.3, 0.2, 0.5)),
job_title == "Sales Manager" ~ sample(c("Low", "Medium", "High"), 1, replace = TRUE, prob = c(0.1, 0.3, 0.6)),
job_title == "AI Researcher" ~ sample(c("Low", "Medium", "High"), 1, replace = TRUE, prob = c(0.9, 0.1, 0.0)),
.default = automation_risk
)) |>
ungroup() |>
mutate(
job_title = factor(job_title),
industry = factor(industry),
company_size = factor(company_size, levels = c("Small", "Medium", "Large"), ordered = TRUE),
location = factor(location),
ai_adoption_level = factor(ai_adoption_level, levels = c("Low", "Medium", "High"), ordered = TRUE),
automation_risk = factor(automation_risk, levels = c("Low", "Medium", "High"), ordered = TRUE),
required_skills = factor(required_skills),
remote_friendly = factor(remote_friendly),
job_growth_projection = factor(job_growth_projection, levels = c("Decline", "Stable", "Growth"), ordered = TRUE)
) |>
set_variable_labels(
job_title = "Title of the job role",
industry = "Industry in which the job is located",
company_size = "Size of the company offering the job",
location = "Geographic location of the job",
ai_adoption_level = "Extent to which the company has adopted AI in its operations",
automation_risk = "Estimated risk that the job could be automated within the next 10 years",
required_skills = "Key skills required for the job role",
salary_usd = "Annual salary offered for the job in USD",
remote_friendly = "Indicates whether the job can be performed remotely",
job_growth_projection = "Projected growth or decline of the job role over the next five years"
)
NHANES_adult |>
select(c("SurveyYr", "Gender", "Age", "Race1", "Education", "Weight", "Height", "Pulse", "Diabetes")) |>
gt_preview() |>
tab_header(title = "NHANES Dataset")
#| echo: false
t <- 500
mu <- NULL
stdev <- NULL
for (i in 1:t) {
mu[i] <- NHANES_adult |>
sample_n(i) |>
pull(Weight) |>
mean()
stdev[i] <- sd(mu)
}
d <- data.frame(n = 1:t, mu, stdev)
# See the generated data
# head(d)
#| echo: false
library(latex2exp)
# Plot Animated Graph
p <- d |>
ggplot(aes(x = n, y = mu)) +
geom_line() +
geom_point() +
xlim(0, t) +
ylim(min(mu) - 2, max(mu) + 5) +
geom_hline(yintercept = mean(NHANES_adult$Weight), alpha = 0.4, color = "red") +
#  scale_color_viridis(discrete = TRUE) +
ggtitle("Convergence of Estimate with Sample Size (Law of Large Numbers)") +
geom_label(x = 100, y = max(mu) + 4, label = TeX(paste0("Std Dev ($\\sigma_{\\bar{x}}$) = ", round(stdev, 2))), parse = TRUE) +
theme_ipsum() +
ylab("Mean Weight") +
xlab("Sample Size")
p
# p <- p + transition_reveal(n)
# animate(p)
#| out-width: 100%
#| echo: false
#| fig-cap: "We are drawing a random sample of people from the dataset and calculating the mean weight for that sample. *Sample size* is the number of data points we pull. We then repeat this 5000 times (`n_samples`) to build up the sampling distribution."
sample_sizes <- c(5, 15, 50)
plots <- list()
plots[[1]] <- NHANES_adult |>
ggplot(aes(x = Weight)) +
geom_histogram(bins = 50, fill = "blue", alpha = 0.7) +
geom_vline(
xintercept = mean(NHANES_adult$Weight),
color = "red",
linewidth = 1
) +
xlim(min(NHANES_adult$Weight), max(NHANES_adult$Weight)) +
labs(
title = "Histogram of Population",
x = "Weight (kg)",
y = "Count"
)
# Take 5000 samples and plot distribution
set.seed(123)
for (i in 1:length(sample_sizes)) {
samples_large <- map_df(
1:5000,
~ {
NHANES_adult |>
sample_n(sample_sizes[i]) |>
summarise(mean_weight = mean(Weight))
}
)
plots[[i + 1]] <- ggplot(samples_large, aes(x = mean_weight)) +
geom_histogram(bins = 100 * i, fill = "blue", alpha = 0.7) +
geom_vline(
xintercept = mean(NHANES_adult$Weight),
color = "red",
linewidth = 1
) +
xlim(min(NHANES_adult$Weight), max(NHANES_adult$Weight)) +
labs(
title = paste0("Sample Size = ", sample_sizes[i]),
x = "Sample Mean Weight (kg)",
y = "Count"
)
}
do.call(grid.arrange, plots)
ai_jobs |>
gt_preview() |>
tab_header(title = "AI-Powered Job Market Insights") |>
tab_source_note(source_note = "Source: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights")
#| echo: true
ai_jobs_risk <- ai_jobs |>
filter(automation_risk %in% c("Low", "High"))
ai_jobs_high <- ai_jobs_risk |>
filter(automation_risk == "High")
ai_jobs_low <- ai_jobs_risk |>
filter(automation_risk == "Low")
#| echo: false
#| fig-width: 6
#| fig-height: 5
#| out-width: 95%
# Create simulated normal distribution plot
ggplot(data.frame(x = seq(-4, 4, length.out = 200)), aes(x)) +
# Add shaded regions with stronger colors
stat_function(
fun = dnorm, geom = "area", fill = "#0066CC", alpha = 0.4,
xlim = c(-1, 1)
) +
stat_function(
fun = dnorm, geom = "area", fill = "#3399FF", alpha = 0.3,
xlim = c(-2, -1)
) +
stat_function(
fun = dnorm, geom = "area", fill = "#3399FF", alpha = 0.3,
xlim = c(1, 2)
) +
stat_function(
fun = dnorm, geom = "area", fill = "#66B2FF", alpha = 0.2,
xlim = c(-3, -2)
) +
stat_function(
fun = dnorm, geom = "area", fill = "#66B2FF", alpha = 0.2,
xlim = c(2, 3)
) +
# Add the main curve
stat_function(fun = dnorm, color = "#003366", linewidth = 1.2) +
# Add vertical lines for mean
geom_vline(xintercept = 0, linetype = "dashed", color = "#CC0000", alpha = 0.7) +
# Add annotations with better positioning
annotate("text", x = 0, y = 0.275, label = "68%\n(±1\u03c3)", size = 4, fontface = "bold") +
annotate("text", x = 1.5, y = 0.06, label = "95%\n(±2\u03c3)", size = 4, fontface = "bold") +
annotate("text", x = 3, y = 0.06, label = "99.7% (±3\u03c3)", size = 4, fontface = "bold") +
# Customize theme and labels
labs(
title = "Standard Normal Distribution",
subtitle = "Showing empirical rule percentages and standard deviations (\u03c3)",
x = "Standard Deviations from Mean (\u03c3)",
y = "Probability Density"
) +
theme_minimal() +
theme(
plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
plot.subtitle = element_text(hjust = 0.5, size = 12),
axis.title = element_text(size = 11),
panel.grid.minor = element_blank()
) +
scale_x_continuous(breaks = -3:3)
ai_jobs |>
gt_preview() |>
tab_header(title = "AI-Powered Job Market Insights") |>
tab_source_note(source_note = "Source: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights")
#| echo: true
ai_jobs_risk <- ai_jobs |>
filter(automation_risk %in% c("Low", "High"))
ai_jobs_high <- ai_jobs_risk |>
filter(automation_risk == "High")
ai_jobs_low <- ai_jobs_risk |>
filter(automation_risk == "Low")
#| echo: true
mean_high <- mean(ai_jobs_high$salary_usd)
mean_low <- mean(ai_jobs_low$salary_usd)
s_hat <- mean_high - mean_low
#| echo: true
#| warning: false
t_res <- t.test(ai_jobs_high$salary_usd, ai_jobs_low$salary_usd)
#| echo: true
#| warning: false
t.test(ai_jobs_high$salary_usd, ai_jobs_low$salary_usd)
xpos <- seq(-5, 5, by = 0.01)
degree <- 280
ypos <- dt(xpos, df = degree)
ggplot() +
xlim(-8, 8) +
geom_function(aes(colour = "t, df=280"), fun = dt, args = list(df = 280), linewidth = 1.5) +
geom_vline(xintercept = t_res$statistic, color = "black", linewidth = 1.5) +
xlab("Test statistic under the null hypothesis P(data | H0)") +
ylab("Frequency") +
ggtitle("Student's T distribution")
library(tidyverse)
library(palmerpenguins)
library(NHANES)
NHANES <- NHANES
library(tidyverse)
# Input the values of the samples
sample_1 <- c(1, 2, 3, 4, 5)
sample_2 <- c(6, 7, 8, 9, 10)
sample_3 <- c(11, 12, 13, 14, 15)
# sample_4 <- c(..., ..., ..., ...)
sample_1
sample_2
mean(sample_1)
sum(sample_1) / length(sample_1)
mean_1 <- mean(sample_1)
mean_2 <- mean(sample_2)
mean_3 <- mean(sample_3)
mean_1
means <- c(mean_1, mean_2, mean_3)
means
data <- tibble(sample_1, sample_2, sample_3)
data
data <- tibble(sample_1, sample_2, sample_3) |>
gather(key = "sample", value = "value")
data
data <- tibble(sample_1, sample_2, sample_3) |>
gather(key = "sample", value = "value")
source("~/Documents/UCL/Teaching/BSSC0021_25/Code/Week4/sampling-exercise.R", echo=TRUE)
library(tidyverse)
# Input the values of the samples
sample_1 <- c(83.2, 82.6, 82.6, 82.6, 93.2, 94, 94, 48.5, 33.6, 33.6)
sample_2 <- c(88, 48, 23, 23, 23, 23, 23, 23, 23, 23)
sample_3 <- c(83.2, 82.6, 82.6, 99, 100, 145, 123, 222, 111, 111)
# sample_4 <- c(..., ..., ..., ...)
# ... and so on
# Create a table with the samples and an id column
data <- tibble(sample_1, sample_2, sample_3) |>
gather(key = "sample", value = "value")
data # Look at the table
source("~/Documents/UCL/Teaching/BSSC0021_25/Code/Week4/sampling-exercise.R", echo=TRUE)
data
data <- tibble(sample_1, sample_2, sample_3) |>
gather(key = "sample", value = "value")
data # Look at the table
sample_means
sample_means <- data |>
group_by(sample) |>
summarise(mean = mean(value))
sample_means # Look at the means
source("~/Documents/UCL/Teaching/BSSC0021_25/Code/Week4/sampling-exercise.R", echo=TRUE)
renv::status()
renv::snapshot()
quarto render --profile full_site
quarto render --profile full-site
quarto render
