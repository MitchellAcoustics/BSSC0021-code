labs(
title = "Distribution of Sample Means by Automation Risk",
x = "Mean Salary (USD)",
y = "Count",
fill = "Automation Risk"
)
?pnorm
sd(sd_df$stdev)
sd(sd_df$stdev, na.rm=TRUE)
xpnorm(mean(sd_df$stdev, na.rm=TRUE), sd=sd(sd_df$stdev, na.rm=TRUE))
??xpnorm
pnorm(mean(sd_df$stdev, na.rm=TRUE), sd=sd(sd_df$stdev, na.rm=TRUE))
pnorm(1000, mean(sd_df$stdev, na.rm=TRUE), sd=sd(sd_df$stdev, na.rm=TRUE))
climate_data
climate_data |> select(where(year(year) > 2015))
climate_data |> select(where(year > 2015))
climate_data <- read_csv("data/annual.csv") |>
janitor::clean_names()
# mutate(year = ymd(year, truncated=2L))
stdevs <- c()
for(i in 1:length(climate_data$year)) {
sample <- slice(climate_data, i:(i+7))
stdevs <- append(stdevs, sd(sample$mean))
}
sd_df <- tibble(stdev=stdevs)
# climate_data
ggplot(sd_df, aes(x = stdev)) +
geom_histogram(alpha = 0.5) +
theme_minimal() +
labs(
title = "Distribution of Sample Means by Automation Risk",
x = "Mean Salary (USD)",
y = "Count",
fill = "Automation Risk"
)
climate_data |> select(where(year > 2015))
climate_data
climate_data$year >= 2015
climate_data[climate_data$year >= 2015]
climate_data[, climate_data$year >= 2015]
climate_data[climate_data$year >= 2015,]
climate_data <- read_csv("data/annual.csv") |>
janitor::clean_names() |>
filter(source == "GISTEMP")
# mutate(year = ymd(year, truncated=2L))
stdevs <- c()
for(i in 1:length(climate_data$year)) {
sample <- slice(climate_data, i:(i+7))
stdevs <- append(stdevs, sd(sample$mean))
}
sd_df <- tibble(stdev=stdevs)
# climate_data
ggplot(sd_df, aes(x = stdev)) +
geom_histogram(alpha = 0.5) +
theme_minimal() +
labs(
title = "Distribution of Sample Means by Automation Risk",
x = "Mean Salary (USD)",
y = "Count",
fill = "Automation Risk"
)
climate_data[climate_data$year >= 2015,]
climate_data[climate_data$year >= 2015 & climate_data$year <= 2022,]
sd(climate_data[climate_data$year >= 2015 & climate_data$year <= 2022,])
climate_data[climate_data$year >= 2015 & climate_data$year <= 2022,]$mean
sd(climate_data[climate_data$year >= 2015 & climate_data$year <= 2022,]$mean)
period_sd <- sd(climate_data[climate_data$year >= 2015 & climate_data$year <= 2022,]$mean)
pnorm(period_sd, mean(sd_df$stdev, na.rm=TRUE), sd=sd(sd_df$stdev, na.rm=TRUE))
period_sd <- sd(climate_data[climate_data$year >= 2015 & climate_data$year <= 2022,]$mean)
pnorm(period_sd, mean(sd_df$stdev, na.rm=TRUE), sd=sd(sd_df$stdev, na.rm=TRUE))
data <- read_csv("data/Apple_Emissions/carbon_footprint_by_product.csv")
data
table1
data
data <- |>
data <- data |>
janitor::clean_names()
data
data <- data |>
janitor::clean_names()
data
data |>
group_by(product) |>
summarise(mean_carbon = mean(carbon_footprint))
data <- read_csv("data/Apple_Emissions/greenhouse_gas_emissions.csv")
data <- data |>
janitor::clean_names()
data
names(data)
View(data)
data |>
group_by(category) |>
summarise(
mean_emissions = mean(emissions),
)
data |>
group_by(category) |>
summarise(
mean_emissions = mean(emissions, na.rm=TRUE),
)
data |>
ggplot(aes(x=fiscal_year, y=emissions, color=type)) +
geom_line()
?group_by
data |>
group_by(c(type, fiscal_year)) |>
summarise(emissions = sum(emissions)) |>
ggplot(aes(x=fiscal_year, y=emissions, color=type)) +
geom_line()
data |>
group_by(c(type, fiscal_year)) |>
summarise(emissions = sum(emissions)) |>
data |>
group_by(c(type, fiscal_year)) |>
summarise(emissions = sum(emissions))
data |>
group_by(type, fiscal_year) |>
summarise(emissions = sum(emissions)) |>
data |>
group_by(type, fiscal_year) |>
summarise(emissions = sum(emissions))
data |>
group_by(type, fiscal_year) |>
summarise(emissions = sum(emissions, na.rm=TRUE)) |>
ggplot(aes(x = fiscal_year, y = emissions, color = type)) +
geom_line()
data |>
group_by(category, fiscal_year) |>
summarise(emissions = sum(emissions, na.rm=TRUE)) |>
ggplot(aes(x = fiscal_year, y = emissions, color = category)) +
geom_line()
renv::snapshot()
renv::install('Rdatasets')
renv::install('datasets')
data("ConsumerGood")
renv::install('TeachingDemos')
library(TeachingDemos)
?TeachingDemos
ci.examp
ci.examp()
clt.examp(5)
mle.demo()
?mle.demo
put.points.demo()
SensSpec.demo()
run.cor.examp()
renv::install('tkrplot')
run.cor.examp()
run.hist.demo()
vis.binom()
run.Pvalue.norm.sim()
# Creating scatter plots in R
library(ggplot2)
# Simulated data: study time vs. grades
set.seed(123)
studyTime <- runif(20, 0, 5)
grades <- 70 + 5 * studyTime + rnorm(20, 0, 3)
df <- data.frame(studyTime, grades)
# Plot relationship
library(ggplot2)
ggplot(df, aes(x = studyTime, y = grades)) +
geom_point() +
geom_smooth(method = "lm", se = TRUE) +
labs(
x = "Study Time (hours)",
y = "Grade (percent)",
title = "Relationship between Study Time and Grades"
) +
theme_minimal()
# Simulate study time and grades data
set.seed(123)
studyTime <- runif(20, 0, 5) # 0-5 hours
grades <- 70 + 5 * studyTime + rnorm(20, 0, 3)
df <- data.frame(studyTime, grades)
# Examine the data
head(df)
##   studyTime    grades
## 1 0.2875775 71.67156
## 2 0.7883051 74.91224
## 3 0.4089769 72.59636
## 4 0.8830174 74.27857
## 5 0.9404673 75.06098
## 6 0.0455565 69.01599
# Visualize relationship
ggplot(df, aes(x = studyTime, y = grades)) +
geom_point() +
labs(
title = "Relationship between Study Time and Grades",
x = "Study Time (hours)",
y = "Grade (%)"
) +
theme_minimal()
# Testing correlation significance
result <- cor.test(
df$studyTime,
df$grades
)
print(result)
# Output shows:
# - Correlation coefficient
# - t-statistic
# - p-value
# - 95% confidence interval for the correlation
# Extract key information
result$estimate # Correlation coefficient
result$p.value # p-value
result$conf.int # Confidence interval
# Function for randomization test
shuffleCorr <- function(x, y, n_iter = 1000) {
observed <- cor(x, y)
null_dist <- replicate(
n_iter,
cor(sample(x), y)
)
p_value <- mean(abs(null_dist) >=
abs(observed))
return(list(observed = observed, p = p_value))
}
# Run the test
set.seed(123) # For reproducibility
result <- shuffleCorr(df$studyTime, df$grades)
print(paste(
"Observed correlation:",
round(result$observed, 3)
))
print(paste(
"p-value:",
round(result$p, 3)
))
# Simulate study time and grades data
set.seed(123)
studyTime <- runif(20, 0, 5) # 0-5 hours
grades <- 70 + 5 * studyTime + rnorm(20, 0, 3)
df <- data.frame(studyTime, grades)
# Examine the data
head(df)
##   studyTime    grades
## 1 0.2875775 71.67156
## 2 0.7883051 74.91224
## 3 0.4089769 72.59636
## 4 0.8830174 74.27857
## 5 0.9404673 75.06098
## 6 0.0455565 69.01599
# Visualize relationship
ggplot(df, aes(x = studyTime, y = grades)) +
geom_point() +
labs(
title = "Relationship between Study Time and Grades",
x = "Study Time (hours)",
y = "Grade (%)"
) +
theme_minimal()
# Calculate correlation in R
cor(df$studyTime, df$grades)
cor.test(df$studyTime, df$grades)
# Interpreting the output
# - cor.test() gives both the correlation coefficient
# - and performs a statistical test of Hâ‚€: r = 0
# - p-value < 0.05 suggests statistically significant correlation
# Testing correlation significance
result <- cor.test(
df$studyTime,
df$grades
)
print(result)
# Output shows:
# - Correlation coefficient
# - t-statistic
# - p-value
# - 95% confidence interval for the correlation
# Extract key information
result$estimate # Correlation coefficient
result$p.value # p-value
result$conf.int # Confidence interval
# Function for randomization test
shuffleCorr <- function(x, y, n_iter = 1000) {
observed <- cor(x, y)
null_dist <- replicate(
n_iter,
cor(sample(x), y)
)
p_value <- mean(abs(null_dist) >=
abs(observed))
return(list(observed = observed, p = p_value))
}
# Run the test
set.seed(123) # For reproducibility
result <- shuffleCorr(df$studyTime, df$grades)
print(paste(
"Observed correlation:",
round(result$observed, 3)
))
print(paste(
"p-value:",
round(result$p, 3)
))
# Comparing correlation methods
cor(x, y, method = "pearson") # Standard
# Function for randomization test
shuffleCorr <- function(x, y, n_iter = 1000) {
observed <- cor(x, y)
null_dist <- replicate(
n_iter,
cor(sample(x), y)
)
p_value <- mean(abs(null_dist) >=
abs(observed))
return(list(observed = observed, p = p_value))
}
# Run the test
set.seed(123) # For reproducibility
result <- shuffleCorr(df$studyTime, df$grades)
print(paste(
"Observed correlation:",
round(result$observed, 3)
))
print(paste(
"p-value:",
round(result$p, 3)
))
# Comparing correlation methods
x <- df$studyTime
y <- df$grades
cor(x, y, method = "pearson") # Standard
cor(x, y, method = "spearman") # Rank-based
cor(x, y, method = "kendall") # Based on concordance
# Visualizing with and without outlier
plot1 <- ggplot(
data_with_outlier,
aes(x, y)
) +
geom_point() +
ggtitle("With outlier")
# Fitting a linear regression model in R
model <- lm(grades ~ studyTime, data = df)
summary(model)
# The summary() function provides:
# - Estimated coefficients (intercept and slope)
# - Standard errors for each coefficient
# - t-values and p-values for significance tests
# - R-squared and adjusted R-squared values
# - F-statistic and its p-value
# - Residual standard error
resid(model)
plot(model)
ggplot(model.diag.metrics, aes(studyTime, grades)) +
geom_point() +
stat_smooth(method = lm, se = FALSE) +
geom_segment(aes(xend = studyTime, yend = .fitted), color = "red", size = 0.3)
model.diag.metrics <- augment(model)
library(broom)
model.diag.metrics <- augment(model)
ggplot(model.diag.metrics, aes(studyTime, grades)) +
geom_point() +
stat_smooth(method = lm, se = FALSE) +
geom_segment(aes(xend = studyTime, yend = .fitted), color = "red", size = 0.3)
# Demonstrating regression to the mean
set.seed(42)
# Generate two correlated variables
n <- 1000
x <- rnorm(n)
y <- 0.7 * x + rnorm(n, 0, sqrt(1 - 0.7^2))
cor(x, y) # Should be around 0.7
# Identify extreme values in x
extreme_x <- abs(x) > 2
# Compare means of corresponding y values
mean(y[extreme_x]) # Closer to 0 than expected
mean(x[extreme_x]) # Should be around 2.2-2.3
# Visualization
plot(x, y,
col = ifelse(extreme_x, "red", "black"),
main = "Regression to the Mean"
)
abline(h = 0, v = 0, lty = 2)
# Reuse our simulated data
head(df)
##   studyTime    grades
## 1 0.2875775 71.67156
## 2 0.7883051 74.91224
## 3 0.4089769 72.59636
## 4 0.8830174 74.27857
## 5 0.9404673 75.06098
## 6 0.0455565 69.01599
# Fit the linear model
model <- lm(grades ~ studyTime, data = df)
# View model summary
summary(model)
# Visualize the regression line
ggplot(df, aes(x = studyTime, y = grades)) +
geom_point() +
geom_smooth(method = "lm", se = TRUE) +
labs(
title = "Linear Regression: Study Time vs. Grades",
x = "Study Time (hours)",
y = "Grades (%)"
) +
theme_minimal()
# Make predictions
new_data <- data.frame(studyTime = c(1, 3, 5))
predictions <- predict(model, new_data)
predictions
# Confidence intervals for predictions
predict(model, new_data, interval = "confidence")
# Prediction intervals
predict(model, new_data, interval = "prediction")
confint(model, level = 0.95)
##                   2.5 %    97.5 %
## (Intercept) 68.9376954 71.489689
## studyTime    4.5190811  5.465801
# Calculate standard error manually
residuals <- residuals(model)
RSS <- sum(residuals^2) # Residual Sum of Squares
n <- length(df$studyTime)
x_dev_squared <- sum((df$studyTime - mean(df$studyTime))^2)
# SE formula
se_beta1 <- sqrt(RSS / (n - 2) / x_dev_squared)
print(paste(
"SE of slope (manual):",
round(se_beta1, 4)
))
# Compare with lm() output
summary_model <- summary(model)
se_from_lm <- summary_model$coefficients[2, 2]
print(paste(
"SE of slope (lm):",
round(se_from_lm, 4)
))
# Calculate confidence intervals for coefficients
conf_intervals <- confint(model, level = 0.95)
print(conf_intervals)
# For our studyTime coefficient:
# 2.5%: 4.52
# 97.5%: 5.47
# Visualizing the uncertainty in the slope
ggplot(df, aes(x = studyTime, y = grades)) +
geom_point() +
geom_smooth(method = "lm", se = TRUE) +
geom_abline(
slope = conf_intervals[2, 1], # Lower bound
intercept = confint(model)[1, 1],
linetype = "dashed", color = "blue"
) +
geom_abline(
slope = conf_intervals[2, 2], # Upper bound
intercept = confint(model)[1, 2],
linetype = "dashed", color = "blue"
) +
labs(title = "Uncertainty in Regression Line") +
theme_minimal()
# Generate all four diagnostic plots
par(mfrow = c(2, 2))
plot(model)
# Alternative: individual plots
plot(model, which = 1) # Residuals vs Fitted
plot(model, which = 2) # Normal Q-Q
plot(model, which = 3) # Scale-Location
plot(model, which = 4) # Residuals vs Leverage
# Check specific diagnostics
# Normality test for residuals
shapiro.test(residuals(model))
# Breusch-Pagan test for homoscedasticity
library(lmtest)
bptest(model)
# Durbin-Watson test for autocorrelation
dwtest(model)
# Testing for linearity
# Add a quadratic term and test significance
quad_model <- lm(grades ~ studyTime + I(studyTime^2),
data = df
)
summary(quad_model)
# Testing for homoscedasticity
library(lmtest)
bptest(model)
# Testing for normality of residuals
shapiro.test(residuals(model))
# Identifying influential observations
plot(model, which = 4)
influence.measures(model)
# Plot residuals against predictors
plot(df$studyTime, residuals(model),
xlab = "Study Time", ylab = "Residuals",
main = "Residuals vs. Study Time"
)
abline(h = 0, lty = 2)
# Standardize variables
df$z_studyTime <- scale(df$studyTime)
df$z_grades <- scale(df$grades)
# Fit model with standardized variables
std_model <- lm(z_grades ~ z_studyTime, data = df)
summary(std_model)
# Get mean values of x and y
mean_x <- mean(df$studyTime)
mean_y <- mean(df$grades)
# Get coefficient estimates
coef_est <- coef(model)
# Plot with improved confidence bounds for slope
ggplot(df, aes(x = studyTime, y = grades)) +
geom_point() +
geom_smooth(method = "lm", se = TRUE) +
geom_abline(
slope = conf_intervals[2, 1], # Lower bound of slope
intercept = mean_y - conf_intervals[2, 1] * mean_x, # Adjusted intercept
linetype = "dashed", color = "blue"
) +
geom_abline(
slope = conf_intervals[2, 2], # Upper bound of slope
intercept = mean_y - conf_intervals[2, 2] * mean_x, # Adjusted intercept
linetype = "dashed", color = "blue"
) +
labs(title = "Uncertainty in Regression Line") +
theme_minimal()
