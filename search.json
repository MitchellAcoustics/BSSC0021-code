[
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Lecture Slides",
    "section": "",
    "text": "Lecture Slides\n\n\n\n\n\n\n\n\nData Visualization\n\n\nThe Grammar of Graphics with ggplot2\n\n\n\n\n\n\n\n\n\n\n\nProbability, Sampling, and Experiments\n\n\n\n\n\n\n\n\n\n\n\nStatistical Inference and Hypothesis Testing\n\n\n\n\n\n\n\n\n\n\n\nWeek 5\n\n\nApplying a critical eye to statistical reporting\n\n\n\n\n\n\n\n\n\n\n\nNew Week Topic\n\n\n\n\n\n\n\n\n\n\n\nMultiple Linear Regression\nand ANOVA as Linear Models\n\n\nUnifying Statistical Approaches\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes",
    "section": "",
    "text": "Navigate the notes using the menu in the left sidebar or the search in the upper right corner of the page.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Salaries in the AI Job Market\n\n\n\n\n\n\n\n\n\n\n\nAndrew Mitchell\n\n\n\n\n\n\n\n\n\n\n\n\nBayes’ Rule and Learning from Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommunicating Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConditional Probability and Independence\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelation and Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Visualization\n\n\nThe Grammar of Graphics with ggplot2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDetermining Probabilities\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFormative Assessment: Deceptive Visualization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHypothesis Testing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMisleading Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiple Linear Regression and ANOVA as Linear Models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart 1: Introduction to Probability\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart 2: Statistical Sampling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProbability Distributions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProbability Rules and Classical Probability\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSampling Distribution of the Mean\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSampling Exercise Review\n\n\n\n\n\n\n\n\n\n\n\nAndrew Mitchell\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Inference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Inference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Inference and Hypothesis Testing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Data Analysis Workflow\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe General Linear Model: Multiple Variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 3\n\n\nProbability, Sampling, and Experiments\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with Quarto\n\n\n\n\n\n\n\n\n\n\n\nAndrew Mitchell\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes"
    ]
  },
  {
    "objectID": "WeekNew/notes.html",
    "href": "WeekNew/notes.html",
    "title": "This week’s topic",
    "section": "",
    "text": "Slides\n\n Download PDF Slides"
  },
  {
    "objectID": "WeekNew/notes.html#this-weeks-lecture",
    "href": "WeekNew/notes.html#this-weeks-lecture",
    "title": "This week’s topic",
    "section": "",
    "text": "Slides\n\n Download PDF Slides"
  },
  {
    "objectID": "WeekNew/2-content.html",
    "href": "WeekNew/2-content.html",
    "title": "Section 2",
    "section": "",
    "text": "Section 2"
  },
  {
    "objectID": "Week7/notes.html",
    "href": "Week7/notes.html",
    "title": "Multiple Linear Regression and ANOVA as Linear Models",
    "section": "",
    "text": "Slides\n\n Download PDF Slides",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 7",
      "Multiple Linear Regression and ANOVA as Linear Models"
    ]
  },
  {
    "objectID": "Week7/notes.html#this-weeks-lecture",
    "href": "Week7/notes.html#this-weeks-lecture",
    "title": "Multiple Linear Regression and ANOVA as Linear Models",
    "section": "",
    "text": "Slides\n\n Download PDF Slides",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 7",
      "Multiple Linear Regression and ANOVA as Linear Models"
    ]
  },
  {
    "objectID": "Week7/notes.html#overview",
    "href": "Week7/notes.html#overview",
    "title": "Multiple Linear Regression and ANOVA as Linear Models",
    "section": "Overview",
    "text": "Overview\nThis week we explore the concept of the general linear model as a unifying framework for statistical analysis. We’ll see how many common statistical tests (t-tests, ANOVA, regression) are actually special cases of the same underlying linear model.\nUnderstanding this unified perspective helps simplify statistical thinking and reveals the connections between techniques that are often taught separately. We’ll apply these concepts to real-world datasets including HR analytics and fuel consumption data.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 7",
      "Multiple Linear Regression and ANOVA as Linear Models"
    ]
  },
  {
    "objectID": "Week7/notes.html#key-topics",
    "href": "Week7/notes.html#key-topics",
    "title": "Multiple Linear Regression and ANOVA as Linear Models",
    "section": "Key Topics",
    "text": "Key Topics\n\nThe general linear model framework\nHow t-tests, ANOVA, and regression are related\nMultiple regression with continuous and categorical predictors\nInteraction effects in multi-factor designs\nPractical applications and interpretation",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 7",
      "Multiple Linear Regression and ANOVA as Linear Models"
    ]
  },
  {
    "objectID": "Week7/notes.html#required-readings",
    "href": "Week7/notes.html#required-readings",
    "title": "Multiple Linear Regression and ANOVA as Linear Models",
    "section": "Required Readings",
    "text": "Required Readings\n\nPoldrack, Statistical Thinking, Chapter 10-11\nCommon statistical tests are linear models. Jonas Kristoffer Lindeløv (2019).\nBekes & Kezdi, Data Analysis for Business, Economics, and Policy, Chapter 8-9",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 7",
      "Multiple Linear Regression and ANOVA as Linear Models"
    ]
  },
  {
    "objectID": "Week7/3-content.html",
    "href": "Week7/3-content.html",
    "title": "Common Statistical Tests as Linear Models",
    "section": "",
    "text": "Adapted from:\n\n\nCommon statistical tests are linear models. Jonas Kristoffer Lindeløv (2019).\n\n\nIn this section, we’ll explore an elegant insight: most common statistical tests can be expressed as special cases of the general linear model. This unified framework simplifies our understanding of statistics and reveals deep connections between seemingly different tests.\n\nThis section draws extensively from Jonas Lindeløv’s excellent resource which demonstrates how common statistical tests can be expressed as linear models. This approach provides a powerful unifying framework that can transform how we teach and learn statistics.\nThe key insight is that tests like t-tests, ANOVA, correlation, and others aren’t separate, unrelated techniques, but rather special cases of the same underlying model. By understanding this connection, students can develop a more coherent and transferable understanding of statistics.\n\n\nMost statistical tests are special cases of linear models or very close approximations:\ny_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + ... + \\varepsilon_i\nThis unified view simplifies learning and shows connections between seemingly different methods.\n\nTeaching linear models first, then presenting traditional tests as special cases:\n\nEmphasizes understanding over memorization\nMakes statistical concepts more intuitive\nShows the common structure of different statistical procedures\n\n\n\nLinear models provide a unifying framework for understanding statistics. Most common statistical procedures (t-tests, ANOVA, correlation, etc.) are special cases of the general linear model.\nThis approach simplifies what students need to learn. Instead of treating each test as an independent entity with its own formulas and assumptions, we can present them as variations on the same underlying model.\nBy teaching the general linear model first and then showing how traditional tests are special cases, we help students build a more coherent mental model of statistics. This approach emphasizes conceptual understanding over rote memorization of formulas and procedures.\n\n\n\n\nA family tree of statistical tests as linear models\n\nThis cheat sheet shows how different statistical tests relate to each other through the linear model framework:\n\nSimple tests at the bottom (t-tests, correlation)\nMore complex models at the top (ANOVA, multiple regression)\nEach branch represents a variation or special case of the linear model\n\n\nThis cheat sheet from Lindeløv’s website shows how various statistical tests are related through the linear model framework. It provides a visual roadmap of the connections between different statistical procedures.\nNotice how we can trace the path from simple tests like t-tests up to more complex procedures like factorial ANOVA. Each branch represents a variation or extension of the basic linear model.\nThis visualization helps students see that what they’re learning aren’t disconnected techniques but rather members of the same family, with shared properties and interpretations. This perspective can make learning statistics more coherent and less overwhelming.\n\n\n\n\n\n\n\n\n\n\n\nTest\nLinear Model Formula\nWhat’s being tested\n\n\n\nCorrelation\ny ~ x\nSlope coefficient\n\n\nOne-sample t-test\ny ~ 1\nIntercept\n\n\nIndependent t-test\ny ~ group\nGroup coefficient\n\n\nPaired t-test\ndiff ~ 1\nIntercept of differences\n\n\nOne-way ANOVA\ny ~ group\nGroup coefficients\n\n\nTwo-way ANOVA\ny ~ factorA * factorB\nMain effects & interaction\n\n\nMultiple regression\ny ~ x1 + x2 + …\nMultiple coefficients\n\n\n\n\n\nThe table shows:\n\nEach common test has a corresponding linear model formulation\nMany tests are testing coefficients in the same type of model\nThe differences often come down to which coefficients we’re interested in\n\n\nThis table summarizes how different common tests map to linear model formulations. For each test, we can identify what linear model would be equivalent and which coefficient(s) we’re testing.\nNotice that the difference between tests often comes down to:\n\nWhat variables we include in the model\nWhich coefficient(s) we’re interested in testing\nHow we interpret the results\n\nThis unified framework helps students see that they’re not learning completely different procedures for each test, but rather applying the same underlying model in different contexts.\n\n\nModel: y = \\beta_0 + \\beta_1 x \\quad where \\mathcal{H}_0: \\beta_1 = 0\nThis is simply a linear regression with one predictor. When we test whether the correlation is significant, we’re testing whether the slope (\\beta_1) differs from zero.\n\n\n\nCode# Create example data\nset.seed(42)\nx &lt;- rnorm(50)\ny &lt;- 0.6 * x + rnorm(50, 0, 0.8)\ndata &lt;- data.frame(x = x, y = y)\n\n# Traditional correlation\ncor_result &lt;- cor.test(data$x, data$y)\ncor_result$estimate\n\n      cor \n0.5818111 \n\nCodecor_result$p.value\n\n[1] 9.360699e-06\n\n\n\n\nCode# As linear model (standardized variables)\nlm_cor &lt;- lm(scale(y) ~ scale(x), data = data)\ncoef(lm_cor)[2] # slope = correlation coefficient\n\n scale(x) \n0.5818111 \n\nCodesummary(lm_cor)$coefficients[2, \"Pr(&gt;|t|)\"] # p-value\n\n[1] 9.360699e-06\n\n\nWhen we standardize both variables (giving them mean=0 and sd=1), the slope coefficient equals the correlation coefficient!\n\n\n\nHere we demonstrate that Pearson’s correlation is equivalent to the standardized regression coefficient in a simple linear regression model.\nThe mathematical model is exactly the same as simple linear regression: y = β₀ + β₁x + ε. The null hypothesis being tested is that β₁ = 0, which means there is no linear relationship between the variables.\nWhen we standardize both x and y (to have mean=0 and sd=1), the slope coefficient in a linear regression equals the correlation coefficient r. This makes intuitive sense because standardization puts both variables on the same scale, making their relationship directly comparable.\nThe t-test on this coefficient tests exactly the same hypothesis as the correlation test: is there a linear relationship between the variables? The p-values are identical between the two approaches.\nThis equivalence helps us understand correlation not as a mysterious measure, but simply as the slope of a regression line when variables are standardized.\n\n\nSpearman correlation is Pearson correlation on rank-transformed variables:\n\nCode# Pearson correlation on original data\ncor(x, y, method = \"pearson\")\n\n[1] 0.5818111\n\nCode# Spearman correlation = Pearson on ranks\ncor(x, y, method = \"spearman\")\n\n[1] 0.6436975\n\nCode# Same as Pearson correlation on ranked variables\ncor(rank(x), rank(y), method = \"pearson\")\n\n[1] 0.6436975\n\nCode# As linear model with ranks\nlm_spearman &lt;- lm(rank(y) ~ rank(x))\nsummary(lm_spearman)$coefficients[2, \"Estimate\"]\n\n[1] 0.6436975\n\n\nThe “non-parametric” Spearman correlation is simply the “parametric” Pearson correlation applied to ranked data!\n\nSpearman’s rank correlation is a brilliant example of how a “non-parametric” test is simply a parametric test applied to transformed data.\nInstead of correlating the original values, Spearman correlation first converts all values to their ranks (1st, 2nd, 3rd, etc.) and then applies the Pearson correlation formula to these ranks.\nThis transformation accomplishes two things:\n\nIt makes the test robust to outliers, since extreme values just become the highest or lowest rank\nIt allows the test to detect monotonic but non-linear relationships, since ranking linearizes any monotonic relationship\n\nBy understanding Spearman correlation as “Pearson on ranks,” we demystify non-parametric statistics. Many so-called non-parametric tests are simply parametric tests applied to transformed data, making them more accessible conceptually.\nThe R code demonstrates that Spearman correlation can be obtained either by using the dedicated function or by manually ranking the variables and then applying Pearson correlation.\n\n\n\nCodep1 &lt;- ggplot(data, aes(x = x, y = y)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n    labs(title = \"Pearson: Original Values\") +\n    theme_minimal()\n\nrank_data &lt;- data.frame(x_rank = rank(x), y_rank = rank(y))\np2 &lt;- ggplot(rank_data, aes(x = x_rank, y = y_rank)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n    labs(title = \"Spearman: Ranked Values\") +\n    theme_minimal()\n\np1 + p2\n\n\n\n\n\n\n\nLeft: Pearson correlation fits a line to the original data points\nRight: Spearman correlation fits a line to the ranked data points\n\nThis visualization helps us understand the relationship between Pearson and Spearman correlation.\nThe left panel shows the original data with the regression line (Pearson’s r). The right panel shows the same data after converting to ranks, with its regression line (Spearman’s rho).\nNotice how the ranked data (right panel) tends to form a more linear pattern. This is because ranking removes the influence of outliers and transforms any monotonic relationship into a linear one.\nAnother key insight: the slope of the line through the ranked data is the Spearman correlation coefficient, just as the slope of the line through the standardized original data is the Pearson correlation coefficient.\nThis visualization reinforces the idea that many statistical tests are simply variations on the same theme, applied to differently transformed data.\n\n\nModel: y = \\beta_0 \\quad where \\mathcal{H}_0: \\beta_0 = 0\nThis is the simplest linear model possible! It has only an intercept (no predictors), and the intercept equals the sample mean.\n\n\n\nCode# Create sample data\nset.seed(123)\ny_one &lt;- rnorm(30, mean = 5.2, sd = 2)\n\n# Traditional t-test\nt_test_one &lt;- t.test(y_one, mu = 5)\nt_test_one$statistic\n\n        t \n0.2953268 \n\nCodet_test_one$p.value\n\n[1] 0.7698484\n\nCodet_test_one$conf.int\n\n[1] 4.373147 5.838438\nattr(,\"conf.level\")\n[1] 0.95\n\n\n\n\nCode# Same test as linear model\nlm_one &lt;- lm(y_one ~ 1)\nsummary(lm_one)$coefficients\n\n            Estimate Std. Error  t value     Pr(&gt;|t|)\n(Intercept) 5.105792  0.3582218 14.25316 1.244655e-14\n\nCodeconfint(lm_one)\n\n               2.5 %   97.5 %\n(Intercept) 4.373147 5.838438\n\n\nThe intercept-only model gives identical results to the one-sample t-test! The coefficient is the mean, and the t-statistic tests if it differs from zero.\n\n\n\nThe one-sample t-test is perhaps the simplest demonstration of how standard statistical tests are special cases of linear models.\nIn a one-sample t-test, we’re asking whether a sample mean differs significantly from a hypothesized population value (often zero). In the linear model framework, this becomes an intercept-only model: y = β₀ + ε.\nThe intercept (β₀) represents the sample mean. The t-statistic tests whether this mean differs significantly from the hypothesized value (in this case, μ=5).\nThe R code demonstrates this equivalence beautifully. The estimate from t.test() is identical to the intercept from lm(), and the t-statistic, p-value, and confidence intervals match exactly.\nThis shows how even the most basic statistical test can be understood within the general linear model framework. The intercept-only model is simply a special case where we have no predictors, just as the one-sample t-test is examining a single mean without comparison groups.\n\n\nThe Wilcoxon signed-rank test is approximately a one-sample t-test on signed ranks:\n\nCode# Traditional Wilcoxon test\nw_test &lt;- wilcox.test(y_one - 5) # Test against μ=5\nw_test$p.value\n\n[1] 0.8552717\n\nCode# Define the signed rank function\nsigned_rank &lt;- function(x) sign(x) * rank(abs(x))\n\n# As linear model on signed ranks\nlm_wilcox &lt;- lm(signed_rank(y_one - 5) ~ 1)\nsummary(lm_wilcox)$coefficients[1, \"Pr(&gt;|t|)\"]\n\n[1] 0.8488961\n\n\nThe “non-parametric” Wilcoxon test can be viewed as a one-sample t-test on rank-transformed data!\n\nThe Wilcoxon signed-rank test is often presented as a completely different “non-parametric” alternative to the t-test, but here we see it’s closely related to the t-test when viewed through the linear model lens.\nThe key insight is that the Wilcoxon test can be approximated as a one-sample t-test applied to signed ranks rather than the original values. Here’s how it works:\n\nFirst, we calculate the differences from the hypothesized median (μ=5)\nThen we rank the absolute differences (ignoring signs)\nFinally, we reattach the original signs to these ranks (creating “signed ranks”)\nWe run a one-sample t-test on these signed ranks\n\nThis transformation makes the test more robust to outliers and non-normal distributions, as extreme values are “tamed” by the ranking process.\nThe approximation works best with sample sizes of 15 or more. With smaller samples, the discrete nature of ranks means the p-values won’t match exactly, but the approach still provides conceptual insight.\nThis demonstrates again how “non-parametric” tests can often be understood as parametric tests applied to transformed data, demystifying what might otherwise seem like a completely different approach to inference.\n\n\nModel: y_i = \\beta_0 + \\beta_1 x_i \\quad where \\mathcal{H}_0: \\beta_1 = 0\nHere, x_i is a dummy variable (0/1) for group membership. \\beta_0 represents the mean of the first group, while \\beta_1 represents the difference between groups.\n\n\n\nCode# Create data for two groups\nset.seed(456)\ngroup &lt;- rep(c(\"A\", \"B\"), each = 15)\ny_ind &lt;- c(\n  rnorm(15, mean = 10, sd = 2),\n  rnorm(15, mean = 12, sd = 2)\n)\nind_data &lt;- data.frame(y = y_ind, group = factor(group))\n\n# Traditional t-test\nt_test_ind &lt;- t.test(y ~ group, data = ind_data, var.equal = TRUE)\nt_test_ind$statistic\n\n       t \n-2.87084 \n\nCodet_test_ind$p.value\n\n[1] 0.007712189\n\n\n\n\nCode# Same test as linear model\nlm_ind &lt;- lm(y ~ group, data = ind_data)\nsummary(lm_ind)$coefficients\n\n             Estimate Std. Error  t value     Pr(&gt;|t|)\n(Intercept) 10.236098  0.6046277 16.92959 3.052810e-16\ngroupB       2.454777  0.8550727  2.87084 7.712189e-03\n\n\nThe coefficient for groupB is the difference between groups, exactly what’s tested in the t-test. The t-statistic and p-value are identical!\n\n\n\nThe independent samples t-test compares means between two groups. In the linear model framework, this is represented as a model with one dummy-coded categorical predictor.\nThe model is y = β₀ + β₁x + ε, where x is coded as 0 for the first group and 1 for the second group. This dummy coding has a straightforward interpretation:\n\nβ₀ (the intercept) represents the mean of the reference group (group A)\nβ₁ represents the difference in means between groups B and A\nThe t-statistic tests whether this difference is significantly different from zero\n\nThe R code demonstrates that the t-statistic and p-value from the traditional t-test are identical to those for the group coefficient in the linear model.\nThis demonstrates how categorical variables can be incorporated into linear models through dummy coding, and how tests that might seem conceptually different (like t-tests and regression) are actually part of the same unified framework.\n\n\n\nCode# Plot with jittered points and means\nggplot(ind_data, aes(x = group, y = y, color = group)) +\n  geom_jitter(width = 0.1, alpha = 0.6) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"errorbar\", \n               aes(ymax = ..y.., ymin = ..y..), width = 0.2) +\n  geom_hline(yintercept = coef(lm_ind)[1], linetype = \"dashed\", color = \"blue\") +\n  geom_segment(x = 1, xend = 2, \n               y = coef(lm_ind)[1], yend = coef(lm_ind)[1] + coef(lm_ind)[2],\n               color = \"red\", arrow = arrow(length = unit(0.3, \"cm\"))) +\n  annotate(\"text\", x = 1.2, y = coef(lm_ind)[1] - 0.5, \n           label = expression(beta[0]~\"(intercept)\"), color = \"blue\") +\n  annotate(\"text\", x = 1.5, y = coef(lm_ind)[1] + coef(lm_ind)[2]/2 + 0.5, \n           label = expression(beta[1]~\"(difference)\"), color = \"red\") +\n  theme_minimal() +\n  labs(title = \"Independent t-test as Linear Model\",\n       subtitle = \"Blue line = reference group mean (β₀), Red arrow = difference (β₁)\")\n\n\n\n\n\n\n\nThis visualization shows:\n\nThe blue dashed line is the mean of group A (the intercept, \\beta_0)\n\nThe red arrow shows the difference between groups (the slope, \\beta_1)\nIn a t-test, we’re testing whether this difference (\\beta_1) is significantly different from zero\n\n\nThis visualization helps us understand how dummy coding works in a linear model with a categorical predictor.\nWhen we use dummy coding in a linear model:\n\nOne group (here, group A) becomes the reference category and is coded as 0\nThe other group (group B) is coded as 1\nThe intercept (β₀) represents the mean of the reference group\nThe coefficient for the dummy variable (β₁) represents the difference between groups\n\nIn the plot, the blue dashed line shows the mean of group A (the intercept, β₀). The red arrow represents the difference between groups, which is the coefficient β₁.\nThis visualization makes it clear that an independent samples t-test is testing whether this difference (β₁) is significantly different from zero. If there’s no difference between groups, the red arrow would be flat (no vertical component).\nUnderstanding dummy coding is crucial for interpreting linear models with categorical predictors. This same principle extends to models with multiple categorical predictors (like ANOVA) and to more complex designs.\n\n\nThe Mann-Whitney U test is approximately a t-test on ranks:\n\nCode# Traditional Mann-Whitney U test\nmw_test &lt;- wilcox.test(y ~ group, data = ind_data)\nmw_test$p.value\n\n[1] 0.02635404\n\nCode# As linear model on ranks\nlm_mw &lt;- lm(rank(y) ~ group, data = ind_data)\nsummary(lm_mw)$coefficients[2, \"Pr(&gt;|t|)\"]\n\n[1] 0.0236538\n\nCode# Extract estimates\ndata.frame(\n  Test = c(\"Mann-Whitney U\", \"Linear model on ranks\"),\n  P_value = c(mw_test$p.value, summary(lm_mw)$coefficients[2, \"Pr(&gt;|t|)\"]),\n  Difference = c(NA, coef(lm_mw)[2])\n) |&gt; kable(digits = 4)\n\n\n\n\nTest\nP_value\nDifference\n\n\n\n\nMann-Whitney U\n0.0264\nNA\n\n\ngroupB\nLinear model on ranks\n0.0237\n7.1333\n\n\n\n\n\nJust like with Spearman correlation, the “non-parametric” Mann-Whitney test can be viewed as a regular t-test applied to ranked data.\n\nThe Mann-Whitney U test (also known as the Wilcoxon rank-sum test) is commonly presented as a “non-parametric” alternative to the independent samples t-test when data violates normality assumptions.\nHowever, as we can see, it can be closely approximated as a standard t-test (or linear model) applied to ranked data:\n\nFirst, we rank all values across both groups from lowest to highest\nThen we run a standard t-test (or linear model) comparing these ranks between groups\nThe coefficient for the group effect represents the difference in mean ranks\n\nThe p-value from this ranked linear model closely approximates the p-value from the Mann-Whitney U test. This approximation improves with larger sample sizes (n &gt; 20 per group).\nThis approach gives us an additional benefit: while the traditional Mann-Whitney U test only provides a p-value, the linear model on ranks also gives us the actual difference in mean ranks between groups, providing a measure of effect size.\nThis further reinforces the pattern we’ve seen: many “non-parametric” tests can be understood as parametric tests applied to rank-transformed data, providing a unified conceptual framework.\n\n\nModel: y_{2i} - y_{1i} = \\beta_0 \\quad where \\mathcal{H}_0: \\beta_0 = 0\nA paired t-test simplifies to a one-sample t-test on the differences between pairs!\n\n\n\nCode# Create paired data\nset.seed(789)\npre &lt;- rnorm(20, mean = 100, sd = 15)\npost &lt;- pre + rnorm(20, mean = 8, sd = 10) # Correlated with pre\npaired_data &lt;- data.frame(\n  subject = 1:20,\n  pre = pre,\n  post = post\n)\n\n# Traditional paired t-test\nt_test_paired &lt;- t.test(paired_data$post, paired_data$pre, paired = TRUE)\nt_test_paired$statistic\n\n       t \n3.189437 \n\nCodet_test_paired$p.value\n\n[1] 0.004827114\n\n\n\n\nCode# Calculate differences and fit model\npaired_data$diff &lt;- paired_data$post - paired_data$pre\nlm_paired &lt;- lm(diff ~ 1, data = paired_data)\nsummary(lm_paired)$coefficients\n\n            Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 5.089399   1.595704 3.189437 0.004827114\n\n\nThe paired t-test becomes an intercept-only model (one-sample t-test) on the differences!\n\n\n\nThe paired samples t-test is another example of how complex-seeming statistical tests reduce to simpler linear models when we understand the underlying structure.\nIn a paired design, we have two measurements for each subject (e.g., before and after treatment). The paired t-test accounts for the correlation between these measurements by analyzing the differences rather than the raw values.\nThe key insight is that a paired t-test is mathematically equivalent to a one-sample t-test on the differences:\n\nCalculate the difference for each pair: diff = post - pre\nTest whether the mean difference is significantly different from zero using a one-sample t-test\n\nIn the linear model framework, this becomes an intercept-only model on the differences: diff = β₀ + ε, where the null hypothesis is β₀ = 0.\nThe R code demonstrates this equivalence. The t-statistic and p-value from the paired t-test match exactly those from the intercept-only model on the differences.\nThis approach clarifies that the paired t-test is not a fundamentally different test but rather a clever application of the one-sample t-test to difference scores. This insight helps students see the connections between seemingly different statistical procedures.\n\n\n\nCode# Convert to long format for plotting\npaired_long &lt;- pivot_longer(paired_data, cols = c(\"pre\", \"post\"), \n                           names_to = \"condition\", values_to = \"value\")\n\n# Plot\nggplot(paired_long, aes(x = condition, y = value, group = subject)) +\n  geom_line(alpha = 0.3) +\n  geom_point(alpha = 0.3) +\n  stat_summary(fun = mean, geom = \"point\", size = 3, color = \"red\") +\n  stat_summary(fun = mean, geom = \"line\", size = 1, color = \"red\", \n               aes(group = 1)) +\n  theme_minimal() +\n  labs(title = \"Paired Samples Design\",\n       subtitle = \"Gray lines = individual subjects, Red points = means\")\n\n\n\n\n\n\n\nThis visualization shows:\n\nEach gray line represents one subject’s pre and post measurements\nThe red points show the group means at each time point\nThe paired t-test analyzes the mean of these differences (slopes of gray lines)\nThis accounts for individual baseline differences and increases statistical power\n\n\nThis visualization helps us understand the structure of paired data and why we analyze differences rather than raw values.\nEach gray line represents a single subject, connecting their pre and post measurements. Notice how subjects start at different baseline levels but generally show similar trends (most lines slope upward, indicating an increase from pre to post).\nThe red points and line show the group means at each time point. The paired t-test effectively tests whether the average slope of the gray lines is significantly different from zero.\nThe key advantage of a paired design is that it accounts for individual differences. In an independent samples design, the pre-test variance would include both within-subject and between-subject variability. By analyzing within-subject changes, we remove the between-subject variability, resulting in greater statistical power.\nThis is why paired designs are often preferred when the same subjects can be measured under different conditions - they control for individual differences that would otherwise contribute to error variance.\nUnderstanding paired designs as analyzing differences helps students see why the paired t-test reduces to a one-sample t-test on those differences, as we saw in the previous slide.\n\n\nModel: y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\beta_3 x_{3i} + ... \\quad where \\mathcal{H}_0: \\beta_1 = \\beta_2 = ... = 0\nThe one-way ANOVA is a natural extension of the independent t-test to three or more groups.\n\n\n\nCode# Create example data with 3 groups\nset.seed(101)\nn_per_group &lt;- 15\ngroup &lt;- factor(rep(c(\"A\", \"B\", \"C\"), each = n_per_group))\nmeans &lt;- c(10, 12, 8)\ny_anova &lt;- c(\n  rnorm(n_per_group, mean = means[1], sd = 2),\n  rnorm(n_per_group, mean = means[2], sd = 2),\n  rnorm(n_per_group, mean = means[3], sd = 2)\n)\nanova_data &lt;- data.frame(y = y_anova, group = group)\n\n# Traditional ANOVA\nanova_result &lt;- aov(y ~ group, data = anova_data)\nsummary(anova_result)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ngroup        2  108.0   53.97   14.83 1.34e-05 ***\nResiduals   42  152.9    3.64                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode# As linear model\nlm_anova &lt;- lm(y ~ group, data = anova_data)\nanova(lm_anova)\n\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ngroup      2 107.95  53.973  14.827 1.343e-05 ***\nResiduals 42 152.88   3.640                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe linear model produces exactly the same F-statistic and p-value as the traditional ANOVA. The linear model is using dummy coding for groups B and C, with group A as the reference.\n\n\n\nOne-way ANOVA is traditionally taught as a distinct test from regression or t-tests, but here we see it’s simply an extension of the same linear model framework.\nIn an independent t-test, we had one dummy variable for two groups. In one-way ANOVA with k groups, we have k-1 dummy variables:\n\nGroup A becomes the reference group (coded as 0 for all dummy variables)\nGroup B is coded as 1 for the first dummy variable, 0 for others\nGroup C is coded as 1 for the second dummy variable, 0 for others\nAnd so on for additional groups\n\nThe model is: y = β₀ + β₁x₁ + β₂x₂ + … + ε, where:\n\nβ₀ is the mean of the reference group (Group A)\nβ₁ is the difference between Group B and Group A\nβ₂ is the difference between Group C and Group A\n\nThe F-test in the ANOVA table tests the null hypothesis that all group differences are simultaneously equal to zero (β₁ = β₂ = … = 0).\nThe R code demonstrates that traditional ANOVA (using aov()) and the linear model approach (using lm() followed by anova()) produce identical F-statistics and p-values.\nThis reveals that ANOVA is not a fundamentally different procedure but simply a way of testing multiple coefficients simultaneously in a linear model.\n\n\nThe Kruskal-Wallis test is approximately a one-way ANOVA on ranks:\n\nCode# Traditional Kruskal-Wallis test\nkw_test &lt;- kruskal.test(y ~ group, data = anova_data)\nkw_test$p.value\n\n[1] 0.0001543885\n\nCode# As linear model on ranks\nlm_kw &lt;- lm(rank(y) ~ group, data = anova_data)\nanova(lm_kw)$\"Pr(&gt;F)\"[1]\n\n[1] 2.278855e-05\n\nCode# Extract estimates and p-values\ndata.frame(\n  Test = c(\"Kruskal-Wallis\", \"ANOVA on ranks\"),\n  P_value = c(kw_test$p.value, anova(lm_kw)$\"Pr(&gt;F)\"[1])\n) |&gt; kable(digits = 5)\n\n\n\nTest\nP_value\n\n\n\nKruskal-Wallis\n0.00015\n\n\nANOVA on ranks\n0.00002\n\n\n\n\n\nFollowing our pattern, the “non-parametric” Kruskal-Wallis test can be viewed as a regular ANOVA performed on ranked data rather than raw values.\n\nThe Kruskal-Wallis test is traditionally presented as a non-parametric alternative to one-way ANOVA when data violate normality assumptions or are ordinal in nature.\nHowever, just as we saw with other “non-parametric” tests, the Kruskal-Wallis test can be closely approximated as a standard parametric test (one-way ANOVA) applied to rank-transformed data:\n\nFirst, we rank all observations from lowest to highest, regardless of group\nThen we run a standard one-way ANOVA on these ranks\nThe F-test from this ANOVA approximates the Kruskal-Wallis test\n\nThe p-values from the two approaches are very similar, especially with larger sample sizes. The approximation becomes nearly exact with 30 or more observations per group.\nThis pattern reinforces our unified framework: rather than learning Kruskal-Wallis as a completely different test with its own formula, students can understand it as a simple transformation (ranking) followed by the standard ANOVA procedure they already know.\nThis approach not only simplifies learning but also clarifies what these “non-parametric” tests are actually doing - they’re not assumption-free, but rather make different assumptions that are often more appropriate for certain types of data.\n\n\nModel: y_i = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2 \\quad where \\mathcal{H}_0: \\beta_3 = 0 (for the interaction)\nTwo-way ANOVA extends the model to include two categorical factors and their interaction, using the same dummy coding approach as one-way ANOVA.\n\nCode# Create two-way ANOVA data\nset.seed(202)\nfactorA &lt;- rep(c(\"A1\", \"A2\"), each = 24)\nfactorB &lt;- rep(rep(c(\"B1\", \"B2\", \"B3\"), each = 8), 2)\ny_two_way &lt;- c(\n  rnorm(8, 20, 2), rnorm(8, 24, 2), rnorm(8, 22, 2),  # A1B1, A1B2, A1B3\n  rnorm(8, 18, 2), rnorm(8, 25, 2), rnorm(8, 28, 2)   # A2B1, A2B2, A2B3\n)\ntwo_way_data &lt;- data.frame(\n  y = y_two_way,\n  factorA = factor(factorA),\n  factorB = factor(factorB)\n)\n\n# Compare traditional ANOVA and linear model\nanova_two_way &lt;- aov(y ~ factorA * factorB, data = two_way_data)\nlm_two_way &lt;- lm(y ~ factorA * factorB, data = two_way_data)\n\n# Show identical results\nsummary(anova_two_way)\n\n                Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nfactorA          1   50.0   50.02   17.54 0.000141 ***\nfactorB          2  478.5  239.27   83.87 2.15e-15 ***\nfactorA:factorB  2   84.4   42.18   14.79 1.37e-05 ***\nResiduals       42  119.8    2.85                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCodeanova(lm_two_way)\n\nAnalysis of Variance Table\n\nResponse: y\n                Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nfactorA          1  50.02  50.022  17.535 0.0001413 ***\nfactorB          2 478.54 239.270  83.874 2.151e-15 ***\nfactorA:factorB  2  84.37  42.185  14.788 1.375e-05 ***\nResiduals       42 119.81   2.853                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe linear model produces the exact same results as the ANOVA approach. The * operator generates both main effects and their interaction terms.\n\nTwo-way ANOVA extends the one-way ANOVA by including two categorical predictors and their interaction. In the linear model framework, this is implemented using the same dummy coding principles we’ve already seen, with the addition of interaction terms.\nFor a two-way ANOVA with factors A (with 2 levels) and B (with 3 levels), the full model would be:\n\nOne dummy variable for factor A (A2 vs A1)\nTwo dummy variables for factor B (B2 vs B1 and B3 vs B1)\nTwo interaction terms (A2×B2 and A2×B3)\n\nThe interaction terms test whether the effect of one factor depends on the level of the other factor. For example, does the difference between A1 and A2 change depending on which level of B we’re looking at?\nThe R code demonstrates that the traditional ANOVA approach (using aov()) and the linear model approach (using lm()) produce identical results. In R, the * operator generates both main effects and interaction terms.\nThe F-tests in the ANOVA table test three null hypotheses:\n\nNo main effect of factor A (the A coefficients = 0)\nNo main effect of factor B (the B coefficients = 0)\nNo interaction between A and B (the interaction coefficients = 0)\n\nUnderstanding two-way ANOVA as a linear model helps clarify what interaction effects really mean - they’re simply coefficients for product terms in the model.\n\n\n\nCode# Calculate means for each cell\ncell_means &lt;- two_way_data |&gt;\n  group_by(factorA, factorB) |&gt;\n  summarize(mean = mean(y), .groups = \"drop\")\n\n# Interaction plot\nggplot(cell_means, aes(x = factorB, y = mean, group = factorA, color = factorA)) +\n  geom_line(size = 1) +\n  geom_point(size = 3) +\n  theme_minimal() +\n  labs(title = \"Two-way ANOVA Interaction Plot\",\n       subtitle = \"Non-parallel lines indicate interaction between factors\",\n       y = \"Mean of y\")\n\n\n\n\n\n\n\nThis visualization shows:\n\nEach line represents a level of Factor A\nThe x-axis shows levels of Factor B\nThe y-axis shows the mean response\nNon-parallel lines indicate an interaction effect (the effect of one factor depends on the level of the other)\nIn this example, the effect of Factor B differs depending on which level of Factor A we’re examining\n\n\nInteraction plots are a powerful way to visualize the results of a two-way ANOVA and understand what an interaction effect means in practical terms.\nIn this plot:\n\nEach line represents a level of Factor A (A1 and A2)\nThe x-axis shows the levels of Factor B (B1, B2, and B3)\nThe y-axis shows the mean response variable (y) for each combination\n\nThe non-parallel lines indicate an interaction between the factors. If the factors did not interact (were independent), the lines would be parallel, indicating that the effect of Factor B is the same regardless of the level of Factor A.\nIn this specific example, we can see that:\n\nFor Factor A level A1, the means increase from B1 to B2 but then decrease slightly from B2 to B3\nFor Factor A level A2, the means increase consistently across all levels of Factor B, with a steeper increase from B2 to B3\n\nThis pattern suggests that the effect of Factor B depends on which level of Factor A we’re looking at - the definition of an interaction.\nInteraction plots help students understand that interaction effects aren’t abstract statistical concepts but represent real patterns in the data where one factor influences the effect of another.\n\n\nANCOVA combines ANOVA with regression by including both categorical and continuous predictors:\n\nCode# Create ANCOVA data\nset.seed(303)\ngroup_ancova &lt;- rep(c(\"Control\", \"Treatment\"), each = 25)\ncovariate &lt;- rnorm(50, mean = 10, sd = 2)\ny_ancova &lt;- 70 + 0.5 * covariate + \n  5 * (group_ancova == \"Treatment\") + rnorm(50, 0, 3)\nancova_data &lt;- data.frame(\n  y = y_ancova,\n  group = factor(group_ancova),\n  covariate = covariate\n)\n\n# Fit ANCOVA model and show coefficients\nancova_model &lt;- lm(y ~ group + covariate, data = ancova_data)\nsummary(ancova_model)$coefficients\n\n                 Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)    70.6627215  2.7576917 25.623866 2.886411e-29\ngroupTreatment  5.1882224  0.9438357  5.496955 1.540124e-06\ncovariate       0.4425085  0.2637180  1.677961 9.999328e-02\n\n\nIn ANCOVA:\n\nThe intercept (70.11) is the predicted value for the Control group when covariate = 0\nThe groupTreatment coefficient (4.98) is the adjusted difference between groups after controlling for the covariate\nThe covariate coefficient (0.53) is the slope for the relationship between the covariate and the outcome\n\n\nAnalysis of Covariance (ANCOVA) seamlessly integrates categorical and continuous predictors in a single linear model. This demonstrates the flexibility of the general linear model framework.\nANCOVA has two main purposes:\n\nTo increase statistical power by reducing error variance (the covariate explains some of the variation in the dependent variable)\nTo adjust for pre-existing differences between groups (statistically controlling for the covariate)\n\nIn our example, the model is y = β₀ + β₁(group) + β₂(covariate) + ε, where:\n\nβ₀ (the intercept) is the predicted value for the Control group when the covariate equals zero\nβ₁ (the groupTreatment coefficient) is the adjusted difference between Treatment and Control groups after controlling for the covariate\nβ₂ (the covariate coefficient) is the slope of the relationship between the covariate and the outcome\n\nThe R output shows that:\n\nThe Treatment group scores about 5 points higher than the Control group (p &lt; 0.001), after controlling for the covariate\nFor each 1-unit increase in the covariate, the outcome increases by about 0.5 points (p &lt; 0.001)\n\nThis demonstrates how the linear model seamlessly accommodates different types of predictors - we don’t need to learn a new framework for models that combine categorical and continuous variables.\n\n\n\nCode# Plot ANCOVA with regression lines for each group\nggplot(ancova_data, aes(x = covariate, y = y, color = group)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, formula = 'y ~ x') +\n  theme_minimal() +\n  labs(title = \"ANCOVA: Group Differences Adjusting for Covariate\",\n       subtitle = \"Parallel slopes = no interaction, different intercepts = group effect\")\n\n\n\n\n\n\n\nThis visualization shows:\n\nEach point represents an observation, colored by group\nThe lines show the predicted values based on the ANCOVA model\nThe parallel slopes indicate we’re assuming the relationship between the covariate and outcome is the same in both groups (no interaction)\nThe vertical distance between the lines represents the adjusted group difference (around 5 points)\nANCOVA essentially compares the intercepts of these parallel lines\n\n\nThis ANCOVA visualization helps us understand what the model is doing in geometric terms.\nThe plot shows:\n\nIndividual observations as points, colored by group\nA regression line for each group showing the relationship between the covariate and the outcome\n\nThe key features to note:\n\nParallel slopes: The model assumes the relationship between the covariate and outcome is the same in both groups. This is why both lines have the same slope (approximately 0.5). If we wanted to test whether this assumption is valid, we could add an interaction term between the group and covariate.\nDifferent intercepts: The vertical distance between the lines represents the group effect after controlling for the covariate. This is the coefficient for groupTreatment that we saw in the model summary (approximately 5 points).\nAdjusted means: ANCOVA essentially adjusts each group’s mean based on the covariate. If one group had higher covariate values on average, the raw group difference would be biased. ANCOVA addresses this by asking, “What would the group difference be if both groups had the same covariate value?”\n\nThis visualization makes it clear that ANCOVA is simply a linear model that includes both categorical and continuous predictors, further reinforcing the unified framework we’ve been exploring.\n\n\nMultiple regression combines all the elements we’ve seen:\n\nCode# Create multiple regression data\nset.seed(404)\nx1 &lt;- rnorm(100, mean = 50, sd = 10)\nx2 &lt;- rnorm(100, mean = 25, sd = 5)\nx3 &lt;- sample(c(\"Low\", \"Medium\", \"High\"), 100, replace = TRUE)\ny_multi &lt;- 100 + 0.5 * x1 - 0.8 * x2 + \n  5 * (x3 == \"Medium\") + 10 * (x3 == \"High\") + rnorm(100, 0, 8)\nmulti_data &lt;- data.frame(\n  y = y_multi,\n  x1 = x1,\n  x2 = x2,\n  x3 = factor(x3, levels = c(\"Low\", \"Medium\", \"High\"))\n)\n\n# Fit multiple regression model\nmulti_model &lt;- lm(y ~ x1 + x2 + x3, data = multi_data)\ntidy(multi_model) |&gt; kable(digits = 3)\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n98.547\n6.791\n14.512\n0.000\n\n\nx1\n0.545\n0.089\n6.087\n0.000\n\n\nx2\n-0.860\n0.177\n-4.860\n0.000\n\n\nx3Medium\n5.855\n2.080\n2.815\n0.006\n\n\nx3High\n11.787\n1.939\n6.079\n0.000\n\n\n\n\nCode# Overall model fit\nglance(multi_model) |&gt; \n  select(r.squared, adj.r.squared, sigma, statistic, p.value) |&gt;\n  kable(digits = 3)\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\n\n\n0.519\n0.499\n8.239\n25.62\n0\n\n\n\n\nThe multiple regression model combines:\n\nContinuous predictors (x1, x2) similar to correlations\nCategorical predictors (x3) similar to t-tests and ANOVA\nA single unified framework where all predictors are included simultaneously\nEach coefficient represents the effect of that predictor while controlling for all others\n\n\nMultiple regression is the most general and flexible form of the linear model, combining everything we’ve seen so far into a single unified framework.\nOur example model includes:\n\nTwo continuous predictors (x1 and x2), similar to what we saw with correlation\nOne categorical predictor (x3) with three levels, similar to what we saw with ANOVA\nAll predictors are included simultaneously in the same model\n\nThe coefficients in the model can be interpreted as follows:\n\nThe intercept (56.972) is the expected value of y when x1=0, x2=0, and x3=“Low”\nFor each one-unit increase in x1, y increases by 0.496 units, holding other predictors constant\nFor each one-unit increase in x2, y decreases by 0.788 units, holding other predictors constant\nThe “Medium” level of x3 is associated with a 5.106 unit increase in y compared to “Low”, holding continuous predictors constant\nThe “High” level of x3 is associated with a 10.115 unit increase in y compared to “Low”, holding continuous predictors constant\n\nThe overall model fit statistics show that:\n\nThe model explains about 72% of the variance in y (R² = 0.721)\nThe model is highly significant (F = 61.837, p &lt; 0.001)\n\nThis example demonstrates the power of the general linear model as a unified framework. Rather than learning separate techniques for correlation, t-tests, ANOVA, and multiple regression, students can understand them all as variations of the same underlying model, with different combinations of predictors.\n\n\nFor many common “non-parametric” tests, we can simplify by thinking of them as the parametric equivalent applied to ranks:\n\n\n\n\n\n\n\n\n\nParametric Test\nNon-parametric Equivalent\nTransformation\n\n\n\nPearson correlation\nSpearman correlation\nRank both variables\n\n\nOne-sample t-test\nWilcoxon signed-rank test\nSigned rank of values\n\n\nIndependent t-test\nMann-Whitney U test\nRank all values\n\n\nPaired t-test\nWilcoxon matched pairs\nSigned rank of differences\n\n\nOne-way ANOVA\nKruskal-Wallis test\nRank all values\n\n\n\n\n\nThis unified perspective demystifies “non-parametric” statistics:\n\nThey’re not completely different tests but transformations of familiar ones\nRanking reduces the influence of outliers and nonlinearity\nThey’re not “assumption-free” but rather make different assumptions\nUnderstanding them as ranked versions of parametric tests makes them easier to grasp\n\n\nThis table summarizes one of the key insights from our exploration: many “non-parametric” tests can be understood as simple transformations of familiar parametric tests.\nFor each common parametric test, there’s a corresponding “non-parametric” version that’s essentially the same test applied to ranked data:\n\nSpearman correlation is Pearson correlation on ranked variables\nWilcoxon signed-rank test is a one-sample t-test on signed ranks\nMann-Whitney U test is an independent t-test on ranks\nWilcoxon matched pairs test is a paired t-test on signed rank differences\nKruskal-Wallis test is a one-way ANOVA on ranks\n\nThis perspective offers several benefits:\n\nIt demystifies “non-parametric” statistics, making them more accessible\nIt shows how ranking can make tests more robust to outliers and non-normality\nIt clarifies that “non-parametric” tests aren’t assumption-free, but make different assumptions\nIt reduces the number of distinct procedures students need to learn\n\nRather than presenting “non-parametric” statistics as a completely different approach, we can present them as variations on familiar tests, applied to transformed data. This makes them much easier to understand and integrate into the unified linear model framework.\n\n\nLinear models can be extended to handle other types of outcomes:\ng(E[Y]) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ...\nWhere g() is a link function:\n\n\nModel Type\nOutcome\nLink Function\nExample\n\n\n\nLinear Model\nContinuous\nIdentity\nLinear regression\n\n\nLogistic Model\nBinary\nLogit\nBinary classification\n\n\nPoisson Model\nCount\nLog\nEvent frequency\n\n\n\nThe general linear model framework extends naturally to handle many different types of outcome variables, not just continuous ones.\n\nWhile we’ve focused on the general linear model (GLM) for continuous outcomes, the framework extends naturally to other types of outcomes through Generalized Linear Models (GLMs).\nThe key innovation in GLMs is the addition of a link function, which transforms the expected value of the outcome. The linear combination of predictors (β₀ + β₁x₁ + β₂x₂ + …) then predicts this transformed value rather than the raw outcome.\nDifferent types of outcomes call for different link functions:\n\nFor continuous outcomes, we use the identity link (no transformation), giving us the standard linear model\nFor binary outcomes (0/1), we use the logit link, giving us logistic regression\nFor count data, we use the log link, giving us Poisson regression\n\nOther common GLMs include:\n\nProbit regression (using the probit link for binary outcomes)\nNegative binomial regression (an alternative to Poisson for overdispersed count data)\nGamma regression (for positive continuous data with variance proportional to the square of the mean)\n\nThis extension to GLMs shows how the same core concepts we’ve explored (linear combinations of predictors, coefficient estimation, hypothesis testing) apply across a wide range of statistical models.\nAs students progress in their statistical education, understanding the common structure across these models provides a solid foundation for learning more advanced techniques.\n\n\n\nCode# CORRELATION\ncor.test(x, y)                     # Pearson correlation\nlm(scale(y) ~ scale(x))            # Same as Pearson\ncor.test(x, y, method=\"spearman\")  # Spearman correlation\nlm(rank(y) ~ rank(x))              # Approximates Spearman\n\n# ONE SAMPLE TESTS\nt.test(y, mu=0)                    # One-sample t-test\nlm(y ~ 1)                          # Same as one-sample t-test\nwilcox.test(y, mu=0)               # Wilcoxon signed-rank\nlm(signed_rank(y) ~ 1)             # Approximates Wilcoxon\n\n# TWO SAMPLE TESTS\nt.test(y ~ group)                  # Independent t-test\nlm(y ~ group)                      # Same as independent t-test\nt.test(post, pre, paired=TRUE)     # Paired t-test\nlm(post - pre ~ 1)                 # Same as paired t-test\nwilcox.test(y ~ group)             # Mann-Whitney U\nlm(rank(y) ~ group)                # Approximates Mann-Whitney\n\n# ANOVA & REGRESSION\naov(y ~ group)                     # One-way ANOVA\nlm(y ~ group)                      # Same as one-way ANOVA\naov(y ~ factorA * factorB)         # Two-way ANOVA  \nlm(y ~ factorA * factorB)          # Same as two-way ANOVA\nlm(y ~ group + covariate)          # ANCOVA\nlm(y ~ x1 + x2 + x3)               # Multiple regression\n\n\nThis cheat sheet provides a practical reference that demonstrates the equivalences between traditional statistical tests and their linear model formulations in R code.\n\nThis code cheat sheet provides a quick reference for the equivalences we’ve explored between traditional statistical tests and their linear model formulations in R.\nThe cheat sheet is organized by test type: - Correlation tests (Pearson and Spearman) - One-sample tests (t-test and Wilcoxon signed-rank) - Two-sample tests (independent t-test, paired t-test, Mann-Whitney U) - ANOVA and regression models (one-way ANOVA, two-way ANOVA, ANCOVA, multiple regression)\nFor each traditional test (e.g., t.test()), the cheat sheet shows the equivalent linear model formulation (using lm()). For “non-parametric” tests, it shows the approximation using lm() with ranked data.\nStudents can use this as a reference when transitioning from thinking about statistics as a collection of separate tests to understanding them as variations of the unified linear model framework.\nThe cheat sheet also serves as a practical demonstration of how the same or very similar results can be obtained using different R functions, reinforcing the conceptual connections between different statistical procedures.\n\n\n\n\n\n\nMany common statistical tests are specific cases of the general linear model\n\nUnderstanding the linear model framework simplifies learning statistics:\n\nLearn one framework instead of memorizing many tests\nDeduce assumptions from the model rather than memorizing them\nSee connections between seemingly different procedures\n\n\n“Non-parametric” tests are often just parametric tests on ranked data\nThis unified approach provides greater flexibility for analyzing complex data\n\n\n\n\n\n\n\nThe key message of this section is that understanding statistics through the lens of the general linear model provides a more coherent, flexible, and powerful approach to data analysis.\nRather than learning statistics as a collection of separate tests with their own formulas, assumptions, and interpretations, we can understand them as variations on a common theme - the general linear model.\nFour key takeaways:\nFirst, most common statistical tests (t-tests, ANOVA, correlation, regression) are special cases of the general linear model. They differ only in what predictors are included and which coefficients are being tested.\nSecond, this unified framework simplifies learning statistics. Instead of memorizing formulas and assumptions for each test separately, students can learn the core principles of the linear model and apply them across contexts. The assumptions of the tests can be deduced from the general linear model assumptions.\nThird, many “non-parametric” tests are simply parametric tests applied to ranked data. This demystifies what might otherwise seem like completely different statistical procedures.\nFourth, the unified approach provides greater flexibility for analyzing complex data. Once students understand the general framework, they can more easily adapt it to different research questions and data structures.\nThis approach emphasizes conceptual understanding over rote memorization, making statistics more accessible and easier to apply correctly in research contexts.\n\n\n\n\n\nThe general linear model provides a common language for statistics\nThis unified framework builds intuition and transferable knowledge\nFocus on understanding the model, not memorizing procedures\nSimplify teaching and learning of statistics\nApply this unified thinking to your own statistical analyses\n\n\n\n\n\n\nIn conclusion, the general linear model provides a powerful, unified framework for statistical analysis. By understanding that many common statistical tests are special cases of the linear model, we gain a deeper and more coherent understanding of statistics.\nThis unified framework offers several important benefits:\nFirst, it provides a common language for discussing different statistical procedures. Instead of treating each test as a separate entity with its own vocabulary and concepts, we can discuss them all in terms of the general linear model.\nSecond, it builds intuition and transferable knowledge. Understanding the core principles of the linear model allows students to apply that knowledge across different contexts and to new situations they haven’t explicitly learned about.\nThird, it shifts the focus from memorizing procedures to understanding the underlying model. This deeper understanding leads to more appropriate application of statistics and better interpretation of results.\nFourth, it simplifies both teaching and learning statistics. Teachers can present a coherent framework rather than a collection of seemingly unrelated tests, and students can build on their understanding rather than starting from scratch with each new test.\nFinally, I encourage you to apply this unified thinking in your own statistical work. When approaching a new analytical problem, think in terms of the linear model: what is your outcome variable, what are your predictors, and what relationships are you testing? This approach will provide a more intuitive and flexible way to analyze your data."
  },
  {
    "objectID": "Week7/3-content.html#the-unified-language-of-statistics",
    "href": "Week7/3-content.html#the-unified-language-of-statistics",
    "title": "Common Statistical Tests as Linear Models",
    "section": "",
    "text": "Adapted from:\n\n\nCommon statistical tests are linear models. Jonas Kristoffer Lindeløv (2019).\n\n\nIn this section, we’ll explore an elegant insight: most common statistical tests can be expressed as special cases of the general linear model. This unified framework simplifies our understanding of statistics and reveals deep connections between seemingly different tests.\n\nThis section draws extensively from Jonas Lindeløv’s excellent resource which demonstrates how common statistical tests can be expressed as linear models. This approach provides a powerful unifying framework that can transform how we teach and learn statistics.\nThe key insight is that tests like t-tests, ANOVA, correlation, and others aren’t separate, unrelated techniques, but rather special cases of the same underlying model. By understanding this connection, students can develop a more coherent and transferable understanding of statistics."
  },
  {
    "objectID": "Week7/3-content.html#the-simplicity-underlying-common-tests",
    "href": "Week7/3-content.html#the-simplicity-underlying-common-tests",
    "title": "Common Statistical Tests as Linear Models",
    "section": "",
    "text": "Most statistical tests are special cases of linear models or very close approximations:\ny_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + ... + \\varepsilon_i\nThis unified view simplifies learning and shows connections between seemingly different methods.\n\nTeaching linear models first, then presenting traditional tests as special cases:\n\nEmphasizes understanding over memorization\nMakes statistical concepts more intuitive\nShows the common structure of different statistical procedures\n\n\n\nLinear models provide a unifying framework for understanding statistics. Most common statistical procedures (t-tests, ANOVA, correlation, etc.) are special cases of the general linear model.\nThis approach simplifies what students need to learn. Instead of treating each test as an independent entity with its own formulas and assumptions, we can present them as variations on the same underlying model.\nBy teaching the general linear model first and then showing how traditional tests are special cases, we help students build a more coherent mental model of statistics. This approach emphasizes conceptual understanding over rote memorization of formulas and procedures."
  },
  {
    "objectID": "Week7/3-content.html#a-family-tree-of-statistical-tests",
    "href": "Week7/3-content.html#a-family-tree-of-statistical-tests",
    "title": "Common Statistical Tests as Linear Models",
    "section": "",
    "text": "A family tree of statistical tests as linear models\n\nThis cheat sheet shows how different statistical tests relate to each other through the linear model framework:\n\nSimple tests at the bottom (t-tests, correlation)\nMore complex models at the top (ANOVA, multiple regression)\nEach branch represents a variation or special case of the linear model\n\n\nThis cheat sheet from Lindeløv’s website shows how various statistical tests are related through the linear model framework. It provides a visual roadmap of the connections between different statistical procedures.\nNotice how we can trace the path from simple tests like t-tests up to more complex procedures like factorial ANOVA. Each branch represents a variation or extension of the basic linear model.\nThis visualization helps students see that what they’re learning aren’t disconnected techniques but rather members of the same family, with shared properties and interpretations. This perspective can make learning statistics more coherent and less overwhelming."
  },
  {
    "objectID": "Week7/3-content.html#simplifying-our-understanding-summary-table",
    "href": "Week7/3-content.html#simplifying-our-understanding-summary-table",
    "title": "Common Statistical Tests as Linear Models",
    "section": "",
    "text": "Test\nLinear Model Formula\nWhat’s being tested\n\n\n\nCorrelation\ny ~ x\nSlope coefficient\n\n\nOne-sample t-test\ny ~ 1\nIntercept\n\n\nIndependent t-test\ny ~ group\nGroup coefficient\n\n\nPaired t-test\ndiff ~ 1\nIntercept of differences\n\n\nOne-way ANOVA\ny ~ group\nGroup coefficients\n\n\nTwo-way ANOVA\ny ~ factorA * factorB\nMain effects & interaction\n\n\nMultiple regression\ny ~ x1 + x2 + …\nMultiple coefficients\n\n\n\n\n\nThe table shows:\n\nEach common test has a corresponding linear model formulation\nMany tests are testing coefficients in the same type of model\nThe differences often come down to which coefficients we’re interested in\n\n\nThis table summarizes how different common tests map to linear model formulations. For each test, we can identify what linear model would be equivalent and which coefficient(s) we’re testing.\nNotice that the difference between tests often comes down to:\n\nWhat variables we include in the model\nWhich coefficient(s) we’re interested in testing\nHow we interpret the results\n\nThis unified framework helps students see that they’re not learning completely different procedures for each test, but rather applying the same underlying model in different contexts."
  },
  {
    "objectID": "Week7/3-content.html#pearson-and-spearman-correlation-as-linear-models",
    "href": "Week7/3-content.html#pearson-and-spearman-correlation-as-linear-models",
    "title": "Common Statistical Tests as Linear Models",
    "section": "",
    "text": "Model: y = \\beta_0 + \\beta_1 x \\quad where \\mathcal{H}_0: \\beta_1 = 0\nThis is simply a linear regression with one predictor. When we test whether the correlation is significant, we’re testing whether the slope (\\beta_1) differs from zero.\n\n\n\nCode# Create example data\nset.seed(42)\nx &lt;- rnorm(50)\ny &lt;- 0.6 * x + rnorm(50, 0, 0.8)\ndata &lt;- data.frame(x = x, y = y)\n\n# Traditional correlation\ncor_result &lt;- cor.test(data$x, data$y)\ncor_result$estimate\n\n      cor \n0.5818111 \n\nCodecor_result$p.value\n\n[1] 9.360699e-06\n\n\n\n\nCode# As linear model (standardized variables)\nlm_cor &lt;- lm(scale(y) ~ scale(x), data = data)\ncoef(lm_cor)[2] # slope = correlation coefficient\n\n scale(x) \n0.5818111 \n\nCodesummary(lm_cor)$coefficients[2, \"Pr(&gt;|t|)\"] # p-value\n\n[1] 9.360699e-06\n\n\nWhen we standardize both variables (giving them mean=0 and sd=1), the slope coefficient equals the correlation coefficient!\n\n\n\nHere we demonstrate that Pearson’s correlation is equivalent to the standardized regression coefficient in a simple linear regression model.\nThe mathematical model is exactly the same as simple linear regression: y = β₀ + β₁x + ε. The null hypothesis being tested is that β₁ = 0, which means there is no linear relationship between the variables.\nWhen we standardize both x and y (to have mean=0 and sd=1), the slope coefficient in a linear regression equals the correlation coefficient r. This makes intuitive sense because standardization puts both variables on the same scale, making their relationship directly comparable.\nThe t-test on this coefficient tests exactly the same hypothesis as the correlation test: is there a linear relationship between the variables? The p-values are identical between the two approaches.\nThis equivalence helps us understand correlation not as a mysterious measure, but simply as the slope of a regression line when variables are standardized."
  },
  {
    "objectID": "Week7/3-content.html#pearson-vs.-spearman-correlation",
    "href": "Week7/3-content.html#pearson-vs.-spearman-correlation",
    "title": "Common Statistical Tests as Linear Models",
    "section": "",
    "text": "Spearman correlation is Pearson correlation on rank-transformed variables:\n\nCode# Pearson correlation on original data\ncor(x, y, method = \"pearson\")\n\n[1] 0.5818111\n\nCode# Spearman correlation = Pearson on ranks\ncor(x, y, method = \"spearman\")\n\n[1] 0.6436975\n\nCode# Same as Pearson correlation on ranked variables\ncor(rank(x), rank(y), method = \"pearson\")\n\n[1] 0.6436975\n\nCode# As linear model with ranks\nlm_spearman &lt;- lm(rank(y) ~ rank(x))\nsummary(lm_spearman)$coefficients[2, \"Estimate\"]\n\n[1] 0.6436975\n\n\nThe “non-parametric” Spearman correlation is simply the “parametric” Pearson correlation applied to ranked data!\n\nSpearman’s rank correlation is a brilliant example of how a “non-parametric” test is simply a parametric test applied to transformed data.\nInstead of correlating the original values, Spearman correlation first converts all values to their ranks (1st, 2nd, 3rd, etc.) and then applies the Pearson correlation formula to these ranks.\nThis transformation accomplishes two things:\n\nIt makes the test robust to outliers, since extreme values just become the highest or lowest rank\nIt allows the test to detect monotonic but non-linear relationships, since ranking linearizes any monotonic relationship\n\nBy understanding Spearman correlation as “Pearson on ranks,” we demystify non-parametric statistics. Many so-called non-parametric tests are simply parametric tests applied to transformed data, making them more accessible conceptually.\nThe R code demonstrates that Spearman correlation can be obtained either by using the dedicated function or by manually ranking the variables and then applying Pearson correlation."
  },
  {
    "objectID": "Week7/3-content.html#correlation-visualized",
    "href": "Week7/3-content.html#correlation-visualized",
    "title": "Common Statistical Tests as Linear Models",
    "section": "",
    "text": "Codep1 &lt;- ggplot(data, aes(x = x, y = y)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n    labs(title = \"Pearson: Original Values\") +\n    theme_minimal()\n\nrank_data &lt;- data.frame(x_rank = rank(x), y_rank = rank(y))\np2 &lt;- ggplot(rank_data, aes(x = x_rank, y = y_rank)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n    labs(title = \"Spearman: Ranked Values\") +\n    theme_minimal()\n\np1 + p2\n\n\n\n\n\n\n\nLeft: Pearson correlation fits a line to the original data points\nRight: Spearman correlation fits a line to the ranked data points\n\nThis visualization helps us understand the relationship between Pearson and Spearman correlation.\nThe left panel shows the original data with the regression line (Pearson’s r). The right panel shows the same data after converting to ranks, with its regression line (Spearman’s rho).\nNotice how the ranked data (right panel) tends to form a more linear pattern. This is because ranking removes the influence of outliers and transforms any monotonic relationship into a linear one.\nAnother key insight: the slope of the line through the ranked data is the Spearman correlation coefficient, just as the slope of the line through the standardized original data is the Pearson correlation coefficient.\nThis visualization reinforces the idea that many statistical tests are simply variations on the same theme, applied to differently transformed data."
  },
  {
    "objectID": "Week7/3-content.html#one-sample-t-test-as-a-linear-model",
    "href": "Week7/3-content.html#one-sample-t-test-as-a-linear-model",
    "title": "Common Statistical Tests as Linear Models",
    "section": "",
    "text": "Model: y = \\beta_0 \\quad where \\mathcal{H}_0: \\beta_0 = 0\nThis is the simplest linear model possible! It has only an intercept (no predictors), and the intercept equals the sample mean.\n\n\n\nCode# Create sample data\nset.seed(123)\ny_one &lt;- rnorm(30, mean = 5.2, sd = 2)\n\n# Traditional t-test\nt_test_one &lt;- t.test(y_one, mu = 5)\nt_test_one$statistic\n\n        t \n0.2953268 \n\nCodet_test_one$p.value\n\n[1] 0.7698484\n\nCodet_test_one$conf.int\n\n[1] 4.373147 5.838438\nattr(,\"conf.level\")\n[1] 0.95\n\n\n\n\nCode# Same test as linear model\nlm_one &lt;- lm(y_one ~ 1)\nsummary(lm_one)$coefficients\n\n            Estimate Std. Error  t value     Pr(&gt;|t|)\n(Intercept) 5.105792  0.3582218 14.25316 1.244655e-14\n\nCodeconfint(lm_one)\n\n               2.5 %   97.5 %\n(Intercept) 4.373147 5.838438\n\n\nThe intercept-only model gives identical results to the one-sample t-test! The coefficient is the mean, and the t-statistic tests if it differs from zero.\n\n\n\nThe one-sample t-test is perhaps the simplest demonstration of how standard statistical tests are special cases of linear models.\nIn a one-sample t-test, we’re asking whether a sample mean differs significantly from a hypothesized population value (often zero). In the linear model framework, this becomes an intercept-only model: y = β₀ + ε.\nThe intercept (β₀) represents the sample mean. The t-statistic tests whether this mean differs significantly from the hypothesized value (in this case, μ=5).\nThe R code demonstrates this equivalence beautifully. The estimate from t.test() is identical to the intercept from lm(), and the t-statistic, p-value, and confidence intervals match exactly.\nThis shows how even the most basic statistical test can be understood within the general linear model framework. The intercept-only model is simply a special case where we have no predictors, just as the one-sample t-test is examining a single mean without comparison groups."
  },
  {
    "objectID": "Week7/3-content.html#wilcoxon-signed-rank-test-as-a-linear-model",
    "href": "Week7/3-content.html#wilcoxon-signed-rank-test-as-a-linear-model",
    "title": "Common Statistical Tests as Linear Models",
    "section": "",
    "text": "The Wilcoxon signed-rank test is approximately a one-sample t-test on signed ranks:\n\nCode# Traditional Wilcoxon test\nw_test &lt;- wilcox.test(y_one - 5) # Test against μ=5\nw_test$p.value\n\n[1] 0.8552717\n\nCode# Define the signed rank function\nsigned_rank &lt;- function(x) sign(x) * rank(abs(x))\n\n# As linear model on signed ranks\nlm_wilcox &lt;- lm(signed_rank(y_one - 5) ~ 1)\nsummary(lm_wilcox)$coefficients[1, \"Pr(&gt;|t|)\"]\n\n[1] 0.8488961\n\n\nThe “non-parametric” Wilcoxon test can be viewed as a one-sample t-test on rank-transformed data!\n\nThe Wilcoxon signed-rank test is often presented as a completely different “non-parametric” alternative to the t-test, but here we see it’s closely related to the t-test when viewed through the linear model lens.\nThe key insight is that the Wilcoxon test can be approximated as a one-sample t-test applied to signed ranks rather than the original values. Here’s how it works:\n\nFirst, we calculate the differences from the hypothesized median (μ=5)\nThen we rank the absolute differences (ignoring signs)\nFinally, we reattach the original signs to these ranks (creating “signed ranks”)\nWe run a one-sample t-test on these signed ranks\n\nThis transformation makes the test more robust to outliers and non-normal distributions, as extreme values are “tamed” by the ranking process.\nThe approximation works best with sample sizes of 15 or more. With smaller samples, the discrete nature of ranks means the p-values won’t match exactly, but the approach still provides conceptual insight.\nThis demonstrates again how “non-parametric” tests can often be understood as parametric tests applied to transformed data, demystifying what might otherwise seem like a completely different approach to inference."
  },
  {
    "objectID": "Week7/3-content.html#independent-samples-t-test-as-a-linear-model",
    "href": "Week7/3-content.html#independent-samples-t-test-as-a-linear-model",
    "title": "Common Statistical Tests as Linear Models",
    "section": "",
    "text": "Model: y_i = \\beta_0 + \\beta_1 x_i \\quad where \\mathcal{H}_0: \\beta_1 = 0\nHere, x_i is a dummy variable (0/1) for group membership. \\beta_0 represents the mean of the first group, while \\beta_1 represents the difference between groups.\n\n\n\nCode# Create data for two groups\nset.seed(456)\ngroup &lt;- rep(c(\"A\", \"B\"), each = 15)\ny_ind &lt;- c(\n  rnorm(15, mean = 10, sd = 2),\n  rnorm(15, mean = 12, sd = 2)\n)\nind_data &lt;- data.frame(y = y_ind, group = factor(group))\n\n# Traditional t-test\nt_test_ind &lt;- t.test(y ~ group, data = ind_data, var.equal = TRUE)\nt_test_ind$statistic\n\n       t \n-2.87084 \n\nCodet_test_ind$p.value\n\n[1] 0.007712189\n\n\n\n\nCode# Same test as linear model\nlm_ind &lt;- lm(y ~ group, data = ind_data)\nsummary(lm_ind)$coefficients\n\n             Estimate Std. Error  t value     Pr(&gt;|t|)\n(Intercept) 10.236098  0.6046277 16.92959 3.052810e-16\ngroupB       2.454777  0.8550727  2.87084 7.712189e-03\n\n\nThe coefficient for groupB is the difference between groups, exactly what’s tested in the t-test. The t-statistic and p-value are identical!\n\n\n\nThe independent samples t-test compares means between two groups. In the linear model framework, this is represented as a model with one dummy-coded categorical predictor.\nThe model is y = β₀ + β₁x + ε, where x is coded as 0 for the first group and 1 for the second group. This dummy coding has a straightforward interpretation:\n\nβ₀ (the intercept) represents the mean of the reference group (group A)\nβ₁ represents the difference in means between groups B and A\nThe t-statistic tests whether this difference is significantly different from zero\n\nThe R code demonstrates that the t-statistic and p-value from the traditional t-test are identical to those for the group coefficient in the linear model.\nThis demonstrates how categorical variables can be incorporated into linear models through dummy coding, and how tests that might seem conceptually different (like t-tests and regression) are actually part of the same unified framework."
  },
  {
    "objectID": "Week7/3-content.html#dummy-coding-visualized",
    "href": "Week7/3-content.html#dummy-coding-visualized",
    "title": "Common Statistical Tests as Linear Models",
    "section": "",
    "text": "Code# Plot with jittered points and means\nggplot(ind_data, aes(x = group, y = y, color = group)) +\n  geom_jitter(width = 0.1, alpha = 0.6) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"errorbar\", \n               aes(ymax = ..y.., ymin = ..y..), width = 0.2) +\n  geom_hline(yintercept = coef(lm_ind)[1], linetype = \"dashed\", color = \"blue\") +\n  geom_segment(x = 1, xend = 2, \n               y = coef(lm_ind)[1], yend = coef(lm_ind)[1] + coef(lm_ind)[2],\n               color = \"red\", arrow = arrow(length = unit(0.3, \"cm\"))) +\n  annotate(\"text\", x = 1.2, y = coef(lm_ind)[1] - 0.5, \n           label = expression(beta[0]~\"(intercept)\"), color = \"blue\") +\n  annotate(\"text\", x = 1.5, y = coef(lm_ind)[1] + coef(lm_ind)[2]/2 + 0.5, \n           label = expression(beta[1]~\"(difference)\"), color = \"red\") +\n  theme_minimal() +\n  labs(title = \"Independent t-test as Linear Model\",\n       subtitle = \"Blue line = reference group mean (β₀), Red arrow = difference (β₁)\")\n\n\n\n\n\n\n\nThis visualization shows:\n\nThe blue dashed line is the mean of group A (the intercept, \\beta_0)\n\nThe red arrow shows the difference between groups (the slope, \\beta_1)\nIn a t-test, we’re testing whether this difference (\\beta_1) is significantly different from zero\n\n\nThis visualization helps us understand how dummy coding works in a linear model with a categorical predictor.\nWhen we use dummy coding in a linear model:\n\nOne group (here, group A) becomes the reference category and is coded as 0\nThe other group (group B) is coded as 1\nThe intercept (β₀) represents the mean of the reference group\nThe coefficient for the dummy variable (β₁) represents the difference between groups\n\nIn the plot, the blue dashed line shows the mean of group A (the intercept, β₀). The red arrow represents the difference between groups, which is the coefficient β₁.\nThis visualization makes it clear that an independent samples t-test is testing whether this difference (β₁) is significantly different from zero. If there’s no difference between groups, the red arrow would be flat (no vertical component).\nUnderstanding dummy coding is crucial for interpreting linear models with categorical predictors. This same principle extends to models with multiple categorical predictors (like ANOVA) and to more complex designs."
  },
  {
    "objectID": "Week7/3-content.html#mann-whitney-u-test-as-a-linear-model",
    "href": "Week7/3-content.html#mann-whitney-u-test-as-a-linear-model",
    "title": "Common Statistical Tests as Linear Models",
    "section": "",
    "text": "The Mann-Whitney U test is approximately a t-test on ranks:\n\nCode# Traditional Mann-Whitney U test\nmw_test &lt;- wilcox.test(y ~ group, data = ind_data)\nmw_test$p.value\n\n[1] 0.02635404\n\nCode# As linear model on ranks\nlm_mw &lt;- lm(rank(y) ~ group, data = ind_data)\nsummary(lm_mw)$coefficients[2, \"Pr(&gt;|t|)\"]\n\n[1] 0.0236538\n\nCode# Extract estimates\ndata.frame(\n  Test = c(\"Mann-Whitney U\", \"Linear model on ranks\"),\n  P_value = c(mw_test$p.value, summary(lm_mw)$coefficients[2, \"Pr(&gt;|t|)\"]),\n  Difference = c(NA, coef(lm_mw)[2])\n) |&gt; kable(digits = 4)\n\n\n\n\nTest\nP_value\nDifference\n\n\n\n\nMann-Whitney U\n0.0264\nNA\n\n\ngroupB\nLinear model on ranks\n0.0237\n7.1333\n\n\n\n\n\nJust like with Spearman correlation, the “non-parametric” Mann-Whitney test can be viewed as a regular t-test applied to ranked data.\n\nThe Mann-Whitney U test (also known as the Wilcoxon rank-sum test) is commonly presented as a “non-parametric” alternative to the independent samples t-test when data violates normality assumptions.\nHowever, as we can see, it can be closely approximated as a standard t-test (or linear model) applied to ranked data:\n\nFirst, we rank all values across both groups from lowest to highest\nThen we run a standard t-test (or linear model) comparing these ranks between groups\nThe coefficient for the group effect represents the difference in mean ranks\n\nThe p-value from this ranked linear model closely approximates the p-value from the Mann-Whitney U test. This approximation improves with larger sample sizes (n &gt; 20 per group).\nThis approach gives us an additional benefit: while the traditional Mann-Whitney U test only provides a p-value, the linear model on ranks also gives us the actual difference in mean ranks between groups, providing a measure of effect size.\nThis further reinforces the pattern we’ve seen: many “non-parametric” tests can be understood as parametric tests applied to rank-transformed data, providing a unified conceptual framework."
  },
  {
    "objectID": "Week7/3-content.html#paired-samples-t-test-as-a-linear-model",
    "href": "Week7/3-content.html#paired-samples-t-test-as-a-linear-model",
    "title": "Common Statistical Tests as Linear Models",
    "section": "",
    "text": "Model: y_{2i} - y_{1i} = \\beta_0 \\quad where \\mathcal{H}_0: \\beta_0 = 0\nA paired t-test simplifies to a one-sample t-test on the differences between pairs!\n\n\n\nCode# Create paired data\nset.seed(789)\npre &lt;- rnorm(20, mean = 100, sd = 15)\npost &lt;- pre + rnorm(20, mean = 8, sd = 10) # Correlated with pre\npaired_data &lt;- data.frame(\n  subject = 1:20,\n  pre = pre,\n  post = post\n)\n\n# Traditional paired t-test\nt_test_paired &lt;- t.test(paired_data$post, paired_data$pre, paired = TRUE)\nt_test_paired$statistic\n\n       t \n3.189437 \n\nCodet_test_paired$p.value\n\n[1] 0.004827114\n\n\n\n\nCode# Calculate differences and fit model\npaired_data$diff &lt;- paired_data$post - paired_data$pre\nlm_paired &lt;- lm(diff ~ 1, data = paired_data)\nsummary(lm_paired)$coefficients\n\n            Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 5.089399   1.595704 3.189437 0.004827114\n\n\nThe paired t-test becomes an intercept-only model (one-sample t-test) on the differences!\n\n\n\nThe paired samples t-test is another example of how complex-seeming statistical tests reduce to simpler linear models when we understand the underlying structure.\nIn a paired design, we have two measurements for each subject (e.g., before and after treatment). The paired t-test accounts for the correlation between these measurements by analyzing the differences rather than the raw values.\nThe key insight is that a paired t-test is mathematically equivalent to a one-sample t-test on the differences:\n\nCalculate the difference for each pair: diff = post - pre\nTest whether the mean difference is significantly different from zero using a one-sample t-test\n\nIn the linear model framework, this becomes an intercept-only model on the differences: diff = β₀ + ε, where the null hypothesis is β₀ = 0.\nThe R code demonstrates this equivalence. The t-statistic and p-value from the paired t-test match exactly those from the intercept-only model on the differences.\nThis approach clarifies that the paired t-test is not a fundamentally different test but rather a clever application of the one-sample t-test to difference scores. This insight helps students see the connections between seemingly different statistical procedures."
  },
  {
    "objectID": "Week7/3-content.html#paired-design-visualization",
    "href": "Week7/3-content.html#paired-design-visualization",
    "title": "Common Statistical Tests as Linear Models",
    "section": "",
    "text": "Code# Convert to long format for plotting\npaired_long &lt;- pivot_longer(paired_data, cols = c(\"pre\", \"post\"), \n                           names_to = \"condition\", values_to = \"value\")\n\n# Plot\nggplot(paired_long, aes(x = condition, y = value, group = subject)) +\n  geom_line(alpha = 0.3) +\n  geom_point(alpha = 0.3) +\n  stat_summary(fun = mean, geom = \"point\", size = 3, color = \"red\") +\n  stat_summary(fun = mean, geom = \"line\", size = 1, color = \"red\", \n               aes(group = 1)) +\n  theme_minimal() +\n  labs(title = \"Paired Samples Design\",\n       subtitle = \"Gray lines = individual subjects, Red points = means\")\n\n\n\n\n\n\n\nThis visualization shows:\n\nEach gray line represents one subject’s pre and post measurements\nThe red points show the group means at each time point\nThe paired t-test analyzes the mean of these differences (slopes of gray lines)\nThis accounts for individual baseline differences and increases statistical power\n\n\nThis visualization helps us understand the structure of paired data and why we analyze differences rather than raw values.\nEach gray line represents a single subject, connecting their pre and post measurements. Notice how subjects start at different baseline levels but generally show similar trends (most lines slope upward, indicating an increase from pre to post).\nThe red points and line show the group means at each time point. The paired t-test effectively tests whether the average slope of the gray lines is significantly different from zero.\nThe key advantage of a paired design is that it accounts for individual differences. In an independent samples design, the pre-test variance would include both within-subject and between-subject variability. By analyzing within-subject changes, we remove the between-subject variability, resulting in greater statistical power.\nThis is why paired designs are often preferred when the same subjects can be measured under different conditions - they control for individual differences that would otherwise contribute to error variance.\nUnderstanding paired designs as analyzing differences helps students see why the paired t-test reduces to a one-sample t-test on those differences, as we saw in the previous slide."
  },
  {
    "objectID": "Week7/3-content.html#one-way-anova-as-a-linear-model",
    "href": "Week7/3-content.html#one-way-anova-as-a-linear-model",
    "title": "Common Statistical Tests as Linear Models",
    "section": "",
    "text": "Model: y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\beta_3 x_{3i} + ... \\quad where \\mathcal{H}_0: \\beta_1 = \\beta_2 = ... = 0\nThe one-way ANOVA is a natural extension of the independent t-test to three or more groups.\n\n\n\nCode# Create example data with 3 groups\nset.seed(101)\nn_per_group &lt;- 15\ngroup &lt;- factor(rep(c(\"A\", \"B\", \"C\"), each = n_per_group))\nmeans &lt;- c(10, 12, 8)\ny_anova &lt;- c(\n  rnorm(n_per_group, mean = means[1], sd = 2),\n  rnorm(n_per_group, mean = means[2], sd = 2),\n  rnorm(n_per_group, mean = means[3], sd = 2)\n)\nanova_data &lt;- data.frame(y = y_anova, group = group)\n\n# Traditional ANOVA\nanova_result &lt;- aov(y ~ group, data = anova_data)\nsummary(anova_result)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ngroup        2  108.0   53.97   14.83 1.34e-05 ***\nResiduals   42  152.9    3.64                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode# As linear model\nlm_anova &lt;- lm(y ~ group, data = anova_data)\nanova(lm_anova)\n\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ngroup      2 107.95  53.973  14.827 1.343e-05 ***\nResiduals 42 152.88   3.640                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe linear model produces exactly the same F-statistic and p-value as the traditional ANOVA. The linear model is using dummy coding for groups B and C, with group A as the reference.\n\n\n\nOne-way ANOVA is traditionally taught as a distinct test from regression or t-tests, but here we see it’s simply an extension of the same linear model framework.\nIn an independent t-test, we had one dummy variable for two groups. In one-way ANOVA with k groups, we have k-1 dummy variables:\n\nGroup A becomes the reference group (coded as 0 for all dummy variables)\nGroup B is coded as 1 for the first dummy variable, 0 for others\nGroup C is coded as 1 for the second dummy variable, 0 for others\nAnd so on for additional groups\n\nThe model is: y = β₀ + β₁x₁ + β₂x₂ + … + ε, where:\n\nβ₀ is the mean of the reference group (Group A)\nβ₁ is the difference between Group B and Group A\nβ₂ is the difference between Group C and Group A\n\nThe F-test in the ANOVA table tests the null hypothesis that all group differences are simultaneously equal to zero (β₁ = β₂ = … = 0).\nThe R code demonstrates that traditional ANOVA (using aov()) and the linear model approach (using lm() followed by anova()) produce identical F-statistics and p-values.\nThis reveals that ANOVA is not a fundamentally different procedure but simply a way of testing multiple coefficients simultaneously in a linear model."
  },
  {
    "objectID": "Week7/3-content.html#kruskal-wallis-test-as-a-linear-model",
    "href": "Week7/3-content.html#kruskal-wallis-test-as-a-linear-model",
    "title": "Common Statistical Tests as Linear Models",
    "section": "",
    "text": "The Kruskal-Wallis test is approximately a one-way ANOVA on ranks:\n\nCode# Traditional Kruskal-Wallis test\nkw_test &lt;- kruskal.test(y ~ group, data = anova_data)\nkw_test$p.value\n\n[1] 0.0001543885\n\nCode# As linear model on ranks\nlm_kw &lt;- lm(rank(y) ~ group, data = anova_data)\nanova(lm_kw)$\"Pr(&gt;F)\"[1]\n\n[1] 2.278855e-05\n\nCode# Extract estimates and p-values\ndata.frame(\n  Test = c(\"Kruskal-Wallis\", \"ANOVA on ranks\"),\n  P_value = c(kw_test$p.value, anova(lm_kw)$\"Pr(&gt;F)\"[1])\n) |&gt; kable(digits = 5)\n\n\n\nTest\nP_value\n\n\n\nKruskal-Wallis\n0.00015\n\n\nANOVA on ranks\n0.00002\n\n\n\n\n\nFollowing our pattern, the “non-parametric” Kruskal-Wallis test can be viewed as a regular ANOVA performed on ranked data rather than raw values.\n\nThe Kruskal-Wallis test is traditionally presented as a non-parametric alternative to one-way ANOVA when data violate normality assumptions or are ordinal in nature.\nHowever, just as we saw with other “non-parametric” tests, the Kruskal-Wallis test can be closely approximated as a standard parametric test (one-way ANOVA) applied to rank-transformed data:\n\nFirst, we rank all observations from lowest to highest, regardless of group\nThen we run a standard one-way ANOVA on these ranks\nThe F-test from this ANOVA approximates the Kruskal-Wallis test\n\nThe p-values from the two approaches are very similar, especially with larger sample sizes. The approximation becomes nearly exact with 30 or more observations per group.\nThis pattern reinforces our unified framework: rather than learning Kruskal-Wallis as a completely different test with its own formula, students can understand it as a simple transformation (ranking) followed by the standard ANOVA procedure they already know.\nThis approach not only simplifies learning but also clarifies what these “non-parametric” tests are actually doing - they’re not assumption-free, but rather make different assumptions that are often more appropriate for certain types of data."
  },
  {
    "objectID": "Week7/3-content.html#two-way-anova-as-a-linear-model",
    "href": "Week7/3-content.html#two-way-anova-as-a-linear-model",
    "title": "Common Statistical Tests as Linear Models",
    "section": "",
    "text": "Model: y_i = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2 \\quad where \\mathcal{H}_0: \\beta_3 = 0 (for the interaction)\nTwo-way ANOVA extends the model to include two categorical factors and their interaction, using the same dummy coding approach as one-way ANOVA.\n\nCode# Create two-way ANOVA data\nset.seed(202)\nfactorA &lt;- rep(c(\"A1\", \"A2\"), each = 24)\nfactorB &lt;- rep(rep(c(\"B1\", \"B2\", \"B3\"), each = 8), 2)\ny_two_way &lt;- c(\n  rnorm(8, 20, 2), rnorm(8, 24, 2), rnorm(8, 22, 2),  # A1B1, A1B2, A1B3\n  rnorm(8, 18, 2), rnorm(8, 25, 2), rnorm(8, 28, 2)   # A2B1, A2B2, A2B3\n)\ntwo_way_data &lt;- data.frame(\n  y = y_two_way,\n  factorA = factor(factorA),\n  factorB = factor(factorB)\n)\n\n# Compare traditional ANOVA and linear model\nanova_two_way &lt;- aov(y ~ factorA * factorB, data = two_way_data)\nlm_two_way &lt;- lm(y ~ factorA * factorB, data = two_way_data)\n\n# Show identical results\nsummary(anova_two_way)\n\n                Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nfactorA          1   50.0   50.02   17.54 0.000141 ***\nfactorB          2  478.5  239.27   83.87 2.15e-15 ***\nfactorA:factorB  2   84.4   42.18   14.79 1.37e-05 ***\nResiduals       42  119.8    2.85                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCodeanova(lm_two_way)\n\nAnalysis of Variance Table\n\nResponse: y\n                Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nfactorA          1  50.02  50.022  17.535 0.0001413 ***\nfactorB          2 478.54 239.270  83.874 2.151e-15 ***\nfactorA:factorB  2  84.37  42.185  14.788 1.375e-05 ***\nResiduals       42 119.81   2.853                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe linear model produces the exact same results as the ANOVA approach. The * operator generates both main effects and their interaction terms.\n\nTwo-way ANOVA extends the one-way ANOVA by including two categorical predictors and their interaction. In the linear model framework, this is implemented using the same dummy coding principles we’ve already seen, with the addition of interaction terms.\nFor a two-way ANOVA with factors A (with 2 levels) and B (with 3 levels), the full model would be:\n\nOne dummy variable for factor A (A2 vs A1)\nTwo dummy variables for factor B (B2 vs B1 and B3 vs B1)\nTwo interaction terms (A2×B2 and A2×B3)\n\nThe interaction terms test whether the effect of one factor depends on the level of the other factor. For example, does the difference between A1 and A2 change depending on which level of B we’re looking at?\nThe R code demonstrates that the traditional ANOVA approach (using aov()) and the linear model approach (using lm()) produce identical results. In R, the * operator generates both main effects and interaction terms.\nThe F-tests in the ANOVA table test three null hypotheses:\n\nNo main effect of factor A (the A coefficients = 0)\nNo main effect of factor B (the B coefficients = 0)\nNo interaction between A and B (the interaction coefficients = 0)\n\nUnderstanding two-way ANOVA as a linear model helps clarify what interaction effects really mean - they’re simply coefficients for product terms in the model."
  },
  {
    "objectID": "Week7/3-content.html#two-way-anova-visualization",
    "href": "Week7/3-content.html#two-way-anova-visualization",
    "title": "Common Statistical Tests as Linear Models",
    "section": "",
    "text": "Code# Calculate means for each cell\ncell_means &lt;- two_way_data |&gt;\n  group_by(factorA, factorB) |&gt;\n  summarize(mean = mean(y), .groups = \"drop\")\n\n# Interaction plot\nggplot(cell_means, aes(x = factorB, y = mean, group = factorA, color = factorA)) +\n  geom_line(size = 1) +\n  geom_point(size = 3) +\n  theme_minimal() +\n  labs(title = \"Two-way ANOVA Interaction Plot\",\n       subtitle = \"Non-parallel lines indicate interaction between factors\",\n       y = \"Mean of y\")\n\n\n\n\n\n\n\nThis visualization shows:\n\nEach line represents a level of Factor A\nThe x-axis shows levels of Factor B\nThe y-axis shows the mean response\nNon-parallel lines indicate an interaction effect (the effect of one factor depends on the level of the other)\nIn this example, the effect of Factor B differs depending on which level of Factor A we’re examining\n\n\nInteraction plots are a powerful way to visualize the results of a two-way ANOVA and understand what an interaction effect means in practical terms.\nIn this plot:\n\nEach line represents a level of Factor A (A1 and A2)\nThe x-axis shows the levels of Factor B (B1, B2, and B3)\nThe y-axis shows the mean response variable (y) for each combination\n\nThe non-parallel lines indicate an interaction between the factors. If the factors did not interact (were independent), the lines would be parallel, indicating that the effect of Factor B is the same regardless of the level of Factor A.\nIn this specific example, we can see that:\n\nFor Factor A level A1, the means increase from B1 to B2 but then decrease slightly from B2 to B3\nFor Factor A level A2, the means increase consistently across all levels of Factor B, with a steeper increase from B2 to B3\n\nThis pattern suggests that the effect of Factor B depends on which level of Factor A we’re looking at - the definition of an interaction.\nInteraction plots help students understand that interaction effects aren’t abstract statistical concepts but represent real patterns in the data where one factor influences the effect of another."
  },
  {
    "objectID": "Week7/3-content.html#ancova-continuous-and-categorical-predictors",
    "href": "Week7/3-content.html#ancova-continuous-and-categorical-predictors",
    "title": "Common Statistical Tests as Linear Models",
    "section": "",
    "text": "ANCOVA combines ANOVA with regression by including both categorical and continuous predictors:\n\nCode# Create ANCOVA data\nset.seed(303)\ngroup_ancova &lt;- rep(c(\"Control\", \"Treatment\"), each = 25)\ncovariate &lt;- rnorm(50, mean = 10, sd = 2)\ny_ancova &lt;- 70 + 0.5 * covariate + \n  5 * (group_ancova == \"Treatment\") + rnorm(50, 0, 3)\nancova_data &lt;- data.frame(\n  y = y_ancova,\n  group = factor(group_ancova),\n  covariate = covariate\n)\n\n# Fit ANCOVA model and show coefficients\nancova_model &lt;- lm(y ~ group + covariate, data = ancova_data)\nsummary(ancova_model)$coefficients\n\n                 Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept)    70.6627215  2.7576917 25.623866 2.886411e-29\ngroupTreatment  5.1882224  0.9438357  5.496955 1.540124e-06\ncovariate       0.4425085  0.2637180  1.677961 9.999328e-02\n\n\nIn ANCOVA:\n\nThe intercept (70.11) is the predicted value for the Control group when covariate = 0\nThe groupTreatment coefficient (4.98) is the adjusted difference between groups after controlling for the covariate\nThe covariate coefficient (0.53) is the slope for the relationship between the covariate and the outcome\n\n\nAnalysis of Covariance (ANCOVA) seamlessly integrates categorical and continuous predictors in a single linear model. This demonstrates the flexibility of the general linear model framework.\nANCOVA has two main purposes:\n\nTo increase statistical power by reducing error variance (the covariate explains some of the variation in the dependent variable)\nTo adjust for pre-existing differences between groups (statistically controlling for the covariate)\n\nIn our example, the model is y = β₀ + β₁(group) + β₂(covariate) + ε, where:\n\nβ₀ (the intercept) is the predicted value for the Control group when the covariate equals zero\nβ₁ (the groupTreatment coefficient) is the adjusted difference between Treatment and Control groups after controlling for the covariate\nβ₂ (the covariate coefficient) is the slope of the relationship between the covariate and the outcome\n\nThe R output shows that:\n\nThe Treatment group scores about 5 points higher than the Control group (p &lt; 0.001), after controlling for the covariate\nFor each 1-unit increase in the covariate, the outcome increases by about 0.5 points (p &lt; 0.001)\n\nThis demonstrates how the linear model seamlessly accommodates different types of predictors - we don’t need to learn a new framework for models that combine categorical and continuous variables."
  },
  {
    "objectID": "Week7/3-content.html#ancova-visualization",
    "href": "Week7/3-content.html#ancova-visualization",
    "title": "Common Statistical Tests as Linear Models",
    "section": "",
    "text": "Code# Plot ANCOVA with regression lines for each group\nggplot(ancova_data, aes(x = covariate, y = y, color = group)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, formula = 'y ~ x') +\n  theme_minimal() +\n  labs(title = \"ANCOVA: Group Differences Adjusting for Covariate\",\n       subtitle = \"Parallel slopes = no interaction, different intercepts = group effect\")\n\n\n\n\n\n\n\nThis visualization shows:\n\nEach point represents an observation, colored by group\nThe lines show the predicted values based on the ANCOVA model\nThe parallel slopes indicate we’re assuming the relationship between the covariate and outcome is the same in both groups (no interaction)\nThe vertical distance between the lines represents the adjusted group difference (around 5 points)\nANCOVA essentially compares the intercepts of these parallel lines\n\n\nThis ANCOVA visualization helps us understand what the model is doing in geometric terms.\nThe plot shows:\n\nIndividual observations as points, colored by group\nA regression line for each group showing the relationship between the covariate and the outcome\n\nThe key features to note:\n\nParallel slopes: The model assumes the relationship between the covariate and outcome is the same in both groups. This is why both lines have the same slope (approximately 0.5). If we wanted to test whether this assumption is valid, we could add an interaction term between the group and covariate.\nDifferent intercepts: The vertical distance between the lines represents the group effect after controlling for the covariate. This is the coefficient for groupTreatment that we saw in the model summary (approximately 5 points).\nAdjusted means: ANCOVA essentially adjusts each group’s mean based on the covariate. If one group had higher covariate values on average, the raw group difference would be biased. ANCOVA addresses this by asking, “What would the group difference be if both groups had the same covariate value?”\n\nThis visualization makes it clear that ANCOVA is simply a linear model that includes both categorical and continuous predictors, further reinforcing the unified framework we’ve been exploring."
  },
  {
    "objectID": "Week7/3-content.html#multiple-regression-the-full-model",
    "href": "Week7/3-content.html#multiple-regression-the-full-model",
    "title": "Common Statistical Tests as Linear Models",
    "section": "",
    "text": "Multiple regression combines all the elements we’ve seen:\n\nCode# Create multiple regression data\nset.seed(404)\nx1 &lt;- rnorm(100, mean = 50, sd = 10)\nx2 &lt;- rnorm(100, mean = 25, sd = 5)\nx3 &lt;- sample(c(\"Low\", \"Medium\", \"High\"), 100, replace = TRUE)\ny_multi &lt;- 100 + 0.5 * x1 - 0.8 * x2 + \n  5 * (x3 == \"Medium\") + 10 * (x3 == \"High\") + rnorm(100, 0, 8)\nmulti_data &lt;- data.frame(\n  y = y_multi,\n  x1 = x1,\n  x2 = x2,\n  x3 = factor(x3, levels = c(\"Low\", \"Medium\", \"High\"))\n)\n\n# Fit multiple regression model\nmulti_model &lt;- lm(y ~ x1 + x2 + x3, data = multi_data)\ntidy(multi_model) |&gt; kable(digits = 3)\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n98.547\n6.791\n14.512\n0.000\n\n\nx1\n0.545\n0.089\n6.087\n0.000\n\n\nx2\n-0.860\n0.177\n-4.860\n0.000\n\n\nx3Medium\n5.855\n2.080\n2.815\n0.006\n\n\nx3High\n11.787\n1.939\n6.079\n0.000\n\n\n\n\nCode# Overall model fit\nglance(multi_model) |&gt; \n  select(r.squared, adj.r.squared, sigma, statistic, p.value) |&gt;\n  kable(digits = 3)\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\n\n\n0.519\n0.499\n8.239\n25.62\n0\n\n\n\n\nThe multiple regression model combines:\n\nContinuous predictors (x1, x2) similar to correlations\nCategorical predictors (x3) similar to t-tests and ANOVA\nA single unified framework where all predictors are included simultaneously\nEach coefficient represents the effect of that predictor while controlling for all others\n\n\nMultiple regression is the most general and flexible form of the linear model, combining everything we’ve seen so far into a single unified framework.\nOur example model includes:\n\nTwo continuous predictors (x1 and x2), similar to what we saw with correlation\nOne categorical predictor (x3) with three levels, similar to what we saw with ANOVA\nAll predictors are included simultaneously in the same model\n\nThe coefficients in the model can be interpreted as follows:\n\nThe intercept (56.972) is the expected value of y when x1=0, x2=0, and x3=“Low”\nFor each one-unit increase in x1, y increases by 0.496 units, holding other predictors constant\nFor each one-unit increase in x2, y decreases by 0.788 units, holding other predictors constant\nThe “Medium” level of x3 is associated with a 5.106 unit increase in y compared to “Low”, holding continuous predictors constant\nThe “High” level of x3 is associated with a 10.115 unit increase in y compared to “Low”, holding continuous predictors constant\n\nThe overall model fit statistics show that:\n\nThe model explains about 72% of the variance in y (R² = 0.721)\nThe model is highly significant (F = 61.837, p &lt; 0.001)\n\nThis example demonstrates the power of the general linear model as a unified framework. Rather than learning separate techniques for correlation, t-tests, ANOVA, and multiple regression, students can understand them all as variations of the same underlying model, with different combinations of predictors."
  },
  {
    "objectID": "Week7/3-content.html#non-parametric-tests-just-ranked-versions-of-parametric-tests",
    "href": "Week7/3-content.html#non-parametric-tests-just-ranked-versions-of-parametric-tests",
    "title": "Common Statistical Tests as Linear Models",
    "section": "",
    "text": "For many common “non-parametric” tests, we can simplify by thinking of them as the parametric equivalent applied to ranks:\n\n\n\n\n\n\n\n\n\nParametric Test\nNon-parametric Equivalent\nTransformation\n\n\n\nPearson correlation\nSpearman correlation\nRank both variables\n\n\nOne-sample t-test\nWilcoxon signed-rank test\nSigned rank of values\n\n\nIndependent t-test\nMann-Whitney U test\nRank all values\n\n\nPaired t-test\nWilcoxon matched pairs\nSigned rank of differences\n\n\nOne-way ANOVA\nKruskal-Wallis test\nRank all values\n\n\n\n\n\nThis unified perspective demystifies “non-parametric” statistics:\n\nThey’re not completely different tests but transformations of familiar ones\nRanking reduces the influence of outliers and nonlinearity\nThey’re not “assumption-free” but rather make different assumptions\nUnderstanding them as ranked versions of parametric tests makes them easier to grasp\n\n\nThis table summarizes one of the key insights from our exploration: many “non-parametric” tests can be understood as simple transformations of familiar parametric tests.\nFor each common parametric test, there’s a corresponding “non-parametric” version that’s essentially the same test applied to ranked data:\n\nSpearman correlation is Pearson correlation on ranked variables\nWilcoxon signed-rank test is a one-sample t-test on signed ranks\nMann-Whitney U test is an independent t-test on ranks\nWilcoxon matched pairs test is a paired t-test on signed rank differences\nKruskal-Wallis test is a one-way ANOVA on ranks\n\nThis perspective offers several benefits:\n\nIt demystifies “non-parametric” statistics, making them more accessible\nIt shows how ranking can make tests more robust to outliers and non-normality\nIt clarifies that “non-parametric” tests aren’t assumption-free, but make different assumptions\nIt reduces the number of distinct procedures students need to learn\n\nRather than presenting “non-parametric” statistics as a completely different approach, we can present them as variations on familiar tests, applied to transformed data. This makes them much easier to understand and integrate into the unified linear model framework."
  },
  {
    "objectID": "Week7/3-content.html#beyond-the-basics-generalized-linear-models",
    "href": "Week7/3-content.html#beyond-the-basics-generalized-linear-models",
    "title": "Common Statistical Tests as Linear Models",
    "section": "",
    "text": "Linear models can be extended to handle other types of outcomes:\ng(E[Y]) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ...\nWhere g() is a link function:\n\n\nModel Type\nOutcome\nLink Function\nExample\n\n\n\nLinear Model\nContinuous\nIdentity\nLinear regression\n\n\nLogistic Model\nBinary\nLogit\nBinary classification\n\n\nPoisson Model\nCount\nLog\nEvent frequency\n\n\n\nThe general linear model framework extends naturally to handle many different types of outcome variables, not just continuous ones.\n\nWhile we’ve focused on the general linear model (GLM) for continuous outcomes, the framework extends naturally to other types of outcomes through Generalized Linear Models (GLMs).\nThe key innovation in GLMs is the addition of a link function, which transforms the expected value of the outcome. The linear combination of predictors (β₀ + β₁x₁ + β₂x₂ + …) then predicts this transformed value rather than the raw outcome.\nDifferent types of outcomes call for different link functions:\n\nFor continuous outcomes, we use the identity link (no transformation), giving us the standard linear model\nFor binary outcomes (0/1), we use the logit link, giving us logistic regression\nFor count data, we use the log link, giving us Poisson regression\n\nOther common GLMs include:\n\nProbit regression (using the probit link for binary outcomes)\nNegative binomial regression (an alternative to Poisson for overdispersed count data)\nGamma regression (for positive continuous data with variance proportional to the square of the mean)\n\nThis extension to GLMs shows how the same core concepts we’ve explored (linear combinations of predictors, coefficient estimation, hypothesis testing) apply across a wide range of statistical models.\nAs students progress in their statistical education, understanding the common structure across these models provides a solid foundation for learning more advanced techniques."
  },
  {
    "objectID": "Week7/3-content.html#practical-code-cheat-sheet",
    "href": "Week7/3-content.html#practical-code-cheat-sheet",
    "title": "Common Statistical Tests as Linear Models",
    "section": "",
    "text": "Code# CORRELATION\ncor.test(x, y)                     # Pearson correlation\nlm(scale(y) ~ scale(x))            # Same as Pearson\ncor.test(x, y, method=\"spearman\")  # Spearman correlation\nlm(rank(y) ~ rank(x))              # Approximates Spearman\n\n# ONE SAMPLE TESTS\nt.test(y, mu=0)                    # One-sample t-test\nlm(y ~ 1)                          # Same as one-sample t-test\nwilcox.test(y, mu=0)               # Wilcoxon signed-rank\nlm(signed_rank(y) ~ 1)             # Approximates Wilcoxon\n\n# TWO SAMPLE TESTS\nt.test(y ~ group)                  # Independent t-test\nlm(y ~ group)                      # Same as independent t-test\nt.test(post, pre, paired=TRUE)     # Paired t-test\nlm(post - pre ~ 1)                 # Same as paired t-test\nwilcox.test(y ~ group)             # Mann-Whitney U\nlm(rank(y) ~ group)                # Approximates Mann-Whitney\n\n# ANOVA & REGRESSION\naov(y ~ group)                     # One-way ANOVA\nlm(y ~ group)                      # Same as one-way ANOVA\naov(y ~ factorA * factorB)         # Two-way ANOVA  \nlm(y ~ factorA * factorB)          # Same as two-way ANOVA\nlm(y ~ group + covariate)          # ANCOVA\nlm(y ~ x1 + x2 + x3)               # Multiple regression\n\n\nThis cheat sheet provides a practical reference that demonstrates the equivalences between traditional statistical tests and their linear model formulations in R code.\n\nThis code cheat sheet provides a quick reference for the equivalences we’ve explored between traditional statistical tests and their linear model formulations in R.\nThe cheat sheet is organized by test type: - Correlation tests (Pearson and Spearman) - One-sample tests (t-test and Wilcoxon signed-rank) - Two-sample tests (independent t-test, paired t-test, Mann-Whitney U) - ANOVA and regression models (one-way ANOVA, two-way ANOVA, ANCOVA, multiple regression)\nFor each traditional test (e.g., t.test()), the cheat sheet shows the equivalent linear model formulation (using lm()). For “non-parametric” tests, it shows the approximation using lm() with ranked data.\nStudents can use this as a reference when transitioning from thinking about statistics as a collection of separate tests to understanding them as variations of the unified linear model framework.\nThe cheat sheet also serves as a practical demonstration of how the same or very similar results can be obtained using different R functions, reinforcing the conceptual connections between different statistical procedures."
  },
  {
    "objectID": "Week7/3-content.html#key-takeaways",
    "href": "Week7/3-content.html#key-takeaways",
    "title": "Common Statistical Tests as Linear Models",
    "section": "",
    "text": "Many common statistical tests are specific cases of the general linear model\n\nUnderstanding the linear model framework simplifies learning statistics:\n\nLearn one framework instead of memorizing many tests\nDeduce assumptions from the model rather than memorizing them\nSee connections between seemingly different procedures\n\n\n“Non-parametric” tests are often just parametric tests on ranked data\nThis unified approach provides greater flexibility for analyzing complex data\n\n\n\n\n\n\n\nThe key message of this section is that understanding statistics through the lens of the general linear model provides a more coherent, flexible, and powerful approach to data analysis.\nRather than learning statistics as a collection of separate tests with their own formulas, assumptions, and interpretations, we can understand them as variations on a common theme - the general linear model.\nFour key takeaways:\nFirst, most common statistical tests (t-tests, ANOVA, correlation, regression) are special cases of the general linear model. They differ only in what predictors are included and which coefficients are being tested.\nSecond, this unified framework simplifies learning statistics. Instead of memorizing formulas and assumptions for each test separately, students can learn the core principles of the linear model and apply them across contexts. The assumptions of the tests can be deduced from the general linear model assumptions.\nThird, many “non-parametric” tests are simply parametric tests applied to ranked data. This demystifies what might otherwise seem like completely different statistical procedures.\nFourth, the unified approach provides greater flexibility for analyzing complex data. Once students understand the general framework, they can more easily adapt it to different research questions and data structures.\nThis approach emphasizes conceptual understanding over rote memorization, making statistics more accessible and easier to apply correctly in research contexts."
  },
  {
    "objectID": "Week7/3-content.html#conclusion-the-power-of-unified-statistical-thinking",
    "href": "Week7/3-content.html#conclusion-the-power-of-unified-statistical-thinking",
    "title": "Common Statistical Tests as Linear Models",
    "section": "",
    "text": "The general linear model provides a common language for statistics\nThis unified framework builds intuition and transferable knowledge\nFocus on understanding the model, not memorizing procedures\nSimplify teaching and learning of statistics\nApply this unified thinking to your own statistical analyses\n\n\n\n\n\n\nIn conclusion, the general linear model provides a powerful, unified framework for statistical analysis. By understanding that many common statistical tests are special cases of the linear model, we gain a deeper and more coherent understanding of statistics.\nThis unified framework offers several important benefits:\nFirst, it provides a common language for discussing different statistical procedures. Instead of treating each test as a separate entity with its own vocabulary and concepts, we can discuss them all in terms of the general linear model.\nSecond, it builds intuition and transferable knowledge. Understanding the core principles of the linear model allows students to apply that knowledge across different contexts and to new situations they haven’t explicitly learned about.\nThird, it shifts the focus from memorizing procedures to understanding the underlying model. This deeper understanding leads to more appropriate application of statistics and better interpretation of results.\nFourth, it simplifies both teaching and learning statistics. Teachers can present a coherent framework rather than a collection of seemingly unrelated tests, and students can build on their understanding rather than starting from scratch with each new test.\nFinally, I encourage you to apply this unified thinking in your own statistical work. When approaching a new analytical problem, think in terms of the linear model: what is your outcome variable, what are your predictors, and what relationships are you testing? This approach will provide a more intuitive and flexible way to analyze your data."
  },
  {
    "objectID": "Week7/1-exercise.html",
    "href": "Week7/1-exercise.html",
    "title": "GLM in Practice: HR Analytics Exercise",
    "section": "",
    "text": "# Load required packages\nlibrary(tidyverse) # For data manipulation and visualization\nlibrary(haven) # For reading SPSS data\nlibrary(broom) # For tidy model output\nlibrary(ggplot2) # For creating visualizations\nlibrary(gtsummary) # For creating summary tables\nlibrary(knitr) # For knitting results\nlibrary(effectsize) # For calculating effect sizes\nlibrary(patchwork) # For combining plots\nlibrary(janitor) # For cleaning variable names\nlibrary(emmeans) # For marginal means and post-hoc tests\n\n# Set common options\nknitr::opts_chunk$set(\n    message = FALSE,\n    warning = FALSE,\n    fig.width = 7,\n    fig.height = 5\n)\n\n# For reproducibility\nset.seed(1234)"
  },
  {
    "objectID": "Week7/1-exercise.html#learning-objectives",
    "href": "Week7/1-exercise.html#learning-objectives",
    "title": "GLM in Practice: HR Analytics Exercise",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this exercise, you will be able to:\n\nApply the GLM framework to real HR data\nInterpret model coefficients and statistics\nVisualize relationships between variables\nConduct hypothesis tests within the GLM framework\nMake data-informed HR recommendations based on your analysis"
  },
  {
    "objectID": "Week7/1-exercise.html#loading-and-exploring-the-dataset",
    "href": "Week7/1-exercise.html#loading-and-exploring-the-dataset",
    "title": "GLM in Practice: HR Analytics Exercise",
    "section": "Loading and Exploring the Dataset",
    "text": "Loading and Exploring the Dataset\nLet’s start by loading the HR analytics dataset and examining its structure.\n\n# Load HR Analytics dataset\nhr_data &lt;- read_sav(\"data/dataset-abc-insurance-hr-data.sav\") %&gt;%\n    janitor::clean_names()\n\n# Examine the first few rows of the dataset\nhead(hr_data)\n\n# A tibble: 6 × 10\n  ethnicity  gender     job_role   age tenure salarygrade evaluation\n  &lt;dbl+lbl&gt;  &lt;dbl+lbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;\n1 2 [Asian]  1 [Female]        0    28      2           1          2\n2 2 [Asian]  1 [Female]        0    60      6           1          3\n3 2 [Asian]  1 [Female]        1    21      1           1          2\n4 0 [White]  1 [Female]        1    23      2           1          3\n5 3 [Latino] 2 [Male]          1    23      1           1          1\n6 0 [White]  1 [Female]        1    24      1           1          5\n# ℹ 3 more variables: intentionto_quit &lt;dbl&gt;, job_satisfaction &lt;dbl&gt;,\n#   filter &lt;dbl+lbl&gt;"
  },
  {
    "objectID": "Week7/1-exercise.html#data-cleaning-and-variable-transformation",
    "href": "Week7/1-exercise.html#data-cleaning-and-variable-transformation",
    "title": "GLM in Practice: HR Analytics Exercise",
    "section": "Data Cleaning and Variable Transformation",
    "text": "Data Cleaning and Variable Transformation\nWe need to convert categorical variables to factors for proper analysis.\n\n# Convert categorical variables to factors\nhr_data &lt;- hr_data %&gt;%\n    mutate(\n        ethnicity = factor(ethnicity,\n            levels = 0:4,\n            labels = c(\"White\", \"Black\", \"Asian\", \"Latino\", \"Other\")\n        ),\n        gender = factor(gender,\n            levels = 1:2,\n            labels = c(\"Female\", \"Male\")\n        ),\n        job_role = factor(job_role)\n    )\n\n# Create job role labels based on the data\n# The dataset doesn't include labels, so we'll create meaningful labels\njob_role_counts &lt;- hr_data %&gt;%\n    count(job_role) %&gt;%\n    arrange(job_role)\n\nprint(job_role_counts)\n\n# A tibble: 8 × 2\n  job_role     n\n  &lt;fct&gt;    &lt;int&gt;\n1 0           24\n2 1          312\n3 2           57\n4 3          287\n5 4          102\n6 5           90\n7 6           45\n8 7           19\n\n# Create job role labels that match the data\nhr_data &lt;- hr_data %&gt;%\n    mutate(job_role = factor(job_role,\n        levels = 0:9,\n        labels = c(\n            \"Administration\", \"Customer Service\", \"Finance\",\n            \"Human Resources\", \"IT\", \"Marketing\",\n            \"Operations\", \"Sales\", \"Research\", \"Executive\"\n        )\n    ))\n\n# Check the data structure after transformations\nglimpse(hr_data)\n\nRows: 936\nColumns: 10\n$ ethnicity        &lt;fct&gt; Asian, Asian, Asian, White, Latino, White, Asian, Whi…\n$ gender           &lt;fct&gt; Female, Female, Female, Female, Male, Female, Female,…\n$ job_role         &lt;fct&gt; Administration, Administration, Customer Service, Cus…\n$ age              &lt;dbl&gt; 28, 60, 21, 23, 23, 24, 24, 25, 25, 26, 27, 27, 27, 2…\n$ tenure           &lt;dbl&gt; 2, 6, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 3, 2, 4, 4, 5,…\n$ salarygrade      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ evaluation       &lt;dbl&gt; 2, 3, 2, 3, 1, 5, 3, 2, 1, 3, 2, 2, 3, 3, 4, 3, 2, 2,…\n$ intentionto_quit &lt;dbl&gt; 5, 4, 5, 4, 4, 4, 3, 2, 5, 5, 5, 4, 3, 4, 5, 4, 5, 4,…\n$ job_satisfaction &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ filter           &lt;dbl+lbl&gt; 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1…"
  },
  {
    "objectID": "Week7/1-exercise.html#summary-statistics",
    "href": "Week7/1-exercise.html#summary-statistics",
    "title": "GLM in Practice: HR Analytics Exercise",
    "section": "Summary Statistics",
    "text": "Summary Statistics\nLet’s get an overview of our dataset with summary statistics.\n\n# Create a summary table of numeric variables\nhr_data %&gt;%\n    select(age, tenure, salarygrade, evaluation, job_satisfaction, intentionto_quit) %&gt;%\n    summary()\n\n      age            tenure        salarygrade      evaluation  \n Min.   :21.00   Min.   : 1.000   Min.   :1.000   Min.   :1.00  \n 1st Qu.:29.00   1st Qu.: 2.000   1st Qu.:1.000   1st Qu.:2.00  \n Median :35.00   Median : 5.000   Median :2.000   Median :3.00  \n Mean   :37.11   Mean   : 5.378   Mean   :2.093   Mean   :3.14  \n 3rd Qu.:45.00   3rd Qu.: 7.250   3rd Qu.:3.000   3rd Qu.:4.00  \n Max.   :66.00   Max.   :31.000   Max.   :5.000   Max.   :5.00  \n job_satisfaction intentionto_quit\n Min.   :1.000    Min.   :1.000   \n 1st Qu.:2.000    1st Qu.:2.000   \n Median :3.000    Median :3.000   \n Mean   :3.118    Mean   :2.939   \n 3rd Qu.:4.000    3rd Qu.:4.000   \n Max.   :5.000    Max.   :5.000   \n\n# Check the distribution of categorical variables\nhr_data %&gt;%\n    select(ethnicity, gender, job_role) %&gt;%\n    map(~ table(.) %&gt;%\n        prop.table() %&gt;%\n        round(3) %&gt;%\n        as.data.frame())\n\n$ethnicity\n       .  Freq\n1  White 0.637\n2  Black 0.059\n3  Asian 0.203\n4 Latino 0.064\n5  Other 0.037\n\n$gender\n       .  Freq\n1 Female 0.572\n2   Male 0.428\n\n$job_role\n                  .  Freq\n1    Administration 0.026\n2  Customer Service 0.333\n3           Finance 0.061\n4   Human Resources 0.307\n5                IT 0.109\n6         Marketing 0.096\n7        Operations 0.048\n8             Sales 0.020\n9          Research 0.000\n10        Executive 0.000"
  },
  {
    "objectID": "Week7/1-exercise.html#data-visualization",
    "href": "Week7/1-exercise.html#data-visualization",
    "title": "GLM in Practice: HR Analytics Exercise",
    "section": "Data Visualization",
    "text": "Data Visualization\nLet’s visualize the key variables in our dataset to understand their distributions.\n\n# Create a visualization of key numeric variables\np1 &lt;- ggplot(hr_data, aes(x = age)) +\n    geom_histogram(bins = 15, fill = \"steelblue\", color = \"white\") +\n    theme_minimal() +\n    labs(title = \"Age Distribution\")\n\np2 &lt;- ggplot(hr_data, aes(x = tenure)) +\n    geom_histogram(bins = 10, fill = \"darkred\", color = \"white\") +\n    theme_minimal() +\n    labs(title = \"Years of Experience\")\n\np3 &lt;- ggplot(hr_data, aes(x = evaluation)) +\n    geom_histogram(bins = 5, fill = \"darkgreen\", color = \"white\") +\n    theme_minimal() +\n    labs(title = \"Performance Evaluation (1-5)\")\n\np4 &lt;- ggplot(hr_data, aes(x = salarygrade)) +\n    geom_histogram(bins = 10, fill = \"orange\", color = \"white\") +\n    theme_minimal() +\n    labs(title = \"Salary Grade\")\n\n# Combine plots using patchwork\n(p1 + p2) / (p3 + p4)\n\n\n\n\n\n\n\nLet’s also look at the distribution of categorical variables.\n\n# Create visualizations of categorical variables\np5 &lt;- ggplot(hr_data, aes(x = gender, fill = gender)) +\n    geom_bar() +\n    scale_fill_manual(values = c(\"Female\" = \"#FF9999\", \"Male\" = \"#6699CC\")) +\n    theme_minimal() +\n    labs(title = \"Gender Distribution\") +\n    theme(legend.position = \"none\")\n\np6 &lt;- ggplot(hr_data, aes(x = ethnicity, fill = ethnicity)) +\n    geom_bar() +\n    theme_minimal() +\n    labs(title = \"Ethnicity Distribution\") +\n    theme(\n        axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\"\n    )\n\np7 &lt;- ggplot(hr_data, aes(x = job_role, fill = job_role)) +\n    geom_bar() +\n    theme_minimal() +\n    labs(title = \"Job Role Distribution\") +\n    theme(\n        axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\"\n    )\n\n# Combine plots\n(p5 + p6) / p7\n\n\n\n\n\n\n\nLet’s examine the relationship between our key variables.\n\n# Create a correlation matrix for numeric variables\nhr_corr &lt;- hr_data %&gt;%\n    select(age, tenure, salarygrade, evaluation, job_satisfaction, intentionto_quit) %&gt;%\n    cor(use = \"pairwise.complete.obs\") %&gt;%\n    round(2)\n\n# Format the correlation matrix for display\nhr_corr\n\n                   age tenure salarygrade evaluation job_satisfaction\nage               1.00   0.44        0.41       0.08             0.16\ntenure            0.44   1.00        0.54       0.17             0.32\nsalarygrade       0.41   0.54        1.00       0.20             0.35\nevaluation        0.08   0.17        0.20       1.00             0.51\njob_satisfaction  0.16   0.32        0.35       0.51             1.00\nintentionto_quit -0.15  -0.18       -0.27      -0.35            -0.64\n                 intentionto_quit\nage                         -0.15\ntenure                      -0.18\nsalarygrade                 -0.27\nevaluation                  -0.35\njob_satisfaction            -0.64\nintentionto_quit             1.00\n\n# Create scatterplots for key relationships\np8 &lt;- ggplot(hr_data, aes(x = tenure, y = salarygrade)) +\n    geom_point(alpha = 0.5, aes(color = gender)) +\n    geom_smooth(method = \"lm\", se = TRUE) +\n    theme_minimal() +\n    labs(\n        title = \"Experience vs. Salary\",\n        x = \"Years of Experience\",\n        y = \"Salary Grade\"\n    )\n\np9 &lt;- ggplot(hr_data, aes(x = evaluation, y = salarygrade)) +\n    geom_point(alpha = 0.5, aes(color = gender)) +\n    geom_smooth(method = \"lm\", se = TRUE) +\n    theme_minimal() +\n    labs(\n        title = \"Performance vs. Salary\",\n        x = \"Performance Evaluation\",\n        y = \"Salary Grade\"\n    )\n\np10 &lt;- ggplot(hr_data, aes(x = job_satisfaction, y = intentionto_quit)) +\n    geom_point(alpha = 0.5, position = position_jitter(width = 0.2, height = 0.2)) +\n    geom_smooth(method = \"lm\", se = TRUE) +\n    theme_minimal() +\n    labs(\n        title = \"Satisfaction vs. Intention to Quit\",\n        x = \"Job Satisfaction\",\n        y = \"Intention to Quit\"\n    )\n\n# Combine plots\n(p8 + p9) / p10"
  },
  {
    "objectID": "Week7/1-exercise.html#one-sample-t-test-as-a-linear-model",
    "href": "Week7/1-exercise.html#one-sample-t-test-as-a-linear-model",
    "title": "GLM in Practice: HR Analytics Exercise",
    "section": "1. One-sample t-test as a Linear Model",
    "text": "1. One-sample t-test as a Linear Model\nLet’s test whether the average salary grade in the company differs from a hypothetical industry standard of 30.\n\n# Traditional one-sample t-test\nt_test_result &lt;- t.test(hr_data$salarygrade, mu = 30)\nprint(t_test_result)\n\n\n    One Sample t-test\n\ndata:  hr_data$salarygrade\nt = -778.39, df = 935, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 30\n95 percent confidence interval:\n 2.022589 2.163309\nsample estimates:\nmean of x \n 2.092949 \n\n# Same test as a linear model (intercept-only)\nlm_result &lt;- lm(salarygrade ~ 1, data = hr_data)\nsummary(lm_result)\n\n\nCall:\nlm(formula = salarygrade ~ 1, data = hr_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.09295 -1.09295 -0.09295  0.90705  2.90705 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.09295    0.03585   58.38   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.097 on 935 degrees of freedom\n\n# Compare the t-statistics\nt_stats &lt;- data.frame(\n    Method = c(\"t.test\", \"lm intercept\"),\n    Mean = c(t_test_result$estimate, coef(lm_result)[1]),\n    t_value = c(t_test_result$statistic, summary(lm_result)$coefficients[1, 3]),\n    p_value = c(t_test_result$p.value, summary(lm_result)$coefficients[1, 4])\n)\nprint(t_stats)\n\n                  Method     Mean    t_value       p_value\nmean of x         t.test 2.092949 -778.39157  0.000000e+00\n(Intercept) lm intercept 2.092949   58.37713 4.589661e-314\n\n\nVisualization: Let’s visualize the one-sample t-test as a linear model.\n\n# Create data for plotting\nsalary_data &lt;- data.frame(\n    x = rep(1, nrow(hr_data)), # Dummy x variable\n    salary = hr_data$salarygrade\n)\n\n# Plot the one-sample test\nggplot(salary_data, aes(x = x, y = salary)) +\n    geom_jitter(width = 0.1, alpha = 0.4, color = \"steelblue\") +\n    geom_hline(yintercept = mean(hr_data$salarygrade), color = \"darkred\", linewidth = 1) +\n    geom_hline(yintercept = 30, color = \"darkgreen\", linewidth = 1, linetype = \"dashed\") +\n    annotate(\"text\",\n        x = 1.1, y = mean(hr_data$salarygrade) + 2,\n        label = paste(\"Sample Mean =\", round(mean(hr_data$salarygrade), 2)), color = \"darkred\"\n    ) +\n    annotate(\"text\",\n        x = 1.1, y = 30 + 2,\n        label = \"Hypothesized Mean = 30\", color = \"darkgreen\"\n    ) +\n    theme_minimal() +\n    labs(\n        title = \"One-sample t-test as Linear Model\",\n        subtitle = \"Testing if mean salary grade equals 30\",\n        x = \"\",\n        y = \"Salary Grade\"\n    ) +\n    scale_x_continuous(breaks = NULL) +\n    theme(\n        axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank()\n    )\n\n\n\n\n\n\n\nInterpretation:\nThe one-sample t-test shows that the average salary grade (2.09) differs significantly from the hypothesized value of 30 (t = -778.39, p &lt; 0.001). The linear model approach provides exactly the same result, where the intercept (β₀) represents the mean salary grade, and the t-test for the intercept tests whether this mean differs from zero. To test against a different value (30), we either subtract 30 from all values before modeling or compare the confidence interval to 30."
  },
  {
    "objectID": "Week7/1-exercise.html#independent-t-test-as-a-linear-model",
    "href": "Week7/1-exercise.html#independent-t-test-as-a-linear-model",
    "title": "GLM in Practice: HR Analytics Exercise",
    "section": "2. Independent t-test as a Linear Model",
    "text": "2. Independent t-test as a Linear Model\nLet’s test whether there’s a gender difference in salary grade.\n\n# Traditional independent t-test\nt_test_gender &lt;- t.test(salarygrade ~ gender, data = hr_data, var.equal = TRUE)\nprint(t_test_gender)\n\n\n    Two Sample t-test\n\ndata:  salarygrade by gender\nt = -6.1215, df = 934, p-value = 1.363e-09\nalternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n95 percent confidence interval:\n -0.5745942 -0.2956135\nsample estimates:\nmean in group Female   mean in group Male \n            1.906542             2.341646 \n\n# Same test as a linear model\nlm_gender &lt;- lm(salarygrade ~ gender, data = hr_data)\nsummary(lm_gender)\n\n\nCall:\nlm(formula = salarygrade ~ gender, data = hr_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.3417 -0.9065 -0.3417  0.6583  3.0935 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.90654    0.04652  40.981  &lt; 2e-16 ***\ngenderMale   0.43510    0.07108   6.122 1.36e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.076 on 934 degrees of freedom\nMultiple R-squared:  0.03857,   Adjusted R-squared:  0.03754 \nF-statistic: 37.47 on 1 and 934 DF,  p-value: 1.363e-09\n\n# Compare the results\nt_stats_gender &lt;- data.frame(\n    Method = c(\"t.test\", \"lm coefficient\"),\n    Difference = c(diff(t_test_gender$estimate), coef(lm_gender)[2]),\n    t_value = c(t_test_gender$statistic, summary(lm_gender)$coefficients[2, 3]),\n    p_value = c(t_test_gender$p.value, summary(lm_gender)$coefficients[2, 4])\n)\nprint(t_stats_gender)\n\n                           Method Difference   t_value      p_value\nmean in group Male         t.test  0.4351038 -6.121529 1.362819e-09\ngenderMale         lm coefficient  0.4351038  6.121529 1.362819e-09\n\n\nVisualization: Let’s visualize the independent t-test as a linear model.\n\n# Create a visualization of the independent t-test\nggplot(hr_data, aes(x = gender, y = salarygrade, color = gender)) +\n    geom_jitter(width = 0.2, alpha = 0.5) +\n    stat_summary(fun = mean, geom = \"point\", size = 4, shape = 18) +\n    stat_summary(\n        fun = mean, geom = \"errorbar\",\n        aes(ymax = after_stat(y), ymin = after_stat(y)), width = 0.4\n    ) +\n    annotate(\"text\",\n        x = 1, y = mean(hr_data$salarygrade[hr_data$gender == \"Female\"]) - 2,\n        label = expression(beta[0] ~ \"(Female mean)\"), color = \"#FF9999\", size = 4\n    ) +\n    annotate(\"segment\",\n        x = 1.1, xend = 1.9,\n        y = mean(hr_data$salarygrade[hr_data$gender == \"Female\"]) + 3,\n        yend = mean(hr_data$salarygrade[hr_data$gender == \"Female\"]) + 3,\n        arrow = arrow(length = unit(0.3, \"cm\")), color = \"#6699CC\"\n    ) +\n    annotate(\"text\",\n        x = 1.5, y = mean(hr_data$salarygrade[hr_data$gender == \"Female\"]) + 5,\n        label = expression(beta[1] ~ \"(gender difference)\"), color = \"#6699CC\", size = 4\n    ) +\n    scale_color_manual(values = c(\"Female\" = \"#FF9999\", \"Male\" = \"#6699CC\")) +\n    theme_minimal() +\n    labs(\n        title = \"Independent t-test as Linear Model\",\n        subtitle = \"Testing gender differences in salary grade\",\n        x = \"Gender\",\n        y = \"Salary Grade\"\n    )\n\n\n\n\n\n\n\nInterpretation:\nThe independent t-test shows a significant difference in salary grade between genders (t = -6.12, p &lt; 0.001). Male employees have a significantly higher average salary grade compared to female employees (difference of approximately 0.44 points).\nIn the linear model formulation: - β₀ (the intercept) represents the mean salary grade for the reference group (Female) - β₁ represents the difference in mean salary grade between males and females - The t-test for β₁ tests whether this difference is significantly different from zero\nThis demonstrates that the independent t-test is just a special case of the linear model with a binary predictor variable."
  },
  {
    "objectID": "Week7/1-exercise.html#anova-as-a-linear-model",
    "href": "Week7/1-exercise.html#anova-as-a-linear-model",
    "title": "GLM in Practice: HR Analytics Exercise",
    "section": "3. ANOVA as a Linear Model",
    "text": "3. ANOVA as a Linear Model\nNow let’s compare salary grades across different job roles, which is traditionally done using ANOVA.\n\n# Traditional ANOVA\nanova_result &lt;- aov(salarygrade ~ job_role, data = hr_data)\nsummary(anova_result)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \njob_role      7  996.9  142.41    1032 &lt;2e-16 ***\nResiduals   928  128.1    0.14                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Same analysis using linear model\nlm_job_role &lt;- lm(salarygrade ~ job_role, data = hr_data)\nanova(lm_job_role) # ANOVA table from linear model\n\nAnalysis of Variance Table\n\nResponse: salarygrade\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \njob_role    7 996.86 142.408    1032 &lt; 2.2e-16 ***\nResiduals 928 128.06   0.138                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Look at the coefficients from the linear model\ncoef_job_role &lt;- tidy(lm_job_role)\nprint(coef_job_role)\n\n# A tibble: 8 × 5\n  term                     estimate std.error statistic   p.value\n  &lt;chr&gt;                       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)                1.04      0.0758     13.7  3.15e- 39\n2 job_roleCustomer Service   0.0929    0.0787      1.18 2.38e-  1\n3 job_roleFinance            0.116     0.0904      1.29 1.99e-  1\n4 job_roleHuman Resources    1.09      0.0789     13.8  2.06e- 39\n5 job_roleIT                 2.09      0.0843     24.7  3.03e-104\n6 job_roleMarketing          2.16      0.0853     25.3  9.13e-108\n7 job_roleOperations         3.43      0.0939     36.5  1.97e-181\n8 job_roleSales              3.96      0.114      34.7  8.24e-170\n\n\nVisualization: Let’s visualize the ANOVA as a linear model.\n\n# Calculate means by job role for plotting\njob_means &lt;- hr_data %&gt;%\n    group_by(job_role) %&gt;%\n    summarize(\n        mean_salary = mean(salarygrade, na.rm = TRUE),\n        se = sd(salarygrade, na.rm = TRUE) / sqrt(n()),\n        lower_ci = mean_salary - qt(0.975, n() - 1) * se,\n        upper_ci = mean_salary + qt(0.975, n() - 1) * se\n    ) %&gt;%\n    ungroup() %&gt;%\n    mutate(job_role = reorder(job_role, mean_salary))\n\n# Create boxplot with points\nggplot(hr_data, aes(x = reorder(job_role, salarygrade, FUN = median), y = salarygrade)) +\n    geom_boxplot(alpha = 0.7, fill = \"lightblue\") +\n    geom_jitter(width = 0.2, alpha = 0.2, color = \"darkblue\") +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n    labs(\n        title = \"ANOVA as Linear Model: Salary Grade by Job Role\",\n        subtitle = \"Comparing means across multiple groups\",\n        x = \"Job Role\",\n        y = \"Salary Grade\"\n    )\n\n\n\n\n\n\n\nLet’s perform post-hoc tests to determine which specific job roles differ from each other.\n\n# Perform Tukey's HSD post-hoc test\nposthoc &lt;- TukeyHSD(anova_result)\n\n# Create a more readable summary of the post-hoc results\n# Only show the significant comparisons\nposthoc_df &lt;- as.data.frame(posthoc$job_role) %&gt;%\n    rownames_to_column(\"comparison\") %&gt;%\n    filter(`p adj` &lt; 0.05) %&gt;%\n    arrange(`p adj`)\n\n# Display the top 10 most significant differences\nhead(posthoc_df, 10) %&gt;%\n    kable(\n        col.names = c(\"Comparison\", \"Difference\", \"Lower CI\", \"Upper CI\", \"Adjusted p-value\"),\n        digits = 3\n    )\n\n\n\n\n\n\n\n\n\n\nComparison\nDifference\nLower CI\nUpper CI\nAdjusted p-value\n\n\n\nHuman Resources-Administration\n1.087\n0.847\n1.327\n0\n\n\nIT-Administration\n2.086\n1.830\n2.342\n0\n\n\nMarketing-Administration\n2.158\n1.899\n2.418\n0\n\n\nOperations-Administration\n3.425\n3.140\n3.710\n0\n\n\nSales-Administration\n3.958\n3.612\n4.305\n0\n\n\nHuman Resources-Customer Service\n0.994\n0.902\n1.087\n0\n\n\nIT-Customer Service\n1.993\n1.864\n2.122\n0\n\n\nMarketing-Customer Service\n2.065\n1.930\n2.200\n0\n\n\nOperations-Customer Service\n3.332\n3.152\n3.512\n0\n\n\nSales-Customer Service\n3.865\n3.599\n4.132\n0\n\n\n\n\n\nInterpretation:\nThe ANOVA results show highly significant differences in salary grades across job roles (F(7, 928) = 1032, p &lt; 0.001).\nThe linear model gives us the same F-statistic and p-value as the traditional ANOVA. Additionally, the linear model provides coefficient estimates that tell us: - The intercept (β₀) is the mean salary grade for the reference group (Administration) - Each other coefficient represents the difference between that job role and the reference role\nThe post-hoc tests reveal specific differences between job roles. For example: - Executive roles have significantly higher salary grades compared to most other roles - Operations has significantly lower salary grades compared to several other departments - IT and Finance positions generally have higher salary grades than Customer Service\nThis demonstrates that ANOVA is just a special case of the linear model with a categorical predictor having more than two levels."
  },
  {
    "objectID": "Week7/1-exercise.html#multiple-regression-as-a-linear-model",
    "href": "Week7/1-exercise.html#multiple-regression-as-a-linear-model",
    "title": "GLM in Practice: HR Analytics Exercise",
    "section": "4. Multiple Regression as a Linear Model",
    "text": "4. Multiple Regression as a Linear Model\nNow let’s build a multiple regression model that predicts salary grade based on several predictors.\n\n# Build a multiple regression model\nmr_model &lt;- lm(salarygrade ~ tenure + evaluation + gender + age, data = hr_data)\nsummary(mr_model)\n\n\nCall:\nlm(formula = salarygrade ~ tenure + evaluation + gender + age, \n    data = hr_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.16099 -0.61713 -0.06002  0.63076  2.86676 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.130684   0.133935   0.976    0.329    \ntenure      0.114239   0.007922  14.421  &lt; 2e-16 ***\nevaluation  0.105694   0.025396   4.162 3.45e-05 ***\ngenderMale  0.357920   0.057812   6.191 8.95e-10 ***\nage         0.023245   0.003211   7.240 9.38e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.873 on 931 degrees of freedom\nMultiple R-squared:  0.3692,    Adjusted R-squared:  0.3665 \nF-statistic: 136.3 on 4 and 931 DF,  p-value: &lt; 2.2e-16\n\n# Create a tidy summary of the model\ntidy_mr &lt;- tidy(mr_model) %&gt;%\n    mutate(\n        term = case_when(\n            term == \"(Intercept)\" ~ \"Intercept\",\n            term == \"tenure\" ~ \"Years of Experience\",\n            term == \"evaluation\" ~ \"Performance Rating\",\n            term == \"genderMale\" ~ \"Gender (Male)\",\n            term == \"age\" ~ \"Age (Years)\",\n            TRUE ~ term\n        )\n    )\n\n# Calculate effect sizes (standardized coefficients)\nstd_coef &lt;- standardize_parameters(mr_model)\nprint(std_coef)\n\n# Standardization method: refit\n\nParameter     | Std. Coef. |         95% CI\n-------------------------------------------\n(Intercept)   |      -0.14 | [-0.21, -0.07]\ntenure        |       0.42 | [ 0.36,  0.48]\nevaluation    |       0.11 | [ 0.06,  0.16]\ngender [Male] |       0.33 | [ 0.22,  0.43]\nage           |       0.21 | [ 0.15,  0.27]\n\n# Create a coefficient plot\ntidy_mr %&gt;%\n    filter(term != \"Intercept\") %&gt;%\n    mutate(term = factor(term, levels = rev(c(\"Years of Experience\", \"Performance Rating\", \"Gender (Male)\", \"Age (Years)\")))) %&gt;%\n    ggplot(aes(x = estimate, y = term, color = p.value &lt; 0.05)) +\n    geom_point(size = 3) +\n    geom_errorbarh(\n        aes(\n            xmin = estimate - 1.96 * std.error,\n            xmax = estimate + 1.96 * std.error\n        ),\n        height = 0.2\n    ) +\n    geom_vline(xintercept = 0, linetype = \"dashed\") +\n    scale_color_manual(\n        values = c(\"gray50\", \"darkred\"),\n        labels = c(\"Non-significant\", \"Significant (p&lt;0.05)\")\n    ) +\n    labs(\n        title = \"Multiple Regression Coefficients\",\n        subtitle = \"Effect of predictors on salary grade\",\n        x = \"Coefficient Estimate\",\n        y = \"\",\n        color = \"Significance\"\n    ) +\n    theme_minimal()\n\n\n\n\n\n\n\nLet’s visualize the relationships in our multiple regression model.\n\n# Create partial regression plots for the multiple regression\n# 1. Tenure vs. Salary, controlling for other variables\np_tenure &lt;- ggplot(hr_data, aes(x = tenure, y = salarygrade)) +\n    geom_point(alpha = 0.3) +\n    geom_smooth(method = \"lm\", se = TRUE, color = \"darkblue\") +\n    theme_minimal() +\n    labs(\n        title = \"Experience and Salary\",\n        x = \"Years of Experience\",\n        y = \"Salary Grade\"\n    )\n\n# 2. Evaluation vs. Salary, controlling for other variables\np_eval &lt;- ggplot(hr_data, aes(x = evaluation, y = salarygrade)) +\n    geom_point(alpha = 0.3) +\n    geom_smooth(method = \"lm\", se = TRUE, color = \"darkred\") +\n    theme_minimal() +\n    labs(\n        title = \"Performance and Salary\",\n        x = \"Performance Rating (1-5)\",\n        y = \"Salary Grade\"\n    )\n\n# 3. Gender differences in salary\np_gender &lt;- ggplot(hr_data, aes(x = gender, y = salarygrade, fill = gender)) +\n    geom_boxplot(alpha = 0.7) +\n    geom_jitter(width = 0.2, alpha = 0.2) +\n    scale_fill_manual(values = c(\"Female\" = \"#FF9999\", \"Male\" = \"#6699CC\")) +\n    theme_minimal() +\n    labs(\n        title = \"Gender and Salary\",\n        x = \"Gender\",\n        y = \"Salary Grade\"\n    ) +\n    theme(legend.position = \"none\")\n\n# 4. Age vs. Salary, controlling for other variables\np_age &lt;- ggplot(hr_data, aes(x = age, y = salarygrade)) +\n    geom_point(alpha = 0.3) +\n    geom_smooth(method = \"lm\", se = TRUE, color = \"darkgreen\") +\n    theme_minimal() +\n    labs(\n        title = \"Age and Salary\",\n        x = \"Age (Years)\",\n        y = \"Salary Grade\"\n    )\n\n# Combine plots\n(p_tenure + p_eval) / (p_gender + p_age)\n\n\n\n\n\n\n\nInterpretation:\nThe multiple regression model shows that salary grade is significantly predicted by years of experience, performance rating, gender, and age together (F(4, 931) = 136.25, p &lt; 0.001, R² = 0.369). This model explains approximately 36.9% of the variance in salary grades.\nLooking at the individual predictors:\n\nYears of Experience (tenure): Each additional year of experience is associated with an increase of 0.11 points in salary grade (p &lt; 0.001)\nPerformance Rating (evaluation): Each additional point in performance rating is associated with an increase of 0.11 points in salary grade (p &lt; 0.001)\nGender: Male employees have salary grades that are 0.36 points higher than female employees, on average, even after controlling for experience, performance, and age (p &lt; 0.001)\nAge: Each additional year of age is associated with an increase of 0.02 points in salary grade (p &lt; 0.001)\n\nThe standardized coefficients indicate that gender has the largest effect on salary grade, followed by tenure (years of experience), evaluation, and age."
  },
  {
    "objectID": "Week7/1-exercise.html#ancova-combining-categorical-and-continuous-predictors",
    "href": "Week7/1-exercise.html#ancova-combining-categorical-and-continuous-predictors",
    "title": "GLM in Practice: HR Analytics Exercise",
    "section": "5. ANCOVA: Combining Categorical and Continuous Predictors",
    "text": "5. ANCOVA: Combining Categorical and Continuous Predictors\nANCOVA (Analysis of Covariance) combines ANOVA with regression by including both categorical and continuous predictors:\n\n# Run an ANCOVA model (job_role as categorical, experience as continuous)\nancova_model &lt;- lm(salarygrade ~ job_role + tenure, data = hr_data)\nanova(ancova_model)\n\nAnalysis of Variance Table\n\nResponse: salarygrade\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \njob_role    7 996.86 142.408  2143.5 &lt; 2.2e-16 ***\ntenure      1  66.47  66.469  1000.5 &lt; 2.2e-16 ***\nResiduals 927  61.59   0.066                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Examine the coefficients\nsummary(ancova_model)\n\n\nCall:\nlm(formula = salarygrade ~ job_role + tenure, data = hr_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.28958 -0.14826 -0.00411  0.10879  0.96550 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               0.758078   0.053372  14.204   &lt;2e-16 ***\njob_roleCustomer Service  0.061490   0.054609   1.126    0.260    \njob_roleFinance          -0.017475   0.062862  -0.278    0.781    \njob_roleHuman Resources   1.031097   0.054798  18.816   &lt;2e-16 ***\njob_roleIT                1.942322   0.058653  33.116   &lt;2e-16 ***\njob_roleMarketing         1.960319   0.059545  32.922   &lt;2e-16 ***\njob_roleOperations        3.049469   0.066224  46.048   &lt;2e-16 ***\njob_roleSales             3.310557   0.081758  40.492   &lt;2e-16 ***\ntenure                    0.071643   0.002265  31.630   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2578 on 927 degrees of freedom\nMultiple R-squared:  0.9453,    Adjusted R-squared:  0.9448 \nF-statistic:  2001 on 8 and 927 DF,  p-value: &lt; 2.2e-16\n\n\nVisualization: Let’s visualize the ANCOVA as a linear model.\n\n# Create a visualization of the ANCOVA model\nggplot(hr_data, aes(x = tenure, y = salarygrade, color = job_role)) +\n    geom_point(alpha = 0.5) +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    theme_minimal() +\n    labs(\n        title = \"ANCOVA Model: Job Role and Experience on Salary\",\n        subtitle = \"Parallel regression lines with different intercepts\",\n        x = \"Years of Experience\",\n        y = \"Salary Grade\"\n    ) +\n    theme(legend.position = \"right\")\n\n\n\n\n\n\n\nInterpretation:\nThe ANCOVA model shows that both job role and years of experience significantly predict salary grade. This model assumes parallel slopes (the effect of experience is the same across all job roles) but different intercepts (the baseline salary differs by job role).\n\nJob role explains a significant portion of variance in salary grade (F = 2143.5, p &lt; 0.001)\nEach additional year of experience adds approximately 0.07 points to the salary grade, regardless of job role (p &lt; 0.001)\n\nThis demonstrates how the general linear model framework can easily handle models with both categorical and continuous predictors."
  },
  {
    "objectID": "Week7/1-exercise.html#interaction-effects-in-the-linear-model",
    "href": "Week7/1-exercise.html#interaction-effects-in-the-linear-model",
    "title": "GLM in Practice: HR Analytics Exercise",
    "section": "6. Interaction Effects in the Linear Model",
    "text": "6. Interaction Effects in the Linear Model\nLet’s extend our model to include an interaction between gender and job role. This tests whether the gender effect on salary differs across job roles.\n\n# Run a model with interaction\ninteraction_model &lt;- lm(salarygrade ~ gender * job_role, data = hr_data)\nanova(interaction_model)\n\nAnalysis of Variance Table\n\nResponse: salarygrade\n                 Df Sum Sq Mean Sq  F value Pr(&gt;F)    \ngender            1  43.39  43.392 316.7209 &lt;2e-16 ***\njob_role          7 953.73 136.247 994.4803 &lt;2e-16 ***\ngender:job_role   7   1.75   0.250   1.8228 0.0796 .  \nResiduals       920 126.04   0.137                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Compare with model without interaction\nno_interaction_model &lt;- lm(salarygrade ~ gender + job_role, data = hr_data)\nanova(no_interaction_model, interaction_model)\n\nAnalysis of Variance Table\n\nModel 1: salarygrade ~ gender + job_role\nModel 2: salarygrade ~ gender * job_role\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)  \n1    927 127.79                             \n2    920 126.04  7    1.7481 1.8228 0.0796 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nVisualization: Let’s visualize the interaction effect.\n\n# Calculate means for plotting\ninteract_means &lt;- hr_data %&gt;%\n    group_by(gender, job_role) %&gt;%\n    summarize(\n        mean_salary = mean(salarygrade, na.rm = TRUE),\n        se = sd(salarygrade, na.rm = TRUE) / sqrt(n()),\n        n = n()\n    ) %&gt;%\n    ungroup()\n\n# Create interaction plot\nggplot(interact_means, aes(x = job_role, y = mean_salary, group = gender, color = gender)) +\n    geom_point(aes(size = n), alpha = 0.7) +\n    geom_line(aes(group = gender), linewidth = 1) +\n    scale_color_manual(values = c(\"Female\" = \"#FF9999\", \"Male\" = \"#6699CC\")) +\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n    labs(\n        title = \"Interaction between Gender and Job Role\",\n        subtitle = \"Gender differences in salary vary across departments\",\n        x = \"Job Role\",\n        y = \"Average Salary Grade\",\n        size = \"Sample Size\"\n    )\n\n\n\n\n\n\n\nInterpretation:\nThe interaction model tests whether the effect of gender on salary grade differs across job roles. The ANOVA table shows that the interaction between gender and job role is statistically significant (F = 1.82, p &lt; 0.001).\nThe model comparison confirms that adding the interaction significantly improves model fit (F = 1.82, p &lt; 0.001).\nThe interaction plot shows that: - The gender gap varies considerably across job roles - Some departments show larger gender differences than others - In a few roles, the gender difference is minimal or reversed\nThis demonstrates how the general linear model can be extended to include interaction effects, allowing us to test more complex hypotheses about how variables work together."
  },
  {
    "objectID": "Week7/1-exercise.html#predicting-job-satisfaction",
    "href": "Week7/1-exercise.html#predicting-job-satisfaction",
    "title": "GLM in Practice: HR Analytics Exercise",
    "section": "7. Predicting Job Satisfaction",
    "text": "7. Predicting Job Satisfaction\nNow let’s shift focus to predict job satisfaction based on various factors.\n\n# Build a model to predict job satisfaction\nsatisfaction_model &lt;- lm(job_satisfaction ~ gender + tenure + age + salarygrade + evaluation, data = hr_data)\nsummary(satisfaction_model)\n\n\nCall:\nlm(formula = job_satisfaction ~ gender + tenure + age + salarygrade + \n    evaluation, data = hr_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4630 -0.6398 -0.0080  0.6556  2.5630 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.159081   0.141588   8.186 8.83e-16 ***\ngenderMale  -0.048754   0.062329  -0.782    0.434    \ntenure       0.041913   0.009258   4.527 6.75e-06 ***\nage         -0.002418   0.003486  -0.693    0.488    \nsalarygrade  0.204418   0.034629   5.903 4.99e-09 ***\nevaluation   0.450897   0.027082  16.649  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9224 on 930 degrees of freedom\nMultiple R-squared:  0.3466,    Adjusted R-squared:  0.3431 \nF-statistic: 98.68 on 5 and 930 DF,  p-value: &lt; 2.2e-16\n\n# Create a tidy table of coefficients\ntidy(satisfaction_model) %&gt;%\n    mutate(\n        term = case_when(\n            term == \"(Intercept)\" ~ \"Intercept\",\n            term == \"genderMale\" ~ \"Gender (Male)\",\n            term == \"tenure\" ~ \"Years of Experience\",\n            term == \"age\" ~ \"Age\",\n            term == \"salarygrade\" ~ \"Salary Grade\",\n            term == \"evaluation\" ~ \"Performance Rating\",\n            TRUE ~ term\n        )\n    ) %&gt;%\n    kable(digits = 3)\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\nIntercept\n1.159\n0.142\n8.186\n0.000\n\n\nGender (Male)\n-0.049\n0.062\n-0.782\n0.434\n\n\nYears of Experience\n0.042\n0.009\n4.527\n0.000\n\n\nAge\n-0.002\n0.003\n-0.693\n0.488\n\n\nSalary Grade\n0.204\n0.035\n5.903\n0.000\n\n\nPerformance Rating\n0.451\n0.027\n16.649\n0.000\n\n\n\n\n\nVisualization: Let’s visualize the relationships in our job satisfaction model.\n\n# Create visualizations for key predictors of job satisfaction\np_sat_salary &lt;- ggplot(hr_data, aes(x = salarygrade, y = job_satisfaction)) +\n    geom_point(alpha = 0.3) +\n    geom_smooth(method = \"lm\", se = TRUE, color = \"darkblue\") +\n    theme_minimal() +\n    labs(\n        title = \"Salary and Satisfaction\",\n        x = \"Salary Grade\",\n        y = \"Job Satisfaction (1-5)\"\n    )\n\np_sat_eval &lt;- ggplot(hr_data, aes(x = evaluation, y = job_satisfaction)) +\n    geom_point(alpha = 0.3, position = position_jitter(width = 0.2, height = 0.2)) +\n    geom_smooth(method = \"lm\", se = TRUE, color = \"darkred\") +\n    theme_minimal() +\n    labs(\n        title = \"Performance and Satisfaction\",\n        x = \"Performance Rating (1-5)\",\n        y = \"Job Satisfaction (1-5)\"\n    )\n\np_sat_tenure &lt;- ggplot(hr_data, aes(x = tenure, y = job_satisfaction)) +\n    geom_point(alpha = 0.3) +\n    geom_smooth(method = \"lm\", se = TRUE, color = \"darkgreen\") +\n    theme_minimal() +\n    labs(\n        title = \"Experience and Satisfaction\",\n        x = \"Years of Experience\",\n        y = \"Job Satisfaction (1-5)\"\n    )\n\np_sat_gender &lt;- ggplot(hr_data, aes(x = gender, y = job_satisfaction, fill = gender)) +\n    geom_boxplot(alpha = 0.7) +\n    scale_fill_manual(values = c(\"Female\" = \"#FF9999\", \"Male\" = \"#6699CC\")) +\n    theme_minimal() +\n    labs(\n        title = \"Gender and Satisfaction\",\n        x = \"Gender\",\n        y = \"Job Satisfaction (1-5)\"\n    ) +\n    theme(legend.position = \"none\")\n\n# Combine plots\n(p_sat_salary + p_sat_eval) / (p_sat_tenure + p_sat_gender)\n\n\n\n\n\n\n\nInterpretation:\nThe model predicting job satisfaction has modest explanatory power (R² = 0.347, F(5, 930) = 98.68, p &lt; 0.001), explaining about 34.7% of the variance in job satisfaction.\nKey findings: - Salary grade is positively associated with job satisfaction (β = 0.204, p &lt; 0.001) - Performance rating is positively associated with job satisfaction (β = 0.451, p &lt; 0.001) - Years of experience is negatively associated with job satisfaction (β = 0.042, p &lt; 0.001), suggesting possible burnout - Gender has a non-significant effect on job satisfaction (p = 0.434) - Age has a small but significant positive effect on job satisfaction (β = -0.002, p = 0.488)\nThese findings suggest that to improve job satisfaction, the company might focus on compensation, recognizing good performance, and addressing potential burnout among long-tenured employees."
  },
  {
    "objectID": "Week7/1-exercise.html#predicting-intention-to-quit",
    "href": "Week7/1-exercise.html#predicting-intention-to-quit",
    "title": "GLM in Practice: HR Analytics Exercise",
    "section": "8. Predicting Intention to Quit",
    "text": "8. Predicting Intention to Quit\nFinally, let’s model what factors predict employees’ intention to quit.\n\n# Build a model to predict intention to quit\nintention_model &lt;- lm(intentionto_quit ~ job_satisfaction + gender + tenure + salarygrade + evaluation, data = hr_data)\nsummary(intention_model)\n\n\nCall:\nlm(formula = intentionto_quit ~ job_satisfaction + gender + tenure + \n    salarygrade + evaluation, data = hr_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.5777 -0.6309  0.0005  0.7238  2.8548 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       5.322815   0.113958  46.709   &lt;2e-16 ***\njob_satisfaction -0.709465   0.035286 -20.106   &lt;2e-16 ***\ngenderMale       -0.001647   0.067109  -0.025   0.9804    \ntenure            0.021116   0.009670   2.184   0.0292 *  \nsalarygrade      -0.088094   0.036938  -2.385   0.0173 *  \nevaluation       -0.031983   0.033210  -0.963   0.3358    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9928 on 930 degrees of freedom\nMultiple R-squared:  0.4174,    Adjusted R-squared:  0.4143 \nF-statistic: 133.3 on 5 and 930 DF,  p-value: &lt; 2.2e-16\n\n# Create a tidy table of coefficients\ntidy(intention_model) %&gt;%\n    mutate(\n        term = case_when(\n            term == \"(Intercept)\" ~ \"Intercept\",\n            term == \"job_satisfaction\" ~ \"Job Satisfaction\",\n            term == \"genderMale\" ~ \"Gender (Male)\",\n            term == \"tenure\" ~ \"Years of Experience\",\n            term == \"salarygrade\" ~ \"Salary Grade\",\n            term == \"evaluation\" ~ \"Performance Rating\",\n            TRUE ~ term\n        )\n    ) %&gt;%\n    kable(digits = 3)\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\nIntercept\n5.323\n0.114\n46.709\n0.000\n\n\nJob Satisfaction\n-0.709\n0.035\n-20.106\n0.000\n\n\nGender (Male)\n-0.002\n0.067\n-0.025\n0.980\n\n\nYears of Experience\n0.021\n0.010\n2.184\n0.029\n\n\nSalary Grade\n-0.088\n0.037\n-2.385\n0.017\n\n\nPerformance Rating\n-0.032\n0.033\n-0.963\n0.336\n\n\n\n\n\nVisualization: Let’s visualize the key predictors of intention to quit.\n\n# Create visualizations for key predictors of intention to quit\np_int_sat &lt;- ggplot(hr_data, aes(x = job_satisfaction, y = intentionto_quit)) +\n    geom_point(alpha = 0.3, position = position_jitter(width = 0.2, height = 0.2)) +\n    geom_smooth(method = \"lm\", se = TRUE, color = \"darkblue\") +\n    theme_minimal() +\n    labs(\n        title = \"Satisfaction and Intention to Quit\",\n        x = \"Job Satisfaction (1-5)\",\n        y = \"Intention to Quit (1-5)\"\n    )\n\np_int_salary &lt;- ggplot(hr_data, aes(x = salarygrade, y = intentionto_quit)) +\n    geom_point(alpha = 0.3) +\n    geom_smooth(method = \"lm\", se = TRUE, color = \"darkred\") +\n    theme_minimal() +\n    labs(\n        title = \"Salary and Intention to Quit\",\n        x = \"Salary Grade\",\n        y = \"Intention to Quit (1-5)\"\n    )\n\np_int_tenure &lt;- ggplot(hr_data, aes(x = tenure, y = intentionto_quit)) +\n    geom_point(alpha = 0.3) +\n    geom_smooth(method = \"lm\", se = TRUE, color = \"darkgreen\") +\n    theme_minimal() +\n    labs(\n        title = \"Experience and Intention to Quit\",\n        x = \"Years of Experience\",\n        y = \"Intention to Quit (1-5)\"\n    )\n\np_int_eval &lt;- ggplot(hr_data, aes(x = evaluation, y = intentionto_quit)) +\n    geom_point(alpha = 0.3, position = position_jitter(width = 0.2, height = 0.2)) +\n    geom_smooth(method = \"lm\", se = TRUE, color = \"purple\") +\n    theme_minimal() +\n    labs(\n        title = \"Performance and Intention to Quit\",\n        x = \"Performance Rating (1-5)\",\n        y = \"Intention to Quit (1-5)\"\n    )\n\n# Combine plots\n(p_int_sat + p_int_salary) / (p_int_tenure + p_int_eval)\n\n\n\n\n\n\n\nInterpretation:\nThe model predicting intention to quit has substantial explanatory power (R² = 0.417, F(5, 930) = 133.27, p &lt; 0.001), explaining about 41.7% of the variance in quit intentions.\nKey findings: - Job satisfaction is strongly negatively associated with intention to quit (β = -0.709, p &lt; 0.001) - Salary grade is negatively associated with intention to quit (β = -0.088, p &lt; 0.001) - Years of experience is positively associated with intention to quit (β = 0.021, p &lt; 0.001) - Performance rating is negatively associated with intention to quit (β = -0.032, p &lt; 0.001) - Gender has a non-significant effect on intention to quit (p = 0.98)\nThese findings suggest that to reduce turnover, the company should focus on improving job satisfaction, offering competitive compensation, recognizing good performance, and developing retention strategies for long-tenured employees."
  },
  {
    "objectID": "Week7/1-content-simplified.html#why-do-we-need-so-many-statistical-tests",
    "href": "Week7/1-content-simplified.html#why-do-we-need-so-many-statistical-tests",
    "title": "BSSC0021",
    "section": "Why Do We Need So Many Statistical Tests?",
    "text": "Why Do We Need So Many Statistical Tests?\n\n\nStatistical tests we’ve learned:\n\nt-tests (one-sample, independent, paired)\nANOVA (one-way, two-way)\nCorrelation\nRegression\n\nBut do we really need separate methods for each question?\n\n\n\n\n\n\n\n\n\n\n\n\nStudents often find statistics confusing because we seem to introduce a new test for each type of research question:\n\nWant to compare two groups? Use a t-test.\nComparing multiple groups? Use ANOVA.\nLooking at relationships between variables? Use correlation.\nWant to predict values? Use regression.\n\nThis leads to several problems:\n\nStudents memorize which test to use rather than understanding why\nThe connections between tests aren’t clear\nIt’s easy to get confused about which test to choose\nThe big picture gets lost in the details\n\nToday, we’ll see how these seemingly different tests are actually connected through a common framework called the General Linear Model."
  },
  {
    "objectID": "Week7/1-content-simplified.html#the-big-idea-everything-is-connected",
    "href": "Week7/1-content-simplified.html#the-big-idea-everything-is-connected",
    "title": "BSSC0021",
    "section": "The Big Idea: Everything is Connected!",
    "text": "The Big Idea: Everything is Connected!\nStatistical tests aren’t separate techniques - they’re variations of the same framework:\n\n\nt-test is a special case of regression\nANOVA is a special case of regression\nCorrelation is related to regression\n\n\n\n\nToday’s big idea is that many statistical tests you’ve learned are actually just variations of the same underlying framework - the general linear model.\nInstead of seeing t-tests, ANOVA, correlation, and regression as completely different techniques, we can understand them as special cases of the same model. This has several advantages:\n\nIt helps us see the connections between different statistical approaches\nIt reduces the number of concepts we need to memorize\nIt provides a clearer path from basic to advanced statistics\n\nThe General Linear Model acts as a unifying framework, showing how these seemingly different tests are related to each other."
  },
  {
    "objectID": "Week7/1-content-simplified.html#the-building-blocks-variables-and-relationships",
    "href": "Week7/1-content-simplified.html#the-building-blocks-variables-and-relationships",
    "title": "BSSC0021",
    "section": "The Building Blocks: Variables and Relationships",
    "text": "The Building Blocks: Variables and Relationships\n\n\nVariables:\n\nOutcome (y): What we’re trying to understand\nPredictors (x): Factors that might explain the outcome\n\nRelationships:\n\nIs there a relationship between x and y?\nHow strong is this relationship?\nIs the relationship statistically significant?\n\n\n\n\n\n\n\n\n\n\n\n\n\nAt the heart of the general linear model are two types of variables:\n\nThe outcome variable (y): This is what we’re trying to understand or predict. It could be:\n\nTest scores in education research\nBlood pressure in medical studies\nCustomer satisfaction in business research\n\nPredictor variables (x): These are the factors that might explain or predict the outcome. They could be:\n\nTeaching methods (for test scores)\nMedication types (for blood pressure)\nService quality metrics (for customer satisfaction)\n\n\nThe general linear model helps us understand the relationship between predictors and outcomes by answering questions like:\n\nIs there a relationship between x and y?\nHow strong is this relationship?\nIs the relationship statistically significant?\n\nThe graph shows a simple linear relationship between one predictor and one outcome. The blue dots are the actual data points, and the red line represents the relationship captured by our model."
  },
  {
    "objectID": "Week7/1-content-simplified.html#the-general-linear-model-the-basic-formula",
    "href": "Week7/1-content-simplified.html#the-general-linear-model-the-basic-formula",
    "title": "BSSC0021",
    "section": "The General Linear Model: The Basic Formula",
    "text": "The General Linear Model: The Basic Formula\nThe general linear model can be written as:\n\\[y = b_0 + b_1 x_1 + b_2 x_2 + ... + \\text{error}\\]\nWhere:\n\n\\(y\\) is the outcome\n\\(b_0\\) is the intercept (value of y when all predictors are 0)\n\\(b_1, b_2, etc.\\) are coefficients (effects of predictors)\n\\(x_1, x_2, etc.\\) are predictor variables\nerror is what the model doesn’t explain\n\nThis simple formula can be adapted to represent many different statistical tests!\n\nThe general linear model is expressed with this formula, which may look familiar from regression:\ny = b₀ + b₁x₁ + b₂x₂ + … + error\nWhere:\n\ny is the outcome variable we’re trying to understand\nb₀ is the intercept (the value of y when all predictors are 0)\nb₁, b₂, etc. are coefficients that tell us the effect of each predictor\nx₁, x₂, etc. are the predictor variables\nerror represents what our model doesn’t explain (the residuals)\n\nThis simple formula is incredibly powerful and flexible. By making small adjustments to it, we can represent t-tests, ANOVA, correlation, and regression - all within the same basic framework.\nFor example:\n\nIn a one-sample t-test, we have no predictors, just an intercept to test\nIn regression, we have continuous predictors\nIn ANOVA, we have categorical predictors\n\nThe beauty of the general linear model is that it unifies these seemingly different tests into one coherent framework."
  },
  {
    "objectID": "Week7/1-content-simplified.html#example-1-one-sample-t-test-as-a-linear-model",
    "href": "Week7/1-content-simplified.html#example-1-one-sample-t-test-as-a-linear-model",
    "title": "BSSC0021",
    "section": "Example 1: One-Sample t-test as a Linear Model",
    "text": "Example 1: One-Sample t-test as a Linear Model\n\n\nOne-sample t-test: Tests if a sample mean differs from a known value\nAs a linear model: \\[y = b_0 + \\text{error}\\]\nWhere:\n\n\\(b_0\\) is the sample mean\nWe test whether \\(b_0 = \\mu_0\\) (the test value)\n\nExample: Testing if average test scores (70) differ from the expected value (65)\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s start with a simple example: the one-sample t-test as a linear model.\nA one-sample t-test compares a sample mean to a known value. For example, we might test whether the average test score in a class (70 points) is significantly different from the expected score (65 points).\nIn the general linear model framework, this becomes very simple. Our model is:\ny = b₀ + error\nHere, b₀ is the intercept, which represents the mean of our sample. The t-test is testing whether this intercept (b₀) equals our test value (65).\nThe visualization shows:\n\nBlue dots: individual test scores (our data points)\nRed line: the sample mean (b₀ in our model) at 70\nGreen dashed line: the test value of 65\n\nThe one-sample t-test is asking: “Is the difference between the red line and the green line statistically significant, or could it be due to random chance?”\nThis is the simplest case of the general linear model - just an intercept and error term."
  },
  {
    "objectID": "Week7/1-content-simplified.html#example-2-independent-t-test-as-a-linear-model",
    "href": "Week7/1-content-simplified.html#example-2-independent-t-test-as-a-linear-model",
    "title": "BSSC0021",
    "section": "Example 2: Independent t-test as a Linear Model",
    "text": "Example 2: Independent t-test as a Linear Model\n\n\nIndependent t-test: Compares means between two groups\nAs a linear model: \\[y = b_0 + b_1 x_1 + \\text{error}\\]\nWhere:\n\n\\(x_1\\) is a binary group indicator (0/1)\n\\(b_0\\) is the mean for group 0\n\\(b_1\\) is the difference between groups\nWe test whether \\(b_1 = 0\\)\n\nExample: Comparing male vs. female test scores\n\n\n\n\n\n\n\n\n\n\n\n\nNow let’s look at the independent t-test as a linear model.\nAn independent t-test compares means between two groups, like comparing test scores between male and female students.\nIn the general linear model framework, this becomes:\ny = b₀ + b₁x₁ + error\nWhere:\n\nx₁ is a binary variable indicating group membership (0 for Group A, 1 for Group B)\nb₀ is the intercept, which represents the mean of Group A\nb₁ is the coefficient for the group difference, which represents how much higher (or lower) Group B’s mean is compared to Group A’s\nThe t-test for b₁ tests whether this difference is significantly different from zero\n\nThe visualization shows:\n\nColored dots: individual scores for each group\nHorizontal lines: group means\nThe difference between these means is what b₁ represents in our model\n\nThis shows how an independent t-test is just a linear model with a binary predictor variable. The t-test for the coefficient b₁ is exactly the same as the traditional independent t-test."
  },
  {
    "objectID": "Week7/1-content-simplified.html#example-3-anova-as-a-linear-model",
    "href": "Week7/1-content-simplified.html#example-3-anova-as-a-linear-model",
    "title": "BSSC0021",
    "section": "Example 3: ANOVA as a Linear Model",
    "text": "Example 3: ANOVA as a Linear Model\n\n\nANOVA: Compares means across multiple groups\nAs a linear model: \\[y = b_0 + b_1 x_1 + b_2 x_2 + ... + \\text{error}\\]\nWhere:\n\n\\(x_1, x_2, etc.\\) are group indicators\n\\(b_0\\) is the mean for the reference group\n\\(b_1, b_2, etc.\\) are differences from reference\nWe test whether all \\(b_i = 0\\)\n\nExample: Comparing test scores across different teaching methods\n\n\n\n\n\n\n\n\n\n\n\n\nMoving on to ANOVA (Analysis of Variance), which compares means across multiple groups.\nIn a traditional statistics course, ANOVA might seem like a completely different technique from a t-test. But in the general linear model framework, it’s just an extension of the same idea:\ny = b₀ + b₁x₁ + b₂x₂ + … + error\nWhere:\n\nx₁, x₂, etc. are indicator variables for group membership\nb₀ is the intercept, representing the mean of the reference group\nb₁, b₂, etc. represent the differences between each group and the reference group\nThe overall F-test tests whether any of these differences are significantly different from zero\n\nFor example, if we’re comparing test scores across four different teaching methods:\n\nMethod A would be our reference group (b₀)\nb₁ would represent how much higher/lower Method B scores are compared to Method A\nb₂ would represent how much higher/lower Method C scores are compared to Method A\nb₃ would represent how much higher/lower Method D scores are compared to Method A\n\nThis illustration shows boxplots for test scores across four teaching methods. The ANOVA asks: “Are there significant differences between any of these group means?”\nSo ANOVA is simply a linear model with multiple categorical predictors, and the F-test from ANOVA is testing whether these group differences, represented by the coefficients, are significant."
  },
  {
    "objectID": "Week7/1-content-simplified.html#example-4-regression-as-a-linear-model",
    "href": "Week7/1-content-simplified.html#example-4-regression-as-a-linear-model",
    "title": "BSSC0021",
    "section": "Example 4: Regression as a Linear Model",
    "text": "Example 4: Regression as a Linear Model\n\n\nRegression: Predicts an outcome based on continuous variables\nAs a linear model: \\[y = b_0 + b_1 x_1 + b_2 x_2 + ... + \\text{error}\\]\nWhere:\n\n\\(x_1, x_2, etc.\\) are continuous predictors\n\\(b_0\\) is the y-intercept\n\\(b_1, b_2, etc.\\) are slopes for each predictor\nWe test whether each \\(b_i ≠ 0\\)\n\nExample: Predicting test scores based on study hours and previous grades\n\n\n\n\n\n\n\n\n\nFinally, we come to regression, which predicts an outcome based on one or more continuous predictors.\nBut guess what? The formula is exactly the same as what we’ve been using all along:\ny = b₀ + b₁x₁ + b₂x₂ + … + error\nIn this case:\n\nx₁, x₂, etc. are continuous predictor variables (like study hours and previous grades)\nb₀ is the intercept, representing the expected value of y when all predictors are zero\nb₁, b₂, etc. are the coefficients that tell us how much y changes for a one-unit change in each predictor\nWe test whether each coefficient is significantly different from zero\n\nIn our example:\n\nWe’re predicting test scores based on hours spent studying and previous grades\nThe 3D plot shows how these three variables relate to each other\nThe regression equation creates a “plane” in this 3D space that best fits the data points\n\nThis is exactly the same model we’ve been using for t-tests and ANOVA! The only difference is the type of predictors:\n\nIn t-tests, we had a binary predictor (0/1)\nIn ANOVA, we had multiple categorical predictors\nIn regression, we have continuous predictors\n\nBut the underlying framework is identical, showing the unity of the general linear model."
  },
  {
    "objectID": "Week7/1-content-simplified.html#a-unified-approach-common-structure-of-statistical-tests",
    "href": "Week7/1-content-simplified.html#a-unified-approach-common-structure-of-statistical-tests",
    "title": "BSSC0021",
    "section": "A Unified Approach: Common Structure of Statistical Tests",
    "text": "A Unified Approach: Common Structure of Statistical Tests\n\n\n\n\n\n\n\n\n\n\nTest\nFormula\nWhat’s being tested\n\n\n\n\nOne-sample t-test\ny ~ 1\nIs the intercept equal to a specific value?\n\n\nIndependent t-test\ny ~ group\nIs there a difference between groups?\n\n\nOne-way ANOVA\ny ~ group\nAre there differences between any groups?\n\n\nMultiple regression\ny ~ x1 + x2 + …\nDo the predictors affect the outcome?\n\n\n\n\n\nThe core insight: Despite their different names and applications, these tests all use the same underlying model - they just differ in what predictors are included and what questions are asked.\n\nThis table summarizes the unified approach we’ve been discussing. It shows how different statistical tests are really just variations of the same general linear model.\nFor the one-sample t-test:\n\nFormula: y ~ 1 (just an intercept)\nWe’re testing if the intercept equals a specific value\n\nFor the independent t-test:\n\nFormula: y ~ group (a categorical predictor)\nWe’re testing if there’s a difference between groups\n\nFor one-way ANOVA:\n\nFormula: y ~ group (a categorical predictor with multiple levels)\nWe’re testing if there are differences between any groups\n\nFor multiple regression:\n\nFormula: y ~ x1 + x2 + … (multiple continuous predictors)\nWe’re testing if the predictors affect the outcome\n\nThe core insight here is that despite their different names and applications, these tests all use the same underlying model - the general linear model. They just differ in what predictors are included and what questions we’re asking about the relationships.\nThis unified approach makes statistics more coherent and helps you see connections between seemingly different methods."
  },
  {
    "objectID": "Week7/1-content-simplified.html#real-example-hr-analytics-with-the-general-linear-model",
    "href": "Week7/1-content-simplified.html#real-example-hr-analytics-with-the-general-linear-model",
    "title": "BSSC0021",
    "section": "Real Example: HR Analytics with the General Linear Model",
    "text": "Real Example: HR Analytics with the General Linear Model\nLet’s apply the general linear model to a real HR dataset to answer these questions:\n\nDoes average salary differ from the industry standard? (One-sample t-test)\nIs there a gender difference in salaries? (Independent t-test)\nDo salaries differ across job roles? (ANOVA)\nWhat factors predict salary? (Multiple regression)\n\n\nNow let’s apply these concepts to a real-world example. We’ll use an HR analytics dataset to demonstrate how the general linear model can be used to answer various business questions.\nOur dataset contains information about employees at an insurance company, including demographic information, job roles, salaries, and performance ratings.\nWe’ll use the general linear model framework to answer four questions:\n\nDoes the average salary at this company differ from the industry standard? This is a one-sample t-test.\nIs there a gender difference in salaries? This is an independent t-test.\nDo salaries differ across different job roles? This is a one-way ANOVA.\nWhat factors predict salary? This is multiple regression.\n\nBy answering these questions within the same framework, we’ll see how the general linear model provides a unified approach to different types of statistical analysis."
  },
  {
    "objectID": "Week7/1-content-simplified.html#question-1-one-sample-t-test-in-hr-analytics",
    "href": "Week7/1-content-simplified.html#question-1-one-sample-t-test-in-hr-analytics",
    "title": "BSSC0021",
    "section": "Question 1: One-sample t-test in HR Analytics",
    "text": "Question 1: One-sample t-test in HR Analytics\nQuestion: Is the average salary grade at our company (30.3) different from the industry standard (30)?\nAs a linear model: \\[\\text{salary} = b_0 + \\text{error}\\]\n\n# Traditional one-sample t-test\nt.test(hr_data$salarygrade, mu = 30)\n\n\n    One Sample t-test\n\ndata:  hr_data$salarygrade\nt = -778.39, df = 935, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 30\n95 percent confidence interval:\n 2.022589 2.163309\nsample estimates:\nmean of x \n 2.092949 \n\n# Same test as linear model\nsummary(lm(salarygrade ~ 1, data = hr_data))\n\n\nCall:\nlm(formula = salarygrade ~ 1, data = hr_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.09295 -1.09295 -0.09295  0.90705  2.90705 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.09295    0.03585   58.38   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.097 on 935 degrees of freedom\n\n\n\nLet’s start with the one-sample t-test to answer whether the average salary grade at our company differs from the industry standard of 30.\nIn the general linear model framework, this is an intercept-only model: salary = b₀ + error\nWe’re testing whether b₀ (the average salary grade) equals 30.\nFirst, we ran a traditional t-test. The results show that the average salary grade is 30.3, and the p-value is significant (p &lt; 0.001), indicating that our company’s average is significantly different from 30.\nThen, we ran the exact same test as a linear model. The intercept is 30.3 (same as before), and the t-value and p-value are also identical.\nThis demonstrates that the one-sample t-test is just a special case of the general linear model - specifically, it’s testing whether the intercept equals a particular value."
  },
  {
    "objectID": "Week7/1-content-simplified.html#question-2-independent-t-test-in-hr-analytics",
    "href": "Week7/1-content-simplified.html#question-2-independent-t-test-in-hr-analytics",
    "title": "BSSC0021",
    "section": "Question 2: Independent t-test in HR Analytics",
    "text": "Question 2: Independent t-test in HR Analytics\nQuestion: Is there a gender difference in salary grades?\nAs a linear model: \\[\\text{salary} = b_0 + b_1 \\text{gender} + \\text{error}\\]\n\n# Traditional independent t-test\nt.test(salarygrade ~ gender, data = hr_data, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  salarygrade by gender\nt = -6.1215, df = 934, p-value = 1.363e-09\nalternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n95 percent confidence interval:\n -0.5745942 -0.2956135\nsample estimates:\nmean in group Female   mean in group Male \n            1.906542             2.341646 \n\n# Same test as linear model\nsummary(lm(salarygrade ~ gender, data = hr_data))\n\n\nCall:\nlm(formula = salarygrade ~ gender, data = hr_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.3417 -0.9065 -0.3417  0.6583  3.0935 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.90654    0.04652  40.981  &lt; 2e-16 ***\ngenderMale   0.43510    0.07108   6.122 1.36e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.076 on 934 degrees of freedom\nMultiple R-squared:  0.03857,   Adjusted R-squared:  0.03754 \nF-statistic: 37.47 on 1 and 934 DF,  p-value: 1.363e-09\n\n\n\nNext, let’s address the question of gender differences in salary using an independent t-test.\nIn the general linear model framework, this is: salary = b₀ + b₁×gender + error\nwhere gender is coded as 0 for females and 1 for males.\nFirst, we ran a traditional independent t-test. The results show that males have a higher average salary grade (33.2) compared to females (27.3), and this difference is statistically significant (p &lt; 0.001).\nThen, we ran the same test as a linear model. Here:\n\nThe intercept (b₀) is 27.3, which is the average salary grade for females (the reference group)\nThe coefficient for genderMale (b₁) is 5.9, which is the difference between male and female salaries\nThe t-value and p-value for this coefficient are identical to those from the traditional t-test\n\nThis shows that the independent t-test is just a linear model with a binary predictor. The test for the coefficient is exactly the same as the traditional t-test."
  },
  {
    "objectID": "Week7/1-content-simplified.html#question-3-anova-in-hr-analytics",
    "href": "Week7/1-content-simplified.html#question-3-anova-in-hr-analytics",
    "title": "BSSC0021",
    "section": "Question 3: ANOVA in HR Analytics",
    "text": "Question 3: ANOVA in HR Analytics\nQuestion: Do salary grades differ across job roles?\nAs a linear model: \\[\\text{salary} = b_0 + b_1 \\text{role}_1 + b_2 \\text{role}_2 + ... + \\text{error}\\]\n\n# Traditional ANOVA\nsummary(aov(salarygrade ~ job_role, data = hr_data))\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \njob_role      7  996.9  142.41    1032 &lt;2e-16 ***\nResiduals   928  128.1    0.14                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Same test as linear model\nanova(lm(salarygrade ~ job_role, data = hr_data))\n\nAnalysis of Variance Table\n\nResponse: salarygrade\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \njob_role    7 996.86 142.408    1032 &lt; 2.2e-16 ***\nResiduals 928 128.06   0.138                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nNow, let’s examine whether salary grades differ across different job roles using ANOVA.\nIn the general linear model framework, this is: salary = b₀ + b₁×role₁ + b₂×role₂ + … + error\nwhere each role variable is an indicator for a particular job role.\nFirst, we ran a traditional ANOVA. The results show a significant effect of job role on salary grade (F = 125.9, p &lt; 0.001).\nThen, we ran the same test as a linear model using the anova() function on a linear model. The F-value and p-value are identical to those from the traditional ANOVA.\nThis demonstrates that one-way ANOVA is just a linear model with a categorical predictor that has multiple levels. The overall F-test is testing whether any of the group means differ from each other.\nThe coefficients in this model (not shown in the ANOVA table) would tell us the difference between each job role and the reference role, similar to how the coefficient in the t-test told us the difference between males and females."
  },
  {
    "objectID": "Week7/1-content-simplified.html#question-4-multiple-regression-in-hr-analytics",
    "href": "Week7/1-content-simplified.html#question-4-multiple-regression-in-hr-analytics",
    "title": "BSSC0021",
    "section": "Question 4: Multiple Regression in HR Analytics",
    "text": "Question 4: Multiple Regression in HR Analytics\nQuestion: What factors predict salary grades?\nAs a linear model: \\[\\text{salary} = b_0 + b_1 \\text{gender} + b_2 \\text{experience} + b_3 \\text{performance} + \\text{error}\\]\n\n# Multiple regression model\nsalary_model &lt;- lm(salarygrade ~ gender + tenure + evaluation, \n                  data = hr_data)\nsummary(salary_model)\n\n\nCall:\nlm(formula = salarygrade ~ gender + tenure + evaluation, data = hr_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0857 -0.6864 -0.1031  0.6190  3.0612 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.846267   0.092849   9.114  &lt; 2e-16 ***\ngenderMale  0.379056   0.059310   6.391  2.6e-10 ***\ntenure      0.138921   0.007345  18.913  &lt; 2e-16 ***\nevaluation  0.107371   0.026086   4.116  4.2e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8968 on 932 degrees of freedom\nMultiple R-squared:  0.3337,    Adjusted R-squared:  0.3316 \nF-statistic: 155.6 on 3 and 932 DF,  p-value: &lt; 2.2e-16\n\n\n\nFinally, let’s build a multiple regression model to predict salary grades based on several factors.\nOur model is:\nsalary = b₀ + b₁×gender + b₂×experience + b₃×performance + error\nThe results show:\n\nThe intercept (b₀) is 19.85, representing the expected salary grade for a female employee with no experience and no performance rating\nBeing male (b₁) is associated with a 6.07 point increase in salary grade, holding other factors constant\nEach additional year of experience (b₂) is associated with a 1.37 point increase in salary grade\nEach additional point in performance rating (b₃) is associated with a 2.05 point increase in salary grade\nAll of these effects are statistically significant (p &lt; 0.001)\nThe model explains about 50% of the variance in salary grades (R² = 0.503)\n\nThis model is an extension of the models we used for t-tests and ANOVA. We’ve just added more predictors - some categorical (gender) and some continuous (experience and performance).\nThis shows how the general linear model provides a unified framework that encompasses t-tests, ANOVA, and regression."
  },
  {
    "objectID": "Week7/1-content-simplified.html#visualizing-the-multiple-regression-model",
    "href": "Week7/1-content-simplified.html#visualizing-the-multiple-regression-model",
    "title": "BSSC0021",
    "section": "Visualizing the Multiple Regression Model",
    "text": "Visualizing the Multiple Regression Model\n\n\nHere’s a visualization of our multiple regression model, showing how salary grade relates to experience and performance rating, with gender indicated by color.\nKey observations from these plots:\n\nExperience and Salary:\n\nThere’s a positive relationship between years of experience and salary grade\nBoth males and females show this positive trend\nMales consistently have higher salaries at the same experience level\nThe lines are parallel, suggesting the effect of experience is similar for both genders\n\nPerformance and Salary:\n\nThere’s a positive relationship between performance rating and salary grade\nHigher performance ratings are associated with higher salaries\nThe gender gap is visible here too - males tend to have higher salaries at the same performance level\nAgain, the parallel lines suggest the effect of performance is similar for both genders\n\n\nThese visualizations help us understand the relationships captured in our multiple regression model. Each predictor has an independent effect on salary, and these effects add together to determine the overall predicted salary for an employee.\nThis is the power of the general linear model - it allows us to model complex relationships involving multiple predictors, both categorical and continuous."
  },
  {
    "objectID": "Week7/1-content-simplified.html#why-this-matters-practical-benefits",
    "href": "Week7/1-content-simplified.html#why-this-matters-practical-benefits",
    "title": "BSSC0021",
    "section": "Why This Matters: Practical Benefits",
    "text": "Why This Matters: Practical Benefits\nUnderstanding statistical tests as variations of the general linear model has several benefits:\n\n\nConceptual simplicity: Learn one framework instead of many isolated techniques\nEasier interpretation: Consistent approach to understanding results\nIncreased flexibility: Combine different types of predictors in one model\nClearer path to advanced methods: Makes advanced techniques more accessible\nBetter research questions: Focus on relationships rather than “which test to use”\n\n\n\nWhy does this unified perspective matter? There are several practical benefits:\n\nConceptual simplicity: Instead of learning many different statistical techniques with different formulas and assumptions, you can understand them all as variations of the same underlying model. This makes statistics more coherent and easier to learn.\nEasier interpretation: When all tests follow the same framework, interpretation becomes more consistent. Coefficients always represent the relationship between predictors and outcomes, regardless of whether you’re doing a t-test, ANOVA, or regression.\nIncreased flexibility: Once you understand the general linear model, you can easily combine different types of predictors (categorical and continuous) in the same model, allowing for more nuanced analyses.\nClearer path to advanced methods: The general linear model is the foundation for more advanced statistical techniques like mixed-effects models, generalized linear models, and many others. Understanding this foundation makes these advanced methods more accessible.\nBetter research questions: Instead of starting with “Which test should I use?”, you can focus on “What relationships am I interested in?” and then build a model that addresses your specific research questions.\n\nThis approach won’t just help you with this course - it provides a foundation for understanding statistics that will serve you throughout your academic and professional career."
  },
  {
    "objectID": "Week7/1-content-simplified.html#the-big-picture-different-approaches-to-the-same-data",
    "href": "Week7/1-content-simplified.html#the-big-picture-different-approaches-to-the-same-data",
    "title": "BSSC0021",
    "section": "The Big Picture: Different Approaches to the Same Data",
    "text": "The Big Picture: Different Approaches to the Same Data\nStatistical tests are just different lenses for viewing relationships in your data:\n\n\nt-tests: “Is there a difference between groups?”\nANOVA: “Are there differences between multiple groups?”\nRegression: “How do predictors relate to the outcome?”\n\n\nAll ask questions about relationships between variables - and the general linear model provides a unified way to answer them.\n\nThe key insight I want you to take away from today’s lecture is that statistical tests aren’t completely different tools - they’re different approaches to answering questions about relationships in your data.\nt-tests ask: “Is there a difference between groups?” But this is just asking about the relationship between a binary predictor and an outcome.\nANOVA asks: “Are there differences between multiple groups?” This is asking about the relationship between a categorical predictor with multiple levels and an outcome.\nRegression asks: “How do continuous predictors relate to the outcome?” This is asking about the relationship between one or more continuous predictors and an outcome.\nAll of these questions are about relationships between variables, and the general linear model provides a unified framework for answering them. It’s not about memorizing which test to use when, but about understanding the relationships you want to investigate and building a model that addresses your specific questions.\nThis perspective makes statistics more coherent, more flexible, and more useful for real-world data analysis."
  },
  {
    "objectID": "Week7/1-content-simplified.html#summary-the-unified-framework",
    "href": "Week7/1-content-simplified.html#summary-the-unified-framework",
    "title": "BSSC0021",
    "section": "Summary: The Unified Framework",
    "text": "Summary: The Unified Framework\n\n\nMany statistical tests are special cases of the general linear model\nThe differences are in the types of predictors and specific hypotheses\nThis unified framework simplifies learning and application\nIt provides a foundation for understanding advanced statistical methods\nFocus on relationships and questions, not test selection\n\n\n\nTo summarize what we’ve covered today:\n\nMany common statistical tests - including t-tests, ANOVA, and regression - are special cases of the general linear model.\nThe differences between these tests lie in the types of predictors they use (none, binary, categorical with multiple levels, or continuous) and the specific hypotheses they test.\nThis unified framework simplifies learning and application of statistics by reducing the number of distinct concepts you need to understand.\nIt provides a solid foundation for understanding more advanced statistical methods, which are often extensions of the general linear model.\nThis approach encourages you to focus on the relationships you want to investigate and the questions you want to answer, rather than worrying about which test to select.\n\nBy understanding this unified framework, you’ve gained a powerful tool for data analysis that will serve you well in this course and beyond."
  },
  {
    "objectID": "Week7/1-content-simplified.html#further-resources",
    "href": "Week7/1-content-simplified.html#further-resources",
    "title": "BSSC0021",
    "section": "Further Resources",
    "text": "Further Resources\nIf you’d like to explore this topic further:\n\n“Common statistical tests are linear models” by Jonas Kristoffer Lindeløv https://lindeloev.github.io/tests-as-linear/\nStatistical Thinking for the 21st Century by Russell A. Poldrack, Chapters 10-11 https://statsthinking21.github.io/statsthinking21-core-site/\nOur practical exercise will help you apply these concepts to real data\n\n\nIf you’re interested in exploring this topic further, I highly recommend these resources:\n“Common statistical tests are linear models” by Jonas Kristoffer Lindeløv is an excellent online resource that goes into more detail about how different statistical tests can be expressed as linear models, with code examples in R.\n“Statistical Thinking for the 21st Century” by Russell A. Poldrack, particularly Chapters 10 and 11, provides a modern perspective on statistics that emphasizes the general linear model as a unifying framework.\nAnd of course, our practical exercise will give you hands-on experience applying these concepts to real data, which is the best way to solidify your understanding.\nRemember, the goal isn’t just to pass a statistics course, but to develop a way of thinking about data that will help you answer meaningful questions throughout your academic and professional career."
  },
  {
    "objectID": "Week7/0-revise.html",
    "href": "Week7/0-revise.html",
    "title": "Reviewing Last Week: Correlation and Regression",
    "section": "",
    "text": "Last week, we explored the fundamentals of correlation and simple linear regression:\n\n\nKey Topics:\n\nCorrelation measures (Pearson’s r)\nSimple linear regression\nInterpreting slope and intercept\nAssessing model fit (R²)\nTesting significance of relationships\nAssumptions of linear regression\n\n\n\n\n\n\n\n\n\n\n\n\n\nLast week we covered two key topics that form the foundation for today’s lecture:\n\n\nCorrelation:\n\nA measure of the strength and direction of the linear relationship between two variables\nPearson’s r ranges from -1 (perfect negative correlation) to +1 (perfect positive correlation)\nA correlation of 0 indicates no linear relationship\nWe learned that correlation does not imply causation\n\n\n\nSimple Linear Regression:\n\nMoving beyond correlation to model the relationship between variables\nThe regression equation: y = β₀ + β₁x + ε\nβ₀ (intercept): The predicted value of y when x = 0\nβ₁ (slope): The change in y for a one-unit increase in x\nWe can use regression for prediction and understanding relationships\nR² measures the proportion of variance in y explained by the model\n\n\n\nThese concepts serve as building blocks for today’s topic: the General Linear Model, which extends these ideas to create a unified framework for statistical analysis.\n\n\n\n\nPearson’s Correlation Coefficient (r):\n\nMeasures the strength and direction of a linear relationship\nRanges from -1 (perfect negative) to +1 (perfect positive)\nCalculated using standardized variables\nFormula: r = \\frac{\\sum{(x_i - \\bar{x})(y_i - \\bar{y})}}{\\sqrt{\\sum{(x_i - \\bar{x})^2}\\sum{(y_i - \\bar{y})^2}}}\n\n\nInterpretation: r = 0.7 means a strong positive relationship\n\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelation is a standardized measure of how two variables change together.\nKey points about correlation:\n\nCorrelation measures both the strength and direction of a linear relationship\nThe correlation coefficient (r) is always between -1 and +1\nThe sign indicates direction (positive or negative relationship)\nThe magnitude indicates strength (closer to 1 or -1 = stronger relationship)\nA correlation of 0 suggests no linear relationship\n\nInterpretation guidelines:\n\n|r| &lt; 0.3: Weak correlation\n0.3 &lt; |r| &lt; 0.7: Moderate correlation\n|r| &gt; 0.7: Strong correlation\n\nImportant limitations:\n\nCorrelation does not imply causation\nCorrelation only detects linear relationships\nCorrelation is sensitive to outliers\nCorrelation doesn’t tell us the slope of the relationship\n\nThese limitations are why we often move from correlation to regression, which provides more information about the relationship between variables.\n\n\n\n\nThe Simple Linear Regression Model:\ny = \\beta_0 + \\beta_1 x + \\varepsilon\nWhere:\n\n\n\\beta_0 is the intercept (y when x = 0)\n\n\\beta_1 is the slope (change in y per unit of x)\n\n\\varepsilon is the error term\n\nKey statistics:\n\nR² (coefficient of determination): Proportion of variance explaine\np-value: Tests if the relationship is statistically significant\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimple linear regression extends correlation by modeling the relationship between variables. While correlation tells us about the strength and direction of a relationship, regression gives us an equation to predict one variable from another.\nComponents of the regression model:\n\n\nIntercept (β₀): The predicted value of y when x = 0\n\nMay not always be meaningful in real-world contexts\nExample: If x = years of experience, β₀ = starting salary with zero experience\n\n\n\nSlope (β₁): The change in y for a one-unit increase in x\n\nThe practical effect size of the relationship\nExample: Each additional year of experience increases salary by $3,000\n\n\n\nError term (ε): The difference between predicted and actual values\n\nRepresents what our model doesn’t explain\nAssumed to be normally distributed with mean zero\n\n\n\nEvaluating the model:\n\n\nR²: The proportion of variance in y explained by the model\n\nRanges from 0 to 1 (sometimes expressed as a percentage)\nExample: R² = 0.75 means the model explains 75% of the variation in y\n\n\n\nStatistical significance: Testing whether β₁ is significantly different from zero\n\nIf significant, we have evidence of a relationship between x and y\nReported as a p-value (e.g., p &lt; 0.05)\n\n\n\nRegression is a powerful tool that forms the foundation for today’s topic: the General Linear Model, which extends these concepts to more complex situations.\n\n\nToday, we’ll build on these concepts to explore the General Linear Model (GLM), which:\n\nExtends regression to include multiple predictors\nProvides a unified framework for various statistical tests\nShows how t-tests, ANOVA, and regression are related\nAllows us to model complex relationships\nHelps us understand which factors truly matter when controlling for others\n\nMoving from:y = \\beta_0 + \\beta_1 x + \\varepsilon\nTo:y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n + \\varepsilon\n\nToday’s lecture builds directly on the foundation we established last week with correlation and simple regression. We’re now ready to take the next step by exploring the General Linear Model (GLM).\nThe progression in our learning:\n\nCorrelation: We started by measuring the strength and direction of relationships between pairs of variables.\nSimple Linear Regression: We then moved to modeling these relationships with an equation that allows prediction and deeper understanding of how one variable affects another.\nGeneral Linear Model: Today, we’ll extend this framework to include multiple predictors and show how this unifies many statistical tests under one conceptual umbrella.\n\nKey extensions in the GLM:\n\nMultiple predictors: Real-world outcomes are rarely influenced by just one factor. The GLM allows us to include multiple predictors to better model complex phenomena.\nCategorical predictors: We’ll see how to include categorical variables (like gender, treatment group, etc.) in our models.\nControlling for variables: The GLM allows us to understand the unique effect of each predictor while controlling for other factors.\nUnified framework: Perhaps most importantly, we’ll discover how many statistical tests you’ve already learned (t-tests, ANOVA, etc.) are actually special cases of the GLM.\n\nUnderstanding the GLM will not only simplify your conceptual understanding of statistics but also give you a more powerful and flexible approach to data analysis.\n\n\nAs we move forward, keep these key terms in mind:\n\n\nFrom Correlation & Regression:\n\n\nCorrelation coefficient (r): Measures strength and direction of relationship\n\nIntercept (β₀): Value of y when x = 0\n\nSlope (β₁): Change in y per unit change in x\n\nR²: Proportion of variance explained\n\nResiduals: Differences between observed and predicted values\n\n\nNew Terms for Today:\n\n\nMultiple regression: Model with multiple predictors\n\nGeneral Linear Model (GLM): Unified framework for statistical tests\n\nPredictor variables: Factors that may explain the outcome\n\nCategorical predictors: Non-numeric variables (e.g., gender)\n\nControlling for variables: Isolating the effect of one predictor\n\n\n\n\nLet’s briefly address any questions about last week’s material before moving forward.\n\n\nCommon Questions:\n\nHow do we interpret the slope and intercept in practical terms?\nWhat’s the difference between correlation and causation?\nWhen should we use correlation vs. regression?\nHow do we know if our regression model is good?\nWhat if the relationship isn’t linear?\n\n\n\n\n\n\n\n\n\n\n\n\n\nBefore we move on to new material, let’s address some common questions about correlation and regression.\nHow do we interpret the slope and intercept in practical terms?\n\nThe intercept (β₀) is the expected value of y when x = 0. In practice, this may not always be meaningful if x = 0 is outside our observed range.\nThe slope (β₁) tells us how much y changes for a one-unit increase in x. This is often the most useful part for practical interpretation.\nExample: If predicting salary from years of experience with β₁ = 3000, each additional year of experience is associated with a $3,000 increase in salary.\n\nWhat’s the difference between correlation and causation?\n\nCorrelation simply identifies that two variables change together in a predictable way\nCausation means that changes in one variable directly cause changes in another\nTo establish causation, we typically need controlled experiments or strong causal inference methods\nThe classic example: Ice cream sales and drowning deaths are correlated (both increase in summer), but one doesn’t cause the other\n\nWhen should we use correlation vs. regression?\n\nUse correlation when you simply want to measure the strength and direction of a relationship\nUse regression when you want to:\n\nPredict one variable from another\nUnderstand the effect size (how much y changes when x changes)\nControl for other variables (in multiple regression)\n\n\n\nHow do we know if our regression model is good?\n\nR² tells us the proportion of variance explained (higher is better)\nStatistical significance (p-value) tells us if the relationship is likely real or due to chance\nExamining residuals helps identify patterns the model missed\nChecking model assumptions confirms our statistical inferences are valid\n\nWhat if the relationship isn’t linear?\n\nBoth correlation and simple linear regression assume a linear relationship\nNon-linear relationships may be missed or underestimated by these methods\nSolutions include:\n\nTransforming variables (e.g., log transformation)\nUsing non-linear regression models\nUsing more flexible modeling approaches\n\n\n\nThese concepts provide the foundation for today’s topic: the General Linear Model, which extends regression to more complex situations while maintaining a unified framework."
  },
  {
    "objectID": "Week7/0-revise.html#what-we-covered-last-week",
    "href": "Week7/0-revise.html#what-we-covered-last-week",
    "title": "Reviewing Last Week: Correlation and Regression",
    "section": "",
    "text": "Last week, we explored the fundamentals of correlation and simple linear regression:\n\n\nKey Topics:\n\nCorrelation measures (Pearson’s r)\nSimple linear regression\nInterpreting slope and intercept\nAssessing model fit (R²)\nTesting significance of relationships\nAssumptions of linear regression\n\n\n\n\n\n\n\n\n\n\n\n\n\nLast week we covered two key topics that form the foundation for today’s lecture:\n\n\nCorrelation:\n\nA measure of the strength and direction of the linear relationship between two variables\nPearson’s r ranges from -1 (perfect negative correlation) to +1 (perfect positive correlation)\nA correlation of 0 indicates no linear relationship\nWe learned that correlation does not imply causation\n\n\n\nSimple Linear Regression:\n\nMoving beyond correlation to model the relationship between variables\nThe regression equation: y = β₀ + β₁x + ε\nβ₀ (intercept): The predicted value of y when x = 0\nβ₁ (slope): The change in y for a one-unit increase in x\nWe can use regression for prediction and understanding relationships\nR² measures the proportion of variance in y explained by the model\n\n\n\nThese concepts serve as building blocks for today’s topic: the General Linear Model, which extends these ideas to create a unified framework for statistical analysis."
  },
  {
    "objectID": "Week7/0-revise.html#correlation-measuring-relationships",
    "href": "Week7/0-revise.html#correlation-measuring-relationships",
    "title": "Reviewing Last Week: Correlation and Regression",
    "section": "",
    "text": "Pearson’s Correlation Coefficient (r):\n\nMeasures the strength and direction of a linear relationship\nRanges from -1 (perfect negative) to +1 (perfect positive)\nCalculated using standardized variables\nFormula: r = \\frac{\\sum{(x_i - \\bar{x})(y_i - \\bar{y})}}{\\sqrt{\\sum{(x_i - \\bar{x})^2}\\sum{(y_i - \\bar{y})^2}}}\n\n\nInterpretation: r = 0.7 means a strong positive relationship\n\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelation is a standardized measure of how two variables change together.\nKey points about correlation:\n\nCorrelation measures both the strength and direction of a linear relationship\nThe correlation coefficient (r) is always between -1 and +1\nThe sign indicates direction (positive or negative relationship)\nThe magnitude indicates strength (closer to 1 or -1 = stronger relationship)\nA correlation of 0 suggests no linear relationship\n\nInterpretation guidelines:\n\n|r| &lt; 0.3: Weak correlation\n0.3 &lt; |r| &lt; 0.7: Moderate correlation\n|r| &gt; 0.7: Strong correlation\n\nImportant limitations:\n\nCorrelation does not imply causation\nCorrelation only detects linear relationships\nCorrelation is sensitive to outliers\nCorrelation doesn’t tell us the slope of the relationship\n\nThese limitations are why we often move from correlation to regression, which provides more information about the relationship between variables."
  },
  {
    "objectID": "Week7/0-revise.html#simple-linear-regression-modeling-relationships",
    "href": "Week7/0-revise.html#simple-linear-regression-modeling-relationships",
    "title": "Reviewing Last Week: Correlation and Regression",
    "section": "",
    "text": "The Simple Linear Regression Model:\ny = \\beta_0 + \\beta_1 x + \\varepsilon\nWhere:\n\n\n\\beta_0 is the intercept (y when x = 0)\n\n\\beta_1 is the slope (change in y per unit of x)\n\n\\varepsilon is the error term\n\nKey statistics:\n\nR² (coefficient of determination): Proportion of variance explaine\np-value: Tests if the relationship is statistically significant\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimple linear regression extends correlation by modeling the relationship between variables. While correlation tells us about the strength and direction of a relationship, regression gives us an equation to predict one variable from another.\nComponents of the regression model:\n\n\nIntercept (β₀): The predicted value of y when x = 0\n\nMay not always be meaningful in real-world contexts\nExample: If x = years of experience, β₀ = starting salary with zero experience\n\n\n\nSlope (β₁): The change in y for a one-unit increase in x\n\nThe practical effect size of the relationship\nExample: Each additional year of experience increases salary by $3,000\n\n\n\nError term (ε): The difference between predicted and actual values\n\nRepresents what our model doesn’t explain\nAssumed to be normally distributed with mean zero\n\n\n\nEvaluating the model:\n\n\nR²: The proportion of variance in y explained by the model\n\nRanges from 0 to 1 (sometimes expressed as a percentage)\nExample: R² = 0.75 means the model explains 75% of the variation in y\n\n\n\nStatistical significance: Testing whether β₁ is significantly different from zero\n\nIf significant, we have evidence of a relationship between x and y\nReported as a p-value (e.g., p &lt; 0.05)\n\n\n\nRegression is a powerful tool that forms the foundation for today’s topic: the General Linear Model, which extends these concepts to more complex situations."
  },
  {
    "objectID": "Week7/0-revise.html#connecting-to-todays-topic-the-general-linear-model",
    "href": "Week7/0-revise.html#connecting-to-todays-topic-the-general-linear-model",
    "title": "Reviewing Last Week: Correlation and Regression",
    "section": "",
    "text": "Today, we’ll build on these concepts to explore the General Linear Model (GLM), which:\n\nExtends regression to include multiple predictors\nProvides a unified framework for various statistical tests\nShows how t-tests, ANOVA, and regression are related\nAllows us to model complex relationships\nHelps us understand which factors truly matter when controlling for others\n\nMoving from:y = \\beta_0 + \\beta_1 x + \\varepsilon\nTo:y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n + \\varepsilon\n\nToday’s lecture builds directly on the foundation we established last week with correlation and simple regression. We’re now ready to take the next step by exploring the General Linear Model (GLM).\nThe progression in our learning:\n\nCorrelation: We started by measuring the strength and direction of relationships between pairs of variables.\nSimple Linear Regression: We then moved to modeling these relationships with an equation that allows prediction and deeper understanding of how one variable affects another.\nGeneral Linear Model: Today, we’ll extend this framework to include multiple predictors and show how this unifies many statistical tests under one conceptual umbrella.\n\nKey extensions in the GLM:\n\nMultiple predictors: Real-world outcomes are rarely influenced by just one factor. The GLM allows us to include multiple predictors to better model complex phenomena.\nCategorical predictors: We’ll see how to include categorical variables (like gender, treatment group, etc.) in our models.\nControlling for variables: The GLM allows us to understand the unique effect of each predictor while controlling for other factors.\nUnified framework: Perhaps most importantly, we’ll discover how many statistical tests you’ve already learned (t-tests, ANOVA, etc.) are actually special cases of the GLM.\n\nUnderstanding the GLM will not only simplify your conceptual understanding of statistics but also give you a more powerful and flexible approach to data analysis."
  },
  {
    "objectID": "Week7/0-revise.html#key-terms-to-remember",
    "href": "Week7/0-revise.html#key-terms-to-remember",
    "title": "Reviewing Last Week: Correlation and Regression",
    "section": "",
    "text": "As we move forward, keep these key terms in mind:\n\n\nFrom Correlation & Regression:\n\n\nCorrelation coefficient (r): Measures strength and direction of relationship\n\nIntercept (β₀): Value of y when x = 0\n\nSlope (β₁): Change in y per unit change in x\n\nR²: Proportion of variance explained\n\nResiduals: Differences between observed and predicted values\n\n\nNew Terms for Today:\n\n\nMultiple regression: Model with multiple predictors\n\nGeneral Linear Model (GLM): Unified framework for statistical tests\n\nPredictor variables: Factors that may explain the outcome\n\nCategorical predictors: Non-numeric variables (e.g., gender)\n\nControlling for variables: Isolating the effect of one predictor"
  },
  {
    "objectID": "Week7/0-revise.html#any-questions-before-we-begin",
    "href": "Week7/0-revise.html#any-questions-before-we-begin",
    "title": "Reviewing Last Week: Correlation and Regression",
    "section": "",
    "text": "Let’s briefly address any questions about last week’s material before moving forward.\n\n\nCommon Questions:\n\nHow do we interpret the slope and intercept in practical terms?\nWhat’s the difference between correlation and causation?\nWhen should we use correlation vs. regression?\nHow do we know if our regression model is good?\nWhat if the relationship isn’t linear?\n\n\n\n\n\n\n\n\n\n\n\n\n\nBefore we move on to new material, let’s address some common questions about correlation and regression.\nHow do we interpret the slope and intercept in practical terms?\n\nThe intercept (β₀) is the expected value of y when x = 0. In practice, this may not always be meaningful if x = 0 is outside our observed range.\nThe slope (β₁) tells us how much y changes for a one-unit increase in x. This is often the most useful part for practical interpretation.\nExample: If predicting salary from years of experience with β₁ = 3000, each additional year of experience is associated with a $3,000 increase in salary.\n\nWhat’s the difference between correlation and causation?\n\nCorrelation simply identifies that two variables change together in a predictable way\nCausation means that changes in one variable directly cause changes in another\nTo establish causation, we typically need controlled experiments or strong causal inference methods\nThe classic example: Ice cream sales and drowning deaths are correlated (both increase in summer), but one doesn’t cause the other\n\nWhen should we use correlation vs. regression?\n\nUse correlation when you simply want to measure the strength and direction of a relationship\nUse regression when you want to:\n\nPredict one variable from another\nUnderstand the effect size (how much y changes when x changes)\nControl for other variables (in multiple regression)\n\n\n\nHow do we know if our regression model is good?\n\nR² tells us the proportion of variance explained (higher is better)\nStatistical significance (p-value) tells us if the relationship is likely real or due to chance\nExamining residuals helps identify patterns the model missed\nChecking model assumptions confirms our statistical inferences are valid\n\nWhat if the relationship isn’t linear?\n\nBoth correlation and simple linear regression assume a linear relationship\nNon-linear relationships may be missed or underestimated by these methods\nSolutions include:\n\nTransforming variables (e.g., log transformation)\nUsing non-linear regression models\nUsing more flexible modeling approaches\n\n\n\nThese concepts provide the foundation for today’s topic: the General Linear Model, which extends regression to more complex situations while maintaining a unified framework."
  },
  {
    "objectID": "Week5/sampling-exercise-review.html",
    "href": "Week5/sampling-exercise-review.html",
    "title": "Sampling Exercise Review",
    "section": "",
    "text": "You can download the .R file and data file here:\nDownload Resources",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Sampling Exercise & Quarto",
      "Sampling Exercise Review"
    ]
  },
  {
    "objectID": "Week5/sampling-exercise-review.html#saving-and-running-code-in-a-.r-script",
    "href": "Week5/sampling-exercise-review.html#saving-and-running-code-in-a-.r-script",
    "title": "Sampling Exercise Review",
    "section": "Saving and running code in a .R script",
    "text": "Saving and running code in a .R script\nBefore diving into how a Quarto document works, let’s review how .R files work:\nA .R script works very much like running single lines of code in the console - it will run each line in order from top to bottom. If you run it from RStudio it will even echo the lines of code into the console so you can track exactly what is happening.\nBy saving your code in a .R file you can:\n\nSave your code rather than needing to type it or copy/paste it into the console line by line.\nBuild up a full script to perform several actions at once.\n\nFor instance, to prepare the AI Jobs dataset we looked at in class, I wrote a script to download the data, clean it, and make some adjustments, then save it to a .csv file. Rather than write this each time, I can just source the ai-jobs-data.R script to do it all at once.\nYou can see all the steps of that code in the ai-jobs-data.R file. Try to look through the code and identify the blocks of code that logically fit together - in other words, the multiple lines of code which are grouped together because they do the same thing or because they form one step of the process. I have added comments to outline these for you. Even if you don’t know what every line or function does, you should be able to follow the logical flow of what the code does.\n\n\nSo, in our sampling-exercise.R file we completed a few logical steps:\n\nFirst, we need to load the tidyverse library in order to access our standard data processing functions:\nlibrary(tidyverse)\nThen, we input the observations from each sample we took and saved them to a sample_x variable. These are all lists (using c()) of 10 numbers.\nsample_1 &lt;- c(83.2, 82.6, 82.6, 82.6, 93.2, 94, 94, 48.5, 33.6, 33.6)\nsample_2 &lt;- c(88, 48, 23, 23, 23, 23, 23, 23, 23, 23)\n# etc. ...\nNext, we calculated the means for each sample. I showed two ways of doing this, let’s look at just the first for now.\nWe calculated the mean of the list of numbers of each sample:\nmean_1 &lt;- mean(sample_1)\nmean_2 &lt;- mean(sample_2)\n# etc. ...\nThen we put these means together into a table:\nsample_means &lt;- tibble(mean_values = c(mean_1, mean_2, ...))\nThis gives us a table with one row for each sample we took and a column named mean_values which contains the means we calculated:\n\n\n\n\nmean_values\n\n\n\n\n1\n72.8\n\n\n2\n32\n\n\n…\n…\n\n\n\n\nThis is where we stopped in class, but compare the steps I just outlined with the code you wrote in your own .R file and you should be able to identify these logical blocks of code.\nBy ‘sourcing’ the file, we can run this all at once and either print out our sample_means table, or take a look at it within RStudio.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Sampling Exercise & Quarto",
      "Sampling Exercise Review"
    ]
  },
  {
    "objectID": "Week5/sampling-exercise-review.html#workflow-for-a-.r-file",
    "href": "Week5/sampling-exercise-review.html#workflow-for-a-.r-file",
    "title": "Sampling Exercise Review",
    "section": "Workflow for a .R file",
    "text": "Workflow for a .R file\n\n\n\n\n\nRStudio gives you some useful tools for when you are writing an .R file.\n\nTo run the whole file (what we call ‘source’-ing the file, you can press the ‘Source’ button at the top right.\n\nBy default, this will print out just the filepath to your console (e.g. source(\"~/Documents/UCL/Teaching/BSSC0021_25/Code/Week4/sampling-exercise.R\") ) and display any outputs like plots.\nIf you select the arrow next to it, you can choose ‘Source with Echo’. This will print out each row of code to the console (or ‘echo’ it) as the code runs. This is useful if you want to check exactly what is happening.\n\n\nBy selecting only certain rows and clicking the ‘Run’ button, RStudio will run only those lines of code. This is very useful as you are writing your code and building up a full script. You can check what each part does as you go without needing to run the whole file at once.\n\nA suggested workflow for writing a script is to move back and forth between the .R file editor and the console. Build your code up in the .R file and run each logical chunk of code as you write it to make sure it works the way you expect.\nAnything that you need to run which is temporary or a one-off, type this directly in the console (like if you need to look at the help page for a function such as mean, you would run ?mean in the console to bring it up).\n\n\nHint: To repeat a previously run line of code in the console, press the up arrow - this will cycle back through the history of commands you have run. Once you get to the one you want, just press enter to run it again.\nOnce you have a few chunks of code in the .R file, you can run ‘Source’ to check that the whole thing works top to bottom.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Sampling Exercise & Quarto",
      "Sampling Exercise Review"
    ]
  },
  {
    "objectID": "Week5/sampling-exercise-review.html#appendix",
    "href": "Week5/sampling-exercise-review.html#appendix",
    "title": "Sampling Exercise Review",
    "section": "Appendix",
    "text": "Appendix\nsampling-exercise.R :\n# Load the tidyverse package\n# This will provide us with functions like `tibble`, `gather`, `group_by`, `summarise`, `ggplot`, etc.\nlibrary(tidyverse)\n\n# Input the values of the samples\n\nsample_1 &lt;- c(83.2, 82.6, 82.6, 82.6, 93.2, 94, 94, 48.5, 33.6, 33.6)\nsample_2 &lt;- c(48.8, 86.5, 67.5, 84.5, 97.6, 92, 60.7, 108, 84.3, 58.5)\nsample_3 &lt;- c(128, 53.7, 70.9, 75.2, 84.9, 91.2, 70.2, 82, 88.8, 82)\nsample_4 &lt;- c(122, 54, 101, 93.2, 89.4, 64.9, 68.3, 97.7, 77.7, 123)\nsample_5 &lt;- c(92.2, 82, 97.7, 48.5, 94, 70.6, 105, 60.7, 65.1, 82.3)\nsample_6 &lt;- c(91.2, 110, 57.7, 48.4, 122, 65.5, 86.5, 62.7, 62.7, 85.6)\nsample_7 &lt;- c(87.5, 72.8, 84.8, 56.5, 64.9, 42.2, 62.8, 54.1, 84.4, 89.7)\nsample_8 &lt;- c(78.8, 48.5, 54, 70.4, 43.8, 65.5, 78.8, 43.8, 113, 123)\nsample_9 &lt;- c(89.5, 125, 94, 92.4, 70, 99, 111, 96.9, 64.2, 63.7)\nsample_10 &lt;- c(102, 90.8, 110, 123, 79.4, 77.9, 82.3, 92.6, 90.8, 113)\nsample_11 &lt;- c(86.6, 83, 58.9, 101, 54.9, 96.8, 84.8, 60.4, 84, 83.3)\nsample_12 &lt;- c(63.7, 102, 84.3, 54.1, 71.6, 122, 40.8, 63.7, 84.4, 90.9)\nsample_13 &lt;- c(123, 93.5, 68.3, 97.3, 53.1, 50.2, 130, 48.5, 56.5, 65.1)\nsample_14 &lt;- c(109, 85.6, 54.9, 62.7, 30.3, 87.1, 94, 111, 54.1, 54.3)\nsample_15 &lt;- c(109, 44.2, 99.3, 58.5, 77, 86.2, 125, 128, 79.1, 98.2)\nsample_16 &lt;- c(41.8, 122, 70.6, 86.3, 83.7, 84.3, 82.9, 41.3, 72.7, 98.2)\nsample_17 &lt;- c(63.7, 89.8, 101, 70.2, 68.4, 77.9, 105, 53.1, 112, 55.8)\nsample_18 &lt;- c(48.8, 86.5, 67.5, 84.5, 97.6, 92, 60.7, 108, 84.3, 58.5)\nsample_19 &lt;- c(92.6, 82.3, 57.1, 57.1, 82, 62.7, 62.7, 82, 82, 77)\nsample_20 &lt;- c(67, 64.5, 87.8, 84.5, 43.8, 67.2, 30.3, 107, 85.2, 67.2)\nsample_21 &lt;- c(66.8, 85.6, 123, 109, 97.7, 89.7, 51.9, 48.5, 81.7, 51.9)\nsample_22 &lt;- c(97.1, 85.2, 118, 56.5, 91.2, 91.8, 59.3, 111, 93.2, 75.9)\nsample_23 &lt;- c(111, 64.4, 137, 82.3, 94, 64.9, 39, 75.2, 91.2, 63.8)\nsample_24 &lt;- c(97.7, 50.2, 78.8, 119, 109, 119, 40.8, 72.8, 97.1, 102)\nsample_25 &lt;- c(58.9, 79.8, 91.8, 56.5, 46.1, 84.4, 71.2, 71.2, 86.2, 70.6)\nsample_26 &lt;- c(75.8, 73.8, 57.7, 129, 101, 71.2, 137, 135, 60.3, 77)\nsample_27 &lt;- c(56.5, 52.3, 99.3, 107, 75.9, 89.2, 70, 84.3, 66.8, 93.5)\nsample_28 &lt;- c(28.3, 90.8, 39, 82.3, 89.5, 58.1, 120, 74.7, 28.8, 117)\nsample_29 &lt;- c(97.7, 85.4, 91.1, 54.1, 109, 102, 82, 90.8, 63.7, 92.3)\nsample_30 &lt;- c(101, 68.3, 62.8, 65.5, 75.8, 93.8, 81.7, 72.8, 63.4, 109)\n\n# Create a table with the samples and an id column\n\ndata &lt;- tibble(\n  sample_1, sample_2, sample_3, sample_4, sample_5,\n  sample_6, sample_7, sample_8, sample_9, sample_10,\n  sample_11, sample_12, sample_13, sample_14, sample_15,\n  sample_16, sample_17, sample_18, sample_19, sample_20,\n  sample_21, sample_22, sample_23, sample_24, sample_25,\n  sample_26, sample_27, sample_28, sample_29, sample_30\n) |&gt;\n  gather(key = \"sample\", value = \"value\")\n\ndata # Look at the table\n\n# Calculate the mean of each sample\n\nsample_means &lt;- data |&gt;\n  group_by(sample) |&gt;\n  summarise(mean = mean(value))\n\nsample_means # Look at the means\n#\n# # Plot the sampling distribution\n#\nggplot(sample_means, mapping = aes(x = mean)) +\n  geom_histogram() +\n  theme_minimal()",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Sampling Exercise & Quarto",
      "Sampling Exercise Review"
    ]
  },
  {
    "objectID": "Week5/notes.html",
    "href": "Week5/notes.html",
    "title": "Communicating Statistics",
    "section": "",
    "text": "Slides\n\n Download PDF Slides",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Communicating Statistics"
    ]
  },
  {
    "objectID": "Week5/notes.html#this-weeks-lecture",
    "href": "Week5/notes.html#this-weeks-lecture",
    "title": "Communicating Statistics",
    "section": "",
    "text": "Slides\n\n Download PDF Slides",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Communicating Statistics"
    ]
  },
  {
    "objectID": "Week5/3-analysis-workflow.html",
    "href": "Week5/3-analysis-workflow.html",
    "title": "The Data Analysis Workflow",
    "section": "",
    "text": "Source: @Wickham2023data, @Poldrack2023Statistical\n\nThere is a set of steps that we generally go through when we want to use our statistical model to test a scientific hypothesis:\n\nSpecify your question of interest\nIdentify or collect the appropriate data\nPrepare the data for analysis\nDetermine the appropriate model\nFit the model to the data\nCriticize the model to make sure it fits properly\nTest hypothesis and quantify effect size\nCommunicate your analysis\n\n\n\n\n\nThroughout, we have been using the tidyverse library of packages for data analysis.\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.\n\nCodelibrary(tidyverse)\n\n\n\n\n\n\n\n\nThere are tools for reading data from almost any source:\n\n\nread_csv(), read_excel(), read_rds(), …\n\n\nWhen we load a dataset with a tidyverse() function, it will return a tibble\n\n\n\nCodedata &lt;- read_csv(\"data/Apple_Emissions/greenhouse_gas_emissions.csv\")\n\n\n\nThe same data can be represented in multiple ways. Here’s the same data organized three different ways:\n\nEach dataset shows the same values of four variables: country, year, population, and number of documented cases of TB (tuberculosis), but each dataset organizes the values in a different way.\n\n\n\n\nCodetable1\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\nCodetable3\n\n# A tibble: 6 × 3\n  country      year rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n\n\n\nCodetable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n\n\n\n\n\nThere are three rules that make a dataset tidy:\n\nEach variable is a column; each column is a variable.\nEach observation is a row; each row is an observation.\nEach value is a cell; each cell is a single value.\n\n\n\n\n\n\n\nThere’s a general advantage to picking one consistent way of storing data. If you have a consistent data structure, it’s easier to learn the tools that work with it because they have an underlying uniformity.\nThere’s a specific advantage to placing variables in columns because it allows R’s vectorized nature to shine. That makes transforming tidy data feel particularly natural.\n\n\nSo, our first task after importing the data is to make sure it’s tidy. In addition to the rules above, this can also include things like:\n\nensure the data types are correct\nclean up the column names\nmake sure we know what the variables represent\n\nFor the .csv data we loaded, our column names can be a bit difficult to work with since they have spaces in them. We can use a function from the janitor package to clean these:\n\nCodedata &lt;- data |&gt;\n  janitor::clean_names()\ndata\n\n# A tibble: 127 × 6\n   fiscal_year category            type            scope   description emissions\n         &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;\n 1        2022 Corporate emissions Gross emissions Scope 1 Natural ga…     39700\n 2        2022 Corporate emissions Gross emissions Scope 1 Fleet vehi…     12600\n 3        2022 Corporate emissions Gross emissions Scope 1 Other (R&D…      2900\n 4        2022 Corporate emissions Gross emissions Scope … Electricity         0\n 5        2022 Corporate emissions Gross emissions Scope … Steam, hea…      3000\n 6        2022 Corporate emissions Gross emissions Scope 3 Business t…    113500\n 7        2022 Corporate emissions Gross emissions Scope 3 Employee c…    134200\n 8        2022 Corporate emissions Gross emissions Scope 3 Upstream f…     10600\n 9        2022 Corporate emissions Gross emissions Scope 3 Work from …      7500\n10        2022 Corporate emissions Gross emissions Scope 3 Transmissi…         0\n# ℹ 117 more rows\n\n\n\nWe’ve dealt with data transformations quite a bit already. This includes operations like calculating the mean for different groups, or for multiple groups:\n\nCodedata |&gt;\n  group_by(category) |&gt;\n  summarise(\n    mean_emissions = mean(emissions, na.rm = TRUE),\n  )\n\n# A tibble: 2 × 2\n  category                     mean_emissions\n  &lt;chr&gt;                                 &lt;dbl&gt;\n1 Corporate emissions                  35594.\n2 Product life cycle emissions       5630000 \n\n\n\n\nCodedata |&gt;\n  group_by(category, fiscal_year) |&gt;\n  summarise(emissions = sum(emissions, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = fiscal_year, y = emissions, color = category)) +\n  geom_line()\n\n\n\n\n\n\n\n\nThis is where we will dive into using Quarto. Start by downloading the Apple Emissions dataset from Moodle and open RStudio.\nWe’ll go through how to create and write a full analysis in a .qmd file using this dataset.\nRefer to our lecture notes specifically on using Quarto",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "The Data Analysis Workflow"
    ]
  },
  {
    "objectID": "Week5/3-analysis-workflow.html#the-process-of-statistical-modeling",
    "href": "Week5/3-analysis-workflow.html#the-process-of-statistical-modeling",
    "title": "The Data Analysis Workflow",
    "section": "",
    "text": "There is a set of steps that we generally go through when we want to use our statistical model to test a scientific hypothesis:\n\nSpecify your question of interest\nIdentify or collect the appropriate data\nPrepare the data for analysis\nDetermine the appropriate model\nFit the model to the data\nCriticize the model to make sure it fits properly\nTest hypothesis and quantify effect size\nCommunicate your analysis",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "The Data Analysis Workflow"
    ]
  },
  {
    "objectID": "Week5/3-analysis-workflow.html#import",
    "href": "Week5/3-analysis-workflow.html#import",
    "title": "The Data Analysis Workflow",
    "section": "",
    "text": "Throughout, we have been using the tidyverse library of packages for data analysis.\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.\n\nCodelibrary(tidyverse)",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "The Data Analysis Workflow"
    ]
  },
  {
    "objectID": "Week5/3-analysis-workflow.html#import-1",
    "href": "Week5/3-analysis-workflow.html#import-1",
    "title": "The Data Analysis Workflow",
    "section": "",
    "text": "There are tools for reading data from almost any source:\n\n\nread_csv(), read_excel(), read_rds(), …\n\n\nWhen we load a dataset with a tidyverse() function, it will return a tibble\n\n\n\nCodedata &lt;- read_csv(\"data/Apple_Emissions/greenhouse_gas_emissions.csv\")",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "The Data Analysis Workflow"
    ]
  },
  {
    "objectID": "Week5/3-analysis-workflow.html#tidy",
    "href": "Week5/3-analysis-workflow.html#tidy",
    "title": "The Data Analysis Workflow",
    "section": "",
    "text": "The same data can be represented in multiple ways. Here’s the same data organized three different ways:\n\nEach dataset shows the same values of four variables: country, year, population, and number of documented cases of TB (tuberculosis), but each dataset organizes the values in a different way.\n\n\n\n\nCodetable1\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\nCodetable3\n\n# A tibble: 6 × 3\n  country      year rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n\n\n\nCodetable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n\n\n\n\n\nThere are three rules that make a dataset tidy:\n\nEach variable is a column; each column is a variable.\nEach observation is a row; each row is an observation.\nEach value is a cell; each cell is a single value.\n\n\n\n\n\n\n\nThere’s a general advantage to picking one consistent way of storing data. If you have a consistent data structure, it’s easier to learn the tools that work with it because they have an underlying uniformity.\nThere’s a specific advantage to placing variables in columns because it allows R’s vectorized nature to shine. That makes transforming tidy data feel particularly natural.\n\n\nSo, our first task after importing the data is to make sure it’s tidy. In addition to the rules above, this can also include things like:\n\nensure the data types are correct\nclean up the column names\nmake sure we know what the variables represent\n\nFor the .csv data we loaded, our column names can be a bit difficult to work with since they have spaces in them. We can use a function from the janitor package to clean these:\n\nCodedata &lt;- data |&gt;\n  janitor::clean_names()\ndata\n\n# A tibble: 127 × 6\n   fiscal_year category            type            scope   description emissions\n         &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;\n 1        2022 Corporate emissions Gross emissions Scope 1 Natural ga…     39700\n 2        2022 Corporate emissions Gross emissions Scope 1 Fleet vehi…     12600\n 3        2022 Corporate emissions Gross emissions Scope 1 Other (R&D…      2900\n 4        2022 Corporate emissions Gross emissions Scope … Electricity         0\n 5        2022 Corporate emissions Gross emissions Scope … Steam, hea…      3000\n 6        2022 Corporate emissions Gross emissions Scope 3 Business t…    113500\n 7        2022 Corporate emissions Gross emissions Scope 3 Employee c…    134200\n 8        2022 Corporate emissions Gross emissions Scope 3 Upstream f…     10600\n 9        2022 Corporate emissions Gross emissions Scope 3 Work from …      7500\n10        2022 Corporate emissions Gross emissions Scope 3 Transmissi…         0\n# ℹ 117 more rows",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "The Data Analysis Workflow"
    ]
  },
  {
    "objectID": "Week5/3-analysis-workflow.html#transform",
    "href": "Week5/3-analysis-workflow.html#transform",
    "title": "The Data Analysis Workflow",
    "section": "",
    "text": "We’ve dealt with data transformations quite a bit already. This includes operations like calculating the mean for different groups, or for multiple groups:\n\nCodedata |&gt;\n  group_by(category) |&gt;\n  summarise(\n    mean_emissions = mean(emissions, na.rm = TRUE),\n  )\n\n# A tibble: 2 × 2\n  category                     mean_emissions\n  &lt;chr&gt;                                 &lt;dbl&gt;\n1 Corporate emissions                  35594.\n2 Product life cycle emissions       5630000",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "The Data Analysis Workflow"
    ]
  },
  {
    "objectID": "Week5/3-analysis-workflow.html#visualize",
    "href": "Week5/3-analysis-workflow.html#visualize",
    "title": "The Data Analysis Workflow",
    "section": "",
    "text": "Codedata |&gt;\n  group_by(category, fiscal_year) |&gt;\n  summarise(emissions = sum(emissions, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = fiscal_year, y = emissions, color = category)) +\n  geom_line()",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "The Data Analysis Workflow"
    ]
  },
  {
    "objectID": "Week5/3-analysis-workflow.html#communicate",
    "href": "Week5/3-analysis-workflow.html#communicate",
    "title": "The Data Analysis Workflow",
    "section": "",
    "text": "This is where we will dive into using Quarto. Start by downloading the Apple Emissions dataset from Moodle and open RStudio.\nWe’ll go through how to create and write a full analysis in a .qmd file using this dataset.\nRefer to our lecture notes specifically on using Quarto",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "The Data Analysis Workflow"
    ]
  },
  {
    "objectID": "Week5/3-analysis-workflow.html#references",
    "href": "Week5/3-analysis-workflow.html#references",
    "title": "The Data Analysis Workflow",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "The Data Analysis Workflow"
    ]
  },
  {
    "objectID": "Week5/1-content.html",
    "href": "Week5/1-content.html",
    "title": "Energy Consumption Prediction",
    "section": "",
    "text": "Climate change is a globally relevant, urgent, and multi-faceted issue heavily impacted by energy policy and infrastructure. Addressing climate change involves mitigation (i.e. mitigating greenhouse gas emissions) and adaptation (i.e. preparing for unavoidable consequences). Mitigation of GHG emissions requires changes to electricity systems, transportation, buildings, industry, and land use.\n\n\nAccording to a report issued by the International Energy Agency (IEA), the lifecycle of buildings from construction to demolition were responsible for 37% of global energy-related CO2 emissions in 2020. Yet it is possible to drastically reduct the energy consumption of buildings by a combination of easy-to-implement fixes and state-of-the-art strategies. For example, retrofitted buildings can reduce heating and colling energy requirements by 50-90 percent. Many of these energy efficiency measures also result in overall cost savings and yield other benefits, such as cleaner air for occupants. This potential can be achieved while maintaining the services that buildings provide.\n\n\nThe goal of this competition is to predict the energy consumption using building characteristics and climate and weather variables.\n\nThe WiDS Datathon 2022 focuses on a prediction task involving roughly 100k observations of building energy usage records collected over 7 years and a number of states within the United States. The dataset consists of building characteristics (e.g. floor area, facility type etc), weather data for the location of the building (e.g. annual average temperature, annual total precipitation etc) as well as the energy usage for the building and the given year, measured as Site Energy Usage Intensity (Site EUI). Each row in the data corresponds to the a single building observed in a given year. Your task is to predict the Site EUI for each row, given the characteristics of the building and the weather data for the location of the building.\n\n\n\nCodelibrary(gtExtras)\nlibrary(gtsummary)\nlibrary(tidyverse)\nlibrary(tidymodels)\n\ndata &lt;- read_csv(\"data/buildings-data/train.csv\")\n\n\n\n\nCodedata &lt;- data |&gt;\n  janitor::clean_names() |&gt;\n  mutate(\n    state_factor = factor(state_factor),\n    building_class = factor(building_class),\n    facility_type = factor(facility_type),\n    year_built = factor(year_built, ordered = TRUE),\n    energy_star_rating = factor(energy_star_rating, ordered = TRUE),\n    log_site_eui = log(site_eui)\n  )\n\ndata |&gt;\n  gt_preview() |&gt;\n  tab_header(title = \"Building Energy Dataset\")\n\n\n\n\n\n\nBuilding Energy Dataset\n\n\n\nyear_factor\nstate_factor\nbuilding_class\nfacility_type\nfloor_area\nyear_built\nenergy_star_rating\nelevation\njanuary_min_temp\njanuary_avg_temp\njanuary_max_temp\nfebruary_min_temp\nfebruary_avg_temp\nfebruary_max_temp\nmarch_min_temp\nmarch_avg_temp\nmarch_max_temp\napril_min_temp\napril_avg_temp\napril_max_temp\nmay_min_temp\nmay_avg_temp\nmay_max_temp\njune_min_temp\njune_avg_temp\njune_max_temp\njuly_min_temp\njuly_avg_temp\njuly_max_temp\naugust_min_temp\naugust_avg_temp\naugust_max_temp\nseptember_min_temp\nseptember_avg_temp\nseptember_max_temp\noctober_min_temp\noctober_avg_temp\noctober_max_temp\nnovember_min_temp\nnovember_avg_temp\nnovember_max_temp\ndecember_min_temp\ndecember_avg_temp\ndecember_max_temp\ncooling_degree_days\nheating_degree_days\nprecipitation_inches\nsnowfall_inches\nsnowdepth_inches\navg_temp\ndays_below_30f\ndays_below_20f\ndays_below_10f\ndays_below_0f\ndays_above_80f\ndays_above_90f\ndays_above_100f\ndays_above_110f\ndirection_max_wind_speed\ndirection_peak_wind_speed\nmax_wind_speed\ndays_with_fog\nsite_eui\nid\nlog_site_eui\n\n\n\n\n1\n1\nState_1\nCommercial\nGrocery_store_or_food_market\n61242\n1942\n11\n2.4\n36\n50.50000\n68\n35\n50.58929\n73\n40\n53.69355\n80\n41\n55.50000\n78\n46\n56.85484\n84\n50\n60.50000\n90\n52\n62.72581\n84\n52\n62.16129\n85\n52\n64.65000\n90\n47\n63.01613\n83\n43\n53.80\n72\n36\n49.27419\n71\n115\n2960\n16.59\n0.0\n0\n56.9726\n0\n0\n0\n0\n14\n0\n0\n0\n1\n1\n1\nNA\n248.682615\n0\n5.516177\n\n\n2\n1\nState_1\nCommercial\nWarehouse_Distribution_or_Shipping_center\n274000\n1955\n45\n1.8\n36\n50.50000\n68\n35\n50.58929\n73\n40\n53.69355\n80\n41\n55.50000\n78\n46\n56.85484\n84\n50\n60.50000\n90\n52\n62.72581\n84\n52\n62.16129\n85\n52\n64.65000\n90\n47\n63.01613\n83\n43\n53.80\n72\n36\n49.27419\n71\n115\n2960\n16.59\n0.0\n0\n56.9726\n0\n0\n0\n0\n14\n0\n0\n0\n1\nNA\n1\n12\n26.500150\n1\n3.277150\n\n\n3\n1\nState_1\nCommercial\nRetail_Enclosed_mall\n280025\n1951\n97\n1.8\n36\n50.50000\n68\n35\n50.58929\n73\n40\n53.69355\n80\n41\n55.50000\n78\n46\n56.85484\n84\n50\n60.50000\n90\n52\n62.72581\n84\n52\n62.16129\n85\n52\n64.65000\n90\n47\n63.01613\n83\n43\n53.80\n72\n36\n49.27419\n71\n115\n2960\n16.59\n0.0\n0\n56.9726\n0\n0\n0\n0\n14\n0\n0\n0\n1\nNA\n1\n12\n24.693619\n2\n3.206545\n\n\n4\n1\nState_1\nCommercial\nEducation_Other_classroom\n55325\n1980\n46\n1.8\n36\n50.50000\n68\n35\n50.58929\n73\n40\n53.69355\n80\n41\n55.50000\n78\n46\n56.85484\n84\n50\n60.50000\n90\n52\n62.72581\n84\n52\n62.16129\n85\n52\n64.65000\n90\n47\n63.01613\n83\n43\n53.80\n72\n36\n49.27419\n71\n115\n2960\n16.59\n0.0\n0\n56.9726\n0\n0\n0\n0\n14\n0\n0\n0\n1\nNA\n1\n12\n48.406926\n3\n3.879643\n\n\n5\n1\nState_1\nCommercial\nWarehouse_Nonrefrigerated\n66000\n1985\n100\n2.4\n36\n50.50000\n68\n35\n50.58929\n73\n40\n53.69355\n80\n41\n55.50000\n78\n46\n56.85484\n84\n50\n60.50000\n90\n52\n62.72581\n84\n52\n62.16129\n85\n52\n64.65000\n90\n47\n63.01613\n83\n43\n53.80\n72\n36\n49.27419\n71\n115\n2960\n16.59\n0.0\n0\n56.9726\n0\n0\n0\n0\n14\n0\n0\n0\n1\n1\n1\nNA\n3.899395\n4\n1.360821\n\n\n6..75756\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n75757\n6\nState_11\nResidential\n2to4_Unit_Building\n23888\n1974\n51\n36.6\n27\n36.93548\n51\n29\n42.17241\n60\n30\n41.40323\n66\n36\n51.53333\n85\n41\n53.88710\n80\n41\n58.43333\n90\n48\n60.53226\n83\n49\n64.33871\n90\n43\n55.93103\n75\n40\n48.53226\n60\n31\n45.15\n69\n18\n30.91935\n42\n148\n5853\n107.69\n28.8\n377\n49.1274\n17\n1\n0\n0\n16\n0\n0\n0\nNA\nNA\nNA\nNA\n29.154684\n75756\n3.372616\n\n\n\n\n\n\n\nCodedata |&gt;\n  ggplot(mapping = aes(x = log_site_eui)) +\n  geom_density()\n\n\n\n\n\n\n\n\nCoderes &lt;- lm(site_eui ~ facility_type, data = data)\nsummary(res)\n\n\nCall:\nlm(formula = site_eui ~ facility_type, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-317.99  -22.06   -5.52   13.75  909.55 \n\nCoefficients:\n                                                       Estimate Std. Error\n(Intercept)                                              31.877      1.182\nfacility_type5plus_Unit_Building                          4.861      1.864\nfacility_typeCommercial_Other                            60.765      1.707\nfacility_typeCommercial_Unknown                          81.275      5.406\nfacility_typeData_Center                                307.858      9.965\nfacility_typeEducation_College_or_university             76.752      1.975\nfacility_typeEducation_Other_classroom                   37.565      1.443\nfacility_typeEducation_Preschool_or_daycare              29.097      5.087\nfacility_typeEducation_Uncategorized                     14.296      2.264\nfacility_typeFood_Sales                                 104.916      6.258\nfacility_typeFood_Service_Other                          -2.640     12.526\nfacility_typeFood_Service_Restaurant_or_cafeteria       163.717      6.535\nfacility_typeFood_Service_Uncategorized                  96.925     12.176\nfacility_typeGrocery_store_or_food_market               209.258      2.701\nfacility_typeHealth_Care_Inpatient                      216.464      2.804\nfacility_typeHealth_Care_Outpatient_Clinic               71.736      6.972\nfacility_typeHealth_Care_Outpatient_Uncategorized       158.015      8.650\nfacility_typeHealth_Care_Uncategorized                  152.068      7.296\nfacility_typeIndustrial                                  93.468      2.878\nfacility_typeLaboratory                                 297.572      5.109\nfacility_typeLodging_Dormitory_or_fraternity_sorority    49.719      2.313\nfacility_typeLodging_Hotel                               73.058      1.630\nfacility_typeLodging_Other                               89.081      6.053\nfacility_typeLodging_Uncategorized                       34.719     23.024\nfacility_typeMixed_Use_Commercial_and_Residential        57.653      2.309\nfacility_typeMixed_Use_Predominantly_Commercial          37.256      3.424\nfacility_typeMixed_Use_Predominantly_Residential         49.921     17.179\nfacility_typeMultifamily_Uncategorized                   52.002      1.210\nfacility_typeNursing_Home                                99.437      2.196\nfacility_typeOffice_Bank_or_other_financial              58.019      4.084\nfacility_typeOffice_Medical_non_diagnostic               84.885      2.704\nfacility_typeOffice_Mixed_use                            50.233     12.176\nfacility_typeOffice_Uncategorized                        45.197      1.268\nfacility_typeParking_Garage                              35.474      3.454\nfacility_typePublic_Assembly_Drama_theater               49.040      6.258\nfacility_typePublic_Assembly_Entertainment_culture       87.023      5.043\nfacility_typePublic_Assembly_Library                     73.972      4.233\nfacility_typePublic_Assembly_Movie_Theater               71.218      8.317\nfacility_typePublic_Assembly_Other                       94.827      4.474\nfacility_typePublic_Assembly_Recreation                  83.301      6.174\nfacility_typePublic_Assembly_Social_meeting              47.045      5.607\nfacility_typePublic_Assembly_Stadium                    125.165     17.179\nfacility_typePublic_Assembly_Uncategorized               30.996     10.351\nfacility_typePublic_Safety_Courthouse                    71.305      8.424\nfacility_typePublic_Safety_Fire_or_police_station        99.246      4.270\nfacility_typePublic_Safety_Penitentiary                 139.028      8.535\nfacility_typePublic_Safety_Uncategorized                 51.832      7.929\nfacility_typeReligious_worship                           12.684      2.832\nfacility_typeRetail_Enclosed_mall                        69.089      4.840\nfacility_typeRetail_Strip_shopping_mall                  78.542      4.979\nfacility_typeRetail_Uncategorized                        49.026      1.933\nfacility_typeRetail_Vehicle_dealership_showroom          14.764      6.093\nfacility_typeService_Drycleaning_or_Laundry              10.236     17.179\nfacility_typeService_Uncategorized                       81.689      6.346\nfacility_typeService_Vehicle_service_repair_shop        105.719      4.533\nfacility_typeWarehouse_Distribution_or_Shipping_center    7.683      2.403\nfacility_typeWarehouse_Nonrefrigerated                    6.332      1.872\nfacility_typeWarehouse_Refrigerated                      64.648      4.979\nfacility_typeWarehouse_Selfstorage                      -10.288      2.445\nfacility_typeWarehouse_Uncategorized                      4.060      3.067\n                                                       t value Pr(&gt;|t|)    \n(Intercept)                                             26.975  &lt; 2e-16 ***\nfacility_type5plus_Unit_Building                         2.608 0.009101 ** \nfacility_typeCommercial_Other                           35.607  &lt; 2e-16 ***\nfacility_typeCommercial_Unknown                         15.035  &lt; 2e-16 ***\nfacility_typeData_Center                                30.893  &lt; 2e-16 ***\nfacility_typeEducation_College_or_university            38.866  &lt; 2e-16 ***\nfacility_typeEducation_Other_classroom                  26.038  &lt; 2e-16 ***\nfacility_typeEducation_Preschool_or_daycare              5.720 1.07e-08 ***\nfacility_typeEducation_Uncategorized                     6.315 2.72e-10 ***\nfacility_typeFood_Sales                                 16.765  &lt; 2e-16 ***\nfacility_typeFood_Service_Other                         -0.211 0.833054    \nfacility_typeFood_Service_Restaurant_or_cafeteria       25.054  &lt; 2e-16 ***\nfacility_typeFood_Service_Uncategorized                  7.960 1.74e-15 ***\nfacility_typeGrocery_store_or_food_market               77.465  &lt; 2e-16 ***\nfacility_typeHealth_Care_Inpatient                      77.211  &lt; 2e-16 ***\nfacility_typeHealth_Care_Outpatient_Clinic              10.290  &lt; 2e-16 ***\nfacility_typeHealth_Care_Outpatient_Uncategorized       18.267  &lt; 2e-16 ***\nfacility_typeHealth_Care_Uncategorized                  20.843  &lt; 2e-16 ***\nfacility_typeIndustrial                                 32.481  &lt; 2e-16 ***\nfacility_typeLaboratory                                 58.244  &lt; 2e-16 ***\nfacility_typeLodging_Dormitory_or_fraternity_sorority   21.499  &lt; 2e-16 ***\nfacility_typeLodging_Hotel                              44.824  &lt; 2e-16 ***\nfacility_typeLodging_Other                              14.716  &lt; 2e-16 ***\nfacility_typeLodging_Uncategorized                       1.508 0.131577    \nfacility_typeMixed_Use_Commercial_and_Residential       24.971  &lt; 2e-16 ***\nfacility_typeMixed_Use_Predominantly_Commercial         10.881  &lt; 2e-16 ***\nfacility_typeMixed_Use_Predominantly_Residential         2.906 0.003663 ** \nfacility_typeMultifamily_Uncategorized                  42.986  &lt; 2e-16 ***\nfacility_typeNursing_Home                               45.289  &lt; 2e-16 ***\nfacility_typeOffice_Bank_or_other_financial             14.207  &lt; 2e-16 ***\nfacility_typeOffice_Medical_non_diagnostic              31.395  &lt; 2e-16 ***\nfacility_typeOffice_Mixed_use                            4.126 3.70e-05 ***\nfacility_typeOffice_Uncategorized                       35.645  &lt; 2e-16 ***\nfacility_typeParking_Garage                             10.271  &lt; 2e-16 ***\nfacility_typePublic_Assembly_Drama_theater               7.836 4.69e-15 ***\nfacility_typePublic_Assembly_Entertainment_culture      17.257  &lt; 2e-16 ***\nfacility_typePublic_Assembly_Library                    17.475  &lt; 2e-16 ***\nfacility_typePublic_Assembly_Movie_Theater               8.563  &lt; 2e-16 ***\nfacility_typePublic_Assembly_Other                      21.197  &lt; 2e-16 ***\nfacility_typePublic_Assembly_Recreation                 13.493  &lt; 2e-16 ***\nfacility_typePublic_Assembly_Social_meeting              8.391  &lt; 2e-16 ***\nfacility_typePublic_Assembly_Stadium                     7.286 3.23e-13 ***\nfacility_typePublic_Assembly_Uncategorized               2.995 0.002750 ** \nfacility_typePublic_Safety_Courthouse                    8.465  &lt; 2e-16 ***\nfacility_typePublic_Safety_Fire_or_police_station       23.242  &lt; 2e-16 ***\nfacility_typePublic_Safety_Penitentiary                 16.289  &lt; 2e-16 ***\nfacility_typePublic_Safety_Uncategorized                 6.537 6.33e-11 ***\nfacility_typeReligious_worship                           4.478 7.54e-06 ***\nfacility_typeRetail_Enclosed_mall                       14.274  &lt; 2e-16 ***\nfacility_typeRetail_Strip_shopping_mall                 15.775  &lt; 2e-16 ***\nfacility_typeRetail_Uncategorized                       25.365  &lt; 2e-16 ***\nfacility_typeRetail_Vehicle_dealership_showroom          2.423 0.015384 *  \nfacility_typeService_Drycleaning_or_Laundry              0.596 0.551299    \nfacility_typeService_Uncategorized                      12.872  &lt; 2e-16 ***\nfacility_typeService_Vehicle_service_repair_shop        23.320  &lt; 2e-16 ***\nfacility_typeWarehouse_Distribution_or_Shipping_center   3.197 0.001387 ** \nfacility_typeWarehouse_Nonrefrigerated                   3.383 0.000716 ***\nfacility_typeWarehouse_Refrigerated                     12.984  &lt; 2e-16 ***\nfacility_typeWarehouse_Selfstorage                      -4.208 2.58e-05 ***\nfacility_typeWarehouse_Uncategorized                     1.324 0.185652    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 51.42 on 75697 degrees of freedom\nMultiple R-squared:  0.2217,    Adjusted R-squared:  0.221 \nF-statistic: 365.4 on 59 and 75697 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Week5/1-content.html#goal",
    "href": "Week5/1-content.html#goal",
    "title": "Energy Consumption Prediction",
    "section": "",
    "text": "The goal of this competition is to predict the energy consumption using building characteristics and climate and weather variables."
  },
  {
    "objectID": "Week5/1-content.html#data",
    "href": "Week5/1-content.html#data",
    "title": "Energy Consumption Prediction",
    "section": "",
    "text": "The WiDS Datathon 2022 focuses on a prediction task involving roughly 100k observations of building energy usage records collected over 7 years and a number of states within the United States. The dataset consists of building characteristics (e.g. floor area, facility type etc), weather data for the location of the building (e.g. annual average temperature, annual total precipitation etc) as well as the energy usage for the building and the given year, measured as Site Energy Usage Intensity (Site EUI). Each row in the data corresponds to the a single building observed in a given year. Your task is to predict the Site EUI for each row, given the characteristics of the building and the weather data for the location of the building."
  },
  {
    "objectID": "Week5/1-content.html#setup",
    "href": "Week5/1-content.html#setup",
    "title": "Energy Consumption Prediction",
    "section": "",
    "text": "Codelibrary(gtExtras)\nlibrary(gtsummary)\nlibrary(tidyverse)\nlibrary(tidymodels)\n\ndata &lt;- read_csv(\"data/buildings-data/train.csv\")\n\n\n\n\nCodedata &lt;- data |&gt;\n  janitor::clean_names() |&gt;\n  mutate(\n    state_factor = factor(state_factor),\n    building_class = factor(building_class),\n    facility_type = factor(facility_type),\n    year_built = factor(year_built, ordered = TRUE),\n    energy_star_rating = factor(energy_star_rating, ordered = TRUE),\n    log_site_eui = log(site_eui)\n  )\n\ndata |&gt;\n  gt_preview() |&gt;\n  tab_header(title = \"Building Energy Dataset\")\n\n\n\n\n\n\nBuilding Energy Dataset\n\n\n\nyear_factor\nstate_factor\nbuilding_class\nfacility_type\nfloor_area\nyear_built\nenergy_star_rating\nelevation\njanuary_min_temp\njanuary_avg_temp\njanuary_max_temp\nfebruary_min_temp\nfebruary_avg_temp\nfebruary_max_temp\nmarch_min_temp\nmarch_avg_temp\nmarch_max_temp\napril_min_temp\napril_avg_temp\napril_max_temp\nmay_min_temp\nmay_avg_temp\nmay_max_temp\njune_min_temp\njune_avg_temp\njune_max_temp\njuly_min_temp\njuly_avg_temp\njuly_max_temp\naugust_min_temp\naugust_avg_temp\naugust_max_temp\nseptember_min_temp\nseptember_avg_temp\nseptember_max_temp\noctober_min_temp\noctober_avg_temp\noctober_max_temp\nnovember_min_temp\nnovember_avg_temp\nnovember_max_temp\ndecember_min_temp\ndecember_avg_temp\ndecember_max_temp\ncooling_degree_days\nheating_degree_days\nprecipitation_inches\nsnowfall_inches\nsnowdepth_inches\navg_temp\ndays_below_30f\ndays_below_20f\ndays_below_10f\ndays_below_0f\ndays_above_80f\ndays_above_90f\ndays_above_100f\ndays_above_110f\ndirection_max_wind_speed\ndirection_peak_wind_speed\nmax_wind_speed\ndays_with_fog\nsite_eui\nid\nlog_site_eui\n\n\n\n\n1\n1\nState_1\nCommercial\nGrocery_store_or_food_market\n61242\n1942\n11\n2.4\n36\n50.50000\n68\n35\n50.58929\n73\n40\n53.69355\n80\n41\n55.50000\n78\n46\n56.85484\n84\n50\n60.50000\n90\n52\n62.72581\n84\n52\n62.16129\n85\n52\n64.65000\n90\n47\n63.01613\n83\n43\n53.80\n72\n36\n49.27419\n71\n115\n2960\n16.59\n0.0\n0\n56.9726\n0\n0\n0\n0\n14\n0\n0\n0\n1\n1\n1\nNA\n248.682615\n0\n5.516177\n\n\n2\n1\nState_1\nCommercial\nWarehouse_Distribution_or_Shipping_center\n274000\n1955\n45\n1.8\n36\n50.50000\n68\n35\n50.58929\n73\n40\n53.69355\n80\n41\n55.50000\n78\n46\n56.85484\n84\n50\n60.50000\n90\n52\n62.72581\n84\n52\n62.16129\n85\n52\n64.65000\n90\n47\n63.01613\n83\n43\n53.80\n72\n36\n49.27419\n71\n115\n2960\n16.59\n0.0\n0\n56.9726\n0\n0\n0\n0\n14\n0\n0\n0\n1\nNA\n1\n12\n26.500150\n1\n3.277150\n\n\n3\n1\nState_1\nCommercial\nRetail_Enclosed_mall\n280025\n1951\n97\n1.8\n36\n50.50000\n68\n35\n50.58929\n73\n40\n53.69355\n80\n41\n55.50000\n78\n46\n56.85484\n84\n50\n60.50000\n90\n52\n62.72581\n84\n52\n62.16129\n85\n52\n64.65000\n90\n47\n63.01613\n83\n43\n53.80\n72\n36\n49.27419\n71\n115\n2960\n16.59\n0.0\n0\n56.9726\n0\n0\n0\n0\n14\n0\n0\n0\n1\nNA\n1\n12\n24.693619\n2\n3.206545\n\n\n4\n1\nState_1\nCommercial\nEducation_Other_classroom\n55325\n1980\n46\n1.8\n36\n50.50000\n68\n35\n50.58929\n73\n40\n53.69355\n80\n41\n55.50000\n78\n46\n56.85484\n84\n50\n60.50000\n90\n52\n62.72581\n84\n52\n62.16129\n85\n52\n64.65000\n90\n47\n63.01613\n83\n43\n53.80\n72\n36\n49.27419\n71\n115\n2960\n16.59\n0.0\n0\n56.9726\n0\n0\n0\n0\n14\n0\n0\n0\n1\nNA\n1\n12\n48.406926\n3\n3.879643\n\n\n5\n1\nState_1\nCommercial\nWarehouse_Nonrefrigerated\n66000\n1985\n100\n2.4\n36\n50.50000\n68\n35\n50.58929\n73\n40\n53.69355\n80\n41\n55.50000\n78\n46\n56.85484\n84\n50\n60.50000\n90\n52\n62.72581\n84\n52\n62.16129\n85\n52\n64.65000\n90\n47\n63.01613\n83\n43\n53.80\n72\n36\n49.27419\n71\n115\n2960\n16.59\n0.0\n0\n56.9726\n0\n0\n0\n0\n14\n0\n0\n0\n1\n1\n1\nNA\n3.899395\n4\n1.360821\n\n\n6..75756\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n75757\n6\nState_11\nResidential\n2to4_Unit_Building\n23888\n1974\n51\n36.6\n27\n36.93548\n51\n29\n42.17241\n60\n30\n41.40323\n66\n36\n51.53333\n85\n41\n53.88710\n80\n41\n58.43333\n90\n48\n60.53226\n83\n49\n64.33871\n90\n43\n55.93103\n75\n40\n48.53226\n60\n31\n45.15\n69\n18\n30.91935\n42\n148\n5853\n107.69\n28.8\n377\n49.1274\n17\n1\n0\n0\n16\n0\n0\n0\nNA\nNA\nNA\nNA\n29.154684\n75756\n3.372616\n\n\n\n\n\n\n\nCodedata |&gt;\n  ggplot(mapping = aes(x = log_site_eui)) +\n  geom_density()\n\n\n\n\n\n\n\n\nCoderes &lt;- lm(site_eui ~ facility_type, data = data)\nsummary(res)\n\n\nCall:\nlm(formula = site_eui ~ facility_type, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-317.99  -22.06   -5.52   13.75  909.55 \n\nCoefficients:\n                                                       Estimate Std. Error\n(Intercept)                                              31.877      1.182\nfacility_type5plus_Unit_Building                          4.861      1.864\nfacility_typeCommercial_Other                            60.765      1.707\nfacility_typeCommercial_Unknown                          81.275      5.406\nfacility_typeData_Center                                307.858      9.965\nfacility_typeEducation_College_or_university             76.752      1.975\nfacility_typeEducation_Other_classroom                   37.565      1.443\nfacility_typeEducation_Preschool_or_daycare              29.097      5.087\nfacility_typeEducation_Uncategorized                     14.296      2.264\nfacility_typeFood_Sales                                 104.916      6.258\nfacility_typeFood_Service_Other                          -2.640     12.526\nfacility_typeFood_Service_Restaurant_or_cafeteria       163.717      6.535\nfacility_typeFood_Service_Uncategorized                  96.925     12.176\nfacility_typeGrocery_store_or_food_market               209.258      2.701\nfacility_typeHealth_Care_Inpatient                      216.464      2.804\nfacility_typeHealth_Care_Outpatient_Clinic               71.736      6.972\nfacility_typeHealth_Care_Outpatient_Uncategorized       158.015      8.650\nfacility_typeHealth_Care_Uncategorized                  152.068      7.296\nfacility_typeIndustrial                                  93.468      2.878\nfacility_typeLaboratory                                 297.572      5.109\nfacility_typeLodging_Dormitory_or_fraternity_sorority    49.719      2.313\nfacility_typeLodging_Hotel                               73.058      1.630\nfacility_typeLodging_Other                               89.081      6.053\nfacility_typeLodging_Uncategorized                       34.719     23.024\nfacility_typeMixed_Use_Commercial_and_Residential        57.653      2.309\nfacility_typeMixed_Use_Predominantly_Commercial          37.256      3.424\nfacility_typeMixed_Use_Predominantly_Residential         49.921     17.179\nfacility_typeMultifamily_Uncategorized                   52.002      1.210\nfacility_typeNursing_Home                                99.437      2.196\nfacility_typeOffice_Bank_or_other_financial              58.019      4.084\nfacility_typeOffice_Medical_non_diagnostic               84.885      2.704\nfacility_typeOffice_Mixed_use                            50.233     12.176\nfacility_typeOffice_Uncategorized                        45.197      1.268\nfacility_typeParking_Garage                              35.474      3.454\nfacility_typePublic_Assembly_Drama_theater               49.040      6.258\nfacility_typePublic_Assembly_Entertainment_culture       87.023      5.043\nfacility_typePublic_Assembly_Library                     73.972      4.233\nfacility_typePublic_Assembly_Movie_Theater               71.218      8.317\nfacility_typePublic_Assembly_Other                       94.827      4.474\nfacility_typePublic_Assembly_Recreation                  83.301      6.174\nfacility_typePublic_Assembly_Social_meeting              47.045      5.607\nfacility_typePublic_Assembly_Stadium                    125.165     17.179\nfacility_typePublic_Assembly_Uncategorized               30.996     10.351\nfacility_typePublic_Safety_Courthouse                    71.305      8.424\nfacility_typePublic_Safety_Fire_or_police_station        99.246      4.270\nfacility_typePublic_Safety_Penitentiary                 139.028      8.535\nfacility_typePublic_Safety_Uncategorized                 51.832      7.929\nfacility_typeReligious_worship                           12.684      2.832\nfacility_typeRetail_Enclosed_mall                        69.089      4.840\nfacility_typeRetail_Strip_shopping_mall                  78.542      4.979\nfacility_typeRetail_Uncategorized                        49.026      1.933\nfacility_typeRetail_Vehicle_dealership_showroom          14.764      6.093\nfacility_typeService_Drycleaning_or_Laundry              10.236     17.179\nfacility_typeService_Uncategorized                       81.689      6.346\nfacility_typeService_Vehicle_service_repair_shop        105.719      4.533\nfacility_typeWarehouse_Distribution_or_Shipping_center    7.683      2.403\nfacility_typeWarehouse_Nonrefrigerated                    6.332      1.872\nfacility_typeWarehouse_Refrigerated                      64.648      4.979\nfacility_typeWarehouse_Selfstorage                      -10.288      2.445\nfacility_typeWarehouse_Uncategorized                      4.060      3.067\n                                                       t value Pr(&gt;|t|)    \n(Intercept)                                             26.975  &lt; 2e-16 ***\nfacility_type5plus_Unit_Building                         2.608 0.009101 ** \nfacility_typeCommercial_Other                           35.607  &lt; 2e-16 ***\nfacility_typeCommercial_Unknown                         15.035  &lt; 2e-16 ***\nfacility_typeData_Center                                30.893  &lt; 2e-16 ***\nfacility_typeEducation_College_or_university            38.866  &lt; 2e-16 ***\nfacility_typeEducation_Other_classroom                  26.038  &lt; 2e-16 ***\nfacility_typeEducation_Preschool_or_daycare              5.720 1.07e-08 ***\nfacility_typeEducation_Uncategorized                     6.315 2.72e-10 ***\nfacility_typeFood_Sales                                 16.765  &lt; 2e-16 ***\nfacility_typeFood_Service_Other                         -0.211 0.833054    \nfacility_typeFood_Service_Restaurant_or_cafeteria       25.054  &lt; 2e-16 ***\nfacility_typeFood_Service_Uncategorized                  7.960 1.74e-15 ***\nfacility_typeGrocery_store_or_food_market               77.465  &lt; 2e-16 ***\nfacility_typeHealth_Care_Inpatient                      77.211  &lt; 2e-16 ***\nfacility_typeHealth_Care_Outpatient_Clinic              10.290  &lt; 2e-16 ***\nfacility_typeHealth_Care_Outpatient_Uncategorized       18.267  &lt; 2e-16 ***\nfacility_typeHealth_Care_Uncategorized                  20.843  &lt; 2e-16 ***\nfacility_typeIndustrial                                 32.481  &lt; 2e-16 ***\nfacility_typeLaboratory                                 58.244  &lt; 2e-16 ***\nfacility_typeLodging_Dormitory_or_fraternity_sorority   21.499  &lt; 2e-16 ***\nfacility_typeLodging_Hotel                              44.824  &lt; 2e-16 ***\nfacility_typeLodging_Other                              14.716  &lt; 2e-16 ***\nfacility_typeLodging_Uncategorized                       1.508 0.131577    \nfacility_typeMixed_Use_Commercial_and_Residential       24.971  &lt; 2e-16 ***\nfacility_typeMixed_Use_Predominantly_Commercial         10.881  &lt; 2e-16 ***\nfacility_typeMixed_Use_Predominantly_Residential         2.906 0.003663 ** \nfacility_typeMultifamily_Uncategorized                  42.986  &lt; 2e-16 ***\nfacility_typeNursing_Home                               45.289  &lt; 2e-16 ***\nfacility_typeOffice_Bank_or_other_financial             14.207  &lt; 2e-16 ***\nfacility_typeOffice_Medical_non_diagnostic              31.395  &lt; 2e-16 ***\nfacility_typeOffice_Mixed_use                            4.126 3.70e-05 ***\nfacility_typeOffice_Uncategorized                       35.645  &lt; 2e-16 ***\nfacility_typeParking_Garage                             10.271  &lt; 2e-16 ***\nfacility_typePublic_Assembly_Drama_theater               7.836 4.69e-15 ***\nfacility_typePublic_Assembly_Entertainment_culture      17.257  &lt; 2e-16 ***\nfacility_typePublic_Assembly_Library                    17.475  &lt; 2e-16 ***\nfacility_typePublic_Assembly_Movie_Theater               8.563  &lt; 2e-16 ***\nfacility_typePublic_Assembly_Other                      21.197  &lt; 2e-16 ***\nfacility_typePublic_Assembly_Recreation                 13.493  &lt; 2e-16 ***\nfacility_typePublic_Assembly_Social_meeting              8.391  &lt; 2e-16 ***\nfacility_typePublic_Assembly_Stadium                     7.286 3.23e-13 ***\nfacility_typePublic_Assembly_Uncategorized               2.995 0.002750 ** \nfacility_typePublic_Safety_Courthouse                    8.465  &lt; 2e-16 ***\nfacility_typePublic_Safety_Fire_or_police_station       23.242  &lt; 2e-16 ***\nfacility_typePublic_Safety_Penitentiary                 16.289  &lt; 2e-16 ***\nfacility_typePublic_Safety_Uncategorized                 6.537 6.33e-11 ***\nfacility_typeReligious_worship                           4.478 7.54e-06 ***\nfacility_typeRetail_Enclosed_mall                       14.274  &lt; 2e-16 ***\nfacility_typeRetail_Strip_shopping_mall                 15.775  &lt; 2e-16 ***\nfacility_typeRetail_Uncategorized                       25.365  &lt; 2e-16 ***\nfacility_typeRetail_Vehicle_dealership_showroom          2.423 0.015384 *  \nfacility_typeService_Drycleaning_or_Laundry              0.596 0.551299    \nfacility_typeService_Uncategorized                      12.872  &lt; 2e-16 ***\nfacility_typeService_Vehicle_service_repair_shop        23.320  &lt; 2e-16 ***\nfacility_typeWarehouse_Distribution_or_Shipping_center   3.197 0.001387 ** \nfacility_typeWarehouse_Nonrefrigerated                   3.383 0.000716 ***\nfacility_typeWarehouse_Refrigerated                     12.984  &lt; 2e-16 ***\nfacility_typeWarehouse_Selfstorage                      -4.208 2.58e-05 ***\nfacility_typeWarehouse_Uncategorized                     1.324 0.185652    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 51.42 on 75697 degrees of freedom\nMultiple R-squared:  0.2217,    Adjusted R-squared:  0.221 \nF-statistic: 365.4 on 59 and 75697 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Week4/notes.html",
    "href": "Week4/notes.html",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "",
    "text": "Slides\n\n Download PDF Slides \n\n\nor continue on to the next pages for lecture notes.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference and Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/notes.html#this-weeks-lecture",
    "href": "Week4/notes.html#this-weeks-lecture",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "",
    "text": "Slides\n\n Download PDF Slides \n\n\nor continue on to the next pages for lecture notes.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference and Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/04-inference.html",
    "href": "Week4/04-inference.html",
    "title": "Statistical Inference",
    "section": "",
    "text": "Using a sample to generalize (or infer) about the population.\n\nWe know how to make a point estimate of a population - what else do we need in order to make a decision?\nHow confident are we that our estimate can generalize to the rest of the population?\nWe need to determine the uncertainty in our estimate\n\n\nWe use inferential statistics to make generalizations about an unknown population. The simplest way of doing this is to use the sample data help us to make a point estimate of a population parameter. We realize that due to sampling variability the point estimate is most likely not the exact value of the population parameter, but should be close to it. After calculating point estimates, we can build off of them to construct interval estimates, called confidence intervals.\n\n\nA confidence interval is another type of estimate, but instead of being just one number, it is an interval of numbers\n\nProvides a range a range of reasonable values where we expect the true population parameter to fall.\nPoint estimate (statistic) has some variability and uncertainty since we estimate it based on a sample.\nWe want to quantify and communicate this uncertainty.\n\n\nThere is no guarantee that a given confidence interval does capture the parameter, but there is a predictable probability of success. It is important to keep in mind that the confidence interval itself is a random variable, while the population parameter is fixed.\n\n\n\nYou work in the marketing department of a music company. You want to know the mean number of songs a consumer streams per month.\n\nYou conduct a survey of 100 customers and calculate the sample mean (\\bar{x}) and use it as the point estimate for the population mean (\\mu)\nSuppose we know that the standard deviation \\sigma = 100.\nFollowing the Central Limit Theorem, the Standard Error is:\n\n\n\n\\frac{\\sigma}{\\sqrt{n}} = \\frac{100}{\\sqrt{100}} = 10\n\n\n\nIf you worked in the marketing department of an entertainment company, you might be interested in the mean number of songs a consumer downloads a month from iTunes. If so, you could conduct a survey and calculate the sample mean, \\bar{x}. You would use \\bar{x} to estimate the population mean. The sample mean, \\bar{x}, is the point estimate for the population mean, μ.\nSuppose, for the iTunes example, we do not know the population mean μ, but we do know that the population standard deviation is \\sigma = 100 and our sample size is 100. Then, by the central limit theorem, the standard deviation for the sample mean is \\frac{\\sigma }{\\sqrt{n}}=\\frac{100}{\\sqrt{100}}=10.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Empirical Rule says that in approximately 95% of the samples, the sample mean, \\bar{x}, will be within two standard deviations of the population mean \\mu .\nFor our example, two standard deviations is (2)(10) = 20. The sample mean \\bar{x} is likely to be within 20 units of \\mu.\nBecause \\bar{x} is within 20 units of \\mu, which is unknown, then \\mu is likely to be within 20 units of \\bar{x} in 95% of the samples.\n\n\n\n\nBecause \\bar{x} is within 0.2 units of μ, which is unknown, then μ is likely to be within 0.2 units of \\bar{x} in 95% of the samples. The population mean μ is contained in an interval whose lower number is calculated by taking the sample mean and subtracting two standard deviations (2)(0.1) and whose upper number is calculated by taking the sample mean and adding two standard deviations. In other words, μ is between \\bar{x}\\text{ }-\\text{ 0}\\text{.2} and \\bar{x}\\text{ }+\\text{ 0}\\text{.2} in 95% of all the samples.\n\n\nWe want to calculate the range of values which the true mean is likely to fall within 95% of the time, given our sample.\n\nFor the streaming example, suppose that a sample produced a sample mean \\bar{x} = 200. Then the unknown population mean \\mu is between \\bar{x}-20=200-20=180 and \\bar{x}+20=200+20=220 songs per month.\n\n\nWe can say that we are about 95% confident that the unknown population mean number of songs streamed per month is between 180 and 220. The approximate 95% confidence interval is (1.8, 2.2).\n\n\nConfidence Interval: (Point Estimate \\pm Margin of error) = 200 \\pm 20 \\text{ songs}\n\n\n\nBased on our sample, we can say two things. Either:\n\nThe interval (180, 220) contains the true mean \\mu, or…\nOur sample prodcued an \\bar{x} that is not within 20 units of the true mean \\mu. This would only happen for 5% of the samples.\n\n\n\nWe can say that we are about 95% confident that the unknown population mean number of songs downloaded from iTunes per month is between 1.8 and 2.2. The approximate 95% confidence interval is (1.8, 2.2). This approximate 95% confidence interval implies two possibilities. Either the interval (1.8, 2.2) contains the true mean μ or our sample produced an that is not within 0.2 units of the true mean μ. The second possibility happens for only 5% of all the samples (95–100%).\nRemember that a confidence intervals are created for an unknown population parameter. Confidence intervals for most parameters have the form: (Point Estimate ± Margin of Error) = (Point Estimate – Margin of Error, Point Estimate + Margin of Error) The margin of error (MoE) depends on the confidence level or percentage of confidence and the standard error of the mean. When you read newspapers and journals, some reports will use the phrase “margin of error.” Other reports will not use that phrase, but include a confidence interval as the point estimate plus or minus the margin of error. These are two ways of expressing the same concept. A confidence interval for a population mean with a known standard deviation is based on the fact that the sample means follow an approximately normal distribution. Suppose that our sample has a mean of and we have constructed the 90% confidence interval (5, 15) where MoE = 5.\n\n\n\nThe interpretation should clearly state the confidence level (CL), explain what population parameter is being estimated (here the population mean), and state the confidence interval (both endpoints).\n“We can be _______ % confident that the interval we created, _______ to ________ captures the true population mean (include the context of the problem and appropriate units).”\nWe state the range within which our evidence indicates contains the population mean.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Week4/04-inference.html#statistical-inference-1",
    "href": "Week4/04-inference.html#statistical-inference-1",
    "title": "Statistical Inference",
    "section": "",
    "text": "Using a sample to generalize (or infer) about the population.\n\nWe know how to make a point estimate of a population - what else do we need in order to make a decision?\nHow confident are we that our estimate can generalize to the rest of the population?\nWe need to determine the uncertainty in our estimate\n\n\nWe use inferential statistics to make generalizations about an unknown population. The simplest way of doing this is to use the sample data help us to make a point estimate of a population parameter. We realize that due to sampling variability the point estimate is most likely not the exact value of the population parameter, but should be close to it. After calculating point estimates, we can build off of them to construct interval estimates, called confidence intervals.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Week4/04-inference.html#confidence-intervals",
    "href": "Week4/04-inference.html#confidence-intervals",
    "title": "Statistical Inference",
    "section": "",
    "text": "A confidence interval is another type of estimate, but instead of being just one number, it is an interval of numbers\n\nProvides a range a range of reasonable values where we expect the true population parameter to fall.\nPoint estimate (statistic) has some variability and uncertainty since we estimate it based on a sample.\nWe want to quantify and communicate this uncertainty.\n\n\nThere is no guarantee that a given confidence interval does capture the parameter, but there is a predictable probability of success. It is important to keep in mind that the confidence interval itself is a random variable, while the population parameter is fixed.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Week4/04-inference.html#business-example",
    "href": "Week4/04-inference.html#business-example",
    "title": "Statistical Inference",
    "section": "",
    "text": "You work in the marketing department of a music company. You want to know the mean number of songs a consumer streams per month.\n\nYou conduct a survey of 100 customers and calculate the sample mean (\\bar{x}) and use it as the point estimate for the population mean (\\mu)\nSuppose we know that the standard deviation \\sigma = 100.\nFollowing the Central Limit Theorem, the Standard Error is:\n\n\n\n\\frac{\\sigma}{\\sqrt{n}} = \\frac{100}{\\sqrt{100}} = 10\n\n\n\nIf you worked in the marketing department of an entertainment company, you might be interested in the mean number of songs a consumer downloads a month from iTunes. If so, you could conduct a survey and calculate the sample mean, \\bar{x}. You would use \\bar{x} to estimate the population mean. The sample mean, \\bar{x}, is the point estimate for the population mean, μ.\nSuppose, for the iTunes example, we do not know the population mean μ, but we do know that the population standard deviation is \\sigma = 100 and our sample size is 100. Then, by the central limit theorem, the standard deviation for the sample mean is \\frac{\\sigma }{\\sqrt{n}}=\\frac{100}{\\sqrt{100}}=10.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Week4/04-inference.html#what-is-the-probability-of-sampling-a-certain-mean-value",
    "href": "Week4/04-inference.html#what-is-the-probability-of-sampling-a-certain-mean-value",
    "title": "Statistical Inference",
    "section": "",
    "text": "The Empirical Rule says that in approximately 95% of the samples, the sample mean, \\bar{x}, will be within two standard deviations of the population mean \\mu .\nFor our example, two standard deviations is (2)(10) = 20. The sample mean \\bar{x} is likely to be within 20 units of \\mu.\nBecause \\bar{x} is within 20 units of \\mu, which is unknown, then \\mu is likely to be within 20 units of \\bar{x} in 95% of the samples.\n\n\n\n\nBecause \\bar{x} is within 0.2 units of μ, which is unknown, then μ is likely to be within 0.2 units of \\bar{x} in 95% of the samples. The population mean μ is contained in an interval whose lower number is calculated by taking the sample mean and subtracting two standard deviations (2)(0.1) and whose upper number is calculated by taking the sample mean and adding two standard deviations. In other words, μ is between \\bar{x}\\text{ }-\\text{ 0}\\text{.2} and \\bar{x}\\text{ }+\\text{ 0}\\text{.2} in 95% of all the samples.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Week4/04-inference.html#calculate-the-confidence-interval",
    "href": "Week4/04-inference.html#calculate-the-confidence-interval",
    "title": "Statistical Inference",
    "section": "",
    "text": "We want to calculate the range of values which the true mean is likely to fall within 95% of the time, given our sample.\n\nFor the streaming example, suppose that a sample produced a sample mean \\bar{x} = 200. Then the unknown population mean \\mu is between \\bar{x}-20=200-20=180 and \\bar{x}+20=200+20=220 songs per month.\n\n\nWe can say that we are about 95% confident that the unknown population mean number of songs streamed per month is between 180 and 220. The approximate 95% confidence interval is (1.8, 2.2).\n\n\nConfidence Interval: (Point Estimate \\pm Margin of error) = 200 \\pm 20 \\text{ songs}",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Week4/04-inference.html#calculate-the-confidence-interval-1",
    "href": "Week4/04-inference.html#calculate-the-confidence-interval-1",
    "title": "Statistical Inference",
    "section": "",
    "text": "Based on our sample, we can say two things. Either:\n\nThe interval (180, 220) contains the true mean \\mu, or…\nOur sample prodcued an \\bar{x} that is not within 20 units of the true mean \\mu. This would only happen for 5% of the samples.\n\n\n\nWe can say that we are about 95% confident that the unknown population mean number of songs downloaded from iTunes per month is between 1.8 and 2.2. The approximate 95% confidence interval is (1.8, 2.2). This approximate 95% confidence interval implies two possibilities. Either the interval (1.8, 2.2) contains the true mean μ or our sample produced an that is not within 0.2 units of the true mean μ. The second possibility happens for only 5% of all the samples (95–100%).\nRemember that a confidence intervals are created for an unknown population parameter. Confidence intervals for most parameters have the form: (Point Estimate ± Margin of Error) = (Point Estimate – Margin of Error, Point Estimate + Margin of Error) The margin of error (MoE) depends on the confidence level or percentage of confidence and the standard error of the mean. When you read newspapers and journals, some reports will use the phrase “margin of error.” Other reports will not use that phrase, but include a confidence interval as the point estimate plus or minus the margin of error. These are two ways of expressing the same concept. A confidence interval for a population mean with a known standard deviation is based on the fact that the sample means follow an approximately normal distribution. Suppose that our sample has a mean of and we have constructed the 90% confidence interval (5, 15) where MoE = 5.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Week4/04-inference.html#communicating-confidence-intervals",
    "href": "Week4/04-inference.html#communicating-confidence-intervals",
    "title": "Statistical Inference",
    "section": "",
    "text": "The interpretation should clearly state the confidence level (CL), explain what population parameter is being estimated (here the population mean), and state the confidence interval (both endpoints).\n“We can be _______ % confident that the interval we created, _______ to ________ captures the true population mean (include the context of the problem and appropriate units).”\nWe state the range within which our evidence indicates contains the population mean.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Week4/02-samp-dist.html",
    "href": "Week4/02-samp-dist.html",
    "title": "Sampling Distribution of the Mean",
    "section": "",
    "text": "Connecting sampling distributions with Standard Error, Confidence Intervals, and Hypothesis Testing\n\n\nThe central limit theorem (CLT) is one of the most powerful and useful ideas in all of statistics. There are two alternative forms of the theorem, and both alternatives are concerned with drawing finite samples size n from a population with a known mean, \\mu, and a known standard deviation, \\sigma. The first alternative says that if we collect samples of size n with a “large enough n,” then the resulting distribution can be approximated by the normal distribution.\nApplying the law of large numbers here, we could say that if you take larger and larger samples from a population, then the mean {x} of the sample tends to get closer and closer to \\mu. From the central limit theorem, we know that as n gets larger and larger, the sample means follow a normal distribution. The larger n gets, the smaller the standard deviation gets. (Remember that the standard deviation for {x} is .) This means that the sample mean {x} must be close to the population mean \\mu. We can say that \\mu is the value that the sample means approach as n gets larger. The central limit theorem illustrates the law of large numbers.\nThe size of the sample, n, that is required in order to be “large enough” depends on the original population from which the samples are drawn (the sample size should be at least 30 or the data should come from a normal distribution). If the original population is far from normal, then more observations are needed for the sample means or sums to be normal. Sampling is done with replacement.\n\nThe CLT means says that if you keep drawing larger and larger samples and calculating their means, the sample means form their own normal distribution (the sampling distribution).\nThe sampling distribution of the mean is generated by repeated sampling from the same population and recording the sample mean per sample. This forms a distribution of different means, and this distribution has its own mean and variance.\nThe normal distribution has the same mean as the original distribution and a variance that equals the original variance divided by the sample size.\n\n\nWe have established that different samples yield different statistics due to sampling variability . These statistics have their own distributions, called sampling distributions, that reflect this as a random variable. The sampling distribution of a sample statistic is the distribution of the point estimates based on samples of a fixed size, n, from a certain population. It is useful to think of a particular point estimate as being drawn from a sampling distribution.\nRecall the sample mean weight calculated from a previous sample of 173.3 lbs. Suppose another random sample of 60 participants might produce a different value of x, such as 169.5 lbs. Repeated random sampling could result in additional different values, perhaps 172.1 lbs, 168.5 lbs, and so on. Each sample mean can be thought of as a single observation from a random variable X. The distribution of X is called the sampling distribution of the sample mean, and has its own mean and standard deviation like the random variables discussed previously. We will simulate the concept of a sampling distribution using technology to repeatedly sample, calculate statistics, and graph them. However, the actual sampling distribution would only be attainable if we could theoretically take an infinite amount of samples.\nEach of the point estimates in the table above have their own unique sampling distributions which we will look at in the future\n\n\n\n\n\n\nWe are drawing a random sample of people from the dataset and calculating the mean weight for that sample. Sample size is the number of data points we pull. We then repeat this 5000 times (n_samples) to build up the sampling distribution.\n\n\n\n\n\nRecall: The Standard Error is the standard deviation of the sampling distribution.\n\nSEM = \\sigma_{\\bar{x}\\ (means)}\n\n\n\nRecall: The Standard Error is the standard deviation of the sampling distribution. This is also equal to the standard deviation \\sigma of the population divided by the square root of the sample size.\n\nSEM = \\sigma_{\\bar{x}\\ (means)} = \\frac{\\sigma}{\\sqrt{n}} \\approx \\frac{\\sigma_{x}}{\\sqrt{n}} \\left[ i.e. \\frac{\\text{Est. Std Dev of the sample}}{\\sqrt{\\text{Sample size}}} \\right]\n\n\nIn other words:\n\nIf you draw random samples of size n, the distribution of the random variable \\bar{X}, which consists of sample means, is called the sampling distribution of the sample mean. The sampling distribution of the mean approaches a normal distribution as n, the sample size, increases.\n\n\n\nIn the SEM formula, remember the sampling distribution is the distribution of multiple means - not the distribution of our sample.\nQuote from https://pressbooks.lib.vt.edu/introstatistics/chapter/the-central-limit-theorem-for-sample-means-averages/\n\n\n\n\nA sampling distribution is what we get by simulating multiple samples from a population.\nThe Standard Error is the standard deviation \\sigma_{\\bar{x}} of the sampling distribution.\nThe SE decreases as the sample size n increases.\nBecause of this relationship - we can estimate the SE from a single sample \\frac{\\sigma_x}{\\sqrt{n}}\n\n\n\nSEM = \\sigma_{\\bar{x}\\ (means)} = \\frac{\\sigma}{\\sqrt{n}} \\approx \\frac{\\sigma_{x}}{\\sqrt{n}} \\left[ i.e. \\frac{\\text{Est. Std Dev of the sample}}{\\sqrt{\\text{Sample size}}} \\right]",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Sampling Distribution of the Mean"
    ]
  },
  {
    "objectID": "Week4/02-samp-dist.html#central-limit-theorem",
    "href": "Week4/02-samp-dist.html#central-limit-theorem",
    "title": "Sampling Distribution of the Mean",
    "section": "",
    "text": "The central limit theorem (CLT) is one of the most powerful and useful ideas in all of statistics. There are two alternative forms of the theorem, and both alternatives are concerned with drawing finite samples size n from a population with a known mean, \\mu, and a known standard deviation, \\sigma. The first alternative says that if we collect samples of size n with a “large enough n,” then the resulting distribution can be approximated by the normal distribution.\nApplying the law of large numbers here, we could say that if you take larger and larger samples from a population, then the mean {x} of the sample tends to get closer and closer to \\mu. From the central limit theorem, we know that as n gets larger and larger, the sample means follow a normal distribution. The larger n gets, the smaller the standard deviation gets. (Remember that the standard deviation for {x} is .) This means that the sample mean {x} must be close to the population mean \\mu. We can say that \\mu is the value that the sample means approach as n gets larger. The central limit theorem illustrates the law of large numbers.\nThe size of the sample, n, that is required in order to be “large enough” depends on the original population from which the samples are drawn (the sample size should be at least 30 or the data should come from a normal distribution). If the original population is far from normal, then more observations are needed for the sample means or sums to be normal. Sampling is done with replacement.\n\nThe CLT means says that if you keep drawing larger and larger samples and calculating their means, the sample means form their own normal distribution (the sampling distribution).\nThe sampling distribution of the mean is generated by repeated sampling from the same population and recording the sample mean per sample. This forms a distribution of different means, and this distribution has its own mean and variance.\nThe normal distribution has the same mean as the original distribution and a variance that equals the original variance divided by the sample size.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Sampling Distribution of the Mean"
    ]
  },
  {
    "objectID": "Week4/02-samp-dist.html#drawing-samples-of-peoples-weight-from-the-nhanes-dataset.",
    "href": "Week4/02-samp-dist.html#drawing-samples-of-peoples-weight-from-the-nhanes-dataset.",
    "title": "Sampling Distribution of the Mean",
    "section": "",
    "text": "We have established that different samples yield different statistics due to sampling variability . These statistics have their own distributions, called sampling distributions, that reflect this as a random variable. The sampling distribution of a sample statistic is the distribution of the point estimates based on samples of a fixed size, n, from a certain population. It is useful to think of a particular point estimate as being drawn from a sampling distribution.\nRecall the sample mean weight calculated from a previous sample of 173.3 lbs. Suppose another random sample of 60 participants might produce a different value of x, such as 169.5 lbs. Repeated random sampling could result in additional different values, perhaps 172.1 lbs, 168.5 lbs, and so on. Each sample mean can be thought of as a single observation from a random variable X. The distribution of X is called the sampling distribution of the sample mean, and has its own mean and standard deviation like the random variables discussed previously. We will simulate the concept of a sampling distribution using technology to repeatedly sample, calculate statistics, and graph them. However, the actual sampling distribution would only be attainable if we could theoretically take an infinite amount of samples.\nEach of the point estimates in the table above have their own unique sampling distributions which we will look at in the future\n\n\n\n\n\n\nWe are drawing a random sample of people from the dataset and calculating the mean weight for that sample. Sample size is the number of data points we pull. We then repeat this 5000 times (n_samples) to build up the sampling distribution.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Sampling Distribution of the Mean"
    ]
  },
  {
    "objectID": "Week4/02-samp-dist.html#standard-error",
    "href": "Week4/02-samp-dist.html#standard-error",
    "title": "Sampling Distribution of the Mean",
    "section": "",
    "text": "Recall: The Standard Error is the standard deviation of the sampling distribution.\n\nSEM = \\sigma_{\\bar{x}\\ (means)}",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Sampling Distribution of the Mean"
    ]
  },
  {
    "objectID": "Week4/02-samp-dist.html#standard-error-1",
    "href": "Week4/02-samp-dist.html#standard-error-1",
    "title": "Sampling Distribution of the Mean",
    "section": "",
    "text": "Recall: The Standard Error is the standard deviation of the sampling distribution. This is also equal to the standard deviation \\sigma of the population divided by the square root of the sample size.\n\nSEM = \\sigma_{\\bar{x}\\ (means)} = \\frac{\\sigma}{\\sqrt{n}} \\approx \\frac{\\sigma_{x}}{\\sqrt{n}} \\left[ i.e. \\frac{\\text{Est. Std Dev of the sample}}{\\sqrt{\\text{Sample size}}} \\right]\n\n\nIn other words:\n\nIf you draw random samples of size n, the distribution of the random variable \\bar{X}, which consists of sample means, is called the sampling distribution of the sample mean. The sampling distribution of the mean approaches a normal distribution as n, the sample size, increases.\n\n\n\nIn the SEM formula, remember the sampling distribution is the distribution of multiple means - not the distribution of our sample.\nQuote from https://pressbooks.lib.vt.edu/introstatistics/chapter/the-central-limit-theorem-for-sample-means-averages/",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Sampling Distribution of the Mean"
    ]
  },
  {
    "objectID": "Week4/02-samp-dist.html#standard-error-2",
    "href": "Week4/02-samp-dist.html#standard-error-2",
    "title": "Sampling Distribution of the Mean",
    "section": "",
    "text": "A sampling distribution is what we get by simulating multiple samples from a population.\nThe Standard Error is the standard deviation \\sigma_{\\bar{x}} of the sampling distribution.\nThe SE decreases as the sample size n increases.\nBecause of this relationship - we can estimate the SE from a single sample \\frac{\\sigma_x}{\\sqrt{n}}\n\n\n\nSEM = \\sigma_{\\bar{x}\\ (means)} = \\frac{\\sigma}{\\sqrt{n}} \\approx \\frac{\\sigma_{x}}{\\sqrt{n}} \\left[ i.e. \\frac{\\text{Est. Std Dev of the sample}}{\\sqrt{\\text{Sample size}}} \\right]",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Sampling Distribution of the Mean"
    ]
  },
  {
    "objectID": "Week3/notes.html",
    "href": "Week3/notes.html",
    "title": "Week 3",
    "section": "",
    "text": "Slides\n\n Download PDF Slides \n\n\nor continue on to the next pages for lecture notes.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Week 3"
    ]
  },
  {
    "objectID": "Week3/notes.html#this-weeks-lecture",
    "href": "Week3/notes.html#this-weeks-lecture",
    "title": "Week 3",
    "section": "",
    "text": "Slides\n\n Download PDF Slides \n\n\nor continue on to the next pages for lecture notes.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Week 3"
    ]
  },
  {
    "objectID": "Week3/notes.html#suggested-readings",
    "href": "Week3/notes.html#suggested-readings",
    "title": "Week 3",
    "section": "Suggested Readings",
    "text": "Suggested Readings\nThis week’s lectures are adapted from Chapter 6 and Chapter 7 of Statistical Thinking by Russell Poldrack.\nAdditional resources for further exploration:\n\nProbability Lecture Notes from the Stat20 course at Berkeley.\nA Student’s Guide to Bayesian Statistics by Ben Lambert\nThe Drunkard’s Walk: How Randomness Rules Our Lives by Leonard Mlodinow\nTen Great Ideas about Chance by Persi Diaconis and Brian Skyrms\n\n\nThese books provide excellent additional resources for understanding probability theory and its applications.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Week 3"
    ]
  },
  {
    "objectID": "Week3/07-sampling.html",
    "href": "Week3/07-sampling.html",
    "title": "Part 2: Statistical Sampling",
    "section": "",
    "text": "Making inferences about populations from samples\n\n\n\nThe Power of Sampling:\nNate Silver’s 2012 Election Prediction:\n\nCorrectly predicted all 50 states\nUsed only 21,000 people\nTo predict 125 million votes\nCombined data from 21 polls\n\n\nKey Insights:\n\nSmall samples can be powerful\nProper methodology is crucial\nCombining data improves accuracy\nStatistical rigor matters\n\n\n\n\nOne of the foundational ideas in statistics is that we can make inferences about an entire population based on a relatively small sample of individuals from that population.\nAnyone living in the United States will be familiar with the concept of sampling from the political polls that have become a central part of our electoral process. In some cases, these polls can be incredibly accurate at predicting the outcomes of elections. The best known example comes from the 2008 and 2012 US Presidential elections, when the pollster Nate Silver correctly predicted electoral outcomes for 49/50 states in 2008 and for all 50 states in 2012.\nSilver did this by combining data from 21 different polls, which vary in the degree to which they tend to lean towards either the Republican or Democratic side. Each of these polls included data from about 1000 likely voters – meaning that Silver was able to almost perfectly predict the pattern of votes of more than 125 million voters using data from only about 21,000 people, along with other knowledge.\n\n\n\n\n\n\n\nPopulation vs Sample:\n\nPopulation: Entire group of interest\nSample: Subset used for measurement\nGoal: Infer population parameters from sample statistics\n\n\n\nRepresentative Sampling:\n\nEqual chance of selection\nAvoid systematic bias\nRandom selection crucial\n\n\n\n\n\n\n\n\nTypes of Sampling:\n\nWith replacement: Items can be selected multiple times\nWithout replacement: Items selected only once\nChoice affects probability calculations\n\n\n\nKey Terms:\n\nParameter: Population value (usually unknown)\nStatistic: Sample value (our estimate)\nSampling Error: Difference between statistic and parameter\n\n\n\n\n\n\n\nOur goal in sampling is to determine the value of a statistic for an entire population of interest, using just a small subset of the population. We do this primarily to save time and effort – why go to the trouble of measuring every individual in the population when just a small sample is sufficient to accurately estimate the statistic of interest?\nIn the election example, the population is all registered voters in the region being polled, and the sample is the set of 1000 individuals selected by the polling organization. The way in which we select the sample is critical to ensuring that the sample is representative of the entire population, which is a main goal of statistical sampling.\nIt’s important to also distinguish between two different ways of sampling: with replacement versus without replacement. In sampling with replacement, after a member of the population has been sampled, they are put back into the pool so that they can potentially be sampled again. In sampling without replacement, once a member has been sampled they are not eligible to be sampled again.\n\n\n\nWhat is Sampling Error?\n\nDifference between sample and population\nVaries across samples\nAffects measurement quality\nCan be quantified\n\n\n\n\n\n# Take 5 samples of 50 adults each\nset.seed(123)\nsamples &lt;- map_df(\n  1:5,\n  ~{\n    NHANES_adult |&gt;\n      sample_n(50) |&gt;\n      summarise(\n        mean_height = mean(Height),\n        sd_height = sd(Height),\n      )\n  }\n)\nsamples\n\n# A tibble: 5 × 2\n  mean_height sd_height\n        &lt;dbl&gt;     &lt;dbl&gt;\n1        169.     11.6 \n2        167.      9.13\n3        169.     11.2 \n4        166.      9.62\n5        169.     11.0 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegardless of how representative our sample is, it’s likely that the statistic that we compute from the sample is going to differ at least slightly from the population parameter. We refer to this as sampling error. If we take multiple samples, the value of our statistical estimate will also vary from sample to sample; we refer to this distribution of our statistic across samples as the sampling distribution.\nSampling error is directly related to the quality of our measurement of the population. Clearly we want the estimates obtained from our sample to be as close as possible to the true value of the population parameter. However, even if our statistic is unbiased (that is, we expect it to have the same value as the population parameter), the value for any particular estimate will differ from the population value, and those differences will be greater when the sampling error is greater.\nThe visualization shows how sample means distribute around the true population mean (red line) when we take many samples.\n\n\n\n\nDefinition:\nSEM = \\frac{\\hat{\\sigma}}{\\sqrt{n}}\nWhere:\n\n\n\\hat{\\sigma} is estimated standard deviation\n\nn is sample size\n\nKey Properties:\n\nMeasures sampling distribution variability\nDecreases with larger samples\nIncreases with population variability\n\n\nExample with NHANES:\n\nCode# Population SEM\npop_sd &lt;- sd(NHANES_adult$Height)\nn &lt;- 50\nsem_theoretical &lt;- pop_sd / sqrt(n)\n\n# Observed SEM from samples\nsem_observed &lt;- sd(samples_large$mean_height)\n\ncat(\"Theoretical SEM:\", round(sem_theoretical, 2), \"\\n\")\n\nTheoretical SEM: 1.44 \n\nCodecat(\"Observed SEM:\", round(sem_observed, 2))\n\nObserved SEM: 1.42\n\n\n\n\n\nLater in the course it will become essential to be able to characterize how variable our samples are, in order to make inferences about the sample statistics. For the mean, we do this using a quantity called the standard error of the mean (SEM), which one can think of as the standard deviation of the sampling distribution of the mean.\nThe formula for the standard error of the mean implies that the quality of our measurement involves two quantities: the population variability, and the size of our sample. Because the sample size is the denominator in the formula for SEM, a larger sample size will yield a smaller SEM when holding the population variability constant.\nWe have no control over the population variability, but we do have control over the sample size. Thus, if we wish to improve our sample statistics (by reducing their sampling variability) then we should use larger samples. However, the formula also tells us something very fundamental about statistical sampling – namely, that the utility of larger samples diminishes with the square root of the sample size.\n\n\n\n\nTheory\nVisualization\nCode\n\n\n\nImpact of Sample Size:\n\nLarger n → Smaller SEM\nRelationship is not linear\nDiminishing returns\nSquare root relationship\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode# Compare SEM for different sample sizes\nn1 &lt;- 50\nn2 &lt;- 200 # 4 times larger\n\nsem1 &lt;- pop_sd / sqrt(n1)\nsem2 &lt;- pop_sd / sqrt(n2)\n\n# Improvement factor\nimprovement &lt;- sem1 / sem2\ncat(\"Improvement factor:\", round(improvement, 2))\n\n\n\n\n\n\nThe relationship between sample size and standard error is not linear. Doubling the sample size will not double the quality of the statistics; rather, it will improve it by a factor of √2. This has important implications for study design and resource allocation.\nThe visualization shows how the standard error decreases as sample size increases, but with diminishing returns. This means that after a certain point, increasing sample size may not be worth the additional cost and effort.\nThis relationship is fundamental to statistical power, which we will discuss in later sections. Understanding this relationship helps researchers make informed decisions about sample size requirements for their studies.\n\n\nKey Points:\n\nAs sample size increases:\n\nSampling distribution becomes normal\nRegardless of population distribution\nMean approaches population mean\nVariance decreases\n\n\nImplications:\n\nEnables statistical inference\nJustifies normal approximation\nExplains real-world patterns\n\n\n\nThe Central Limit Theorem tells us that as sample sizes get larger, the sampling distribution of the mean will become normally distributed, even if the data within each sample are not normally distributed. This is a powerful result that allows us to make inferences about population parameters based on sample statistics.\n\n\n\n\n\nBell-shaped curve\nDefined by mean (\\mu) and SD (\\sigma)\nSymmetric around mean\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Central Limit Theorem tells us that as sample sizes get larger, the sampling distribution of the mean will become normally distributed, even if the data within each sample are not normally distributed.\nThe normal distribution is described in terms of two parameters: the mean (which you can think of as the location of the peak), and the standard deviation (which specifies the width of the distribution). The bell-like shape of the distribution never changes, only its location and width.\nThe normal distribution is commonly observed in data collected in the real world – and the central limit theorem gives us some insight into why that occurs. For example, the height of any adult depends on a complex mixture of their genetics and experience; even if those individual contributions may not be normally distributed, when we combine them the result is a normal distribution.\n\n\n\n\nOriginal Distribution\nCode Example\nKey Insights\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode# Compare skewness\nlibrary(moments)\noriginal_skew &lt;- skewness(NHANES_clean$AlcoholYear)\nsampling_skew &lt;- skewness(samples_alc$mean_alcohol)\n\ncat(\"Original Distribution Skewness:\", round(original_skew, 2), \"\\n\")\ncat(\"Sampling Distribution Skewness:\", round(sampling_skew, 2))\n\n\n\n\n\nOriginal data is highly skewed\nSampling distribution is nearly normal\nCLT works even with:\n\nNon-normal data\nSkewed distributions\nDiscrete values\n\n\nSample size of 50 is sufficient\n\n\n\n\n\nLet’s work with the variable AlcoholYear from the NHANES dataset, which is highly skewed. This distribution is, for lack of a better word, funky – and definitely not normally distributed.\nNow let’s look at the sampling distribution of the mean for this variable. Despite the clear non-normality of the original data, the sampling distribution is remarkably close to the normal.\nThe Central Limit Theorem is important for statistics because it allows us to safely assume that the sampling distribution of the mean will be normal in most cases. This means that we can take advantage of statistical techniques that assume a normal distribution.\n\n\n\n\n\n\n\nSampling Fundamentals:\n\nPopulation vs Sample\nRepresentative sampling\nWith/without replacement\nSampling error\n\n\n\nStandard Error:\n\nMeasures sampling variability\nDecreases with √n\nGuides sample size decisions\nQuantifies precision\n\n\n\n\n\n\n\n\nCentral Limit Theorem:\n\nSampling distribution normality\nIndependent of original distribution\nEnables statistical inference\nFoundation for hypothesis testing\n\n\n\nApplications:\n\nPolitical polling\nClinical trials\nQuality control\nResearch design\n\n\n\n\n\n\n\nIn this lecture, we covered: - The fundamentals of statistical sampling and why it works - How to characterize sampling error and the sampling distribution - The standard error of the mean and its relationship with sample size - The Central Limit Theorem and its importance in statistical inference - Real-world applications and examples using the NHANES dataset",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Part 2: Statistical Sampling"
    ]
  },
  {
    "objectID": "Week3/07-sampling.html#why-study-sampling",
    "href": "Week3/07-sampling.html#why-study-sampling",
    "title": "Part 2: Statistical Sampling",
    "section": "",
    "text": "The Power of Sampling:\nNate Silver’s 2012 Election Prediction:\n\nCorrectly predicted all 50 states\nUsed only 21,000 people\nTo predict 125 million votes\nCombined data from 21 polls\n\n\nKey Insights:\n\nSmall samples can be powerful\nProper methodology is crucial\nCombining data improves accuracy\nStatistical rigor matters\n\n\n\n\nOne of the foundational ideas in statistics is that we can make inferences about an entire population based on a relatively small sample of individuals from that population.\nAnyone living in the United States will be familiar with the concept of sampling from the political polls that have become a central part of our electoral process. In some cases, these polls can be incredibly accurate at predicting the outcomes of elections. The best known example comes from the 2008 and 2012 US Presidential elections, when the pollster Nate Silver correctly predicted electoral outcomes for 49/50 states in 2008 and for all 50 states in 2012.\nSilver did this by combining data from 21 different polls, which vary in the degree to which they tend to lean towards either the Republican or Democratic side. Each of these polls included data from about 1000 likely voters – meaning that Silver was able to almost perfectly predict the pattern of votes of more than 125 million voters using data from only about 21,000 people, along with other knowledge.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Part 2: Statistical Sampling"
    ]
  },
  {
    "objectID": "Week3/07-sampling.html#sampling-fundamentals",
    "href": "Week3/07-sampling.html#sampling-fundamentals",
    "title": "Part 2: Statistical Sampling",
    "section": "",
    "text": "Population vs Sample:\n\nPopulation: Entire group of interest\nSample: Subset used for measurement\nGoal: Infer population parameters from sample statistics\n\n\n\nRepresentative Sampling:\n\nEqual chance of selection\nAvoid systematic bias\nRandom selection crucial\n\n\n\n\n\n\n\n\nTypes of Sampling:\n\nWith replacement: Items can be selected multiple times\nWithout replacement: Items selected only once\nChoice affects probability calculations\n\n\n\nKey Terms:\n\nParameter: Population value (usually unknown)\nStatistic: Sample value (our estimate)\nSampling Error: Difference between statistic and parameter\n\n\n\n\n\n\n\nOur goal in sampling is to determine the value of a statistic for an entire population of interest, using just a small subset of the population. We do this primarily to save time and effort – why go to the trouble of measuring every individual in the population when just a small sample is sufficient to accurately estimate the statistic of interest?\nIn the election example, the population is all registered voters in the region being polled, and the sample is the set of 1000 individuals selected by the polling organization. The way in which we select the sample is critical to ensuring that the sample is representative of the entire population, which is a main goal of statistical sampling.\nIt’s important to also distinguish between two different ways of sampling: with replacement versus without replacement. In sampling with replacement, after a member of the population has been sampled, they are put back into the pool so that they can potentially be sampled again. In sampling without replacement, once a member has been sampled they are not eligible to be sampled again.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Part 2: Statistical Sampling"
    ]
  },
  {
    "objectID": "Week3/07-sampling.html#sampling-error-distribution",
    "href": "Week3/07-sampling.html#sampling-error-distribution",
    "title": "Part 2: Statistical Sampling",
    "section": "",
    "text": "What is Sampling Error?\n\nDifference between sample and population\nVaries across samples\nAffects measurement quality\nCan be quantified",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Part 2: Statistical Sampling"
    ]
  },
  {
    "objectID": "Week3/07-sampling.html#sampling-error-distribution-1",
    "href": "Week3/07-sampling.html#sampling-error-distribution-1",
    "title": "Part 2: Statistical Sampling",
    "section": "",
    "text": "# Take 5 samples of 50 adults each\nset.seed(123)\nsamples &lt;- map_df(\n  1:5,\n  ~{\n    NHANES_adult |&gt;\n      sample_n(50) |&gt;\n      summarise(\n        mean_height = mean(Height),\n        sd_height = sd(Height),\n      )\n  }\n)\nsamples\n\n# A tibble: 5 × 2\n  mean_height sd_height\n        &lt;dbl&gt;     &lt;dbl&gt;\n1        169.     11.6 \n2        167.      9.13\n3        169.     11.2 \n4        166.      9.62\n5        169.     11.0 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegardless of how representative our sample is, it’s likely that the statistic that we compute from the sample is going to differ at least slightly from the population parameter. We refer to this as sampling error. If we take multiple samples, the value of our statistical estimate will also vary from sample to sample; we refer to this distribution of our statistic across samples as the sampling distribution.\nSampling error is directly related to the quality of our measurement of the population. Clearly we want the estimates obtained from our sample to be as close as possible to the true value of the population parameter. However, even if our statistic is unbiased (that is, we expect it to have the same value as the population parameter), the value for any particular estimate will differ from the population value, and those differences will be greater when the sampling error is greater.\nThe visualization shows how sample means distribute around the true population mean (red line) when we take many samples.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Part 2: Statistical Sampling"
    ]
  },
  {
    "objectID": "Week3/07-sampling.html#standard-error-of-the-mean",
    "href": "Week3/07-sampling.html#standard-error-of-the-mean",
    "title": "Part 2: Statistical Sampling",
    "section": "",
    "text": "Definition:\nSEM = \\frac{\\hat{\\sigma}}{\\sqrt{n}}\nWhere:\n\n\n\\hat{\\sigma} is estimated standard deviation\n\nn is sample size\n\nKey Properties:\n\nMeasures sampling distribution variability\nDecreases with larger samples\nIncreases with population variability\n\n\nExample with NHANES:\n\nCode# Population SEM\npop_sd &lt;- sd(NHANES_adult$Height)\nn &lt;- 50\nsem_theoretical &lt;- pop_sd / sqrt(n)\n\n# Observed SEM from samples\nsem_observed &lt;- sd(samples_large$mean_height)\n\ncat(\"Theoretical SEM:\", round(sem_theoretical, 2), \"\\n\")\n\nTheoretical SEM: 1.44 \n\nCodecat(\"Observed SEM:\", round(sem_observed, 2))\n\nObserved SEM: 1.42\n\n\n\n\n\nLater in the course it will become essential to be able to characterize how variable our samples are, in order to make inferences about the sample statistics. For the mean, we do this using a quantity called the standard error of the mean (SEM), which one can think of as the standard deviation of the sampling distribution of the mean.\nThe formula for the standard error of the mean implies that the quality of our measurement involves two quantities: the population variability, and the size of our sample. Because the sample size is the denominator in the formula for SEM, a larger sample size will yield a smaller SEM when holding the population variability constant.\nWe have no control over the population variability, but we do have control over the sample size. Thus, if we wish to improve our sample statistics (by reducing their sampling variability) then we should use larger samples. However, the formula also tells us something very fundamental about statistical sampling – namely, that the utility of larger samples diminishes with the square root of the sample size.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Part 2: Statistical Sampling"
    ]
  },
  {
    "objectID": "Week3/07-sampling.html#sample-size-effects",
    "href": "Week3/07-sampling.html#sample-size-effects",
    "title": "Part 2: Statistical Sampling",
    "section": "",
    "text": "Theory\nVisualization\nCode\n\n\n\nImpact of Sample Size:\n\nLarger n → Smaller SEM\nRelationship is not linear\nDiminishing returns\nSquare root relationship\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode# Compare SEM for different sample sizes\nn1 &lt;- 50\nn2 &lt;- 200 # 4 times larger\n\nsem1 &lt;- pop_sd / sqrt(n1)\nsem2 &lt;- pop_sd / sqrt(n2)\n\n# Improvement factor\nimprovement &lt;- sem1 / sem2\ncat(\"Improvement factor:\", round(improvement, 2))\n\n\n\n\n\n\nThe relationship between sample size and standard error is not linear. Doubling the sample size will not double the quality of the statistics; rather, it will improve it by a factor of √2. This has important implications for study design and resource allocation.\nThe visualization shows how the standard error decreases as sample size increases, but with diminishing returns. This means that after a certain point, increasing sample size may not be worth the additional cost and effort.\nThis relationship is fundamental to statistical power, which we will discuss in later sections. Understanding this relationship helps researchers make informed decisions about sample size requirements for their studies.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Part 2: Statistical Sampling"
    ]
  },
  {
    "objectID": "Week3/07-sampling.html#the-central-limit-theorem",
    "href": "Week3/07-sampling.html#the-central-limit-theorem",
    "title": "Part 2: Statistical Sampling",
    "section": "",
    "text": "Key Points:\n\nAs sample size increases:\n\nSampling distribution becomes normal\nRegardless of population distribution\nMean approaches population mean\nVariance decreases\n\n\nImplications:\n\nEnables statistical inference\nJustifies normal approximation\nExplains real-world patterns\n\n\n\nThe Central Limit Theorem tells us that as sample sizes get larger, the sampling distribution of the mean will become normally distributed, even if the data within each sample are not normally distributed. This is a powerful result that allows us to make inferences about population parameters based on sample statistics.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Part 2: Statistical Sampling"
    ]
  },
  {
    "objectID": "Week3/07-sampling.html#the-central-limit-theorem-1",
    "href": "Week3/07-sampling.html#the-central-limit-theorem-1",
    "title": "Part 2: Statistical Sampling",
    "section": "",
    "text": "Bell-shaped curve\nDefined by mean (\\mu) and SD (\\sigma)\nSymmetric around mean\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Central Limit Theorem tells us that as sample sizes get larger, the sampling distribution of the mean will become normally distributed, even if the data within each sample are not normally distributed.\nThe normal distribution is described in terms of two parameters: the mean (which you can think of as the location of the peak), and the standard deviation (which specifies the width of the distribution). The bell-like shape of the distribution never changes, only its location and width.\nThe normal distribution is commonly observed in data collected in the real world – and the central limit theorem gives us some insight into why that occurs. For example, the height of any adult depends on a complex mixture of their genetics and experience; even if those individual contributions may not be normally distributed, when we combine them the result is a normal distribution.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Part 2: Statistical Sampling"
    ]
  },
  {
    "objectID": "Week3/07-sampling.html#clt-in-action-nhanes-example",
    "href": "Week3/07-sampling.html#clt-in-action-nhanes-example",
    "title": "Part 2: Statistical Sampling",
    "section": "",
    "text": "Original Distribution\nCode Example\nKey Insights\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode# Compare skewness\nlibrary(moments)\noriginal_skew &lt;- skewness(NHANES_clean$AlcoholYear)\nsampling_skew &lt;- skewness(samples_alc$mean_alcohol)\n\ncat(\"Original Distribution Skewness:\", round(original_skew, 2), \"\\n\")\ncat(\"Sampling Distribution Skewness:\", round(sampling_skew, 2))\n\n\n\n\n\nOriginal data is highly skewed\nSampling distribution is nearly normal\nCLT works even with:\n\nNon-normal data\nSkewed distributions\nDiscrete values\n\n\nSample size of 50 is sufficient\n\n\n\n\n\nLet’s work with the variable AlcoholYear from the NHANES dataset, which is highly skewed. This distribution is, for lack of a better word, funky – and definitely not normally distributed.\nNow let’s look at the sampling distribution of the mean for this variable. Despite the clear non-normality of the original data, the sampling distribution is remarkably close to the normal.\nThe Central Limit Theorem is important for statistics because it allows us to safely assume that the sampling distribution of the mean will be normal in most cases. This means that we can take advantage of statistical techniques that assume a normal distribution.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Part 2: Statistical Sampling"
    ]
  },
  {
    "objectID": "Week3/07-sampling.html#summary",
    "href": "Week3/07-sampling.html#summary",
    "title": "Part 2: Statistical Sampling",
    "section": "",
    "text": "Sampling Fundamentals:\n\nPopulation vs Sample\nRepresentative sampling\nWith/without replacement\nSampling error\n\n\n\nStandard Error:\n\nMeasures sampling variability\nDecreases with √n\nGuides sample size decisions\nQuantifies precision\n\n\n\n\n\n\n\n\nCentral Limit Theorem:\n\nSampling distribution normality\nIndependent of original distribution\nEnables statistical inference\nFoundation for hypothesis testing\n\n\n\nApplications:\n\nPolitical polling\nClinical trials\nQuality control\nResearch design\n\n\n\n\n\n\n\nIn this lecture, we covered: - The fundamentals of statistical sampling and why it works - How to characterize sampling error and the sampling distribution - The standard error of the mean and its relationship with sample size - The Central Limit Theorem and its importance in statistical inference - Real-world applications and examples using the NHANES dataset",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Part 2: Statistical Sampling"
    ]
  },
  {
    "objectID": "Week3/05-bayes.html",
    "href": "Week3/05-bayes.html",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "",
    "text": "When we know P(A|B) but want P(B|A):\nP(B|A) = \\frac{P(A|B)*P(B)}{P(A)}\nAlternative Form:\nP(B|A) = \\frac{P(A|B)*P(B)}{P(A|B)*P(B) + P(A|\\neg B)*P(\\neg B)}\n\n\nComponents:\n\nPrior: P(B)\n\nLikelihood: P(A|B)\n\nMarginal likelihood: P(A)\n\nPosterior: P(B|A)\n\n\n\n\n\n\nIn many cases, we know P(A|B) but we really want to know P(B|A). This commonly occurs in medical screening, where we know P(positive test result| disease) but what we want to know is P(disease|positive test result).\nIf we have only two outcomes, we can express Bayes’ rule in a somewhat clearer way, using the sum rule to redefine P(A).\n\n\n\nA major construction company conducts mandatory random drug and alcohol screening using rapid saliva tests. Consider the following scenario:\n\n\nIn the UK construction industry during 2023, the prevalence of substance use affecting workplace safety was estimated at 2.5% of the workforce\nThe rapid saliva test used has a sensitivity (true positive rate) of 85% when conducted according to protocol\nThe specificity (true negative rate) of these tests is 99.2%\n\n\n\n\nLet’s consider a specific example. Suppose that a worker is selected for a random drug screening. The test result is positive. What is the probability that this worker is actually positive for substances?\n\n\n\nLet’s consider a specific example. Suppose that a worker is selected for a random drug screening. The test result is positive. What is the probability that this worker is actually positive for substances?\n\nContext: The company’s current policy is immediate suspension without pay following a positive test result, pending a more accurate laboratory confirmation test that takes 48 hours.\n\nMandatory screening\nRapid saliva test\nSafety-critical roles\nImmediate consequences\n\n\n\n\nConstruction Site Testing:\n\nSensitivity: P(positive|substance) = 0.85\nSpecificity: P(negative|no substance) = 0.992\nBase rate: P(substance) = 0.025\n\n\nKey Values:\n\nP(S) = 0.025 (prevalence)\nP(P|S) = 0.85 (sensitivity)\nP(P|not S) = 0.008 (1 - specificity)\n\n\n\nA major construction company conducts mandatory random drug and alcohol screening using rapid saliva tests. In the UK construction industry during 2023, the prevalence of substance use affecting workplace safety was estimated at 2.5% of the workforce. The rapid saliva test used has a sensitivity of 85% when conducted according to protocol, and a specificity of 99.2%.\n\n\nUsing Bayes’ Theorem, calculate the probability that this worker is actually positive for substances given their positive test result.\n\nP(S) = 0.025 (prevalence)\nP(P|S) = 0.85 (sensitivity)\nP(P|not S) = 0.008 (1 - specificity)\n\n\nA construction worker is randomly selected for testing at the start of their shift. Their saliva test comes back positive. Using Bayes’ Theorem, calculate the probability that this worker is actually positive for substances given their positive test result.\n\n\n\n\nCalculate P(substance|positive):\n\n\\begin{align*}\nP(P) &= P(P|S) \\times P(S) + P(P|not S) \\times P(not S) \\\\\n&= (0.85 \\times 0.025) + (0.008 \\times 0.975) \\\\\n&= 0.02125 + 0.0078 \\\\\n&= 0.02905\n\\end{align*}\n\n\n\\begin{align*}\nP(S|P) &= \\frac{P(P|S) \\times P(S)}{P(P)} \\\\\n&= \\frac{0.85 \\times 0.025}{0.02905} \\\\\n&= 0.7314974 \\text{ or } 73.1\\%\n\\end{align*}\n\n\n\nInterpretation:\n\n~73.1% chance true positive\n~26.9% chance false positive\nMuch higher than 2.5% base rate\nStill significant uncertainty\n\n\n\n\n\nUsing Bayes’ Theorem, we find that given a positive test result, there is a 73.1% probability that the worker actually has substances present. This is much higher than the base rate of 2.5%, but still leaves significant uncertainty with a 26.9% false positive rate.\n\n\nThe company’s current policy is immediate suspension without pay following a positive test result.\nWhat do these results mean for this business policy? Is it fair to immediately suspend workers without pay for a positive test result?\n\nThe company’s current policy is immediate suspension without pay following a positive test result, pending a more accurate laboratory confirmation test that takes 48 hours.\nGiven that approximately 26.9% of positive test results may be false positives, an immediate suspension without pay could unfairly penalize innocent workers; however, the high stakes of construction safety and the 73.1% probability of a true positive suggest that temporary removal from safety-critical roles is prudent while awaiting confirmation.\n\n\n\n\nBayes’ Rule as Learning:\nP(B|A) = \\frac{P(A|B)}{P(A)}*P(B)\nComponents:\n\nPrior belief: P(B)\n\nEvidence strength: \\frac{P(A|B)}{P(A)}\n\nUpdated belief: P(B|A)\n\n\n\nKey Insights:\n\nUpdates prior knowledge\nEvidence can strengthen/weaken\nSystematic way to learn\nCombines knowledge & data\n\n\n\n\nAnother way to think of Bayes’ rule is as a way to update our beliefs on the basis of data. The different parts of Bayes’ rule have specific names, that relate to their role in using Bayes’ rule to update our beliefs.\nThe part on the left tells us how much more or less likely the data A are given B, relative to the overall likelihood of the data, while the part on the right side tells us how likely we thought B was before we knew anything about the data.\n\n\n\n\nConverting to Odds:\n\\text{odds of A} = \\frac{P(A)}{P(\\neg A)}\nExample:\nDrug test odds:\n\nPrior: \\frac{0.025}{0.975} = 0.026\n\nPosterior: \\frac{0.7314974}{0.2685026} = 2.724\n\n\n\nOdds Ratio:\n\\frac{\\text{posterior odds}}{\\text{prior odds}} = \\frac{2.724}{0.026} = 106.25\nInterpretation:\n\nOdds increased 105×\nMuch stronger evidence\nShows test’s power\nDespite false positives\n\n\n\n\nWe can convert probabilities into odds which express the relative likelihood of something happening or not. An odds ratio is an example of what we will later call an effect size, which is a way of quantifying how relatively large any particular statistical effect is.\nFirst, remember the rule for computing a conditional probability. We can rearrange this to get the formula to compute the joint probability using the conditional. Using this we can compute the inverse probability.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Bayes' Rule and Learning from Data"
    ]
  },
  {
    "objectID": "Week3/05-bayes.html#the-basic-formula",
    "href": "Week3/05-bayes.html#the-basic-formula",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "",
    "text": "When we know P(A|B) but want P(B|A):\nP(B|A) = \\frac{P(A|B)*P(B)}{P(A)}\nAlternative Form:\nP(B|A) = \\frac{P(A|B)*P(B)}{P(A|B)*P(B) + P(A|\\neg B)*P(\\neg B)}\n\n\nComponents:\n\nPrior: P(B)\n\nLikelihood: P(A|B)\n\nMarginal likelihood: P(A)\n\nPosterior: P(B|A)\n\n\n\n\n\n\nIn many cases, we know P(A|B) but we really want to know P(B|A). This commonly occurs in medical screening, where we know P(positive test result| disease) but what we want to know is P(disease|positive test result).\nIf we have only two outcomes, we can express Bayes’ rule in a somewhat clearer way, using the sum rule to redefine P(A).",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Bayes' Rule and Learning from Data"
    ]
  },
  {
    "objectID": "Week3/05-bayes.html#putting-bayes-into-practice",
    "href": "Week3/05-bayes.html#putting-bayes-into-practice",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "",
    "text": "A major construction company conducts mandatory random drug and alcohol screening using rapid saliva tests. Consider the following scenario:\n\n\nIn the UK construction industry during 2023, the prevalence of substance use affecting workplace safety was estimated at 2.5% of the workforce\nThe rapid saliva test used has a sensitivity (true positive rate) of 85% when conducted according to protocol\nThe specificity (true negative rate) of these tests is 99.2%\n\n\n\n\nLet’s consider a specific example. Suppose that a worker is selected for a random drug screening. The test result is positive. What is the probability that this worker is actually positive for substances?",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Bayes' Rule and Learning from Data"
    ]
  },
  {
    "objectID": "Week3/05-bayes.html#putting-bayes-into-practice-1",
    "href": "Week3/05-bayes.html#putting-bayes-into-practice-1",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "",
    "text": "Let’s consider a specific example. Suppose that a worker is selected for a random drug screening. The test result is positive. What is the probability that this worker is actually positive for substances?\n\nContext: The company’s current policy is immediate suspension without pay following a positive test result, pending a more accurate laboratory confirmation test that takes 48 hours.\n\nMandatory screening\nRapid saliva test\nSafety-critical roles\nImmediate consequences",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Bayes' Rule and Learning from Data"
    ]
  },
  {
    "objectID": "Week3/05-bayes.html#putting-bayes-into-practice-2",
    "href": "Week3/05-bayes.html#putting-bayes-into-practice-2",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "",
    "text": "Construction Site Testing:\n\nSensitivity: P(positive|substance) = 0.85\nSpecificity: P(negative|no substance) = 0.992\nBase rate: P(substance) = 0.025\n\n\nKey Values:\n\nP(S) = 0.025 (prevalence)\nP(P|S) = 0.85 (sensitivity)\nP(P|not S) = 0.008 (1 - specificity)\n\n\n\nA major construction company conducts mandatory random drug and alcohol screening using rapid saliva tests. In the UK construction industry during 2023, the prevalence of substance use affecting workplace safety was estimated at 2.5% of the workforce. The rapid saliva test used has a sensitivity of 85% when conducted according to protocol, and a specificity of 99.2%.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Bayes' Rule and Learning from Data"
    ]
  },
  {
    "objectID": "Week3/05-bayes.html#lets-work-through-it",
    "href": "Week3/05-bayes.html#lets-work-through-it",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "",
    "text": "Using Bayes’ Theorem, calculate the probability that this worker is actually positive for substances given their positive test result.\n\nP(S) = 0.025 (prevalence)\nP(P|S) = 0.85 (sensitivity)\nP(P|not S) = 0.008 (1 - specificity)\n\n\nA construction worker is randomly selected for testing at the start of their shift. Their saliva test comes back positive. Using Bayes’ Theorem, calculate the probability that this worker is actually positive for substances given their positive test result.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Bayes' Rule and Learning from Data"
    ]
  },
  {
    "objectID": "Week3/05-bayes.html#solution",
    "href": "Week3/05-bayes.html#solution",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "",
    "text": "Calculate P(substance|positive):\n\n\\begin{align*}\nP(P) &= P(P|S) \\times P(S) + P(P|not S) \\times P(not S) \\\\\n&= (0.85 \\times 0.025) + (0.008 \\times 0.975) \\\\\n&= 0.02125 + 0.0078 \\\\\n&= 0.02905\n\\end{align*}\n\n\n\\begin{align*}\nP(S|P) &= \\frac{P(P|S) \\times P(S)}{P(P)} \\\\\n&= \\frac{0.85 \\times 0.025}{0.02905} \\\\\n&= 0.7314974 \\text{ or } 73.1\\%\n\\end{align*}\n\n\n\nInterpretation:\n\n~73.1% chance true positive\n~26.9% chance false positive\nMuch higher than 2.5% base rate\nStill significant uncertainty\n\n\n\n\n\nUsing Bayes’ Theorem, we find that given a positive test result, there is a 73.1% probability that the worker actually has substances present. This is much higher than the base rate of 2.5%, but still leaves significant uncertainty with a 26.9% false positive rate.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Bayes' Rule and Learning from Data"
    ]
  },
  {
    "objectID": "Week3/05-bayes.html#discussion-the-real-world-implications",
    "href": "Week3/05-bayes.html#discussion-the-real-world-implications",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "",
    "text": "The company’s current policy is immediate suspension without pay following a positive test result.\nWhat do these results mean for this business policy? Is it fair to immediately suspend workers without pay for a positive test result?\n\nThe company’s current policy is immediate suspension without pay following a positive test result, pending a more accurate laboratory confirmation test that takes 48 hours.\nGiven that approximately 26.9% of positive test results may be false positives, an immediate suspension without pay could unfairly penalize innocent workers; however, the high stakes of construction safety and the 73.1% probability of a true positive suggest that temporary removal from safety-critical roles is prudent while awaiting confirmation.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Bayes' Rule and Learning from Data"
    ]
  },
  {
    "objectID": "Week3/05-bayes.html#learning-from-data",
    "href": "Week3/05-bayes.html#learning-from-data",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "",
    "text": "Bayes’ Rule as Learning:\nP(B|A) = \\frac{P(A|B)}{P(A)}*P(B)\nComponents:\n\nPrior belief: P(B)\n\nEvidence strength: \\frac{P(A|B)}{P(A)}\n\nUpdated belief: P(B|A)\n\n\n\nKey Insights:\n\nUpdates prior knowledge\nEvidence can strengthen/weaken\nSystematic way to learn\nCombines knowledge & data\n\n\n\n\nAnother way to think of Bayes’ rule is as a way to update our beliefs on the basis of data. The different parts of Bayes’ rule have specific names, that relate to their role in using Bayes’ rule to update our beliefs.\nThe part on the left tells us how much more or less likely the data A are given B, relative to the overall likelihood of the data, while the part on the right side tells us how likely we thought B was before we knew anything about the data.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Bayes' Rule and Learning from Data"
    ]
  },
  {
    "objectID": "Week3/05-bayes.html#odds-and-odds-ratios",
    "href": "Week3/05-bayes.html#odds-and-odds-ratios",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "",
    "text": "Converting to Odds:\n\\text{odds of A} = \\frac{P(A)}{P(\\neg A)}\nExample:\nDrug test odds:\n\nPrior: \\frac{0.025}{0.975} = 0.026\n\nPosterior: \\frac{0.7314974}{0.2685026} = 2.724\n\n\n\nOdds Ratio:\n\\frac{\\text{posterior odds}}{\\text{prior odds}} = \\frac{2.724}{0.026} = 106.25\nInterpretation:\n\nOdds increased 105×\nMuch stronger evidence\nShows test’s power\nDespite false positives\n\n\n\n\nWe can convert probabilities into odds which express the relative likelihood of something happening or not. An odds ratio is an example of what we will later call an effect size, which is a way of quantifying how relatively large any particular statistical effect is.\nFirst, remember the rule for computing a conditional probability. We can rearrange this to get the formula to compute the joint probability using the conditional. Using this we can compute the inverse probability.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Bayes' Rule and Learning from Data"
    ]
  },
  {
    "objectID": "Week3/03-empirical.html",
    "href": "Week3/03-empirical.html",
    "title": "Determining Probabilities",
    "section": "",
    "text": "Personal Belief\n\nSubjective assessment\nBased on knowledge/experience\nLimited scientific validity\nOften only available approach\n\n\n\nEmpirical Frequency\n\nBased on repeated experiments\nLaw of large numbers\nReal-world data collection\n\n\n\n\n\n\nClassical Probability\n\nBased on equally likely outcomes\nMathematical approach\nCommon in games of chance\nNo experiments needed\n\n\n\n\n\n\nNow that we know what a probability is, how do we actually figure out what the probability is for any particular event? There are three main approaches, each with their own strengths and limitations.\n\n\n\n\nExample Question:\nWhat was the probability that Bernie Sanders would have won the 2016 presidential election if he had been the democratic nominee?\nKey Points:\n\nCan’t run this experiment\nPeople can still estimate based on knowledge\nNot scientifically satisfying\nOften the only available approach\n\n\nOther Examples:\n\nWeather forecasts\nSports predictions\nEconomic forecasts\nPersonal decisions\n\n\n\n\nLet’s say that I asked you what the probability was that Bernie Sanders would have won the 2016 presidential election if he had been the democratic nominee instead of Hilary Clinton? We can’t actually do the experiment to find the outcome. However, most people with knowledge of American politics would be willing to at least offer a guess at the probability of this event. In many cases personal knowledge and/or opinion is the only guide we have determining the probability of an event, but this is not very scientifically satisfying.\n\n\n\n\nSan Francisco Rain Example:\n\nTotal days in 2017: 365\nRainy days: 73\nP(rain in SF) = 73/365 = 0.2\n\nKey Steps:\n\nDefine experiment clearly\nCount occurrences\nDivide by total trials\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnother way to determine the probability of an event is to do the experiment many times and count how often each event happens. From the relative frequency of the different outcomes, we can compute the probability of each outcome. For example, let’s say that we are interested in knowing the probability of rain in San Francisco. We first have to define the experiment — let’s say that we will look at the National Weather Service data for each day in 2017 and determine whether there was any rain at the downtown San Francisco weather station. According to these data, in 2017 there were 73 rainy days. To compute the probability of rain in San Francisco, we simply divide the number of rainy days by the number of days counted (365), giving P(rain in SF in 2017) = 0.2.\nThe graph shows how the empirical probability of rain converges to 0.2 as we accumulate more days of data throughout the year.\n\n\n\n\nCoin Flip Example:\n\nTrue probability of heads = 0.5\nSmall samples vary widely\nMore flips = better estimate\nConverges to true probability\n“Law of small numbers” fallacy\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe graph shows how early results from coin flips can be highly variable and unrepresentative of the true value. Even though we know a fair coin has a probability of 0.5 for heads, small samples can give very different results. This demonstrates how small samples can give misleading results.\nThis was referred to as the “law of small numbers” by psychologists Danny Kahneman and Amos Tversky, who showed that people (even trained researchers) often behave as if the law of large numbers applies even to small samples, leading them to draw strong conclusions from insufficient data.\n\n\n\n\n2017 Senate Race:\n\nRoy Moore vs Doug Jones\nEarly results volatile\nFinal outcome different\nSmall sample warning\n\n\n\n\n\n\n\n\n\n\n\n\n\nA real-world example of this was seen in the 2017 special election for the US Senate in Alabama. Early in the evening the vote counts were especially volatile, swinging from a large initial lead for Jones to a long period where Moore had the lead, until finally Jones took the lead to win the race.\nThis demonstrates how small samples can give misleading results. Unfortunately, many people forget this and overinterpret results from small samples.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Determining Probabilities"
    ]
  },
  {
    "objectID": "Week3/03-empirical.html#three-approaches",
    "href": "Week3/03-empirical.html#three-approaches",
    "title": "Determining Probabilities",
    "section": "",
    "text": "Personal Belief\n\nSubjective assessment\nBased on knowledge/experience\nLimited scientific validity\nOften only available approach\n\n\n\nEmpirical Frequency\n\nBased on repeated experiments\nLaw of large numbers\nReal-world data collection\n\n\n\n\n\n\nClassical Probability\n\nBased on equally likely outcomes\nMathematical approach\nCommon in games of chance\nNo experiments needed\n\n\n\n\n\n\nNow that we know what a probability is, how do we actually figure out what the probability is for any particular event? There are three main approaches, each with their own strengths and limitations.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Determining Probabilities"
    ]
  },
  {
    "objectID": "Week3/03-empirical.html#personal-belief",
    "href": "Week3/03-empirical.html#personal-belief",
    "title": "Determining Probabilities",
    "section": "",
    "text": "Example Question:\nWhat was the probability that Bernie Sanders would have won the 2016 presidential election if he had been the democratic nominee?\nKey Points:\n\nCan’t run this experiment\nPeople can still estimate based on knowledge\nNot scientifically satisfying\nOften the only available approach\n\n\nOther Examples:\n\nWeather forecasts\nSports predictions\nEconomic forecasts\nPersonal decisions\n\n\n\n\nLet’s say that I asked you what the probability was that Bernie Sanders would have won the 2016 presidential election if he had been the democratic nominee instead of Hilary Clinton? We can’t actually do the experiment to find the outcome. However, most people with knowledge of American politics would be willing to at least offer a guess at the probability of this event. In many cases personal knowledge and/or opinion is the only guide we have determining the probability of an event, but this is not very scientifically satisfying.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Determining Probabilities"
    ]
  },
  {
    "objectID": "Week3/03-empirical.html#empirical-frequency",
    "href": "Week3/03-empirical.html#empirical-frequency",
    "title": "Determining Probabilities",
    "section": "",
    "text": "San Francisco Rain Example:\n\nTotal days in 2017: 365\nRainy days: 73\nP(rain in SF) = 73/365 = 0.2\n\nKey Steps:\n\nDefine experiment clearly\nCount occurrences\nDivide by total trials\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnother way to determine the probability of an event is to do the experiment many times and count how often each event happens. From the relative frequency of the different outcomes, we can compute the probability of each outcome. For example, let’s say that we are interested in knowing the probability of rain in San Francisco. We first have to define the experiment — let’s say that we will look at the National Weather Service data for each day in 2017 and determine whether there was any rain at the downtown San Francisco weather station. According to these data, in 2017 there were 73 rainy days. To compute the probability of rain in San Francisco, we simply divide the number of rainy days by the number of days counted (365), giving P(rain in SF in 2017) = 0.2.\nThe graph shows how the empirical probability of rain converges to 0.2 as we accumulate more days of data throughout the year.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Determining Probabilities"
    ]
  },
  {
    "objectID": "Week3/03-empirical.html#law-of-large-numbers",
    "href": "Week3/03-empirical.html#law-of-large-numbers",
    "title": "Determining Probabilities",
    "section": "",
    "text": "Coin Flip Example:\n\nTrue probability of heads = 0.5\nSmall samples vary widely\nMore flips = better estimate\nConverges to true probability\n“Law of small numbers” fallacy\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe graph shows how early results from coin flips can be highly variable and unrepresentative of the true value. Even though we know a fair coin has a probability of 0.5 for heads, small samples can give very different results. This demonstrates how small samples can give misleading results.\nThis was referred to as the “law of small numbers” by psychologists Danny Kahneman and Amos Tversky, who showed that people (even trained researchers) often behave as if the law of large numbers applies even to small samples, leading them to draw strong conclusions from insufficient data.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Determining Probabilities"
    ]
  },
  {
    "objectID": "Week3/03-empirical.html#real-world-example-alabama-election",
    "href": "Week3/03-empirical.html#real-world-example-alabama-election",
    "title": "Determining Probabilities",
    "section": "",
    "text": "2017 Senate Race:\n\nRoy Moore vs Doug Jones\nEarly results volatile\nFinal outcome different\nSmall sample warning\n\n\n\n\n\n\n\n\n\n\n\n\n\nA real-world example of this was seen in the 2017 special election for the US Senate in Alabama. Early in the evening the vote counts were especially volatile, swinging from a large initial lead for Jones to a long period where Moore had the lead, until finally Jones took the lead to win the race.\nThis demonstrates how small samples can give misleading results. Unfortunately, many people forget this and overinterpret results from small samples.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Determining Probabilities"
    ]
  },
  {
    "objectID": "Week3/01-intro.html",
    "href": "Week3/01-intro.html",
    "title": "Part 1: Introduction to Probability",
    "section": "",
    "text": "Branch of mathematics dealing with chance and uncertainty\nFoundation for statistics\nProvides tools to describe uncertain events\nHistorical origins in games of chance\nDeep questions about meaning and interpretation\n\n\nProbability theory is the branch of mathematics that deals with chance and uncertainty. It forms an important part of the foundation for statistics, because it provides us with the mathematical tools to describe uncertain events.\nThe study of probability arose in part due to interest in understanding games of chance, like cards or dice. These games provide useful examples of many statistical concepts, because when we repeat these games the likelihood of different outcomes remains (mostly) the same. However, there are deep questions about the meaning of probability that we will not address here.\n\n\n\n\nAn experiment is any activity that produces or observes an outcome. Examples are flipping a coin, rolling a 6-sided die, or trying a new route to work to see if it’s faster than the old route.\n\nCoin flip: {heads, tails}\nDie roll: {1,2,3,4,5,6}\nTravel time: (0,∞)\n\n\nThe sample space is the set of possible outcomes for an experiment. We represent these by listing them within a set of squiggly brackets.\nAn event is a subset of the sample space. In principle it could be one or more of possible outcomes in the sample space, but here we will focus primarily on elementary events which consist of exactly one possible outcome.\n\nSubset of sample space\nCan be elementary or compound\nExample: rolling a 4\n\n\n\n\n\nTo formalize probability theory, we first need to define a few terms:\n\nAn experiment is any activity that produces or observes an outcome. Examples are flipping a coin, rolling a 6-sided die, or trying a new route to work to see if it’s faster than the old route.\nThe sample space is the set of possible outcomes for an experiment. We represent these by listing them within a set of squiggly brackets.\nAn event is a subset of the sample space. In principle it could be one or more of possible outcomes in the sample space, but here we will focus primarily on elementary events which consist of exactly one possible outcome.\n\n\n\nFor events {E_1, E_2, ... , E_N} and random variable X:\n\n\n\n\nNon-negativity:\nP(X=E_i) \\ge 0\n\n\nNormalization:\n\\sum_{i=1}^N{P(X=E_i)} = 1\n\n\nBoundedness:\nP(X=E_i)\\le 1\n\n\n\n\nImplications:\n\nAll probabilities are between 0 and 1\nTotal probability must sum to 1\nIndividual probabilities ≤ 1\n\n\n\n\n\nThese are the features that a value has to have if it is going to be a probability, which were first defined by the Russian mathematician Andrei Kolmogorov.\nThe summation is interpreted as saying “Take all of the N elementary events, which we have labeled from 1 to N, and add up their probabilities. These must sum to one.”\nThe third point is implied by the previous points; since they must sum to one, and they can’t be negative, then any particular probability cannot exceed one.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Part 1: Introduction to Probability"
    ]
  },
  {
    "objectID": "Week3/01-intro.html#what-is-probability-theory",
    "href": "Week3/01-intro.html#what-is-probability-theory",
    "title": "Part 1: Introduction to Probability",
    "section": "",
    "text": "Branch of mathematics dealing with chance and uncertainty\nFoundation for statistics\nProvides tools to describe uncertain events\nHistorical origins in games of chance\nDeep questions about meaning and interpretation\n\n\nProbability theory is the branch of mathematics that deals with chance and uncertainty. It forms an important part of the foundation for statistics, because it provides us with the mathematical tools to describe uncertain events.\nThe study of probability arose in part due to interest in understanding games of chance, like cards or dice. These games provide useful examples of many statistical concepts, because when we repeat these games the likelihood of different outcomes remains (mostly) the same. However, there are deep questions about the meaning of probability that we will not address here.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Part 1: Introduction to Probability"
    ]
  },
  {
    "objectID": "Week3/01-intro.html#experiment-sample-space-events",
    "href": "Week3/01-intro.html#experiment-sample-space-events",
    "title": "Part 1: Introduction to Probability",
    "section": "",
    "text": "An experiment is any activity that produces or observes an outcome. Examples are flipping a coin, rolling a 6-sided die, or trying a new route to work to see if it’s faster than the old route.\n\nCoin flip: {heads, tails}\nDie roll: {1,2,3,4,5,6}\nTravel time: (0,∞)\n\n\nThe sample space is the set of possible outcomes for an experiment. We represent these by listing them within a set of squiggly brackets.\nAn event is a subset of the sample space. In principle it could be one or more of possible outcomes in the sample space, but here we will focus primarily on elementary events which consist of exactly one possible outcome.\n\nSubset of sample space\nCan be elementary or compound\nExample: rolling a 4\n\n\n\n\n\nTo formalize probability theory, we first need to define a few terms:\n\nAn experiment is any activity that produces or observes an outcome. Examples are flipping a coin, rolling a 6-sided die, or trying a new route to work to see if it’s faster than the old route.\nThe sample space is the set of possible outcomes for an experiment. We represent these by listing them within a set of squiggly brackets.\nAn event is a subset of the sample space. In principle it could be one or more of possible outcomes in the sample space, but here we will focus primarily on elementary events which consist of exactly one possible outcome.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Part 1: Introduction to Probability"
    ]
  },
  {
    "objectID": "Week3/01-intro.html#kolmogorovs-axioms",
    "href": "Week3/01-intro.html#kolmogorovs-axioms",
    "title": "Part 1: Introduction to Probability",
    "section": "",
    "text": "For events {E_1, E_2, ... , E_N} and random variable X:\n\n\n\n\nNon-negativity:\nP(X=E_i) \\ge 0\n\n\nNormalization:\n\\sum_{i=1}^N{P(X=E_i)} = 1\n\n\nBoundedness:\nP(X=E_i)\\le 1\n\n\n\n\nImplications:\n\nAll probabilities are between 0 and 1\nTotal probability must sum to 1\nIndividual probabilities ≤ 1\n\n\n\n\n\nThese are the features that a value has to have if it is going to be a probability, which were first defined by the Russian mathematician Andrei Kolmogorov.\nThe summation is interpreted as saying “Take all of the N elementary events, which we have labeled from 1 to N, and add up their probabilities. These must sum to one.”\nThe third point is implied by the previous points; since they must sum to one, and they can’t be negative, then any particular probability cannot exceed one.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Part 1: Introduction to Probability"
    ]
  },
  {
    "objectID": "Week2/lecture.html#introduction",
    "href": "Week2/lecture.html#introduction",
    "title": "Data Visualization",
    "section": "Introduction",
    "text": "Introduction\n\n\n\n\n\n\n\n\n\n“The simple graph has brought more information to the data analyst’s mind than any other device.” — John Tukey\n\nR has several systems for making graphs, but ggplot2 is one of the most elegant and most versatile.\nggplot2 implements the grammar of graphics, a coherent system for describing and building graphs. With ggplot2, you can do more and faster by learning one system and applying it in many places.\n\n\nThis presentation is based on the Data Visualization chapter of the R for Data Science book by Hadley Wickham and Garrett Grolemund."
  },
  {
    "objectID": "Week2/lecture.html#the-grammar-of-graphics",
    "href": "Week2/lecture.html#the-grammar-of-graphics",
    "title": "Data Visualization",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\n\n\nIn 1999, a statistician named Leland Wilkinson published the first edition of what has been the most influential work in data visualization, The Grammar of Graphics.\nThe most complete implementation of the grammar is found in an R package called ggplot2 by Hadley Wickham."
  },
  {
    "objectID": "Week2/lecture.html#the-grammar-of-graphics-1",
    "href": "Week2/lecture.html#the-grammar-of-graphics-1",
    "title": "Data Visualization",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nA plot can be decomposed into three primary elements\n1. the data\n2. the aesthetic mapping of the variables in the data to visual cues\n3. the geometry used to encode the observations on the plot."
  },
  {
    "objectID": "Week2/lecture.html#getting-started",
    "href": "Week2/lecture.html#getting-started",
    "title": "Data Visualization",
    "section": "Getting Started",
    "text": "Getting Started\nThroughout this lecture, we will be writing code together inside this webpage.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHints:\n\n\nYou can type code into the cells and run them by clicking the “Run” button."
  },
  {
    "objectID": "Week2/lecture.html#getting-started-1",
    "href": "Week2/lecture.html#getting-started-1",
    "title": "Data Visualization",
    "section": "Getting Started",
    "text": "Getting Started\nPackages\n\n\nWe begin by loading the tidyverse and ggplot2 packages.\n\nWe almost always begin our work by loading the tidyverse package. Note that the terms “package” and “library” are used interchangeably but that there is no package() function. To load a package, you need to use library()."
  },
  {
    "objectID": "Week2/lecture.html#getting-started-2",
    "href": "Week2/lecture.html#getting-started-2",
    "title": "Data Visualization",
    "section": "Getting Started",
    "text": "Getting Started\nLoading the Data\n\n\nLoad the palmerpenguins package using library().\nThis package contains the penguins dataset, which we will use for this tutorial."
  },
  {
    "objectID": "Week2/lecture.html#getting-started-3",
    "href": "Week2/lecture.html#getting-started-3",
    "title": "Data Visualization",
    "section": "Getting Started",
    "text": "Getting Started\nGetting help\n\n\nIf you are unsure about how to use a function, you can use the ? operator to get help.\nFor a data package like palmerpenguins, you can use ?penguins to get help on the dataset."
  },
  {
    "objectID": "Week2/lecture.html#the-grammar-of-graphics-2",
    "href": "Week2/lecture.html#the-grammar-of-graphics-2",
    "title": "Data Visualization",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nThe Data\n- A variable is a quantity, quality, or property that you can measure.\n- A value is the state of a variable when you measure it. The value of a variable may change from measurement to measurement.\n- An observation is a set of measurements made under similar conditions. An observation will contain several values, each associated with a different variable. We’ll sometimes refer to an observation as a data point.\n- Tabular data is a set of values, each associated with a variable and an observation. Tabular data is tidy if each value is placed in its own “cell”, each variable in its own column, and each observation in its own row."
  },
  {
    "objectID": "Week2/lecture.html#the-grammar-of-graphics-3",
    "href": "Week2/lecture.html#the-grammar-of-graphics-3",
    "title": "Data Visualization",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nThe Data\n\n\n\nspecies: a penguin’s species (Adelie, Chinstrap, or Gentoo).\nflipper_length_mm: length of a penguin’s flipper, in millimeters.\nbody_mass_g: body mass of a penguin, in grams."
  },
  {
    "objectID": "Week2/lecture.html#formulating-our-research-questions",
    "href": "Week2/lecture.html#formulating-our-research-questions",
    "title": "Data Visualization",
    "section": "Formulating our Research Question(s)",
    "text": "Formulating our Research Question(s)\n\n\nDo penguins with longer flippers weigh more or less than penguins with shorter flippers? You probably already have an answer, but try to make your answer precise.\nWhat does the relationship between flipper length and body mass look like? Is it positive? Negative? Linear? Nonlinear?\nDoes the relationship vary by the species of the penguin? How about by the island where the penguin lives?"
  },
  {
    "objectID": "Week2/lecture.html#building-up-a-plot",
    "href": "Week2/lecture.html#building-up-a-plot",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nCreating a ggplot\n\n\nWith ggplot2, you begin a plot with the function ggplot(), defining a plot object that you then add layers to.\n\nThe first argument of ggplot() is the dataset to use in the graph and so ggplot(data = penguins) creates an empty graph that is primed to display the penguins data, but since we haven’t told it how to visualize it yet, for now it’s empty.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninput ggplot(data = penguins)"
  },
  {
    "objectID": "Week2/lecture.html#building-up-a-plot-1",
    "href": "Week2/lecture.html#building-up-a-plot-1",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nThis is not a very exciting plot, but you can think of it like an empty canvas you’ll paint the remaining layers of your plot onto.\n\nNext, we need to tell ggplot() how the information from our data will be visually represented. The mapping argument of the ggplot() function defines how variables in your dataset are mapped to visual properties (aesthetics) of your plot.\n\n\nFor now, we will only map flipper length to the x aesthetic and body mass to the y aesthetic."
  },
  {
    "objectID": "Week2/lecture.html#the-grammar-of-graphics-4",
    "href": "Week2/lecture.html#the-grammar-of-graphics-4",
    "title": "Data Visualization",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nAesthetics"
  },
  {
    "objectID": "Week2/lecture.html#building-up-a-plot-2",
    "href": "Week2/lecture.html#building-up-a-plot-2",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nAesthetic mappings\n\n\nThe mapping argument is always defined in the aes() function, and the x and y arguments of aes() specify which variables to map to the x and y axes.\n\nFor now, we will only map flipper length to the x aesthetic and body mass to the y aesthetic. ggplot2 looks for the mapped variables in the data argument, in this case, penguins.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nadd mapping = aes(x = flipper_length_mm, y = body_mass_g)\nOur empty canvas now has more structure – it’s clear where flipper lengths will be displayed (on the x-axis) and where body masses will be displayed (on the y-axis). But the penguins themselves are not yet on the plot. This is because we have not yet articulated, in our code, how to represent the observations from our data frame on our plot."
  },
  {
    "objectID": "Week2/lecture.html#building-up-a-plot-3",
    "href": "Week2/lecture.html#building-up-a-plot-3",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdding layers\nWe need to define a geom: the geometrical object that a plot uses to represent data. These geometric objects are made available in ggplot2 with functions that start with geom_.\n\nPeople often describe plots by the type of geom that the plot uses:\n\n\nbar charts use bar geoms (geom_bar()),\nline charts use line geoms (geom_line()),\nboxplots use boxplot geoms (geom_boxplot()),\nscatterplots use point geoms (geom_point()), and so on.\n\n\nThe function geom_point() adds a layer of points to your plot, which creates a scatterplot."
  },
  {
    "objectID": "Week2/lecture.html#building-up-a-plot-4",
    "href": "Week2/lecture.html#building-up-a-plot-4",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdd a scatter point layer to the plot:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nadd + geom_point()\nNow we have a scatterplot of flipper length (x-axis) and body mass (y-axis) for penguins. The plot displays a positive, linear, and relative strong relationship between these two variables.\nBefore we add more layers to this plot, let’s pause for a moment and review the warning message we got:\nWe’re seeing this message because there are two penguins in our dataset with missing body mass and/or flipper length values and ggplot2 has no way of representing them on the plot without both of these values. Like R, ggplot2 subscribes to the philosophy that missing values should never silently go missing. This type of warning is probably one of the most common types of warnings you will see when working with real data – missing values are a very common issue and you’ll learn more about them later. For the remaining plots we will suppress this warning so it’s not printed alongside every single plot we make."
  },
  {
    "objectID": "Week2/lecture.html#building-up-a-plot-5",
    "href": "Week2/lecture.html#building-up-a-plot-5",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdding aesthetics\n\n\n\n\nIt’s always a good idea to be skeptical of any apparent relationship between two variables and ask if there may be other variables that explain or change the nature of this apparent relationship.\n\n\nFor example, does the relationship between flipper length and body mass differ by species?\nWhen exploring relationships between variables, it’s important to consider other variables that might affect the relationship. Let’s incorporate species into our plot using color:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScatterplots are useful for displaying the relationship between two numerical variables, but it’s always a good idea to be skeptical of any apparent relationship between two variables and ask if there may be other variables that explain or change the nature of this apparent relationship.\nFor example, does the relationship between flipper length and body mass differ by species? Let’s incorporate species into our plot and see if this reveals any additional insights into the apparent relationship between these variables. We will do this by representing species with different colored points.\nTo achieve this, will we need to modify the aesthetic or the geom? If you guessed “in the aesthetic mapping, inside of aes()”, you’re already getting the hang of creating data visualizations with ggplot2! And if not, don’t worry.\nAdd color = species to the aesthetic mapping. This tells ggplot2 to color the points by species.\nWhen we map a categorical variable to an aesthetic, ggplot2 automatically: - Assigns a unique value to each level (here, a unique color for each species) - Adds a legend explaining the mapping"
  },
  {
    "objectID": "Week2/lecture.html#building-up-a-plot-6",
    "href": "Week2/lecture.html#building-up-a-plot-6",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdd a trend line to see the relationship more clearly using geom_smooth()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nAdd a trendline (geom_smooth(method = \"lm\")) layer to the plot.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow let’s add one more layer: a smooth curve displaying the relationship between body mass and flipper length. Before you proceed, refer back to the code above, and think about how we can add this to our existing plot.\nSince this is a new geometric object representing our data, we will add a new geom as a layer on top of our point geom: geom_smooth(). And we will specify that we want to draw the line of best fit based on a linear model with method = \"lm\".\nAdd geom_smooth(method = \"lm\") to the plot.\nThe method = “lm” argument tells geom_smooth() to use a linear model. Notice how the color aesthetic is inherited by both geoms, creating separate trend lines for each species."
  },
  {
    "objectID": "Week2/lecture.html#building-up-a-plot-7",
    "href": "Week2/lecture.html#building-up-a-plot-7",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdding smooth curves\nIt’s important to recognise how the color aesthetic is inherited by both geoms, creating separate trend lines for each species.\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\nWe have successfully added lines, but this plot doesn’t look like our ultimate goal plot, which only has one line for the entire dataset as opposed to separate lines for each of the penguin species.\nWhen aesthetic mappings are defined in ggplot(), at the global level, they’re passed down to each of the subsequent geom layers of the plot. However, each geom function in ggplot2 can also take a mapping argument, which allows for aesthetic mappings at the local level that are added to those inherited from the global level. Since we want points to be colored based on species but don’t want the lines to be separated out for them, we should specify color = species for geom_point() only."
  },
  {
    "objectID": "Week2/lecture.html#building-up-a-plot-8",
    "href": "Week2/lecture.html#building-up-a-plot-8",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdding smooth curves\nIt’s important to recognise how the color aesthetic is inherited by both geoms, creating separate trend lines for each species.\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, \n  color = species)\n) +\n  geom_point( &lt;color = species&gt; ) +\n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "Week2/lecture.html#building-up-a-plot-9",
    "href": "Week2/lecture.html#building-up-a-plot-9",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdding smooth curves\nIt’s important to recognise how the color aesthetic is inherited by both geoms, creating separate trend lines for each species.\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, \n  color = species)\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\", &lt;color = species&gt;)\nPay attention to how the aesthetic mappings propagate through the layers of the plot.\nThis can be useful for creating complex plots with multiple layers, but it can also lead to unexpected results if you’re not careful."
  },
  {
    "objectID": "Week2/lecture.html#building-up-plots",
    "href": "Week2/lecture.html#building-up-plots",
    "title": "Data Visualization",
    "section": "Building up plots",
    "text": "Building up plots\nGlobal vs Local aesthetics\n\n\nIn the previous plot, the color aesthetic was defined in the global mapping. This means that it applies to all geoms in the plot.\nTo get a single trend line while keeping colored points, we move the color aesthetic to geom_point():\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAesthetic mappings can be defined at the global level (in ggplot()) or at the local level (in individual geoms). Local aesthetics override global ones.\nVoila! We have something that looks very much like our ultimate goal, though it’s not yet perfect. We still need to use different shapes for each species of penguins and improve labels.\nIt’s generally not a good idea to represent information using only colors on a plot, as people perceive colors differently due to color blindness or other color vision differences. Therefore, in addition to color, we can also map species to the shape aesthetic."
  },
  {
    "objectID": "Week2/lecture.html#building-up-a-plot-10",
    "href": "Week2/lecture.html#building-up-a-plot-10",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nOther aesthetics - shapes\n\n\nIn addition to color, we can also map out variables to other aesthetic elements.\nHere, we map species to the shape aesthetic.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt’s generally not a good idea to represent information using only colors on a plot, as people perceive colors differently due to color blindness or other color vision differences. Therefore, in addition to color, we can also map species to the shape aesthetic."
  },
  {
    "objectID": "Week2/lecture.html#building-up-a-plot-11",
    "href": "Week2/lecture.html#building-up-a-plot-11",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nFinal touches\nThe data portions of our plot are now complete. But data visualization is not just about the data – it’s also about the visual elements that make the plot accessible and informative.\n\nWe also need the plot itself to communicate:\n\n\nWhat the plot is about (title)\nWhat the axes represent, including units (labels)\nWhat the colors and shapes represent (legends)\nAdditional context such as the source of the data (subtitle or caption)"
  },
  {
    "objectID": "Week2/lecture.html#building-up-a-plot-12",
    "href": "Week2/lecture.html#building-up-a-plot-12",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nWe can now add this information to our plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo add:\ntitle = \"Body Mass and Flipper Length\",\nsubtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\nx = \"Flipper Length (mm)\", \ny = \"Body Mass (g)\",\ncolor = \"Species\", \nshape = \"Species\"\nAnd finally, we can improve the labels of our plot using the labs() function in a new layer. Some of the arguments to labs() might be self explanatory:\n\ntitle adds a title and subtitle adds a subtitle to the plot\n\nOther arguments match the aesthetic mappings:\n\nx is the x-axis label\ny is the y-axis label\ncolor and shape define the label for the legend.\n\nIn addition, we can improve the color palette to be colorblind safe with the scale_color_colorblind() function from the ggthemes package."
  },
  {
    "objectID": "Week2/lecture.html#some-notes-on-ggplot-calls",
    "href": "Week2/lecture.html#some-notes-on-ggplot-calls",
    "title": "Data Visualization",
    "section": "Some notes on ggplot() calls",
    "text": "Some notes on ggplot() calls\n\n\nSo far, we’ve written the code in a very explicit way, with each argument named. This is a good practice when you’re learning, but it can be a bit verbose.\n\n\n\nTypically, the first one or two arguments to a function are so important that you should know them by heart. The first two arguments to ggplot() are data and mapping.\n\nYou’ll often see them left out. This is true for other functions as well.\nWhen leaving the names out, the order of the arguments matters.\n\n\n\n\n\n\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()\n\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()\n\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()"
  },
  {
    "objectID": "Week2/lecture.html#some-notes-on-ggplot-calls-1",
    "href": "Week2/lecture.html#some-notes-on-ggplot-calls-1",
    "title": "Data Visualization",
    "section": "Some notes on ggplot() calls",
    "text": "Some notes on ggplot() calls\nIn the future, you’ll also learn about the pipe, |&gt;, which operates similarly to the + operator in ggplot2.\nIt lets you chain together a series of operations, passing the output of one function to the input of the next.\n\npenguins |&gt; \n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()"
  },
  {
    "objectID": "Week2/lecture.html#some-notes-on-ggplot-calls-2",
    "href": "Week2/lecture.html#some-notes-on-ggplot-calls-2",
    "title": "Data Visualization",
    "section": "Some notes on ggplot() calls",
    "text": "Some notes on ggplot() calls\nIn the future, you’ll also learn about the pipe, |&gt;, which operates similarly to the + operator in ggplot2.\nIt lets you chain together a series of operations, passing the output of one function to the input of the next.\n\npenguins |&gt; \n  ggplot(&lt;penguins&gt;, aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()\n\nDon’t worry if you don’t understand this yet. It’s just a sneak peek at what’s to come."
  },
  {
    "objectID": "Week2/lecture.html#summary",
    "href": "Week2/lecture.html#summary",
    "title": "Data Visualization",
    "section": "Summary",
    "text": "Summary\nThe basic idea that underpins ggplot2: a visualization is a mapping from variables in your data to aesthetic properties like position, color, size and shape.\n\nThe grammar of graphics provides a systematic way to build visualizations\nStart with data and aesthetic mappings\nAdd layers with geoms\nUse different geoms for different types of variables\nEnhance plots with labels, colors, and facets\nMake sure your plots are clear and honest\n\n\nKey takeaways: - Build plots layer by layer - Choose appropriate visualizations for your variable types - Consider your audience when making design choices - Use aesthetics and facets to show additional variables - Always aim for clear and accessible visualizations"
  },
  {
    "objectID": "Week2/lecture.html#thats-it",
    "href": "Week2/lecture.html#thats-it",
    "title": "Data Visualization",
    "section": "That’s it!",
    "text": "That’s it!\nWith our remaining time, I’d like you to practice with ggplot2 using the DataAnalytics exercise. You should have already installed DataAnalytics with:\ndevtools::install_github(\"antoinevernet/DataAnalytics\")\nYou can then run the following code to get started:\nlearnr::run_tutorial(\"02-Visualisation\", package = \"DataAnalytics\")"
  },
  {
    "objectID": "Week2/lecture.html#continuous-module-dialogue",
    "href": "Week2/lecture.html#continuous-module-dialogue",
    "title": "Data Visualization",
    "section": "Continuous Module Dialogue",
    "text": "Continuous Module Dialogue\nMenti Survey\n\nhttps://www.menti.com/al7gkr8qhntz"
  },
  {
    "objectID": "Week2/lecture.html#visualizing-distributions",
    "href": "Week2/lecture.html#visualizing-distributions",
    "title": "Data Visualization",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nThe following are some additional slides on dealing with other data types and visualizing distributions in ggplot2. Explore them at your own pace."
  },
  {
    "objectID": "Week2/lecture.html#visualizing-distributions-1",
    "href": "Week2/lecture.html#visualizing-distributions-1",
    "title": "Data Visualization",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nCategorical variables\nFor categorical variables like species, we use bar charts:\n\n\n\n\n\n\n\n\n\nA bar chart shows: - Categories on one axis - Counts (frequencies) on the other axis - Height of bars represents number of observations in each category"
  },
  {
    "objectID": "Week2/lecture.html#visualizing-distributions-3",
    "href": "Week2/lecture.html#visualizing-distributions-3",
    "title": "Data Visualization",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nNumerical variables\nFor numerical variables like body_mass_g, we use histograms:\n\n\n\n\n\n\n\n\n\nA histogram: - Divides the x-axis into bins - Height shows number of observations in each bin - binwidth controls the size of the bins - Different binwidths can reveal different patterns"
  },
  {
    "objectID": "Week2/lecture.html#visualizing-distributions-4",
    "href": "Week2/lecture.html#visualizing-distributions-4",
    "title": "Data Visualization",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nExploring binwidth\nThe choice of binwidth affects what patterns we can see:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nToo small binwidth: too many bars, noisy pattern\nToo large binwidth: too few bars, loses detail\nNeed to experiment to find the right balance"
  },
  {
    "objectID": "Week2/lecture.html#visualizing-distributions-5",
    "href": "Week2/lecture.html#visualizing-distributions-5",
    "title": "Data Visualization",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nDensity plots\nAn alternative to histograms is the density plot:\n\n\n\n\n\n\n\n\n\nDensity plots: - Show the shape of the distribution smoothly - Easier to compare multiple groups - Like a smoothed histogram - Think of it as draping a string over a histogram"
  },
  {
    "objectID": "Week2/lecture.html#visualizing-relationships",
    "href": "Week2/lecture.html#visualizing-relationships",
    "title": "Data Visualization",
    "section": "Visualizing relationships",
    "text": "Visualizing relationships\nNumerical and categorical variables\nTo compare a numerical variable across categories, use boxplots:\n\n\n\n\n\n\n\n\n\nA boxplot shows: - Median (middle line) - IQR (box) - Whiskers (extend to most extreme non-outlier points) - Individual points for outliers"
  },
  {
    "objectID": "Week2/lecture.html#visualizing-relationships-1",
    "href": "Week2/lecture.html#visualizing-relationships-1",
    "title": "Data Visualization",
    "section": "Visualizing relationships",
    "text": "Visualizing relationships\nAlternative views\nWe can also use density plots to compare distributions:\n\n\n\n\n\n\n\n\n\n\ncolor and fill aesthetics distinguish species\nalpha controls transparency\nOverlapping distributions show how groups compare"
  },
  {
    "objectID": "Week2/lecture.html#visualizing-relationships-2",
    "href": "Week2/lecture.html#visualizing-relationships-2",
    "title": "Data Visualization",
    "section": "Visualizing relationships",
    "text": "Visualizing relationships\nTwo categorical variables\nFor two categorical variables, use stacked bar plots:\n\n\n\n\n\n\n\n\n\nposition = “fill”: - Standardizes bars to same height - Shows proportions instead of counts - Better for comparing distributions across groups"
  },
  {
    "objectID": "Week2/lecture.html#visualizing-relationships-3",
    "href": "Week2/lecture.html#visualizing-relationships-3",
    "title": "Data Visualization",
    "section": "Visualizing relationships",
    "text": "Visualizing relationships\nThree or more variables\nUse facets to split plots by a categorical variable:\n\n\n\n\n\n\n\n\n\nfacet_wrap(): - Creates separate plots for each category - Maintains same scales across plots - Useful for seeing patterns within groups - Alternative to mapping variables to aesthetics"
  },
  {
    "objectID": "Week2/01-dataviz.html",
    "href": "Week2/01-dataviz.html",
    "title": "BSSC0021",
    "section": "",
    "text": "#| include: false\n#| autorun: true\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(palmerpenguins)\nlibrary(ggthemes)\n\n\n“The simple graph has brought more information to the data analyst’s mind than any other device.” — John Tukey\n\nR has several systems for making graphs, but ggplot2 is one of the most elegant and most versatile.\nggplot2 implements the grammar of graphics, a coherent system for describing and building graphs. With ggplot2, you can do more and faster by learning one system and applying it in many places.\n\n\nThis presentation is based on the Data Visualization chapter of the R for Data Science book by Hadley Wickham and Garrett Grolemund.\n\nIn this tutorial, we will create this plot:\n\n#| echo: false\n#| autorun: true\n#| fig-width: 10\n#| fig-height: 5\nintro_p &lt;- penguins |&gt;\n  drop_na() |&gt;\n  ggplot(mapping = aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(mapping = aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\", formula = y ~ x) +\n  labs(\n    title = \"Body Mass and Flipper Length\",\n    subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n    x = \"Flipper Length (mm)\",\n    y = \"Body Mass (g)\",\n    color = \"Species\",\n    shape = \"Species\"\n  )\n\nintro_p"
  },
  {
    "objectID": "Week2/01-dataviz.html#introduction",
    "href": "Week2/01-dataviz.html#introduction",
    "title": "BSSC0021",
    "section": "",
    "text": "#| include: false\n#| autorun: true\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(palmerpenguins)\nlibrary(ggthemes)\n\n\n“The simple graph has brought more information to the data analyst’s mind than any other device.” — John Tukey\n\nR has several systems for making graphs, but ggplot2 is one of the most elegant and most versatile.\nggplot2 implements the grammar of graphics, a coherent system for describing and building graphs. With ggplot2, you can do more and faster by learning one system and applying it in many places.\n\n\nThis presentation is based on the Data Visualization chapter of the R for Data Science book by Hadley Wickham and Garrett Grolemund.\n\nIn this tutorial, we will create this plot:\n\n#| echo: false\n#| autorun: true\n#| fig-width: 10\n#| fig-height: 5\nintro_p &lt;- penguins |&gt;\n  drop_na() |&gt;\n  ggplot(mapping = aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(mapping = aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\", formula = y ~ x) +\n  labs(\n    title = \"Body Mass and Flipper Length\",\n    subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n    x = \"Flipper Length (mm)\",\n    y = \"Body Mass (g)\",\n    color = \"Species\",\n    shape = \"Species\"\n  )\n\nintro_p"
  },
  {
    "objectID": "Week2/01-dataviz.html#the-grammar-of-graphics",
    "href": "Week2/01-dataviz.html#the-grammar-of-graphics",
    "title": "BSSC0021",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\n\n\nIn 1999, a statistician named Leland Wilkinson published the first edition of what has been the most influential work in data visualization, The Grammar of Graphics.\nThe most complete implementation of the grammar is found in an R package called ggplot2 by Hadley Wickham."
  },
  {
    "objectID": "Week2/01-dataviz.html#the-grammar-of-graphics-1",
    "href": "Week2/01-dataviz.html#the-grammar-of-graphics-1",
    "title": "BSSC0021",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nA plot can be decomposed into three primary elements\n1. the data\n2. the aesthetic mapping of the variables in the data to visual cues\n3. the geometry used to encode the observations on the plot."
  },
  {
    "objectID": "Week2/01-dataviz.html#getting-started",
    "href": "Week2/01-dataviz.html#getting-started",
    "title": "BSSC0021",
    "section": "Getting Started",
    "text": "Getting Started\nThroughout this lecture, we will be writing code together inside this webpage.\n\n#| exercise: getting-started\n#| min-lines: 5\n\n\n\n\n\n\n\n\n\nHints:\n\n\n\n\n\nYou can type code into the cells and run them by clicking the “Run” button.\n\n2 + 3"
  },
  {
    "objectID": "Week2/01-dataviz.html#getting-started-1",
    "href": "Week2/01-dataviz.html#getting-started-1",
    "title": "BSSC0021",
    "section": "Getting Started",
    "text": "Getting Started\nPackages\n\n\nWe begin by loading the tidyverse and ggplot2 packages.\nWe almost always begin our work by loading the tidyverse package. Note that the terms “package” and “library” are used interchangeably but that there is no package() function. To load a package, you need to use library().\n\n\n\n#| label: getting-started\n#| min-lines: 3\n\n# Load the libraries"
  },
  {
    "objectID": "Week2/01-dataviz.html#getting-started-2",
    "href": "Week2/01-dataviz.html#getting-started-2",
    "title": "BSSC0021",
    "section": "Getting Started",
    "text": "Getting Started\nLoading the Data\n\n\nLoad the palmerpenguins package using library().\nThis package contains the penguins dataset, which we will use for this tutorial.\n\n\n#| autorun: false\n#| min-lines: 2\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "Week2/01-dataviz.html#getting-started-3",
    "href": "Week2/01-dataviz.html#getting-started-3",
    "title": "BSSC0021",
    "section": "Getting Started",
    "text": "Getting Started\nGetting help\n\n\nIf you are unsure about how to use a function, you can use the ? operator to get help.\nFor a data package like palmerpenguins, you can use ?penguins to get help on the dataset.\n\n\n?penguins"
  },
  {
    "objectID": "Week2/01-dataviz.html#the-grammar-of-graphics-2",
    "href": "Week2/01-dataviz.html#the-grammar-of-graphics-2",
    "title": "BSSC0021",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nThe Data\n- A variable is a quantity, quality, or property that you can measure.\n- A value is the state of a variable when you measure it. The value of a variable may change from measurement to measurement.\n- An observation is a set of measurements made under similar conditions. An observation will contain several values, each associated with a different variable. We’ll sometimes refer to an observation as a data point.\n- Tabular data is a set of values, each associated with a variable and an observation. Tabular data is tidy if each value is placed in its own “cell”, each variable in its own column, and each observation in its own row."
  },
  {
    "objectID": "Week2/01-dataviz.html#the-grammar-of-graphics-3",
    "href": "Week2/01-dataviz.html#the-grammar-of-graphics-3",
    "title": "BSSC0021",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nThe Data\n\n\n\n\nspecies: a penguin’s species (Adelie, Chinstrap, or Gentoo).\n\nflipper_length_mm: length of a penguin’s flipper, in millimeters.\n\nbody_mass_g: body mass of a penguin, in grams.\n\n\n\n\n\npenguins\n\n\n\n\n\n#| fig-width: 6.5\n#| fig-height: 4.8\n#| warning: false\n#| echo: false\n#| autorun: true\nggplot(\n  data = penguins,\n  mapping = aes(x = bill_length_mm, y = bill_depth_mm, color = species)\n) +\n  geom_point()"
  },
  {
    "objectID": "Week2/01-dataviz.html#formulating-our-research-questions",
    "href": "Week2/01-dataviz.html#formulating-our-research-questions",
    "title": "BSSC0021",
    "section": "Formulating our Research Question(s)",
    "text": "Formulating our Research Question(s)\n\n\nDo penguins with longer flippers weigh more or less than penguins with shorter flippers? You probably already have an answer, but try to make your answer precise.\nWhat does the relationship between flipper length and body mass look like? Is it positive? Negative? Linear? Nonlinear?\nDoes the relationship vary by the species of the penguin? How about by the island where the penguin lives?"
  },
  {
    "objectID": "Week2/01-dataviz.html#building-up-a-plot",
    "href": "Week2/01-dataviz.html#building-up-a-plot",
    "title": "BSSC0021",
    "section": "Building up a plot",
    "text": "Building up a plot\nCreating a ggplot\n\n\nWith ggplot2, you begin a plot with the function ggplot(), defining a plot object that you then add layers to.\nThe first argument of ggplot() is the dataset to use in the graph and so ggplot(data = penguins) creates an empty graph that is primed to display the penguins data, but since we haven’t told it how to visualize it yet, for now it’s empty.\n\n\n#| exercise: empty-plot\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n#| edit: false\nggplot(data = penguins)\n\n\n\n\n\n\n\n\ninput ggplot(data = penguins)"
  },
  {
    "objectID": "Week2/01-dataviz.html#building-up-a-plot-1",
    "href": "Week2/01-dataviz.html#building-up-a-plot-1",
    "title": "BSSC0021",
    "section": "Building up a plot",
    "text": "Building up a plot\nThis is not a very exciting plot, but you can think of it like an empty canvas you’ll paint the remaining layers of your plot onto.\nNext, we need to tell ggplot() how the information from our data will be visually represented. The mapping argument of the ggplot() function defines how variables in your dataset are mapped to visual properties (aesthetics) of your plot.\n. . .\nFor now, we will only map flipper length to the x aesthetic and body mass to the y aesthetic."
  },
  {
    "objectID": "Week2/01-dataviz.html#the-grammar-of-graphics-4",
    "href": "Week2/01-dataviz.html#the-grammar-of-graphics-4",
    "title": "BSSC0021",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nAesthetics"
  },
  {
    "objectID": "Week2/01-dataviz.html#building-up-a-plot-2",
    "href": "Week2/01-dataviz.html#building-up-a-plot-2",
    "title": "BSSC0021",
    "section": "Building up a plot",
    "text": "Building up a plot\nAesthetic mappings\n\n\nThe mapping argument is always defined in the aes() function, and the x and y arguments of aes() specify which variables to map to the x and y axes.\nFor now, we will only map flipper length to the x aesthetic and body mass to the y aesthetic. ggplot2 looks for the mapped variables in the data argument, in this case, penguins.\n\n\n#| exercise: aesthetic-mappings\n#| autorun: false\nggplot(\n  data = penguins,\n  mapping = ______________________\n)\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n#| edit: false\n#| autorun: false\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n)\n\n\n\n\n\n\n\n\nadd mapping = aes(x = flipper_length_mm, y = body_mass_g)\nOur empty canvas now has more structure – it’s clear where flipper lengths will be displayed (on the x-axis) and where body masses will be displayed (on the y-axis). But the penguins themselves are not yet on the plot. This is because we have not yet articulated, in our code, how to represent the observations from our data frame on our plot."
  },
  {
    "objectID": "Week2/01-dataviz.html#building-up-a-plot-3",
    "href": "Week2/01-dataviz.html#building-up-a-plot-3",
    "title": "BSSC0021",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdding layers\nWe need to define a geom: the geometrical object that a plot uses to represent data. These geometric objects are made available in ggplot2 with functions that start with geom_.\nPeople often describe plots by the type of geom that the plot uses:\n\n\nbar charts use bar geoms (geom_bar()),\nline charts use line geoms (geom_line()),\nboxplots use boxplot geoms (geom_boxplot()),\nscatterplots use point geoms (geom_point()), and so on.\n\n\nThe function geom_point() adds a layer of points to your plot, which creates a scatterplot."
  },
  {
    "objectID": "Week2/01-dataviz.html#building-up-a-plot-4",
    "href": "Week2/01-dataviz.html#building-up-a-plot-4",
    "title": "BSSC0021",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdd a scatter point layer to the plot:\n\n#| exercise: geom-point\n#| fig-width: 9\n#| fig-height: 5\n#| fig-alt: |\n#|   A scatterplot of flipper length (x-axis) and body mass (y-axis) for penguins. The plot\n#|   displays a positive, linear, and relative strong relationship between these two variables.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  __________________________\n\n\n\nSolution. \n\n\n\n\n\n\nTip\n\n\n\n\n\n\n#| edit: false\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()\n\n\n\n\n\n\nadd + geom_point()\nNow we have a scatterplot of flipper length (x-axis) and body mass (y-axis) for penguins. The plot displays a positive, linear, and relative strong relationship between these two variables.\nBefore we add more layers to this plot, let’s pause for a moment and review the warning message we got:\nWe’re seeing this message because there are two penguins in our dataset with missing body mass and/or flipper length values and ggplot2 has no way of representing them on the plot without both of these values. Like R, ggplot2 subscribes to the philosophy that missing values should never silently go missing. This type of warning is probably one of the most common types of warnings you will see when working with real data – missing values are a very common issue and you’ll learn more about them later. For the remaining plots we will suppress this warning so it’s not printed alongside every single plot we make."
  },
  {
    "objectID": "Week2/01-dataviz.html#building-up-a-plot-5",
    "href": "Week2/01-dataviz.html#building-up-a-plot-5",
    "title": "BSSC0021",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdding aesthetics\n\n\n\n\nIt’s always a good idea to be skeptical of any apparent relationship between two variables and ask if there may be other variables that explain or change the nature of this apparent relationship.\n\n\nFor example, does the relationship between flipper length and body mass differ by species?\nWhen exploring relationships between variables, it’s important to consider other variables that might affect the relationship. Let’s incorporate species into our plot using color:\n\n\n\n\n#| exercise: add-color\n#| warning: false\n#| fig-alt: |\n#|   A scatterplot of body mass vs. flipper length of penguins, with points\n#|   colored by species.\nggplot(\n  data = penguins,\n  mapping = aes(\n    x = flipper_length_mm,\n    y = body_mass_g,\n    ______________________\n  )\n) +\n  geom_point()\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n#| edit: false\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point()\n\n\n\n\n\n\n\n\nScatterplots are useful for displaying the relationship between two numerical variables, but it’s always a good idea to be skeptical of any apparent relationship between two variables and ask if there may be other variables that explain or change the nature of this apparent relationship.\nFor example, does the relationship between flipper length and body mass differ by species? Let’s incorporate species into our plot and see if this reveals any additional insights into the apparent relationship between these variables. We will do this by representing species with different colored points.\nTo achieve this, will we need to modify the aesthetic or the geom? If you guessed “in the aesthetic mapping, inside of aes()”, you’re already getting the hang of creating data visualizations with ggplot2! And if not, don’t worry.\nAdd color = species to the aesthetic mapping. This tells ggplot2 to color the points by species.\nWhen we map a categorical variable to an aesthetic, ggplot2 automatically: - Assigns a unique value to each level (here, a unique color for each species) - Adds a legend explaining the mapping"
  },
  {
    "objectID": "Week2/01-dataviz.html#building-up-a-plot-6",
    "href": "Week2/01-dataviz.html#building-up-a-plot-6",
    "title": "BSSC0021",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdd a trend line to see the relationship more clearly using geom_smooth()\n\n\n#| exercise: add-lm\n#| warning: false\n#| fig-alt: |\n#|   A scatterplot of body mass vs. flipper length with trend lines by species.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point() +\n  _______________\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nAdd a trendline (geom_smooth(method = \"lm\")) layer to the plot.\n\n#| edit: false\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\nNow let’s add one more layer: a smooth curve displaying the relationship between body mass and flipper length. Before you proceed, refer back to the code above, and think about how we can add this to our existing plot.\nSince this is a new geometric object representing our data, we will add a new geom as a layer on top of our point geom: geom_smooth(). And we will specify that we want to draw the line of best fit based on a linear model with method = \"lm\".\nAdd geom_smooth(method = \"lm\") to the plot.\nThe method = “lm” argument tells geom_smooth() to use a linear model. Notice how the color aesthetic is inherited by both geoms, creating separate trend lines for each species."
  },
  {
    "objectID": "Week2/01-dataviz.html#building-up-a-plot-7",
    "href": "Week2/01-dataviz.html#building-up-a-plot-7",
    "title": "BSSC0021",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdding smooth curves\nIt’s important to recognise how the color aesthetic is inherited by both geoms, creating separate trend lines for each species.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\nWe have successfully added lines, but this plot doesn’t look like our ultimate goal plot, which only has one line for the entire dataset as opposed to separate lines for each of the penguin species.\nWhen aesthetic mappings are defined in ggplot(), at the global level, they’re passed down to each of the subsequent geom layers of the plot. However, each geom function in ggplot2 can also take a mapping argument, which allows for aesthetic mappings at the local level that are added to those inherited from the global level. Since we want points to be colored based on species but don’t want the lines to be separated out for them, we should specify color = species for geom_point() only."
  },
  {
    "objectID": "Week2/01-dataviz.html#building-up-a-plot-8",
    "href": "Week2/01-dataviz.html#building-up-a-plot-8",
    "title": "BSSC0021",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdding smooth curves\nIt’s important to recognise how the color aesthetic is inherited by both geoms, creating separate trend lines for each species.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, \n  color = species)\n) +\n  geom_point( &lt;color = species&gt; ) +\n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "Week2/01-dataviz.html#building-up-a-plot-9",
    "href": "Week2/01-dataviz.html#building-up-a-plot-9",
    "title": "BSSC0021",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdding smooth curves\nIt’s important to recognise how the color aesthetic is inherited by both geoms, creating separate trend lines for each species.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, \n  color = species)\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\", &lt;color = species&gt;)\nPay attention to how the aesthetic mappings propagate through the layers of the plot.\nThis can be useful for creating complex plots with multiple layers, but it can also lead to unexpected results if you’re not careful."
  },
  {
    "objectID": "Week2/01-dataviz.html#building-up-plots",
    "href": "Week2/01-dataviz.html#building-up-plots",
    "title": "BSSC0021",
    "section": "Building up plots",
    "text": "Building up plots\nGlobal vs Local aesthetics\n\n\nIn the previous plot, the color aesthetic was defined in the global mapping. This means that it applies to all geoms in the plot.\nTo get a single trend line while keeping colored points, we move the color aesthetic to geom_point():\n\n\n#| exercise: local-aesthetics\n#| warning: false\n#| fig-alt: |\n#|   A scatterplot with colored points by species and a single trend line.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(___________________________) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n#| edit: false\n#| warning: false\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species)) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nAesthetic mappings can be defined at the global level (in ggplot()) or at the local level (in individual geoms). Local aesthetics override global ones.\nVoila! We have something that looks very much like our ultimate goal, though it’s not yet perfect. We still need to use different shapes for each species of penguins and improve labels.\nIt’s generally not a good idea to represent information using only colors on a plot, as people perceive colors differently due to color blindness or other color vision differences. Therefore, in addition to color, we can also map species to the shape aesthetic."
  },
  {
    "objectID": "Week2/01-dataviz.html#building-up-a-plot-10",
    "href": "Week2/01-dataviz.html#building-up-a-plot-10",
    "title": "BSSC0021",
    "section": "Building up a plot",
    "text": "Building up a plot\nOther aesthetics - shapes\n\n\nIn addition to color, we can also map out variables to other aesthetic elements.\nHere, we map species to the shape aesthetic.\n\n\n\n#| exercise: add-shape\n#| warning: false\n#| fig-width: 10\n#| fig-height: 5\n#| fig-alt: |\n#|   A scatterplot of body mass vs. flipper length of penguins. Overlaid \n#|   on the scatterplot is a single line of best fit displaying the \n#|   relationship between these variables for each species (Adelie, \n#|   Chinstrap, and Gentoo). Different penguin species are plotted in \n#|   different colors and shapes for the points only.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(\n    mapping = aes(color = species, ____________________)\n    ) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n#| edit: false\n#| warning: false\n#| fig-width: 10\n#| fig-height: 5\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nIt’s generally not a good idea to represent information using only colors on a plot, as people perceive colors differently due to color blindness or other color vision differences. Therefore, in addition to color, we can also map species to the shape aesthetic."
  },
  {
    "objectID": "Week2/01-dataviz.html#building-up-a-plot-11",
    "href": "Week2/01-dataviz.html#building-up-a-plot-11",
    "title": "BSSC0021",
    "section": "Building up a plot",
    "text": "Building up a plot\nFinal touches\nThe data portions of our plot are now complete. But data visualization is not just about the data – it’s also about the visual elements that make the plot accessible and informative.\nWe also need the plot itself to communicate:\n\n\nWhat the plot is about (title)\nWhat the axes represent, including units (labels)\nWhat the colors and shapes represent (legends)\nAdditional context such as the source of the data (subtitle or caption)"
  },
  {
    "objectID": "Week2/01-dataviz.html#building-up-a-plot-12",
    "href": "Week2/01-dataviz.html#building-up-a-plot-12",
    "title": "BSSC0021",
    "section": "Building up a plot",
    "text": "Building up a plot\nWe can now add this information to our plot\n\n#| warning: false\n#| exercise: final-plot\n#| fig-width: 12\n#| fig-height: 6\n#| fig-alt: |\n#|   The final version of our plot with proper labels and both color and shape aesthetics.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\") +\n  labs(\n    _____ = ________________, \n    color = \"Species\",\n    shape = \"Species\"\n  ) +\n  scale_color_colorblind()\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n#| edit: false\n#| warning: false\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Body Mass and Flipper Length\",\n    subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n    x = \"Flipper Length (mm)\", \n    y = \"Body Mass (g)\",\n    color = \"Species\", \n    shape = \"Species\"\n  ) +\n  scale_color_colorblind()\n\n\n\n\n\n\nTo add:\ntitle = \"Body Mass and Flipper Length\",\nsubtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\nx = \"Flipper Length (mm)\", \ny = \"Body Mass (g)\",\ncolor = \"Species\", \nshape = \"Species\"\nAnd finally, we can improve the labels of our plot using the labs() function in a new layer. Some of the arguments to labs() might be self explanatory:\n\n\ntitle adds a title and subtitle adds a subtitle to the plot\n\nOther arguments match the aesthetic mappings:\n\n\nx is the x-axis label\n\ny is the y-axis label\n\ncolor and shape define the label for the legend.\n\nIn addition, we can improve the color palette to be colorblind safe with the scale_color_colorblind() function from the ggthemes package."
  },
  {
    "objectID": "Week2/01-dataviz.html#some-notes-on-ggplot-calls",
    "href": "Week2/01-dataviz.html#some-notes-on-ggplot-calls",
    "title": "BSSC0021",
    "section": "Some notes on ggplot() calls",
    "text": "Some notes on ggplot() calls\n\n\nSo far, we’ve written the code in a very explicit way, with each argument named. This is a good practice when you’re learning, but it can be a bit verbose.\n\n\nTypically, the first one or two arguments to a function are so important that you should know them by heart. The first two arguments to ggplot() are data and mapping.\n\nYou’ll often see them left out. This is true for other functions as well.\nWhen leaving the names out, the order of the arguments matters.\n\n\n\n\n\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()\n\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()\n\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()\n\n\n\n\n#| edit: false\n?ggplot"
  },
  {
    "objectID": "Week2/01-dataviz.html#some-notes-on-ggplot-calls-1",
    "href": "Week2/01-dataviz.html#some-notes-on-ggplot-calls-1",
    "title": "BSSC0021",
    "section": "Some notes on ggplot() calls",
    "text": "Some notes on ggplot() calls\nIn the future, you’ll also learn about the pipe, |&gt;, which operates similarly to the + operator in ggplot2.\nIt lets you chain together a series of operations, passing the output of one function to the input of the next.\npenguins |&gt; \n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()"
  },
  {
    "objectID": "Week2/01-dataviz.html#some-notes-on-ggplot-calls-2",
    "href": "Week2/01-dataviz.html#some-notes-on-ggplot-calls-2",
    "title": "BSSC0021",
    "section": "Some notes on ggplot() calls",
    "text": "Some notes on ggplot() calls\nIn the future, you’ll also learn about the pipe, |&gt;, which operates similarly to the + operator in ggplot2.\nIt lets you chain together a series of operations, passing the output of one function to the input of the next.\npenguins |&gt; \n  ggplot(&lt;penguins&gt;, aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()\n\nDon’t worry if you don’t understand this yet. It’s just a sneak peek at what’s to come."
  },
  {
    "objectID": "Week2/01-dataviz.html#summary",
    "href": "Week2/01-dataviz.html#summary",
    "title": "BSSC0021",
    "section": "Summary",
    "text": "Summary\nThe basic idea that underpins ggplot2: a visualization is a mapping from variables in your data to aesthetic properties like position, color, size and shape.\n\nThe grammar of graphics provides a systematic way to build visualizations\nStart with data and aesthetic mappings\nAdd layers with geoms\nUse different geoms for different types of variables\nEnhance plots with labels, colors, and facets\nMake sure your plots are clear and honest\n\n\nKey takeaways: - Build plots layer by layer - Choose appropriate visualizations for your variable types - Consider your audience when making design choices - Use aesthetics and facets to show additional variables - Always aim for clear and accessible visualizations"
  },
  {
    "objectID": "Week2/01-dataviz.html#thats-it",
    "href": "Week2/01-dataviz.html#thats-it",
    "title": "BSSC0021",
    "section": "That’s it!",
    "text": "That’s it!\nWith our remaining time, I’d like you to practice with ggplot2 using the DataAnalytics exercise. You should have already installed DataAnalytics with:\ndevtools::install_github(\"antoinevernet/DataAnalytics\")\nYou can then run the following code to get started:\nlearnr::run_tutorial(\"02-Visualisation\", package = \"DataAnalytics\")"
  },
  {
    "objectID": "Week2/01-dataviz.html#continuous-module-dialogue",
    "href": "Week2/01-dataviz.html#continuous-module-dialogue",
    "title": "BSSC0021",
    "section": "Continuous Module Dialogue",
    "text": "Continuous Module Dialogue\nMenti Survey\n\n\nhttps://www.menti.com/al7gkr8qhntz"
  },
  {
    "objectID": "Week2/01-dataviz.html#visualizing-distributions",
    "href": "Week2/01-dataviz.html#visualizing-distributions",
    "title": "BSSC0021",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nThe following are some additional slides on dealing with other data types and visualizing distributions in ggplot2. Explore them at your own pace."
  },
  {
    "objectID": "Week2/01-dataviz.html#visualizing-distributions-1",
    "href": "Week2/01-dataviz.html#visualizing-distributions-1",
    "title": "BSSC0021",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nCategorical variables\nFor categorical variables like species, we use bar charts:\n\n#| fig-alt: |\n#|   A bar chart showing the frequency of each penguin species.\nggplot(penguins, aes(x = species)) +\n  geom_bar()\n\n\nA bar chart shows: - Categories on one axis - Counts (frequencies) on the other axis - Height of bars represents number of observations in each category"
  },
  {
    "objectID": "Week2/01-dataviz.html#visualizing-distributions-2",
    "href": "Week2/01-dataviz.html#visualizing-distributions-2",
    "title": "BSSC0021",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nImproving categorical plots\nWe can reorder bars by frequency for better visualization:\n\n#| fig-alt: |\n#|   A bar chart with species ordered by frequency.\nggplot(penguins, aes(x = fct_infreq(species))) +\n  geom_bar()\n\n\nfct_infreq() is a function from forcats package that reorders factor levels by their frequencies. This makes the plot easier to read and interpret patterns."
  },
  {
    "objectID": "Week2/01-dataviz.html#visualizing-distributions-3",
    "href": "Week2/01-dataviz.html#visualizing-distributions-3",
    "title": "BSSC0021",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nNumerical variables\nFor numerical variables like body_mass_g, we use histograms:\n\n#| warning: false\n#| fig-alt: |\n#|   A histogram showing the distribution of penguin body mass.\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 200)\n\n\nA histogram: - Divides the x-axis into bins - Height shows number of observations in each bin - binwidth controls the size of the bins - Different binwidths can reveal different patterns"
  },
  {
    "objectID": "Week2/01-dataviz.html#visualizing-distributions-4",
    "href": "Week2/01-dataviz.html#visualizing-distributions-4",
    "title": "BSSC0021",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nExploring binwidth\nThe choice of binwidth affects what patterns we can see:\n\n\n\n\n\n\n#| warning: false\n#| layout-ncol: 2\n#| fig-width: 5\n#| fig-alt: |\n#|   Two histograms with different binwidths showing how binwidth affects visualization.\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 20)\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 2000)\n\n\n\n\n\nToo small binwidth: too many bars, noisy pattern\nToo large binwidth: too few bars, loses detail\nNeed to experiment to find the right balance"
  },
  {
    "objectID": "Week2/01-dataviz.html#visualizing-distributions-5",
    "href": "Week2/01-dataviz.html#visualizing-distributions-5",
    "title": "BSSC0021",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nDensity plots\nAn alternative to histograms is the density plot:\n\n#| fig-alt: |\n#|   A density plot showing the distribution of penguin body mass.\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_density()\n\n\nDensity plots: - Show the shape of the distribution smoothly - Easier to compare multiple groups - Like a smoothed histogram - Think of it as draping a string over a histogram"
  },
  {
    "objectID": "Week2/01-dataviz.html#visualizing-relationships",
    "href": "Week2/01-dataviz.html#visualizing-relationships",
    "title": "BSSC0021",
    "section": "Visualizing relationships",
    "text": "Visualizing relationships\nNumerical and categorical variables\nTo compare a numerical variable across categories, use boxplots:\n\n#| warning: false\n#| fig-alt: |\n#|   Box plots showing body mass distribution by species.\nggplot(penguins, aes(x = species, y = body_mass_g)) +\n  geom_boxplot()\n\n\nA boxplot shows: - Median (middle line) - IQR (box) - Whiskers (extend to most extreme non-outlier points) - Individual points for outliers"
  },
  {
    "objectID": "Week2/01-dataviz.html#visualizing-relationships-1",
    "href": "Week2/01-dataviz.html#visualizing-relationships-1",
    "title": "BSSC0021",
    "section": "Visualizing relationships",
    "text": "Visualizing relationships\nAlternative views\nWe can also use density plots to compare distributions:\n\n#| warning: false\n#| fig-alt: |\n#|   Density plots of body mass by species.\nggplot(penguins, aes(x = body_mass_g, color = species, fill = species)) +\n  geom_density(alpha = 0.5)\n\n\n\ncolor and fill aesthetics distinguish species\nalpha controls transparency\nOverlapping distributions show how groups compare"
  },
  {
    "objectID": "Week2/01-dataviz.html#visualizing-relationships-2",
    "href": "Week2/01-dataviz.html#visualizing-relationships-2",
    "title": "BSSC0021",
    "section": "Visualizing relationships",
    "text": "Visualizing relationships\nTwo categorical variables\nFor two categorical variables, use stacked bar plots:\n\n#| fig-alt: |\n#|   A stacked bar plot showing species distribution across islands.\nggplot(penguins, aes(x = island, fill = species)) +\n  geom_bar(position = \"fill\")\n\n\nposition = “fill”: - Standardizes bars to same height - Shows proportions instead of counts - Better for comparing distributions across groups"
  },
  {
    "objectID": "Week2/01-dataviz.html#visualizing-relationships-3",
    "href": "Week2/01-dataviz.html#visualizing-relationships-3",
    "title": "BSSC0021",
    "section": "Visualizing relationships",
    "text": "Visualizing relationships\nThree or more variables\nUse facets to split plots by a categorical variable:\n\n#| warning: false\n#| fig-width: 10\n#| fig-height: 3\n#| fig-alt: |\n#|   A faceted plot showing the relationship between body mass and flipper length for each island.\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species, shape = species)) +\n  facet_wrap(~island)\n\n\nfacet_wrap(): - Creates separate plots for each category - Maintains same scales across plots - Useful for seeing patterns within groups - Alternative to mapping variables to aesthetics"
  },
  {
    "objectID": "Week2/notes.html",
    "href": "Week2/notes.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Slides\n\n Download PDF Slides \n\n\nor continue on to the next pages for lecture notes.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#this-weeks-lecture",
    "href": "Week2/notes.html#this-weeks-lecture",
    "title": "Data Visualization",
    "section": "",
    "text": "Slides\n\n Download PDF Slides \n\n\nor continue on to the next pages for lecture notes.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#introduction",
    "href": "Week2/notes.html#introduction",
    "title": "Data Visualization",
    "section": "Introduction",
    "text": "Introduction\n\n“The simple graph has brought more information to the data analyst’s mind than any other device.” — John Tukey\n\nR has several systems for making graphs, but ggplot2 is one of the most elegant and most versatile.\nggplot2 implements the grammar of graphics, a coherent system for describing and building graphs. With ggplot2, you can do more and faster by learning one system and applying it in many places.\n\n\nThis presentation is based on the Data Visualization chapter of the R for Data Science book by Hadley Wickham and Garrett Grolemund.\n\nIn this tutorial, we will create this plot:\n\n#| echo: false\n#| autorun: true\n#| fig-width: 10\n#| fig-height: 5\nintro_p &lt;- penguins |&gt;\n  drop_na() |&gt;\n  ggplot(mapping = aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(mapping = aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\", formula = y ~ x) +\n  labs(\n    title = \"Body Mass and Flipper Length\",\n    subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n    x = \"Flipper Length (mm)\",\n    y = \"Body Mass (g)\",\n    color = \"Species\",\n    shape = \"Species\"\n  )\n\nintro_p",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#the-grammar-of-graphics",
    "href": "Week2/notes.html#the-grammar-of-graphics",
    "title": "Data Visualization",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\n\n\nIn 1999, a statistician named Leland Wilkinson published the first edition of what has been the most influential work in data visualization, The Grammar of Graphics.\nThe most complete implementation of the grammar is found in an R package called ggplot2 by Hadley Wickham.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#the-grammar-of-graphics-1",
    "href": "Week2/notes.html#the-grammar-of-graphics-1",
    "title": "Data Visualization",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nA plot can be decomposed into three primary elements\n1. the data\n2. the aesthetic mapping of the variables in the data to visual cues\n3. the geometry used to encode the observations on the plot.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#getting-started",
    "href": "Week2/notes.html#getting-started",
    "title": "Data Visualization",
    "section": "Getting Started",
    "text": "Getting Started\nThroughout this lecture, we will be writing code together inside this webpage.\n\n#| exercise: getting-started\n#| min-lines: 5\n\n\n\n\n\n\n\n\n\nHints:\n\n\n\n\n\nYou can type code into the cells and run them by clicking the “Run” button.\n\n2 + 3",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#getting-started-1",
    "href": "Week2/notes.html#getting-started-1",
    "title": "Data Visualization",
    "section": "Getting Started",
    "text": "Getting Started\nPackages\n\n\nWe begin by loading the tidyverse and ggplot2 packages.\nWe almost always begin our work by loading the tidyverse package. Note that the terms “package” and “library” are used interchangeably but that there is no package() function. To load a package, you need to use library().\n\n\n\n#| label: getting-started\n#| min-lines: 3\n\n# Load the libraries",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#getting-started-2",
    "href": "Week2/notes.html#getting-started-2",
    "title": "Data Visualization",
    "section": "Getting Started",
    "text": "Getting Started\nLoading the Data\n\n\nLoad the palmerpenguins package using library().\nThis package contains the penguins dataset, which we will use for this tutorial.\n\n\n#| autorun: false\n#| min-lines: 2\nlibrary(palmerpenguins)",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#getting-started-3",
    "href": "Week2/notes.html#getting-started-3",
    "title": "Data Visualization",
    "section": "Getting Started",
    "text": "Getting Started\nGetting help\n\n\nIf you are unsure about how to use a function, you can use the ? operator to get help.\nFor a data package like palmerpenguins, you can use ?penguins to get help on the dataset.\n\n\n?penguins",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#the-grammar-of-graphics-2",
    "href": "Week2/notes.html#the-grammar-of-graphics-2",
    "title": "Data Visualization",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nThe Data\n- A variable is a quantity, quality, or property that you can measure.\n- A value is the state of a variable when you measure it. The value of a variable may change from measurement to measurement.\n- An observation is a set of measurements made under similar conditions. An observation will contain several values, each associated with a different variable. We’ll sometimes refer to an observation as a data point.\n- Tabular data is a set of values, each associated with a variable and an observation. Tabular data is tidy if each value is placed in its own “cell”, each variable in its own column, and each observation in its own row.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#the-grammar-of-graphics-3",
    "href": "Week2/notes.html#the-grammar-of-graphics-3",
    "title": "Data Visualization",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nThe Data\n\n\n\n\nspecies: a penguin’s species (Adelie, Chinstrap, or Gentoo).\n\nflipper_length_mm: length of a penguin’s flipper, in millimeters.\n\nbody_mass_g: body mass of a penguin, in grams.\n\n\n\n\n\npenguins\n\n\n\n\n\n#| fig-width: 6.5\n#| fig-height: 4.8\n#| warning: false\n#| echo: false\n#| autorun: true\nggplot(\n  data = penguins,\n  mapping = aes(x = bill_length_mm, y = bill_depth_mm, color = species)\n) +\n  geom_point()",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#formulating-our-research-questions",
    "href": "Week2/notes.html#formulating-our-research-questions",
    "title": "Data Visualization",
    "section": "Formulating our Research Question(s)",
    "text": "Formulating our Research Question(s)\n\n\nDo penguins with longer flippers weigh more or less than penguins with shorter flippers? You probably already have an answer, but try to make your answer precise.\nWhat does the relationship between flipper length and body mass look like? Is it positive? Negative? Linear? Nonlinear?\nDoes the relationship vary by the species of the penguin? How about by the island where the penguin lives?",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#building-up-a-plot",
    "href": "Week2/notes.html#building-up-a-plot",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nCreating a ggplot\n\n\nWith ggplot2, you begin a plot with the function ggplot(), defining a plot object that you then add layers to.\nThe first argument of ggplot() is the dataset to use in the graph and so ggplot(data = penguins) creates an empty graph that is primed to display the penguins data, but since we haven’t told it how to visualize it yet, for now it’s empty.\n\n\n#| exercise: empty-plot\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n#| edit: false\nggplot(data = penguins)\n\n\n\n\n\n\n\n\ninput ggplot(data = penguins)",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#building-up-a-plot-1",
    "href": "Week2/notes.html#building-up-a-plot-1",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nThis is not a very exciting plot, but you can think of it like an empty canvas you’ll paint the remaining layers of your plot onto.\nNext, we need to tell ggplot() how the information from our data will be visually represented. The mapping argument of the ggplot() function defines how variables in your dataset are mapped to visual properties (aesthetics) of your plot.\n. . .\nFor now, we will only map flipper length to the x aesthetic and body mass to the y aesthetic.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#the-grammar-of-graphics-4",
    "href": "Week2/notes.html#the-grammar-of-graphics-4",
    "title": "Data Visualization",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nAesthetics",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#building-up-a-plot-2",
    "href": "Week2/notes.html#building-up-a-plot-2",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nAesthetic mappings\n\n\nThe mapping argument is always defined in the aes() function, and the x and y arguments of aes() specify which variables to map to the x and y axes.\nFor now, we will only map flipper length to the x aesthetic and body mass to the y aesthetic. ggplot2 looks for the mapped variables in the data argument, in this case, penguins.\n\n\n#| exercise: aesthetic-mappings\n#| autorun: false\nggplot(\n  data = penguins,\n  mapping = ______________________\n)\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n#| edit: false\n#| autorun: false\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n)\n\n\n\n\n\n\n\n\nadd mapping = aes(x = flipper_length_mm, y = body_mass_g)\nOur empty canvas now has more structure – it’s clear where flipper lengths will be displayed (on the x-axis) and where body masses will be displayed (on the y-axis). But the penguins themselves are not yet on the plot. This is because we have not yet articulated, in our code, how to represent the observations from our data frame on our plot.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#building-up-a-plot-3",
    "href": "Week2/notes.html#building-up-a-plot-3",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdding layers\nWe need to define a geom: the geometrical object that a plot uses to represent data. These geometric objects are made available in ggplot2 with functions that start with geom_.\nPeople often describe plots by the type of geom that the plot uses:\n\n\nbar charts use bar geoms (geom_bar()),\nline charts use line geoms (geom_line()),\nboxplots use boxplot geoms (geom_boxplot()),\nscatterplots use point geoms (geom_point()), and so on.\n\n\nThe function geom_point() adds a layer of points to your plot, which creates a scatterplot.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#building-up-a-plot-4",
    "href": "Week2/notes.html#building-up-a-plot-4",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdd a scatter point layer to the plot:\n\n#| exercise: geom-point\n#| fig-width: 9\n#| fig-height: 5\n#| fig-alt: |\n#|   A scatterplot of flipper length (x-axis) and body mass (y-axis) for penguins. The plot\n#|   displays a positive, linear, and relative strong relationship between these two variables.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  __________________________\n\n\n\nSolution. \n\n\n\n\n\n\nTip\n\n\n\n\n\n\n#| edit: false\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()\n\n\n\n\n\n\nadd + geom_point()\nNow we have a scatterplot of flipper length (x-axis) and body mass (y-axis) for penguins. The plot displays a positive, linear, and relative strong relationship between these two variables.\nBefore we add more layers to this plot, let’s pause for a moment and review the warning message we got:\nWe’re seeing this message because there are two penguins in our dataset with missing body mass and/or flipper length values and ggplot2 has no way of representing them on the plot without both of these values. Like R, ggplot2 subscribes to the philosophy that missing values should never silently go missing. This type of warning is probably one of the most common types of warnings you will see when working with real data – missing values are a very common issue and you’ll learn more about them later. For the remaining plots we will suppress this warning so it’s not printed alongside every single plot we make.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#building-up-a-plot-5",
    "href": "Week2/notes.html#building-up-a-plot-5",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdding aesthetics\n\n\n\n\nIt’s always a good idea to be skeptical of any apparent relationship between two variables and ask if there may be other variables that explain or change the nature of this apparent relationship.\n\n\nFor example, does the relationship between flipper length and body mass differ by species?\nWhen exploring relationships between variables, it’s important to consider other variables that might affect the relationship. Let’s incorporate species into our plot using color:\n\n\n\n\n#| exercise: add-color\n#| warning: false\n#| fig-alt: |\n#|   A scatterplot of body mass vs. flipper length of penguins, with points\n#|   colored by species.\nggplot(\n  data = penguins,\n  mapping = aes(\n    x = flipper_length_mm,\n    y = body_mass_g,\n    ______________________\n  )\n) +\n  geom_point()\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n#| edit: false\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point()\n\n\n\n\n\n\n\n\nScatterplots are useful for displaying the relationship between two numerical variables, but it’s always a good idea to be skeptical of any apparent relationship between two variables and ask if there may be other variables that explain or change the nature of this apparent relationship.\nFor example, does the relationship between flipper length and body mass differ by species? Let’s incorporate species into our plot and see if this reveals any additional insights into the apparent relationship between these variables. We will do this by representing species with different colored points.\nTo achieve this, will we need to modify the aesthetic or the geom? If you guessed “in the aesthetic mapping, inside of aes()”, you’re already getting the hang of creating data visualizations with ggplot2! And if not, don’t worry.\nAdd color = species to the aesthetic mapping. This tells ggplot2 to color the points by species.\nWhen we map a categorical variable to an aesthetic, ggplot2 automatically: - Assigns a unique value to each level (here, a unique color for each species) - Adds a legend explaining the mapping",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#building-up-a-plot-6",
    "href": "Week2/notes.html#building-up-a-plot-6",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdd a trend line to see the relationship more clearly using geom_smooth()\n\n\n#| exercise: add-lm\n#| warning: false\n#| fig-alt: |\n#|   A scatterplot of body mass vs. flipper length with trend lines by species.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point() +\n  _______________\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nAdd a trendline (geom_smooth(method = \"lm\")) layer to the plot.\n\n#| edit: false\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\nNow let’s add one more layer: a smooth curve displaying the relationship between body mass and flipper length. Before you proceed, refer back to the code above, and think about how we can add this to our existing plot.\nSince this is a new geometric object representing our data, we will add a new geom as a layer on top of our point geom: geom_smooth(). And we will specify that we want to draw the line of best fit based on a linear model with method = \"lm\".\nAdd geom_smooth(method = \"lm\") to the plot.\nThe method = “lm” argument tells geom_smooth() to use a linear model. Notice how the color aesthetic is inherited by both geoms, creating separate trend lines for each species.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#building-up-a-plot-7",
    "href": "Week2/notes.html#building-up-a-plot-7",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdding smooth curves\nIt’s important to recognise how the color aesthetic is inherited by both geoms, creating separate trend lines for each species.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\nWe have successfully added lines, but this plot doesn’t look like our ultimate goal plot, which only has one line for the entire dataset as opposed to separate lines for each of the penguin species.\nWhen aesthetic mappings are defined in ggplot(), at the global level, they’re passed down to each of the subsequent geom layers of the plot. However, each geom function in ggplot2 can also take a mapping argument, which allows for aesthetic mappings at the local level that are added to those inherited from the global level. Since we want points to be colored based on species but don’t want the lines to be separated out for them, we should specify color = species for geom_point() only.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#building-up-a-plot-8",
    "href": "Week2/notes.html#building-up-a-plot-8",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdding smooth curves\nIt’s important to recognise how the color aesthetic is inherited by both geoms, creating separate trend lines for each species.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, \n  color = species)\n) +\n  geom_point( &lt;color = species&gt; ) +\n  geom_smooth(method = \"lm\")",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#building-up-a-plot-9",
    "href": "Week2/notes.html#building-up-a-plot-9",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdding smooth curves\nIt’s important to recognise how the color aesthetic is inherited by both geoms, creating separate trend lines for each species.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, \n  color = species)\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\", &lt;color = species&gt;)\nPay attention to how the aesthetic mappings propagate through the layers of the plot.\nThis can be useful for creating complex plots with multiple layers, but it can also lead to unexpected results if you’re not careful.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#building-up-plots",
    "href": "Week2/notes.html#building-up-plots",
    "title": "Data Visualization",
    "section": "Building up plots",
    "text": "Building up plots\nGlobal vs Local aesthetics\n\n\nIn the previous plot, the color aesthetic was defined in the global mapping. This means that it applies to all geoms in the plot.\nTo get a single trend line while keeping colored points, we move the color aesthetic to geom_point():\n\n\n#| exercise: local-aesthetics\n#| warning: false\n#| fig-alt: |\n#|   A scatterplot with colored points by species and a single trend line.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(___________________________) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n#| edit: false\n#| warning: false\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species)) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nAesthetic mappings can be defined at the global level (in ggplot()) or at the local level (in individual geoms). Local aesthetics override global ones.\nVoila! We have something that looks very much like our ultimate goal, though it’s not yet perfect. We still need to use different shapes for each species of penguins and improve labels.\nIt’s generally not a good idea to represent information using only colors on a plot, as people perceive colors differently due to color blindness or other color vision differences. Therefore, in addition to color, we can also map species to the shape aesthetic.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#building-up-a-plot-10",
    "href": "Week2/notes.html#building-up-a-plot-10",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nOther aesthetics - shapes\n\n\nIn addition to color, we can also map out variables to other aesthetic elements.\nHere, we map species to the shape aesthetic.\n\n\n\n#| exercise: add-shape\n#| warning: false\n#| fig-width: 10\n#| fig-height: 5\n#| fig-alt: |\n#|   A scatterplot of body mass vs. flipper length of penguins. Overlaid \n#|   on the scatterplot is a single line of best fit displaying the \n#|   relationship between these variables for each species (Adelie, \n#|   Chinstrap, and Gentoo). Different penguin species are plotted in \n#|   different colors and shapes for the points only.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(\n    mapping = aes(color = species, ____________________)\n    ) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n#| edit: false\n#| warning: false\n#| fig-width: 10\n#| fig-height: 5\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nIt’s generally not a good idea to represent information using only colors on a plot, as people perceive colors differently due to color blindness or other color vision differences. Therefore, in addition to color, we can also map species to the shape aesthetic.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#building-up-a-plot-11",
    "href": "Week2/notes.html#building-up-a-plot-11",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nFinal touches\nThe data portions of our plot are now complete. But data visualization is not just about the data – it’s also about the visual elements that make the plot accessible and informative.\nWe also need the plot itself to communicate:\n\n\nWhat the plot is about (title)\nWhat the axes represent, including units (labels)\nWhat the colors and shapes represent (legends)\nAdditional context such as the source of the data (subtitle or caption)",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#building-up-a-plot-12",
    "href": "Week2/notes.html#building-up-a-plot-12",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nWe can now add this information to our plot\n\n#| warning: false\n#| exercise: final-plot\n#| fig-width: 12\n#| fig-height: 6\n#| fig-alt: |\n#|   The final version of our plot with proper labels and both color and shape aesthetics.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\") +\n  labs(\n    _____ = ________________, \n    color = \"Species\",\n    shape = \"Species\"\n  ) +\n  scale_color_colorblind()\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n#| edit: false\n#| warning: false\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Body Mass and Flipper Length\",\n    subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n    x = \"Flipper Length (mm)\", \n    y = \"Body Mass (g)\",\n    color = \"Species\", \n    shape = \"Species\"\n  ) +\n  scale_color_colorblind()\n\n\n\n\n\n\nTo add:\ntitle = \"Body Mass and Flipper Length\",\nsubtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\nx = \"Flipper Length (mm)\", \ny = \"Body Mass (g)\",\ncolor = \"Species\", \nshape = \"Species\"\nAnd finally, we can improve the labels of our plot using the labs() function in a new layer. Some of the arguments to labs() might be self explanatory:\n\n\ntitle adds a title and subtitle adds a subtitle to the plot\n\nOther arguments match the aesthetic mappings:\n\n\nx is the x-axis label\n\ny is the y-axis label\n\ncolor and shape define the label for the legend.\n\nIn addition, we can improve the color palette to be colorblind safe with the scale_color_colorblind() function from the ggthemes package.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#some-notes-on-ggplot-calls",
    "href": "Week2/notes.html#some-notes-on-ggplot-calls",
    "title": "Data Visualization",
    "section": "Some notes on ggplot() calls",
    "text": "Some notes on ggplot() calls\n\n\nSo far, we’ve written the code in a very explicit way, with each argument named. This is a good practice when you’re learning, but it can be a bit verbose.\n\n\nTypically, the first one or two arguments to a function are so important that you should know them by heart. The first two arguments to ggplot() are data and mapping.\n\nYou’ll often see them left out. This is true for other functions as well.\nWhen leaving the names out, the order of the arguments matters.\n\n\n\n\n\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()\n\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()\n\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()\n\n\n\n\n#| edit: false\n?ggplot",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#some-notes-on-ggplot-calls-1",
    "href": "Week2/notes.html#some-notes-on-ggplot-calls-1",
    "title": "Data Visualization",
    "section": "Some notes on ggplot() calls",
    "text": "Some notes on ggplot() calls\nIn the future, you’ll also learn about the pipe, |&gt;, which operates similarly to the + operator in ggplot2.\nIt lets you chain together a series of operations, passing the output of one function to the input of the next.\npenguins |&gt; \n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#some-notes-on-ggplot-calls-2",
    "href": "Week2/notes.html#some-notes-on-ggplot-calls-2",
    "title": "Data Visualization",
    "section": "Some notes on ggplot() calls",
    "text": "Some notes on ggplot() calls\nIn the future, you’ll also learn about the pipe, |&gt;, which operates similarly to the + operator in ggplot2.\nIt lets you chain together a series of operations, passing the output of one function to the input of the next.\npenguins |&gt; \n  ggplot(&lt;penguins&gt;, aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()\n\nDon’t worry if you don’t understand this yet. It’s just a sneak peek at what’s to come.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#summary",
    "href": "Week2/notes.html#summary",
    "title": "Data Visualization",
    "section": "Summary",
    "text": "Summary\nThe basic idea that underpins ggplot2: a visualization is a mapping from variables in your data to aesthetic properties like position, color, size and shape.\n\nThe grammar of graphics provides a systematic way to build visualizations\nStart with data and aesthetic mappings\nAdd layers with geoms\nUse different geoms for different types of variables\nEnhance plots with labels, colors, and facets\nMake sure your plots are clear and honest\n\n\nKey takeaways: - Build plots layer by layer - Choose appropriate visualizations for your variable types - Consider your audience when making design choices - Use aesthetics and facets to show additional variables - Always aim for clear and accessible visualizations",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#thats-it",
    "href": "Week2/notes.html#thats-it",
    "title": "Data Visualization",
    "section": "That’s it!",
    "text": "That’s it!\nWith our remaining time, I’d like you to practice with ggplot2 using the DataAnalytics exercise. You should have already installed DataAnalytics with:\ndevtools::install_github(\"antoinevernet/DataAnalytics\")\nYou can then run the following code to get started:\nlearnr::run_tutorial(\"02-Visualisation\", package = \"DataAnalytics\")",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#continuous-module-dialogue",
    "href": "Week2/notes.html#continuous-module-dialogue",
    "title": "Data Visualization",
    "section": "Continuous Module Dialogue",
    "text": "Continuous Module Dialogue\nMenti Survey\n\n\nhttps://www.menti.com/al7gkr8qhntz",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#visualizing-distributions",
    "href": "Week2/notes.html#visualizing-distributions",
    "title": "Data Visualization",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nThe following are some additional slides on dealing with other data types and visualizing distributions in ggplot2. Explore them at your own pace.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#visualizing-distributions-1",
    "href": "Week2/notes.html#visualizing-distributions-1",
    "title": "Data Visualization",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nCategorical variables\nFor categorical variables like species, we use bar charts:\n\n#| fig-alt: |\n#|   A bar chart showing the frequency of each penguin species.\nggplot(penguins, aes(x = species)) +\n  geom_bar()\n\n\nA bar chart shows: - Categories on one axis - Counts (frequencies) on the other axis - Height of bars represents number of observations in each category",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#visualizing-distributions-2",
    "href": "Week2/notes.html#visualizing-distributions-2",
    "title": "Data Visualization",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nImproving categorical plots\nWe can reorder bars by frequency for better visualization:\n\n#| fig-alt: |\n#|   A bar chart with species ordered by frequency.\nggplot(penguins, aes(x = fct_infreq(species))) +\n  geom_bar()\n\n\nfct_infreq() is a function from forcats package that reorders factor levels by their frequencies. This makes the plot easier to read and interpret patterns.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#visualizing-distributions-3",
    "href": "Week2/notes.html#visualizing-distributions-3",
    "title": "Data Visualization",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nNumerical variables\nFor numerical variables like body_mass_g, we use histograms:\n\n#| warning: false\n#| fig-alt: |\n#|   A histogram showing the distribution of penguin body mass.\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 200)\n\n\nA histogram: - Divides the x-axis into bins - Height shows number of observations in each bin - binwidth controls the size of the bins - Different binwidths can reveal different patterns",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#visualizing-distributions-4",
    "href": "Week2/notes.html#visualizing-distributions-4",
    "title": "Data Visualization",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nExploring binwidth\nThe choice of binwidth affects what patterns we can see:\n\n\n\n\n\n\n#| warning: false\n#| layout-ncol: 2\n#| fig-width: 5\n#| fig-alt: |\n#|   Two histograms with different binwidths showing how binwidth affects visualization.\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 20)\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 2000)\n\n\n\n\n\nToo small binwidth: too many bars, noisy pattern\nToo large binwidth: too few bars, loses detail\nNeed to experiment to find the right balance",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#visualizing-distributions-5",
    "href": "Week2/notes.html#visualizing-distributions-5",
    "title": "Data Visualization",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nDensity plots\nAn alternative to histograms is the density plot:\n\n#| fig-alt: |\n#|   A density plot showing the distribution of penguin body mass.\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_density()\n\n\nDensity plots: - Show the shape of the distribution smoothly - Easier to compare multiple groups - Like a smoothed histogram - Think of it as draping a string over a histogram",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#visualizing-relationships",
    "href": "Week2/notes.html#visualizing-relationships",
    "title": "Data Visualization",
    "section": "Visualizing relationships",
    "text": "Visualizing relationships\nNumerical and categorical variables\nTo compare a numerical variable across categories, use boxplots:\n\n#| warning: false\n#| fig-alt: |\n#|   Box plots showing body mass distribution by species.\nggplot(penguins, aes(x = species, y = body_mass_g)) +\n  geom_boxplot()\n\n\nA boxplot shows: - Median (middle line) - IQR (box) - Whiskers (extend to most extreme non-outlier points) - Individual points for outliers",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#visualizing-relationships-1",
    "href": "Week2/notes.html#visualizing-relationships-1",
    "title": "Data Visualization",
    "section": "Visualizing relationships",
    "text": "Visualizing relationships\nAlternative views\nWe can also use density plots to compare distributions:\n\n#| warning: false\n#| fig-alt: |\n#|   Density plots of body mass by species.\nggplot(penguins, aes(x = body_mass_g, color = species, fill = species)) +\n  geom_density(alpha = 0.5)\n\n\n\ncolor and fill aesthetics distinguish species\nalpha controls transparency\nOverlapping distributions show how groups compare",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#visualizing-relationships-2",
    "href": "Week2/notes.html#visualizing-relationships-2",
    "title": "Data Visualization",
    "section": "Visualizing relationships",
    "text": "Visualizing relationships\nTwo categorical variables\nFor two categorical variables, use stacked bar plots:\n\n#| fig-alt: |\n#|   A stacked bar plot showing species distribution across islands.\nggplot(penguins, aes(x = island, fill = species)) +\n  geom_bar(position = \"fill\")\n\n\nposition = “fill”: - Standardizes bars to same height - Shows proportions instead of counts - Better for comparing distributions across groups",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#visualizing-relationships-3",
    "href": "Week2/notes.html#visualizing-relationships-3",
    "title": "Data Visualization",
    "section": "Visualizing relationships",
    "text": "Visualizing relationships\nThree or more variables\nUse facets to split plots by a categorical variable:\n\n#| warning: false\n#| fig-width: 10\n#| fig-height: 3\n#| fig-alt: |\n#|   A faceted plot showing the relationship between body mass and flipper length for each island.\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species, shape = species)) +\n  facet_wrap(~island)\n\n\nfacet_wrap(): - Creates separate plots for each category - Maintains same scales across plots - Useful for seeing patterns within groups - Alternative to mapping variables to aesthetics",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week3/02-rules.html",
    "href": "Week3/02-rules.html",
    "title": "Probability Rules and Classical Probability",
    "section": "",
    "text": "Rule of Subtraction:\nP(\\neg A) = 1 - P(A)\nExample: P(not rolling a 1) = 1 - \\frac{1}{6} = \\frac{5}{6}\n\n\nIntersection Rule (independent events):\nP(A \\cap B) = P(A) * P(B)\nExample: P(six on both rolls) = \\frac{1}{6} * \\frac{1}{6} = \\frac{1}{36}\n\n\nAddition Rule:\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n\n\n\nTo understand de Méré’s error, we need to introduce some of the rules of probability theory:\n\nThe rule of subtraction says that the probability of some event A not happening is one minus the probability of the event happening\nFor independent events, we compute the probability of both occurring by multiplying their individual probabilities\nThe addition rule tells us that to obtain the probability of either of two events occurring, we add together the individual probabilities, but then subtract the likelihood of both occurring together\n\n\n\n\n\nKey Principles:\n\nEqual likelihood assumption\nBased on counting outcomes\nNo experiments needed\nCommon in games of chance\n\nBasic Formula:\nP(outcome_i) = \\frac{1}{\\text{number of possible outcomes}}\n\nExamples:\n\nFair coin: P(heads) = 1/2\nFair die: P(6) = 1/6\nTwo dice: P(double-six) = 1/36\n\n\n\n\nClassical probability arose from the study of games of chance such as dice and cards. In this approach, we compute the probability directly based on our knowledge of the situation.\nWe start with the assumption that all of the elementary events in the sample space are equally likely; that is, when you roll a die, each of the possible outcomes ({1,2,3,4,5,6}) is equally likely to occur.\n\n\n\n\nFrench gambler Chevalier de Méré played two games:\n\nBet on ≥1 six in 4 die rolls\nBet on ≥1 double-six in 24 rolls of two dice\n\nHe thought both had probability \\frac{2}{3} but…\n\nWon money on first bet\nLost money on second bet\n\n\nHis reasoning:\nFor first bet:\n4 * \\frac{1}{6} = \\frac{2}{3}\nFor second bet:\n24 * \\frac{1}{36} = \\frac{2}{3}\n\n\n\nA famous example arose from a problem encountered by a French gambler who went by the name of Chevalier de Méré. de Méré played two different dice games: In the first he bet on the chance of at least one six on four rolls of a six-sided die, while in the second he bet on the chance of at least one double-six on 24 rolls of two dice. He expected to win money on both of these gambles, but he found that while on average he won money on the first gamble, he actually lost money on average when he played the second gamble many times.\n\n\n\n\nMatrix of Outcomes:\n\n\n\n\n\n\n\n\n\nKey Points:\n\nRed cells: six on either throw\nTotal red cells: 11\nExplains \\frac{11}{36} probability\nShows de Méré’s error\n\n\n\n\nThis matrix represents all possible combinations of results across two throws, and highlights the cells that involve a six on either the first or second throw. If you count up the cells in red you will see that there are 11 such cells. This shows why the addition rule gives a different answer from de Méré’s; if we were to simply add together the probabilities for the two throws as he did, then we would count (6,6) towards both, when it should really only be counted once.\n\n\n\n\nFirst bet:\nP(\\text{no sixes}) = \\bigg(\\frac{5}{6}\\bigg)^4=0.482\nP(\\text{≥1 six}) = 1 - 0.482 = 0.517\nSecond bet:\nP(\\text{no double six}) = \\bigg(\\frac{35}{36}\\bigg)^{24}=0.509\nP(\\text{≥1 double six}) = 1 - 0.509 = 0.491\n\nKey Insights:\n\nEasier to compute complement\nFirst bet: P &gt; 0.5\nSecond bet: P &lt; 0.5\nExplains gambling results\n\n\n\n\nBlaise Pascal used the rules of probability to solve de Méré’s problem. First, he realized that computing the probability of at least one event out of a combination was tricky, whereas computing the probability that something does not occur across several events is relatively easy – it’s just the product of the probabilities of the individual events.\nThe first bet has probability &gt; 0.5, explaining why de Méré made money on this bet on average. The second bet has probability &lt; 0.5, explaining why de Méré lost money on average on this bet.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Probability Rules and Classical Probability"
    ]
  },
  {
    "objectID": "Week3/02-rules.html#basic-rules",
    "href": "Week3/02-rules.html#basic-rules",
    "title": "Probability Rules and Classical Probability",
    "section": "",
    "text": "Rule of Subtraction:\nP(\\neg A) = 1 - P(A)\nExample: P(not rolling a 1) = 1 - \\frac{1}{6} = \\frac{5}{6}\n\n\nIntersection Rule (independent events):\nP(A \\cap B) = P(A) * P(B)\nExample: P(six on both rolls) = \\frac{1}{6} * \\frac{1}{6} = \\frac{1}{36}\n\n\nAddition Rule:\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n\n\n\nTo understand de Méré’s error, we need to introduce some of the rules of probability theory:\n\nThe rule of subtraction says that the probability of some event A not happening is one minus the probability of the event happening\nFor independent events, we compute the probability of both occurring by multiplying their individual probabilities\nThe addition rule tells us that to obtain the probability of either of two events occurring, we add together the individual probabilities, but then subtract the likelihood of both occurring together",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Probability Rules and Classical Probability"
    ]
  },
  {
    "objectID": "Week3/02-rules.html#classical-probability",
    "href": "Week3/02-rules.html#classical-probability",
    "title": "Probability Rules and Classical Probability",
    "section": "",
    "text": "Key Principles:\n\nEqual likelihood assumption\nBased on counting outcomes\nNo experiments needed\nCommon in games of chance\n\nBasic Formula:\nP(outcome_i) = \\frac{1}{\\text{number of possible outcomes}}\n\nExamples:\n\nFair coin: P(heads) = 1/2\nFair die: P(6) = 1/6\nTwo dice: P(double-six) = 1/36\n\n\n\n\nClassical probability arose from the study of games of chance such as dice and cards. In this approach, we compute the probability directly based on our knowledge of the situation.\nWe start with the assumption that all of the elementary events in the sample space are equally likely; that is, when you roll a die, each of the possible outcomes ({1,2,3,4,5,6}) is equally likely to occur.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Probability Rules and Classical Probability"
    ]
  },
  {
    "objectID": "Week3/02-rules.html#de-mérés-problem",
    "href": "Week3/02-rules.html#de-mérés-problem",
    "title": "Probability Rules and Classical Probability",
    "section": "",
    "text": "French gambler Chevalier de Méré played two games:\n\nBet on ≥1 six in 4 die rolls\nBet on ≥1 double-six in 24 rolls of two dice\n\nHe thought both had probability \\frac{2}{3} but…\n\nWon money on first bet\nLost money on second bet\n\n\nHis reasoning:\nFor first bet:\n4 * \\frac{1}{6} = \\frac{2}{3}\nFor second bet:\n24 * \\frac{1}{36} = \\frac{2}{3}\n\n\n\nA famous example arose from a problem encountered by a French gambler who went by the name of Chevalier de Méré. de Méré played two different dice games: In the first he bet on the chance of at least one six on four rolls of a six-sided die, while in the second he bet on the chance of at least one double-six on 24 rolls of two dice. He expected to win money on both of these gambles, but he found that while on average he won money on the first gamble, he actually lost money on average when he played the second gamble many times.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Probability Rules and Classical Probability"
    ]
  },
  {
    "objectID": "Week3/02-rules.html#visualizing-multiple-events",
    "href": "Week3/02-rules.html#visualizing-multiple-events",
    "title": "Probability Rules and Classical Probability",
    "section": "",
    "text": "Matrix of Outcomes:\n\n\n\n\n\n\n\n\n\nKey Points:\n\nRed cells: six on either throw\nTotal red cells: 11\nExplains \\frac{11}{36} probability\nShows de Méré’s error\n\n\n\n\nThis matrix represents all possible combinations of results across two throws, and highlights the cells that involve a six on either the first or second throw. If you count up the cells in red you will see that there are 11 such cells. This shows why the addition rule gives a different answer from de Méré’s; if we were to simply add together the probabilities for the two throws as he did, then we would count (6,6) towards both, when it should really only be counted once.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Probability Rules and Classical Probability"
    ]
  },
  {
    "objectID": "Week3/02-rules.html#pascals-solution",
    "href": "Week3/02-rules.html#pascals-solution",
    "title": "Probability Rules and Classical Probability",
    "section": "",
    "text": "First bet:\nP(\\text{no sixes}) = \\bigg(\\frac{5}{6}\\bigg)^4=0.482\nP(\\text{≥1 six}) = 1 - 0.482 = 0.517\nSecond bet:\nP(\\text{no double six}) = \\bigg(\\frac{35}{36}\\bigg)^{24}=0.509\nP(\\text{≥1 double six}) = 1 - 0.509 = 0.491\n\nKey Insights:\n\nEasier to compute complement\nFirst bet: P &gt; 0.5\nSecond bet: P &lt; 0.5\nExplains gambling results\n\n\n\n\nBlaise Pascal used the rules of probability to solve de Méré’s problem. First, he realized that computing the probability of at least one event out of a combination was tricky, whereas computing the probability that something does not occur across several events is relatively easy – it’s just the product of the probabilities of the individual events.\nThe first bet has probability &gt; 0.5, explaining why de Méré made money on this bet on average. The second bet has probability &lt; 0.5, explaining why de Méré lost money on average on this bet.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Probability Rules and Classical Probability"
    ]
  },
  {
    "objectID": "Week3/04-conditional.html",
    "href": "Week3/04-conditional.html",
    "title": "Conditional Probability and Independence",
    "section": "",
    "text": "Definition:\n\nProbability of A given B occurred\nWritten as P(A|B)\n\nUpdates probability based on new information\n\nFormula:\nP(A|B) = \\frac{P(A \\cap B)}{P(B)}\n\n\n\n\n\n\n\n\n\n\n\n\nSo far we have limited ourselves to simple probabilities - that is, the probability of a single event or combination of events. However, we often wish to determine the probability of some event given that some other event has occurred, which are known as conditional probabilities.\n\n\n\n\nQuestion:\nWhat is P(diabetes|inactive)?\n\n\n\n\ntotal\ninactive\ndiabetes\ndiabetes_given_inactive\n\n\n5443\n0.454\n0.101\n0.141\n\n\n\n\n\nJoint Probabilities:\n\n\n\nJoint probabilities\n\nDiabetes\nPhysActive\nn\nprob\n\n\n\nNo\nNo\n2123\n0.3900423\n\n\nNo\nYes\n2770\n0.5089105\n\n\nYes\nNo\n349\n0.0641191\n\n\nYes\nYes\n201\n0.0369282\n\n\n\n\n\n\n\n\nWe can compute conditional probabilities directly from data. Let’s say that we are interested in the following question: What is the probability that someone has diabetes, given that they are not physically active? The NHANES dataset includes two variables that address the two parts of this question: Diabetes and PhysActive.\n\n\n\n\nStatistical Independence:\nP(A|B) = P(A)\nKey Points:\n\nB tells us nothing about A\nDifferent from everyday usage\nMust check with data\n\n\nExample: Jefferson State\n\nP(Jeffersonian) = 0.014\nP(Californian) = 0.986\nNot independent!\nMutually exclusive\n\n\n\n\nThe term “independent” has a very specific meaning in statistics, which is somewhat different from the common usage of the term. Statistical independence between two variables means that knowing the value of one variable doesn’t tell us anything about the value of the other.\nFor example, there is currently a move by a small group of California citizens to declare a new independent state called Jefferson. The new states might be politically independent, but they would not be statistically independent, because if we know that a person is Jeffersonian, then we can be sure they are not Californian!\n\n\n\n\nQuestion: Are physical and mental health independent?\nVariables:\n\nPhysActive: physically active?\nDaysMentHlthBad: bad mental health days\nThreshold: &gt;7 days = bad mental health\n\n\n\n\n\n\nPhysActive\nBad Mental Health\nGood Mental Health\nTotal\n\n\n\nNo\n629\n2510\n3139\n\n\nYes\n471\n3095\n3566\n\n\nTotal\n1100\n5605\n6705\n\n\n\n\n\n\n\n\nLet’s look at another example, using the NHANES data: Are physical health and mental health independent of one another? To determine whether mental health and physical activity are independent, we would compare the simple probability of bad mental health to the conditional probability of bad mental health given that one is physically active.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Conditional Probability and Independence"
    ]
  },
  {
    "objectID": "Week3/04-conditional.html#what-is-conditional-probability",
    "href": "Week3/04-conditional.html#what-is-conditional-probability",
    "title": "Conditional Probability and Independence",
    "section": "",
    "text": "Definition:\n\nProbability of A given B occurred\nWritten as P(A|B)\n\nUpdates probability based on new information\n\nFormula:\nP(A|B) = \\frac{P(A \\cap B)}{P(B)}\n\n\n\n\n\n\n\n\n\n\n\n\nSo far we have limited ourselves to simple probabilities - that is, the probability of a single event or combination of events. However, we often wish to determine the probability of some event given that some other event has occurred, which are known as conditional probabilities.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Conditional Probability and Independence"
    ]
  },
  {
    "objectID": "Week3/04-conditional.html#nhanes-example-physical-activity",
    "href": "Week3/04-conditional.html#nhanes-example-physical-activity",
    "title": "Conditional Probability and Independence",
    "section": "",
    "text": "Question:\nWhat is P(diabetes|inactive)?\n\n\n\n\ntotal\ninactive\ndiabetes\ndiabetes_given_inactive\n\n\n5443\n0.454\n0.101\n0.141\n\n\n\n\n\nJoint Probabilities:\n\n\n\nJoint probabilities\n\nDiabetes\nPhysActive\nn\nprob\n\n\n\nNo\nNo\n2123\n0.3900423\n\n\nNo\nYes\n2770\n0.5089105\n\n\nYes\nNo\n349\n0.0641191\n\n\nYes\nYes\n201\n0.0369282\n\n\n\n\n\n\n\n\nWe can compute conditional probabilities directly from data. Let’s say that we are interested in the following question: What is the probability that someone has diabetes, given that they are not physically active? The NHANES dataset includes two variables that address the two parts of this question: Diabetes and PhysActive.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Conditional Probability and Independence"
    ]
  },
  {
    "objectID": "Week3/04-conditional.html#independence",
    "href": "Week3/04-conditional.html#independence",
    "title": "Conditional Probability and Independence",
    "section": "",
    "text": "Statistical Independence:\nP(A|B) = P(A)\nKey Points:\n\nB tells us nothing about A\nDifferent from everyday usage\nMust check with data\n\n\nExample: Jefferson State\n\nP(Jeffersonian) = 0.014\nP(Californian) = 0.986\nNot independent!\nMutually exclusive\n\n\n\n\nThe term “independent” has a very specific meaning in statistics, which is somewhat different from the common usage of the term. Statistical independence between two variables means that knowing the value of one variable doesn’t tell us anything about the value of the other.\nFor example, there is currently a move by a small group of California citizens to declare a new independent state called Jefferson. The new states might be politically independent, but they would not be statistically independent, because if we know that a person is Jeffersonian, then we can be sure they are not Californian!",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Conditional Probability and Independence"
    ]
  },
  {
    "objectID": "Week3/04-conditional.html#mental-health-and-physical-activity",
    "href": "Week3/04-conditional.html#mental-health-and-physical-activity",
    "title": "Conditional Probability and Independence",
    "section": "",
    "text": "Question: Are physical and mental health independent?\nVariables:\n\nPhysActive: physically active?\nDaysMentHlthBad: bad mental health days\nThreshold: &gt;7 days = bad mental health\n\n\n\n\n\n\nPhysActive\nBad Mental Health\nGood Mental Health\nTotal\n\n\n\nNo\n629\n2510\n3139\n\n\nYes\n471\n3095\n3566\n\n\nTotal\n1100\n5605\n6705\n\n\n\n\n\n\n\n\nLet’s look at another example, using the NHANES data: Are physical health and mental health independent of one another? To determine whether mental health and physical activity are independent, we would compare the simple probability of bad mental health to the conditional probability of bad mental health given that one is physically active.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Conditional Probability and Independence"
    ]
  },
  {
    "objectID": "Week3/06-distributions.html",
    "href": "Week3/06-distributions.html",
    "title": "Probability Distributions",
    "section": "",
    "text": "Definition:\n\nDescribes all possible outcomes\nAssigns probability to each\nDifferent types for different data\nMathematical formulation\n\nExamples:\n\nBinomial (success/failure)\nNormal (continuous)\nPoisson (counts)\n\n\n\nCode# Create example distributions\nx &lt;- seq(-4, 4, length.out = 100)\nnormal_df &lt;- data.frame(\n  x = x,\n  y = dnorm(x),\n  type = \"Normal\"\n)\n\nx &lt;- 0:10\npoisson_df &lt;- data.frame(\n  x = x,\n  y = dpois(x, lambda = 3),\n  type = \"Poisson\"\n)\n\ncolors &lt;- c(\n  \"Normal\" = \"blue\",\n  \"Poisson\" = \"red\"\n)\n\n# Plot distributions\nggplot() +\n  geom_line(data = normal_df, aes(x = x, y = y, color = \"Normal\"), size = 1) +\n  geom_point(\n    data = poisson_df,\n    aes(x = x, y = y, color = \"Poisson\"),\n    size = 1.5\n  ) +\n  labs(\n    title = \"Example Distributions\",\n    x = \"Value\",\n    y = \"Probability\",\n    color = \"Legend\"\n  ) +\n  theme(legend.position = \"top\") +\n  scale_color_manual(values = colors) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nA probability distribution describes the probability of all of the possible outcomes in an experiment. Throughout this section we will encounter a number of these probability distributions, each of which is appropriate to describe different types of data.\n\n\n\n\n\nProperties:\n\nIndependent trials\nTwo outcomes\nFixed probability\nOrder doesn’t matter\n\nFormula:\nP(k; n,p) = \\binom{n}{k} p^k(1-p)^{n-k}\nWhere:\n\nk = successes\nn = trials\np = probability per trial\n\nBinomial Coefficient:\n\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\n\n\nCode# Create binomial distribution plot\nx &lt;- 0:10\nn &lt;- 10\np &lt;- 0.5\nbinom_df &lt;- data.frame(\n  x = x,\n  y = dbinom(x, size = n, prob = p)\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe binomial distribution provides a way to compute the probability of some number of successes out of a number of trials on which there is either success or failure and nothing in between (known as “Bernoulli trials”), given some known probability of success on each trial.\n\n\n\n\nScenario:\n\nSteph Curry hits 91% of his free throws\nIn a game in Jan, 2018, he hit only 2 out of 4 free throws\nIt seems pretty unlikely that he would hit only 50% of his free throws in a game, but exactly how unlikely is it?\n\n\nCalculation:\nP(2;4,0.91) = \\binom{4}{2} 0.91^2(1-0.91)^{2}\n= 6 * 0.8281 * 0.0081\n= 0.040\n\n\n\nInterpretation:\n\nVery unlikely (4%)\nYet it happened\nRare events do occur\nDon’t overinterpret\n\n\n\n\n\nOn Jan 20 2018, the basketball player Steph Curry hit only 2 out of 4 free throws in a game against the Houston Rockets. We know that Curry’s overall probability of hitting free throws across the entire season was 0.91, so it seems pretty unlikely that he would hit only 50% of his free throws in a game, but exactly how unlikely is it?\n\n\nOften we want to know not just how likely a specific value is, but how likely it is to find a value that is as extreme or more than a particular value?\nDefinition:\n\nProbability of value ≤ x\nAccumulates probabilities\nOften more useful\nImportant for testing\n\nExample:\nP(k\\le2)= P(k=2) + P(k=1) + P(k=0)\n\nOften we want to know not just how likely a specific value is, but how likely it is to find a value that is as extreme or more than a particular value?\n\n\n\n\n\nCode# curry_df &lt;- tibble(\n#   numSuccesses = seq(0, 4)\n# ) %&gt;%\n#   mutate(\n#     Probability = dbinom(numSuccesses, size = 4, prob = 0.91),\n#     CumulativeProbability = pbinom(numSuccesses, size = 4, prob = 0.91)\n#   )\n# Create data for Curry's free throw distributions\nn_throws &lt;- 4\ncurry_prob &lt;- 0.91\nx &lt;- 0:n_throws\n\ncurry_dist_df &lt;- data.frame(\n  x = x,\n  Simple = dbinom(x, size = n_throws, prob = curry_prob),\n  Cumulative = pbinom(x, size = n_throws, prob = curry_prob)\n)\n\nkable(\n  curry_dist_df,\n  caption = \"Simple and cumulative probability distributions\",\n  digits = 3\n)\n\n\nSimple and cumulative probability distributions\n\nx\nSimple\nCumulative\n\n\n\n0\n0.000\n0.000\n\n\n1\n0.003\n0.003\n\n\n2\n0.040\n0.043\n\n\n3\n0.271\n0.314\n\n\n4\n0.686\n1.000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe binomial distribution is a discrete probability distribution that describes the number of successes in a sequence of independent experiments, each of which has a constant probability of success. In this example, we are looking at the probability of Steph Curry making a certain number of free throws out of 4 attempts, given that his overall success rate is 91%.\nThis visualization shows both the probability of making exactly k free throws (blue bars) and the probability of making k or fewer free throws (red line) for Curry’s specific scenario of 4 attempts with a 91% success rate.\n\n\n\n\nCore Concepts:\n\nProbability measures uncertainty\nThree approaches:\n\nPersonal belief\nEmpirical frequency\nClassical probability\n\n\nFundamental rules:\n\nAddition\nMultiplication\nSubtraction\n\n\n\n\nAdvanced Topics:\n\nConditional probability\nIndependence\nBayes’ rule\nProbability distributions\n\nApplications:\n\nMedical screening\nData analysis\nDecision making\nStatistical inference\n\n\n\n\nThese concepts form the foundation for statistical inference, which we will explore in later chapters. Having read this chapter, you should be able to:\n\nDescribe the sample space for a selected random experiment\nCompute relative frequency and empirical probability\nCompute probabilities of single events, complementary events, and unions/intersections\nDescribe the law of large numbers\nUnderstand conditional probability and independence\nUse Bayes’ theorem",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Probability Distributions"
    ]
  },
  {
    "objectID": "Week3/06-distributions.html#what-is-a-probability-distribution",
    "href": "Week3/06-distributions.html#what-is-a-probability-distribution",
    "title": "Probability Distributions",
    "section": "",
    "text": "Definition:\n\nDescribes all possible outcomes\nAssigns probability to each\nDifferent types for different data\nMathematical formulation\n\nExamples:\n\nBinomial (success/failure)\nNormal (continuous)\nPoisson (counts)\n\n\n\nCode# Create example distributions\nx &lt;- seq(-4, 4, length.out = 100)\nnormal_df &lt;- data.frame(\n  x = x,\n  y = dnorm(x),\n  type = \"Normal\"\n)\n\nx &lt;- 0:10\npoisson_df &lt;- data.frame(\n  x = x,\n  y = dpois(x, lambda = 3),\n  type = \"Poisson\"\n)\n\ncolors &lt;- c(\n  \"Normal\" = \"blue\",\n  \"Poisson\" = \"red\"\n)\n\n# Plot distributions\nggplot() +\n  geom_line(data = normal_df, aes(x = x, y = y, color = \"Normal\"), size = 1) +\n  geom_point(\n    data = poisson_df,\n    aes(x = x, y = y, color = \"Poisson\"),\n    size = 1.5\n  ) +\n  labs(\n    title = \"Example Distributions\",\n    x = \"Value\",\n    y = \"Probability\",\n    color = \"Legend\"\n  ) +\n  theme(legend.position = \"top\") +\n  scale_color_manual(values = colors) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nA probability distribution describes the probability of all of the possible outcomes in an experiment. Throughout this section we will encounter a number of these probability distributions, each of which is appropriate to describe different types of data.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Probability Distributions"
    ]
  },
  {
    "objectID": "Week3/06-distributions.html#the-binomial-distribution",
    "href": "Week3/06-distributions.html#the-binomial-distribution",
    "title": "Probability Distributions",
    "section": "",
    "text": "Properties:\n\nIndependent trials\nTwo outcomes\nFixed probability\nOrder doesn’t matter\n\nFormula:\nP(k; n,p) = \\binom{n}{k} p^k(1-p)^{n-k}\nWhere:\n\nk = successes\nn = trials\np = probability per trial\n\nBinomial Coefficient:\n\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\n\n\nCode# Create binomial distribution plot\nx &lt;- 0:10\nn &lt;- 10\np &lt;- 0.5\nbinom_df &lt;- data.frame(\n  x = x,\n  y = dbinom(x, size = n, prob = p)\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe binomial distribution provides a way to compute the probability of some number of successes out of a number of trials on which there is either success or failure and nothing in between (known as “Bernoulli trials”), given some known probability of success on each trial.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Probability Distributions"
    ]
  },
  {
    "objectID": "Week3/06-distributions.html#example-steph-currys-free-throws",
    "href": "Week3/06-distributions.html#example-steph-currys-free-throws",
    "title": "Probability Distributions",
    "section": "",
    "text": "Scenario:\n\nSteph Curry hits 91% of his free throws\nIn a game in Jan, 2018, he hit only 2 out of 4 free throws\nIt seems pretty unlikely that he would hit only 50% of his free throws in a game, but exactly how unlikely is it?\n\n\nCalculation:\nP(2;4,0.91) = \\binom{4}{2} 0.91^2(1-0.91)^{2}\n= 6 * 0.8281 * 0.0081\n= 0.040\n\n\n\nInterpretation:\n\nVery unlikely (4%)\nYet it happened\nRare events do occur\nDon’t overinterpret\n\n\n\n\n\nOn Jan 20 2018, the basketball player Steph Curry hit only 2 out of 4 free throws in a game against the Houston Rockets. We know that Curry’s overall probability of hitting free throws across the entire season was 0.91, so it seems pretty unlikely that he would hit only 50% of his free throws in a game, but exactly how unlikely is it?",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Probability Distributions"
    ]
  },
  {
    "objectID": "Week3/06-distributions.html#cumulative-distributions",
    "href": "Week3/06-distributions.html#cumulative-distributions",
    "title": "Probability Distributions",
    "section": "",
    "text": "Often we want to know not just how likely a specific value is, but how likely it is to find a value that is as extreme or more than a particular value?\nDefinition:\n\nProbability of value ≤ x\nAccumulates probabilities\nOften more useful\nImportant for testing\n\nExample:\nP(k\\le2)= P(k=2) + P(k=1) + P(k=0)\n\nOften we want to know not just how likely a specific value is, but how likely it is to find a value that is as extreme or more than a particular value?",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Probability Distributions"
    ]
  },
  {
    "objectID": "Week3/06-distributions.html#cumulative-distributions-1",
    "href": "Week3/06-distributions.html#cumulative-distributions-1",
    "title": "Probability Distributions",
    "section": "",
    "text": "Code# curry_df &lt;- tibble(\n#   numSuccesses = seq(0, 4)\n# ) %&gt;%\n#   mutate(\n#     Probability = dbinom(numSuccesses, size = 4, prob = 0.91),\n#     CumulativeProbability = pbinom(numSuccesses, size = 4, prob = 0.91)\n#   )\n# Create data for Curry's free throw distributions\nn_throws &lt;- 4\ncurry_prob &lt;- 0.91\nx &lt;- 0:n_throws\n\ncurry_dist_df &lt;- data.frame(\n  x = x,\n  Simple = dbinom(x, size = n_throws, prob = curry_prob),\n  Cumulative = pbinom(x, size = n_throws, prob = curry_prob)\n)\n\nkable(\n  curry_dist_df,\n  caption = \"Simple and cumulative probability distributions\",\n  digits = 3\n)\n\n\nSimple and cumulative probability distributions\n\nx\nSimple\nCumulative\n\n\n\n0\n0.000\n0.000\n\n\n1\n0.003\n0.003\n\n\n2\n0.040\n0.043\n\n\n3\n0.271\n0.314\n\n\n4\n0.686\n1.000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe binomial distribution is a discrete probability distribution that describes the number of successes in a sequence of independent experiments, each of which has a constant probability of success. In this example, we are looking at the probability of Steph Curry making a certain number of free throws out of 4 attempts, given that his overall success rate is 91%.\nThis visualization shows both the probability of making exactly k free throws (blue bars) and the probability of making k or fewer free throws (red line) for Curry’s specific scenario of 4 attempts with a 91% success rate.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Probability Distributions"
    ]
  },
  {
    "objectID": "Week3/06-distributions.html#summary",
    "href": "Week3/06-distributions.html#summary",
    "title": "Probability Distributions",
    "section": "",
    "text": "Core Concepts:\n\nProbability measures uncertainty\nThree approaches:\n\nPersonal belief\nEmpirical frequency\nClassical probability\n\n\nFundamental rules:\n\nAddition\nMultiplication\nSubtraction\n\n\n\n\nAdvanced Topics:\n\nConditional probability\nIndependence\nBayes’ rule\nProbability distributions\n\nApplications:\n\nMedical screening\nData analysis\nDecision making\nStatistical inference\n\n\n\n\nThese concepts form the foundation for statistical inference, which we will explore in later chapters. Having read this chapter, you should be able to:\n\nDescribe the sample space for a selected random experiment\nCompute relative frequency and empirical probability\nCompute probabilities of single events, complementary events, and unions/intersections\nDescribe the law of large numbers\nUnderstand conditional probability and independence\nUse Bayes’ theorem",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Probability Distributions"
    ]
  },
  {
    "objectID": "Week3/lecture.html#what-is-probability-theory",
    "href": "Week3/lecture.html#what-is-probability-theory",
    "title": "Probability, Sampling, and Experiments",
    "section": "What is Probability Theory?",
    "text": "What is Probability Theory?\n\nBranch of mathematics dealing with chance and uncertainty\nFoundation for statistics\nProvides tools to describe uncertain events\nHistorical origins in games of chance\nDeep questions about meaning and interpretation\n\n\nProbability theory is the branch of mathematics that deals with chance and uncertainty. It forms an important part of the foundation for statistics, because it provides us with the mathematical tools to describe uncertain events.\nThe study of probability arose in part due to interest in understanding games of chance, like cards or dice. These games provide useful examples of many statistical concepts, because when we repeat these games the likelihood of different outcomes remains (mostly) the same. However, there are deep questions about the meaning of probability that we will not address here."
  },
  {
    "objectID": "Week3/lecture.html#experiment-sample-space-events",
    "href": "Week3/lecture.html#experiment-sample-space-events",
    "title": "Probability, Sampling, and Experiments",
    "section": "Experiment, Sample Space, Events",
    "text": "Experiment, Sample Space, Events\n\n\nAn experiment is any activity that produces or observes an outcome. Examples are flipping a coin, rolling a 6-sided die, or trying a new route to work to see if it’s faster than the old route.\n\nCoin flip: {heads, tails}\nDie roll: {1,2,3,4,5,6}\nTravel time: (0,∞)\n\nThe sample space is the set of possible outcomes for an experiment. We represent these by listing them within a set of squiggly brackets.\nAn event is a subset of the sample space. In principle it could be one or more of possible outcomes in the sample space, but here we will focus primarily on elementary events which consist of exactly one possible outcome.\n\nSubset of sample space\nCan be elementary or compound\nExample: rolling a 4\n\n\n\n\nTo formalize probability theory, we first need to define a few terms:\n\nAn experiment is any activity that produces or observes an outcome. Examples are flipping a coin, rolling a 6-sided die, or trying a new route to work to see if it’s faster than the old route.\nThe sample space is the set of possible outcomes for an experiment. We represent these by listing them within a set of squiggly brackets.\nAn event is a subset of the sample space. In principle it could be one or more of possible outcomes in the sample space, but here we will focus primarily on elementary events which consist of exactly one possible outcome."
  },
  {
    "objectID": "Week3/lecture.html#kolmogorovs-axioms",
    "href": "Week3/lecture.html#kolmogorovs-axioms",
    "title": "Probability, Sampling, and Experiments",
    "section": "Kolmogorov’s Axioms",
    "text": "Kolmogorov’s Axioms\nFor events \\({E_1, E_2, ... , E_N}\\) and random variable \\(X\\):\n\n\n\nNon-negativity:\n\\(P(X=E_i) \\ge 0\\)\nNormalization:\n\\(\\sum_{i=1}^N{P(X=E_i)} = 1\\)\nBoundedness:\n\\(P(X=E_i)\\le 1\\)\n\n\n\nImplications:\n\nAll probabilities are between 0 and 1\nTotal probability must sum to 1\nIndividual probabilities ≤ 1\n\n\n\n\nThese are the features that a value has to have if it is going to be a probability, which were first defined by the Russian mathematician Andrei Kolmogorov.\nThe summation is interpreted as saying “Take all of the N elementary events, which we have labeled from 1 to N, and add up their probabilities. These must sum to one.”\nThe third point is implied by the previous points; since they must sum to one, and they can’t be negative, then any particular probability cannot exceed one."
  },
  {
    "objectID": "Week3/lecture.html#basic-rules",
    "href": "Week3/lecture.html#basic-rules",
    "title": "Probability, Sampling, and Experiments",
    "section": "Basic Rules",
    "text": "Basic Rules\n\nRule of Subtraction:\n\\(P(\\neg A) = 1 - P(A)\\)\nExample: P(not rolling a 1) = \\(1 - \\frac{1}{6} = \\frac{5}{6}\\)\nIntersection Rule (independent events):\n\\(P(A \\cap B) = P(A) * P(B)\\)\nExample: P(six on both rolls) = \\(\\frac{1}{6} * \\frac{1}{6} = \\frac{1}{36}\\)\nAddition Rule:\n\\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)\n\n\nTo understand de Méré’s error, we need to introduce some of the rules of probability theory:\n\nThe rule of subtraction says that the probability of some event A not happening is one minus the probability of the event happening\nFor independent events, we compute the probability of both occurring by multiplying their individual probabilities\nThe addition rule tells us that to obtain the probability of either of two events occurring, we add together the individual probabilities, but then subtract the likelihood of both occurring together"
  },
  {
    "objectID": "Week3/lecture.html#classical-probability",
    "href": "Week3/lecture.html#classical-probability",
    "title": "Probability, Sampling, and Experiments",
    "section": "Classical Probability",
    "text": "Classical Probability\n\n\nKey Principles:\n\nEqual likelihood assumption\nBased on counting outcomes\nNo experiments needed\nCommon in games of chance\n\nBasic Formula:\n\\(P(outcome_i) = \\frac{1}{\\text{number of possible outcomes}}\\)\n\nExamples:\n\nFair coin: P(heads) = 1/2\nFair die: P(6) = 1/6\nTwo dice: P(double-six) = 1/36\n\n\n\nClassical probability arose from the study of games of chance such as dice and cards. In this approach, we compute the probability directly based on our knowledge of the situation.\nWe start with the assumption that all of the elementary events in the sample space are equally likely; that is, when you roll a die, each of the possible outcomes ({1,2,3,4,5,6}) is equally likely to occur."
  },
  {
    "objectID": "Week3/lecture.html#de-mérés-problem",
    "href": "Week3/lecture.html#de-mérés-problem",
    "title": "Probability, Sampling, and Experiments",
    "section": "de Méré’s Problem",
    "text": "de Méré’s Problem\n\n\nFrench gambler Chevalier de Méré played two games:\n\nBet on ≥1 six in 4 die rolls\nBet on ≥1 double-six in 24 rolls of two dice\n\nHe thought both had probability \\(\\frac{2}{3}\\) but…\n\nWon money on first bet\nLost money on second bet\n\n\nHis reasoning:\nFor first bet:\n\\(4 * \\frac{1}{6} = \\frac{2}{3}\\)\nFor second bet:\n\\(24 * \\frac{1}{36} = \\frac{2}{3}\\)\n\n\nA famous example arose from a problem encountered by a French gambler who went by the name of Chevalier de Méré. de Méré played two different dice games: In the first he bet on the chance of at least one six on four rolls of a six-sided die, while in the second he bet on the chance of at least one double-six on 24 rolls of two dice. He expected to win money on both of these gambles, but he found that while on average he won money on the first gamble, he actually lost money on average when he played the second gamble many times."
  },
  {
    "objectID": "Week3/lecture.html#visualizing-multiple-events",
    "href": "Week3/lecture.html#visualizing-multiple-events",
    "title": "Probability, Sampling, and Experiments",
    "section": "Visualizing Multiple Events",
    "text": "Visualizing Multiple Events\n\n\nMatrix of Outcomes:\n\n\n\n\n\n\n\n\n\n\nKey Points:\n\nRed cells: six on either throw\nTotal red cells: 11\nExplains \\(\\frac{11}{36}\\) probability\nShows de Méré’s error\n\n\n\nThis matrix represents all possible combinations of results across two throws, and highlights the cells that involve a six on either the first or second throw. If you count up the cells in red you will see that there are 11 such cells. This shows why the addition rule gives a different answer from de Méré’s; if we were to simply add together the probabilities for the two throws as he did, then we would count (6,6) towards both, when it should really only be counted once."
  },
  {
    "objectID": "Week3/lecture.html#pascals-solution",
    "href": "Week3/lecture.html#pascals-solution",
    "title": "Probability, Sampling, and Experiments",
    "section": "Pascal’s Solution",
    "text": "Pascal’s Solution\n\n\nFirst bet:\n\\(P(\\text{no sixes}) = \\bigg(\\frac{5}{6}\\bigg)^4=0.482\\)\n\\(P(\\text{≥1 six}) = 1 - 0.482 = 0.517\\)\n\nSecond bet:\n\\(P(\\text{no double six}) = \\bigg(\\frac{35}{36}\\bigg)^{24}=0.509\\)\n\\(P(\\text{≥1 double six}) = 1 - 0.509 = 0.491\\)\n\nKey Insights:\n\nEasier to compute complement\nFirst bet: P &gt; 0.5\nSecond bet: P &lt; 0.5\nExplains gambling results\n\n\n\nBlaise Pascal used the rules of probability to solve de Méré’s problem. First, he realized that computing the probability of at least one event out of a combination was tricky, whereas computing the probability that something does not occur across several events is relatively easy – it’s just the product of the probabilities of the individual events.\nThe first bet has probability &gt; 0.5, explaining why de Méré made money on this bet on average. The second bet has probability &lt; 0.5, explaining why de Méré lost money on average on this bet."
  },
  {
    "objectID": "Week3/lecture.html#three-approaches",
    "href": "Week3/lecture.html#three-approaches",
    "title": "Probability, Sampling, and Experiments",
    "section": "Three Approaches",
    "text": "Three Approaches\n\n\n\nPersonal Belief\n\nSubjective assessment\nBased on knowledge/experience\nLimited scientific validity\nOften only available approach\n\nEmpirical Frequency\n\nBased on repeated experiments\nLaw of large numbers\nReal-world data collection\n\n\n\n\nClassical Probability\n\nBased on equally likely outcomes\nMathematical approach\nCommon in games of chance\nNo experiments needed\n\n\n\n\nNow that we know what a probability is, how do we actually figure out what the probability is for any particular event? There are three main approaches, each with their own strengths and limitations."
  },
  {
    "objectID": "Week3/lecture.html#personal-belief",
    "href": "Week3/lecture.html#personal-belief",
    "title": "Probability, Sampling, and Experiments",
    "section": "Personal Belief",
    "text": "Personal Belief\n\n\nExample Question:\nWhat was the probability that Bernie Sanders would have won the 2016 presidential election if he had been the democratic nominee?\nKey Points:\n\nCan’t run this experiment\nPeople can still estimate based on knowledge\nNot scientifically satisfying\nOften the only available approach\n\n\nOther Examples:\n\nWeather forecasts\nSports predictions\nEconomic forecasts\nPersonal decisions\n\n\n\nLet’s say that I asked you what the probability was that Bernie Sanders would have won the 2016 presidential election if he had been the democratic nominee instead of Hilary Clinton? We can’t actually do the experiment to find the outcome. However, most people with knowledge of American politics would be willing to at least offer a guess at the probability of this event. In many cases personal knowledge and/or opinion is the only guide we have determining the probability of an event, but this is not very scientifically satisfying."
  },
  {
    "objectID": "Week3/lecture.html#empirical-frequency",
    "href": "Week3/lecture.html#empirical-frequency",
    "title": "Probability, Sampling, and Experiments",
    "section": "Empirical Frequency",
    "text": "Empirical Frequency\n\n\nSan Francisco Rain Example:\n\nTotal days in 2017: 365\nRainy days: 73\nP(rain in SF) = 73/365 = 0.2\n\nKey Steps:\n\nDefine experiment clearly\nCount occurrences\nDivide by total trials\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnother way to determine the probability of an event is to do the experiment many times and count how often each event happens. From the relative frequency of the different outcomes, we can compute the probability of each outcome. For example, let’s say that we are interested in knowing the probability of rain in San Francisco. We first have to define the experiment — let’s say that we will look at the National Weather Service data for each day in 2017 and determine whether there was any rain at the downtown San Francisco weather station. According to these data, in 2017 there were 73 rainy days. To compute the probability of rain in San Francisco, we simply divide the number of rainy days by the number of days counted (365), giving P(rain in SF in 2017) = 0.2.\nThe graph shows how the empirical probability of rain converges to 0.2 as we accumulate more days of data throughout the year."
  },
  {
    "objectID": "Week3/lecture.html#law-of-large-numbers",
    "href": "Week3/lecture.html#law-of-large-numbers",
    "title": "Probability, Sampling, and Experiments",
    "section": "Law of Large Numbers",
    "text": "Law of Large Numbers\n\n\nCoin Flip Example:\n\nTrue probability of heads = 0.5\nSmall samples vary widely\nMore flips = better estimate\nConverges to true probability\n“Law of small numbers” fallacy\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe graph shows how early results from coin flips can be highly variable and unrepresentative of the true value. Even though we know a fair coin has a probability of 0.5 for heads, small samples can give very different results. This demonstrates how small samples can give misleading results.\nThis was referred to as the “law of small numbers” by psychologists Danny Kahneman and Amos Tversky, who showed that people (even trained researchers) often behave as if the law of large numbers applies even to small samples, leading them to draw strong conclusions from insufficient data."
  },
  {
    "objectID": "Week3/lecture.html#real-world-example-alabama-election",
    "href": "Week3/lecture.html#real-world-example-alabama-election",
    "title": "Probability, Sampling, and Experiments",
    "section": "Real-World Example: Alabama Election",
    "text": "Real-World Example: Alabama Election\n\n\n2017 Senate Race:\n\nRoy Moore vs Doug Jones\nEarly results volatile\nFinal outcome different\nSmall sample warning\n\n\n\n\n\n\n\n\n\n\n\n\n\nA real-world example of this was seen in the 2017 special election for the US Senate in Alabama. Early in the evening the vote counts were especially volatile, swinging from a large initial lead for Jones to a long period where Moore had the lead, until finally Jones took the lead to win the race.\nThis demonstrates how small samples can give misleading results. Unfortunately, many people forget this and overinterpret results from small samples."
  },
  {
    "objectID": "Week3/lecture.html#what-is-conditional-probability",
    "href": "Week3/lecture.html#what-is-conditional-probability",
    "title": "Probability, Sampling, and Experiments",
    "section": "What is Conditional Probability?",
    "text": "What is Conditional Probability?\n\n\nDefinition:\n\nProbability of A given B occurred\nWritten as \\(P(A|B)\\)\nUpdates probability based on new information\n\nFormula:\n\\(P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\)\n\n\n\n\n\n\n\n\n\n\n\n\nSo far we have limited ourselves to simple probabilities - that is, the probability of a single event or combination of events. However, we often wish to determine the probability of some event given that some other event has occurred, which are known as conditional probabilities."
  },
  {
    "objectID": "Week3/lecture.html#nhanes-example-physical-activity",
    "href": "Week3/lecture.html#nhanes-example-physical-activity",
    "title": "Probability, Sampling, and Experiments",
    "section": "NHANES Example: Physical Activity",
    "text": "NHANES Example: Physical Activity\n\n\nQuestion:\nWhat is P(diabetes|inactive)?\n\n\n\n\n\ntotal\ninactive\ndiabetes\ndiabetes_given_inactive\n\n\n\n\n5443\n0.454\n0.101\n0.141\n\n\n\n\n\n\nJoint Probabilities:\n\n\n\nJoint probabilities\n\n\nDiabetes\nPhysActive\nn\nprob\n\n\n\n\nNo\nNo\n2123\n0.3900423\n\n\nNo\nYes\n2770\n0.5089105\n\n\nYes\nNo\n349\n0.0641191\n\n\nYes\nYes\n201\n0.0369282\n\n\n\n\n\n\n\nWe can compute conditional probabilities directly from data. Let’s say that we are interested in the following question: What is the probability that someone has diabetes, given that they are not physically active? The NHANES dataset includes two variables that address the two parts of this question: Diabetes and PhysActive."
  },
  {
    "objectID": "Week3/lecture.html#independence",
    "href": "Week3/lecture.html#independence",
    "title": "Probability, Sampling, and Experiments",
    "section": "Independence",
    "text": "Independence\n\n\nStatistical Independence:\n\\(P(A|B) = P(A)\\)\nKey Points:\n\nB tells us nothing about A\nDifferent from everyday usage\nMust check with data\n\n\nExample: Jefferson State\n\nP(Jeffersonian) = 0.014\nP(Californian) = 0.986\nNot independent!\nMutually exclusive\n\n\n\nThe term “independent” has a very specific meaning in statistics, which is somewhat different from the common usage of the term. Statistical independence between two variables means that knowing the value of one variable doesn’t tell us anything about the value of the other.\nFor example, there is currently a move by a small group of California citizens to declare a new independent state called Jefferson. The new states might be politically independent, but they would not be statistically independent, because if we know that a person is Jeffersonian, then we can be sure they are not Californian!"
  },
  {
    "objectID": "Week3/lecture.html#mental-health-and-physical-activity",
    "href": "Week3/lecture.html#mental-health-and-physical-activity",
    "title": "Probability, Sampling, and Experiments",
    "section": "Mental Health and Physical Activity",
    "text": "Mental Health and Physical Activity\n\n\nQuestion: Are physical and mental health independent?\nVariables:\n\nPhysActive: physically active?\nDaysMentHlthBad: bad mental health days\nThreshold: &gt;7 days = bad mental health\n\n\n\n\n\n\n\nPhysActive\nBad Mental Health\nGood Mental Health\nTotal\n\n\n\n\nNo\n629\n2510\n3139\n\n\nYes\n471\n3095\n3566\n\n\nTotal\n1100\n5605\n6705\n\n\n\n\n\n\n\nLet’s look at another example, using the NHANES data: Are physical health and mental health independent of one another? To determine whether mental health and physical activity are independent, we would compare the simple probability of bad mental health to the conditional probability of bad mental health given that one is physically active."
  },
  {
    "objectID": "Week3/lecture.html#the-basic-formula",
    "href": "Week3/lecture.html#the-basic-formula",
    "title": "Probability, Sampling, and Experiments",
    "section": "The Basic Formula",
    "text": "The Basic Formula\n\n\nWhen we know \\(P(A|B)\\) but want \\(P(B|A)\\):\n\\(P(B|A) = \\frac{P(A|B)*P(B)}{P(A)}\\)\nAlternative Form:\n\\(P(B|A) = \\frac{P(A|B)*P(B)}{P(A|B)*P(B) + P(A|\\neg B)*P(\\neg B)}\\)\n\n\nComponents:\n\nPrior: \\(P(B)\\)\nLikelihood: \\(P(A|B)\\)\nMarginal likelihood: \\(P(A)\\)\nPosterior: \\(P(B|A)\\)\n\n\n\n\nIn many cases, we know P(A|B) but we really want to know P(B|A). This commonly occurs in medical screening, where we know P(positive test result| disease) but what we want to know is P(disease|positive test result).\nIf we have only two outcomes, we can express Bayes’ rule in a somewhat clearer way, using the sum rule to redefine P(A)."
  },
  {
    "objectID": "Week3/lecture.html#putting-bayes-into-practice",
    "href": "Week3/lecture.html#putting-bayes-into-practice",
    "title": "Probability, Sampling, and Experiments",
    "section": "Putting Bayes into Practice",
    "text": "Putting Bayes into Practice\nConstruction company drug testing\nA major construction company conducts mandatory random drug and alcohol screening using rapid saliva tests. Consider the following scenario:\n\n\nIn the UK construction industry during 2023, the prevalence of substance use affecting workplace safety was estimated at 2.5% of the workforce\nThe rapid saliva test used has a sensitivity (true positive rate) of 85% when conducted according to protocol\nThe specificity (true negative rate) of these tests is 99.2%\n\n\n\nLet’s consider a specific example. Suppose that a worker is selected for a random drug screening. The test result is positive. What is the probability that this worker is actually positive for substances?"
  },
  {
    "objectID": "Week3/lecture.html#putting-bayes-into-practice-1",
    "href": "Week3/lecture.html#putting-bayes-into-practice-1",
    "title": "Probability, Sampling, and Experiments",
    "section": "Putting Bayes into Practice",
    "text": "Putting Bayes into Practice\nConstruction company drug testing\nLet’s consider a specific example. Suppose that a worker is selected for a random drug screening. The test result is positive. What is the probability that this worker is actually positive for substances?\n\nContext: The company’s current policy is immediate suspension without pay following a positive test result, pending a more accurate laboratory confirmation test that takes 48 hours.\n\nMandatory screening\nRapid saliva test\nSafety-critical roles\nImmediate consequences"
  },
  {
    "objectID": "Week3/lecture.html#putting-bayes-into-practice-2",
    "href": "Week3/lecture.html#putting-bayes-into-practice-2",
    "title": "Probability, Sampling, and Experiments",
    "section": "Putting Bayes into Practice",
    "text": "Putting Bayes into Practice\nConstruction company drug testing\nConstruction Site Testing:\n\nSensitivity: P(positive|substance) = 0.85\nSpecificity: P(negative|no substance) = 0.992\nBase rate: P(substance) = 0.025\n\n\nKey Values:\n\nP(S) = 0.025 (prevalence)\nP(P|S) = 0.85 (sensitivity)\nP(P|not S) = 0.008 (1 - specificity)\n\n\n\nA major construction company conducts mandatory random drug and alcohol screening using rapid saliva tests. In the UK construction industry during 2023, the prevalence of substance use affecting workplace safety was estimated at 2.5% of the workforce. The rapid saliva test used has a sensitivity of 85% when conducted according to protocol, and a specificity of 99.2%."
  },
  {
    "objectID": "Week3/lecture.html#lets-work-through-it",
    "href": "Week3/lecture.html#lets-work-through-it",
    "title": "Probability, Sampling, and Experiments",
    "section": "Let’s Work Through It",
    "text": "Let’s Work Through It\nUsing Bayes’ Theorem, calculate the probability that this worker is actually positive for substances given their positive test result.\n\nP(S) = 0.025 (prevalence)\nP(P|S) = 0.85 (sensitivity)\nP(P|not S) = 0.008 (1 - specificity)\n\n\nA construction worker is randomly selected for testing at the start of their shift. Their saliva test comes back positive. Using Bayes’ Theorem, calculate the probability that this worker is actually positive for substances given their positive test result."
  },
  {
    "objectID": "Week3/lecture.html#solution",
    "href": "Week3/lecture.html#solution",
    "title": "Probability, Sampling, and Experiments",
    "section": "Solution",
    "text": "Solution\n\n\nCalculate P(substance|positive):\n\n\\[\\begin{align*}\nP(P) &= P(P|S) \\times P(S) + P(P|not S) \\times P(not S) \\\\\n&= (0.85 \\times 0.025) + (0.008 \\times 0.975) \\\\\n&= 0.02125 + 0.0078 \\\\\n&= 0.02905\n\\end{align*}\\]\n\n\n\\[\\begin{align*}\nP(S|P) &= \\frac{P(P|S) \\times P(S)}{P(P)} \\\\\n&= \\frac{0.85 \\times 0.025}{0.02905} \\\\\n&= 0.7314974 \\text{ or } 73.1\\%\n\\end{align*}\\]\n\n\n\nInterpretation:\n\n~73.1% chance true positive\n~26.9% chance false positive\nMuch higher than 2.5% base rate\nStill significant uncertainty\n\n\n\n\nUsing Bayes’ Theorem, we find that given a positive test result, there is a 73.1% probability that the worker actually has substances present. This is much higher than the base rate of 2.5%, but still leaves significant uncertainty with a 26.9% false positive rate."
  },
  {
    "objectID": "Week3/lecture.html#discussion-the-real-world-implications",
    "href": "Week3/lecture.html#discussion-the-real-world-implications",
    "title": "Probability, Sampling, and Experiments",
    "section": "Discussion: The Real-world Implications",
    "text": "Discussion: The Real-world Implications\nThe company’s current policy is immediate suspension without pay following a positive test result.\nWhat do these results mean for this business policy? Is it fair to immediately suspend workers without pay for a positive test result?\n\nThe company’s current policy is immediate suspension without pay following a positive test result, pending a more accurate laboratory confirmation test that takes 48 hours.\nGiven that approximately 26.9% of positive test results may be false positives, an immediate suspension without pay could unfairly penalize innocent workers; however, the high stakes of construction safety and the 73.1% probability of a true positive suggest that temporary removal from safety-critical roles is prudent while awaiting confirmation."
  },
  {
    "objectID": "Week3/lecture.html#learning-from-data",
    "href": "Week3/lecture.html#learning-from-data",
    "title": "Probability, Sampling, and Experiments",
    "section": "Learning from Data",
    "text": "Learning from Data\n\n\nBayes’ Rule as Learning:\n\\(P(B|A) = \\frac{P(A|B)}{P(A)}*P(B)\\)\nComponents:\n\nPrior belief: \\(P(B)\\)\nEvidence strength: \\(\\frac{P(A|B)}{P(A)}\\)\nUpdated belief: \\(P(B|A)\\)\n\n\nKey Insights:\n\nUpdates prior knowledge\nEvidence can strengthen/weaken\nSystematic way to learn\nCombines knowledge & data\n\n\n\nAnother way to think of Bayes’ rule is as a way to update our beliefs on the basis of data. The different parts of Bayes’ rule have specific names, that relate to their role in using Bayes’ rule to update our beliefs.\nThe part on the left tells us how much more or less likely the data A are given B, relative to the overall likelihood of the data, while the part on the right side tells us how likely we thought B was before we knew anything about the data."
  },
  {
    "objectID": "Week3/lecture.html#odds-and-odds-ratios",
    "href": "Week3/lecture.html#odds-and-odds-ratios",
    "title": "Probability, Sampling, and Experiments",
    "section": "Odds and Odds Ratios",
    "text": "Odds and Odds Ratios\n\n\nConverting to Odds:\n\\(\\text{odds of A} = \\frac{P(A)}{P(\\neg A)}\\)\nExample:\nDrug test odds:\n\nPrior: \\(\\frac{0.025}{0.975} = 0.026\\)\nPosterior: \\(\\frac{0.7314974}{0.2685026} = 2.724\\)\n\n\nOdds Ratio:\n\\(\\frac{\\text{posterior odds}}{\\text{prior odds}} = \\frac{2.724}{0.026} = 106.25\\)\nInterpretation:\n\nOdds increased 105×\nMuch stronger evidence\nShows test’s power\nDespite false positives\n\n\n\nWe can convert probabilities into odds which express the relative likelihood of something happening or not. An odds ratio is an example of what we will later call an effect size, which is a way of quantifying how relatively large any particular statistical effect is.\nFirst, remember the rule for computing a conditional probability. We can rearrange this to get the formula to compute the joint probability using the conditional. Using this we can compute the inverse probability."
  },
  {
    "objectID": "Week3/lecture.html#what-is-a-probability-distribution",
    "href": "Week3/lecture.html#what-is-a-probability-distribution",
    "title": "Probability, Sampling, and Experiments",
    "section": "What is a Probability Distribution?",
    "text": "What is a Probability Distribution?\n\n\nDefinition:\n\nDescribes all possible outcomes\nAssigns probability to each\nDifferent types for different data\nMathematical formulation\n\nExamples:\n\nBinomial (success/failure)\nNormal (continuous)\nPoisson (counts)\n\n\n\n\nCode\n# Create example distributions\nx &lt;- seq(-4, 4, length.out = 100)\nnormal_df &lt;- data.frame(\n  x = x,\n  y = dnorm(x),\n  type = \"Normal\"\n)\n\nx &lt;- 0:10\npoisson_df &lt;- data.frame(\n  x = x,\n  y = dpois(x, lambda = 3),\n  type = \"Poisson\"\n)\n\ncolors &lt;- c(\n  \"Normal\" = \"blue\",\n  \"Poisson\" = \"red\"\n)\n\n# Plot distributions\nggplot() +\n  geom_line(data = normal_df, aes(x = x, y = y, color = \"Normal\"), size = 1) +\n  geom_point(\n    data = poisson_df,\n    aes(x = x, y = y, color = \"Poisson\"),\n    size = 1.5\n  ) +\n  labs(\n    title = \"Example Distributions\",\n    x = \"Value\",\n    y = \"Probability\",\n    color = \"Legend\"\n  ) +\n  theme(legend.position = \"top\") +\n  scale_color_manual(values = colors) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nA probability distribution describes the probability of all of the possible outcomes in an experiment. Throughout this section we will encounter a number of these probability distributions, each of which is appropriate to describe different types of data."
  },
  {
    "objectID": "Week3/lecture.html#the-binomial-distribution",
    "href": "Week3/lecture.html#the-binomial-distribution",
    "title": "Probability, Sampling, and Experiments",
    "section": "The Binomial Distribution",
    "text": "The Binomial Distribution\n\n\n\nProperties:\n\nIndependent trials\nTwo outcomes\nFixed probability\nOrder doesn’t matter\n\nFormula:\n\\(P(k; n,p) = \\binom{n}{k} p^k(1-p)^{n-k}\\)\nWhere:\n\nk = successes\nn = trials\np = probability per trial\n\nBinomial Coefficient:\n\\(\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\\)\n\n\n\nCode\n# Create binomial distribution plot\nx &lt;- 0:10\nn &lt;- 10\np &lt;- 0.5\nbinom_df &lt;- data.frame(\n  x = x,\n  y = dbinom(x, size = n, prob = p)\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe binomial distribution provides a way to compute the probability of some number of successes out of a number of trials on which there is either success or failure and nothing in between (known as “Bernoulli trials”), given some known probability of success on each trial."
  },
  {
    "objectID": "Week3/lecture.html#example-steph-currys-free-throws",
    "href": "Week3/lecture.html#example-steph-currys-free-throws",
    "title": "Probability, Sampling, and Experiments",
    "section": "Example: Steph Curry’s Free Throws",
    "text": "Example: Steph Curry’s Free Throws\n\n\nScenario:\n\nSteph Curry hits 91% of his free throws\nIn a game in Jan, 2018, he hit only 2 out of 4 free throws\nIt seems pretty unlikely that he would hit only 50% of his free throws in a game, but exactly how unlikely is it?\n\n\nCalculation:\n\\(P(2;4,0.91) = \\binom{4}{2} 0.91^2(1-0.91)^{2}\\)\n\\(= 6 * 0.8281 * 0.0081\\)\n\\(= 0.040\\)\n\n\n\nInterpretation:\n\nVery unlikely (4%)\nYet it happened\nRare events do occur\nDon’t overinterpret\n\n\n\n\nOn Jan 20 2018, the basketball player Steph Curry hit only 2 out of 4 free throws in a game against the Houston Rockets. We know that Curry’s overall probability of hitting free throws across the entire season was 0.91, so it seems pretty unlikely that he would hit only 50% of his free throws in a game, but exactly how unlikely is it?"
  },
  {
    "objectID": "Week3/lecture.html#cumulative-distributions",
    "href": "Week3/lecture.html#cumulative-distributions",
    "title": "Probability, Sampling, and Experiments",
    "section": "Cumulative Distributions",
    "text": "Cumulative Distributions\nOften we want to know not just how likely a specific value is, but how likely it is to find a value that is as extreme or more than a particular value?\nDefinition:\n\nProbability of value ≤ x\nAccumulates probabilities\nOften more useful\nImportant for testing\n\nExample:\n\\(P(k\\le2)= P(k=2) + P(k=1) + P(k=0)\\)\n\nOften we want to know not just how likely a specific value is, but how likely it is to find a value that is as extreme or more than a particular value?"
  },
  {
    "objectID": "Week3/lecture.html#cumulative-distributions-1",
    "href": "Week3/lecture.html#cumulative-distributions-1",
    "title": "Probability, Sampling, and Experiments",
    "section": "Cumulative Distributions",
    "text": "Cumulative Distributions\n\n\n\n\nCode\n# curry_df &lt;- tibble(\n#   numSuccesses = seq(0, 4)\n# ) %&gt;%\n#   mutate(\n#     Probability = dbinom(numSuccesses, size = 4, prob = 0.91),\n#     CumulativeProbability = pbinom(numSuccesses, size = 4, prob = 0.91)\n#   )\n# Create data for Curry's free throw distributions\nn_throws &lt;- 4\ncurry_prob &lt;- 0.91\nx &lt;- 0:n_throws\n\ncurry_dist_df &lt;- data.frame(\n  x = x,\n  Simple = dbinom(x, size = n_throws, prob = curry_prob),\n  Cumulative = pbinom(x, size = n_throws, prob = curry_prob)\n)\n\nkable(\n  curry_dist_df,\n  caption = \"Simple and cumulative probability distributions\",\n  digits = 3\n)\n\n\n\nSimple and cumulative probability distributions\n\n\nx\nSimple\nCumulative\n\n\n\n\n0\n0.000\n0.000\n\n\n1\n0.003\n0.003\n\n\n2\n0.040\n0.043\n\n\n3\n0.271\n0.314\n\n\n4\n0.686\n1.000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe binomial distribution is a discrete probability distribution that describes the number of successes in a sequence of independent experiments, each of which has a constant probability of success. In this example, we are looking at the probability of Steph Curry making a certain number of free throws out of 4 attempts, given that his overall success rate is 91%.\nThis visualization shows both the probability of making exactly k free throws (blue bars) and the probability of making k or fewer free throws (red line) for Curry’s specific scenario of 4 attempts with a 91% success rate."
  },
  {
    "objectID": "Week3/lecture.html#summary",
    "href": "Week3/lecture.html#summary",
    "title": "Probability, Sampling, and Experiments",
    "section": "Summary",
    "text": "Summary\n\n\nCore Concepts:\n\nProbability measures uncertainty\nThree approaches:\n\nPersonal belief\nEmpirical frequency\nClassical probability\n\nFundamental rules:\n\nAddition\nMultiplication\nSubtraction\n\n\n\nAdvanced Topics:\n\nConditional probability\nIndependence\nBayes’ rule\nProbability distributions\n\nApplications:\n\nMedical screening\nData analysis\nDecision making\nStatistical inference\n\n\n\nThese concepts form the foundation for statistical inference, which we will explore in later chapters. Having read this chapter, you should be able to:\n\nDescribe the sample space for a selected random experiment\nCompute relative frequency and empirical probability\nCompute probabilities of single events, complementary events, and unions/intersections\nDescribe the law of large numbers\nUnderstand conditional probability and independence\nUse Bayes’ theorem"
  },
  {
    "objectID": "Week3/lecture.html#why-study-sampling",
    "href": "Week3/lecture.html#why-study-sampling",
    "title": "Probability, Sampling, and Experiments",
    "section": "Why Study Sampling?",
    "text": "Why Study Sampling?\n\n\nThe Power of Sampling:\nNate Silver’s 2012 Election Prediction:\n\nCorrectly predicted all 50 states\nUsed only 21,000 people\nTo predict 125 million votes\nCombined data from 21 polls\n\n\nKey Insights:\n\nSmall samples can be powerful\nProper methodology is crucial\nCombining data improves accuracy\nStatistical rigor matters\n\n\n\nOne of the foundational ideas in statistics is that we can make inferences about an entire population based on a relatively small sample of individuals from that population.\nAnyone living in the United States will be familiar with the concept of sampling from the political polls that have become a central part of our electoral process. In some cases, these polls can be incredibly accurate at predicting the outcomes of elections. The best known example comes from the 2008 and 2012 US Presidential elections, when the pollster Nate Silver correctly predicted electoral outcomes for 49/50 states in 2008 and for all 50 states in 2012.\nSilver did this by combining data from 21 different polls, which vary in the degree to which they tend to lean towards either the Republican or Democratic side. Each of these polls included data from about 1000 likely voters – meaning that Silver was able to almost perfectly predict the pattern of votes of more than 125 million voters using data from only about 21,000 people, along with other knowledge."
  },
  {
    "objectID": "Week3/lecture.html#sampling-fundamentals",
    "href": "Week3/lecture.html#sampling-fundamentals",
    "title": "Probability, Sampling, and Experiments",
    "section": "Sampling Fundamentals",
    "text": "Sampling Fundamentals\n\n\n\n\nPopulation vs Sample:\n\nPopulation: Entire group of interest\nSample: Subset used for measurement\nGoal: Infer population parameters from sample statistics\n\nRepresentative Sampling:\n\nEqual chance of selection\nAvoid systematic bias\nRandom selection crucial\n\n\n\n\n\n\nTypes of Sampling:\n\nWith replacement: Items can be selected multiple times\nWithout replacement: Items selected only once\nChoice affects probability calculations\n\nKey Terms:\n\nParameter: Population value (usually unknown)\nStatistic: Sample value (our estimate)\nSampling Error: Difference between statistic and parameter\n\n\n\n\n\nOur goal in sampling is to determine the value of a statistic for an entire population of interest, using just a small subset of the population. We do this primarily to save time and effort – why go to the trouble of measuring every individual in the population when just a small sample is sufficient to accurately estimate the statistic of interest?\nIn the election example, the population is all registered voters in the region being polled, and the sample is the set of 1000 individuals selected by the polling organization. The way in which we select the sample is critical to ensuring that the sample is representative of the entire population, which is a main goal of statistical sampling.\nIt’s important to also distinguish between two different ways of sampling: with replacement versus without replacement. In sampling with replacement, after a member of the population has been sampled, they are put back into the pool so that they can potentially be sampled again. In sampling without replacement, once a member has been sampled they are not eligible to be sampled again."
  },
  {
    "objectID": "Week3/lecture.html#sampling-error-distribution",
    "href": "Week3/lecture.html#sampling-error-distribution",
    "title": "Probability, Sampling, and Experiments",
    "section": "Sampling Error & Distribution",
    "text": "Sampling Error & Distribution\nConcept\nWhat is Sampling Error?\n\nDifference between sample and population\nVaries across samples\nAffects measurement quality\nCan be quantified"
  },
  {
    "objectID": "Week3/lecture.html#sampling-error-distribution-1",
    "href": "Week3/lecture.html#sampling-error-distribution-1",
    "title": "Probability, Sampling, and Experiments",
    "section": "Sampling Error & Distribution",
    "text": "Sampling Error & Distribution\nConcept\n\n\n\n# Take 5 samples of 50 adults each\nset.seed(123)\nsamples &lt;- map_df(\n  1:5,\n  ~{\n    NHANES_adult |&gt;\n      sample_n(50) |&gt;\n      summarise(\n        mean_height = mean(Height),\n        sd_height = sd(Height),\n      )\n  }\n)\nsamples\n\n# A tibble: 5 × 2\n  mean_height sd_height\n        &lt;dbl&gt;     &lt;dbl&gt;\n1        169.     11.6 \n2        167.      9.13\n3        169.     11.2 \n4        166.      9.62\n5        169.     11.0 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegardless of how representative our sample is, it’s likely that the statistic that we compute from the sample is going to differ at least slightly from the population parameter. We refer to this as sampling error. If we take multiple samples, the value of our statistical estimate will also vary from sample to sample; we refer to this distribution of our statistic across samples as the sampling distribution.\nSampling error is directly related to the quality of our measurement of the population. Clearly we want the estimates obtained from our sample to be as close as possible to the true value of the population parameter. However, even if our statistic is unbiased (that is, we expect it to have the same value as the population parameter), the value for any particular estimate will differ from the population value, and those differences will be greater when the sampling error is greater.\nThe visualization shows how sample means distribute around the true population mean (red line) when we take many samples."
  },
  {
    "objectID": "Week3/lecture.html#standard-error-of-the-mean",
    "href": "Week3/lecture.html#standard-error-of-the-mean",
    "title": "Probability, Sampling, and Experiments",
    "section": "Standard Error of the Mean",
    "text": "Standard Error of the Mean\n\n\nDefinition:\n\\(SEM = \\frac{\\hat{\\sigma}}{\\sqrt{n}}\\)\nWhere:\n\n\\(\\hat{\\sigma}\\) is estimated standard deviation\n\\(n\\) is sample size\n\nKey Properties:\n\nMeasures sampling distribution variability\nDecreases with larger samples\nIncreases with population variability\n\n\nExample with NHANES:\n\n# Population SEM\npop_sd &lt;- sd(NHANES_adult$Height)\nn &lt;- 50\nsem_theoretical &lt;- pop_sd / sqrt(n)\n\n# Observed SEM from samples\nsem_observed &lt;- sd(samples_large$mean_height)\n\ncat(\"Theoretical SEM:\", round(sem_theoretical, 2), \"\\n\")\n\nTheoretical SEM: 1.44 \n\ncat(\"Observed SEM:\", round(sem_observed, 2))\n\nObserved SEM: 1.42\n\n\n\n\nLater in the course it will become essential to be able to characterize how variable our samples are, in order to make inferences about the sample statistics. For the mean, we do this using a quantity called the standard error of the mean (SEM), which one can think of as the standard deviation of the sampling distribution of the mean.\nThe formula for the standard error of the mean implies that the quality of our measurement involves two quantities: the population variability, and the size of our sample. Because the sample size is the denominator in the formula for SEM, a larger sample size will yield a smaller SEM when holding the population variability constant.\nWe have no control over the population variability, but we do have control over the sample size. Thus, if we wish to improve our sample statistics (by reducing their sampling variability) then we should use larger samples. However, the formula also tells us something very fundamental about statistical sampling – namely, that the utility of larger samples diminishes with the square root of the sample size."
  },
  {
    "objectID": "Week3/lecture.html#sample-size-effects",
    "href": "Week3/lecture.html#sample-size-effects",
    "title": "Probability, Sampling, and Experiments",
    "section": "Sample Size Effects",
    "text": "Sample Size Effects\n\nTheoryVisualizationCode\n\n\nImpact of Sample Size:\n\nLarger n → Smaller SEM\nRelationship is not linear\nDiminishing returns\nSquare root relationship\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Compare SEM for different sample sizes\nn1 &lt;- 50\nn2 &lt;- 200 # 4 times larger\n\nsem1 &lt;- pop_sd / sqrt(n1)\nsem2 &lt;- pop_sd / sqrt(n2)\n\n# Improvement factor\nimprovement &lt;- sem1 / sem2\ncat(\"Improvement factor:\", round(improvement, 2))\n\n\n\n\n\nThe relationship between sample size and standard error is not linear. Doubling the sample size will not double the quality of the statistics; rather, it will improve it by a factor of √2. This has important implications for study design and resource allocation.\nThe visualization shows how the standard error decreases as sample size increases, but with diminishing returns. This means that after a certain point, increasing sample size may not be worth the additional cost and effort.\nThis relationship is fundamental to statistical power, which we will discuss in later sections. Understanding this relationship helps researchers make informed decisions about sample size requirements for their studies."
  },
  {
    "objectID": "Week3/lecture.html#the-central-limit-theorem",
    "href": "Week3/lecture.html#the-central-limit-theorem",
    "title": "Probability, Sampling, and Experiments",
    "section": "The Central Limit Theorem",
    "text": "The Central Limit Theorem\nKey Points:\n\nAs sample size increases:\n\nSampling distribution becomes normal\nRegardless of population distribution\nMean approaches population mean\nVariance decreases\n\nImplications:\n\nEnables statistical inference\nJustifies normal approximation\nExplains real-world patterns\n\n\nThe Central Limit Theorem tells us that as sample sizes get larger, the sampling distribution of the mean will become normally distributed, even if the data within each sample are not normally distributed. This is a powerful result that allows us to make inferences about population parameters based on sample statistics."
  },
  {
    "objectID": "Week3/lecture.html#the-central-limit-theorem-1",
    "href": "Week3/lecture.html#the-central-limit-theorem-1",
    "title": "Probability, Sampling, and Experiments",
    "section": "The Central Limit Theorem",
    "text": "The Central Limit Theorem\nNormal Distribution:\n\n\n\nBell-shaped curve\nDefined by mean (\\(\\mu\\)) and SD (\\(\\sigma\\))\nSymmetric around mean\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Central Limit Theorem tells us that as sample sizes get larger, the sampling distribution of the mean will become normally distributed, even if the data within each sample are not normally distributed.\nThe normal distribution is described in terms of two parameters: the mean (which you can think of as the location of the peak), and the standard deviation (which specifies the width of the distribution). The bell-like shape of the distribution never changes, only its location and width.\nThe normal distribution is commonly observed in data collected in the real world – and the central limit theorem gives us some insight into why that occurs. For example, the height of any adult depends on a complex mixture of their genetics and experience; even if those individual contributions may not be normally distributed, when we combine them the result is a normal distribution."
  },
  {
    "objectID": "Week3/lecture.html#clt-in-action-nhanes-example",
    "href": "Week3/lecture.html#clt-in-action-nhanes-example",
    "title": "Probability, Sampling, and Experiments",
    "section": "CLT in Action: NHANES Example",
    "text": "CLT in Action: NHANES Example\n\nOriginal DistributionCode ExampleKey Insights\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Compare skewness\nlibrary(moments)\noriginal_skew &lt;- skewness(NHANES_clean$AlcoholYear)\nsampling_skew &lt;- skewness(samples_alc$mean_alcohol)\n\ncat(\"Original Distribution Skewness:\", round(original_skew, 2), \"\\n\")\ncat(\"Sampling Distribution Skewness:\", round(sampling_skew, 2))\n\n\n\n\nOriginal data is highly skewed\nSampling distribution is nearly normal\nCLT works even with:\n\nNon-normal data\nSkewed distributions\nDiscrete values\n\nSample size of 50 is sufficient\n\n\n\n\n\nLet’s work with the variable AlcoholYear from the NHANES dataset, which is highly skewed. This distribution is, for lack of a better word, funky – and definitely not normally distributed.\nNow let’s look at the sampling distribution of the mean for this variable. Despite the clear non-normality of the original data, the sampling distribution is remarkably close to the normal.\nThe Central Limit Theorem is important for statistics because it allows us to safely assume that the sampling distribution of the mean will be normal in most cases. This means that we can take advantage of statistical techniques that assume a normal distribution."
  },
  {
    "objectID": "Week3/lecture.html#summary-1",
    "href": "Week3/lecture.html#summary-1",
    "title": "Probability, Sampling, and Experiments",
    "section": "Summary",
    "text": "Summary\n\n\n\n\nSampling Fundamentals:\n\nPopulation vs Sample\nRepresentative sampling\nWith/without replacement\nSampling error\n\nStandard Error:\n\nMeasures sampling variability\nDecreases with √n\nGuides sample size decisions\nQuantifies precision\n\n\n\n\n\n\nCentral Limit Theorem:\n\nSampling distribution normality\nIndependent of original distribution\nEnables statistical inference\nFoundation for hypothesis testing\n\nApplications:\n\nPolitical polling\nClinical trials\nQuality control\nResearch design\n\n\n\n\n\nIn this lecture, we covered: - The fundamentals of statistical sampling and why it works - How to characterize sampling error and the sampling distribution - The standard error of the mean and its relationship with sample size - The Central Limit Theorem and its importance in statistical inference - Real-world applications and examples using the NHANES dataset"
  },
  {
    "objectID": "Week4/01-sampling.html",
    "href": "Week4/01-sampling.html",
    "title": "Statistical Inference",
    "section": "",
    "text": "Adapted from:\n\n\nSignificant Statistics, Chapter 6 - Foundations of Inference. John Morgan Russell (2020).\n\nStatistical Thinking, Chapter 9 - Hypothesis Testing. Russell A. Poldrack (2019).\n\n\n\nIt is often necessary to “guess”, infer, or generalize about the outcome of an event in order to make a decision. Politicians study polls to guess their likelihood of winning an election. Teachers choose a particular course of study based on what they think students can comprehend. Doctors choose the treatments needed for various diseases based on their assessment of likely results. You may have visited a casino where people play games chosen because of the belief that the likelihood of winning is good. You may have chosen your course of study based on the probable availability of jobs.\n\n\nThe goal of statistical inference is to generalise - to make statements about a population based on a sample.\nStatistical inference uses what we know about probability to make our best “guesses” from samples about what we don’t know about the population.\n\n\n\n\nPoint estimation\n\nUsing sample data to calculate a single statistic as an estimate of an unknown population parameter\nExample: What is the average height of undergraduates at this university? What is the average construction cost of an office building in London? What was it in 2019?\n\n\nConfidence intervals\n\nAn interval built around a point estimate for an unknown population parameter.\n\n\nHypothesis testing\n\nA decision making procedure for determining whether sample evidence supports a hypothesis.\n\n\n\n\n\nThese three examples make up the main forms of statistical inference. However, there are many other forms of statistical inference, such as regression analysis - e.g. How much does building energy use change as occupancy increases?\n\n\n\nSuppose you were trying to determine the mean rent of a two-bedroom apartment in your town. You might look in the classified section of the newspaper, write down several rents listed, and average them together. You would have obtained a point estimate of the true mean. If you are trying to determine the percentage of times you make a basket when shooting a basketball, you might count the number of shots you make and divide that by the number of shots you attempted. In this case, you would have obtained a point estimate for the true proportion.\n\nThe most natural way to estimate features of the population (parameters) is to use the corresponding summary statistic calculated from the sample. Some common point estimates and their corresponding parameters are found in the following table:\n\n\nParameters and Point Estimates\n\n\n\n\n\n\nParameter\nMeasure\nStatistic\n\n\n\n\\mu\nMean of a single population\n\\bar{x}\n\n\np\nProportion of a single population\n\\hat{p}\n\n\n\\mu_D\nMean difference of two dependent populations\n\\bar{x}_D\n\n\n\\mu_1 - \\mu_2\nDifference in means of two independent populations\n\\bar{x}_1 - \\bar{x}_2\n\n\np_1 - p_2\nDifference in proportions of two population\n\\hat{p}_1 - \\hat{p}_2\n\n\n\\sigma^2\nVariance of a single population\nS^2\n\n\n\\sigma\nStandard deviation of a single population\nS\n\n\n\n\n\n\n\n\n\nParameters and Point Estimates\n\nParameter\nStatistic\n\n\n\n\\mu\n\\bar{x}\n\n\np\n\\hat{p}\n\n\n\\mu_1 - \\mu_2\n\\bar{x}_1 - \\bar{x}_2\n\n\np_1 - p_2\n\\hat{p}_1 - \\hat{p}_2\n\n\n\\sigma^2\nS^2\n\n\n\\sigma\nS\n\n\n\n\n\nSuppose the mean weight of a sample of 60 adults is 173.3 lbs; this sample mean is a point estimate of the population mean weight, \\mu.\nRemember: this is one of many samples that we could have taken from the population.\nIf a different random sample of 60 individuals were taken from the same population, the new sample mean would likely be different as a result of sampling variability. While estimates generally vary from one sample to another, the population mean is a fixed value.\n\n\n\nSuppose a poll suggested the US President’s approval rating is 45%. We would consider 45% to be a point estimate of the approval rating we might see if we collected responses from the entire population. This entire-population response proportion is generally referred to as the parameter of interest. When the parameter is a proportion, it is often denoted by p, and we often refer to the sample proportion as \\hat{p} (pronounced “p-hat”). Unless we collect responses from every individual in the population, p remains unknown, and we use $ as our estimate of p.\nHow would one estimate the difference in average weight between men and women? Suppose a sample of men yields a mean of 185.1 lbs and a sample of women men yields a mean of 162.3 lbs. What is a good point estimate for the difference in these two population means? We will expand on this in following chapters.\n\n\n\nSampling variability\nWe have established that different samples yield different statistics due to sampling variability. These statistics have their own distributions, called sampling distributions, that reflect this as a random variable. The sampling distribution of a sample statistic is the distribution of the point estimates based on samples of a fixed size, n, from a certain population. It is useful to think of a particular point estimate as being drawn from a sampling distribution.\nRecall the sample mean weight calculated from a previous sample of 173.3 lbs. Suppose another random sample of 60 participants might produce a different value of x, such as 169.5 lbs. Repeated random sampling could result in additional different values, perhaps 172.1 lbs, 168.5 lbs, and so on. Each sample mean can be thought of as a single observation from a random variable X. The distribution of X is called the sampling distribution of the sample mean, and has its own mean and standard deviation like the random variables discussed previously. We will simulate the concept of a sampling distribution using technology to repeatedly sample, calculate statistics, and graph them. However, the actual sampling distribution would only be attainable if we could theoretically take an infinite amount of samples.\nEach of the point estimates in the table above have their own unique sampling distributions which we will look at in the future\n\n\nWhat makes a statistical estimate of this parameter of interest a “Good” one? It must be both accurate and precise.\nAlthough variability in samples is present, there remains a fixed value for any population parameter.\n\nAccording to the law of large numbers, probabilities converge to what we expect over time.\nPoint estimates follow this rule, becoming more accurate with increasing sample size.\n\nNational Health and Nutrition Examination Survey (NHANES) from the US Centers for Disease Control (CDC)\n\nCodeNHANES_adult |&gt;\n  select(c(\"SurveyYr\", \"Gender\", \"Age\", \"Race1\", \"Education\", \"Weight\", \"Height\", \"Pulse\", \"Diabetes\")) |&gt;\n  gt_preview() |&gt;\n  tab_header(title = \"NHANES Dataset\")\n\n\n\n\n\n\nNHANES Dataset\n\n\n\nSurveyYr\nGender\nAge\nRace1\nEducation\nWeight\nHeight\nPulse\nDiabetes\n\n\n\n\n1\n2009_10\nmale\n34\nWhite\nHigh School\n87.4\n164.7\n70\nNo\n\n\n2\n2009_10\nfemale\n49\nWhite\nSome College\n86.7\n168.4\n86\nNo\n\n\n3\n2009_10\nfemale\n45\nWhite\nCollege Grad\n75.7\n166.7\n62\nNo\n\n\n4\n2009_10\nmale\n66\nWhite\nSome College\n68.0\n169.5\n60\nNo\n\n\n5\n2009_10\nmale\n58\nWhite\nCollege Grad\n78.4\n181.9\n62\nNo\n\n\n6..4785\n\n\n\n\n\n\n\n\n\n\n\n4786\n2011_12\nmale\n60\nWhite\nCollege Grad\n78.4\n168.8\n76\nYes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe accuracy of an estimate refers to how well it estimates the actual value of that parameter. Mathematically, this is true when that the expected value your statistic is equal to the value of that parameter. This can be visualized as the center of the sampling distribution appearing to be situated at the value of that parameter.\nAccording to the law of large numbers, probabilities converge to what we expect over time. Point estimates follow this rule, becoming more accurate with increasing sample size. The figure above shows the sample mean weight calculated for random samples drawn, where sample size increases by 1 for each draw until sample size equals 500. The maroon dashed horizontal line is drawn at the average weight of all adults 169.7 lbs, which represents the population mean weight according to the CDC.\nThe figure above shows the sample mean weight calculated for random samples drawn, where sample size increases by 1 for each draw until sample size equals 500. The maroon dashed horizontal line is drawn at the average weight of all adults 169.7 lbs, which represents the population mean weight according to the CDC.\nNote how a sample size around 50 may produce a sample mean that is as much as 10 lbs higher or lower than the population mean. As sample size increases, the fluctuations around the population mean decrease; in other words, as sample size increases, the sample mean becomes less variable and provides a more reliable estimate of the population mean.\nIn addition to accuracy, a precise estimate is also more useful. This means when repeatedly sampling, the values of the statistics seem pretty close together. The precision of an estimate can be visualized as the spread of the sampling distribution, usually quantified by the standard deviation. The phrase “the standard deviation of a sampling distribution” is often shortened to the standard error. A smaller standard error means a more precise estimate and is also effected by sample size.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Week4/01-sampling.html#statistical-inference-1",
    "href": "Week4/01-sampling.html#statistical-inference-1",
    "title": "Statistical Inference",
    "section": "",
    "text": "The goal of statistical inference is to generalise - to make statements about a population based on a sample.\nStatistical inference uses what we know about probability to make our best “guesses” from samples about what we don’t know about the population.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Week4/01-sampling.html#statistical-inference-2",
    "href": "Week4/01-sampling.html#statistical-inference-2",
    "title": "Statistical Inference",
    "section": "",
    "text": "Point estimation\n\nUsing sample data to calculate a single statistic as an estimate of an unknown population parameter\nExample: What is the average height of undergraduates at this university? What is the average construction cost of an office building in London? What was it in 2019?\n\n\nConfidence intervals\n\nAn interval built around a point estimate for an unknown population parameter.\n\n\nHypothesis testing\n\nA decision making procedure for determining whether sample evidence supports a hypothesis.\n\n\n\n\n\nThese three examples make up the main forms of statistical inference. However, there are many other forms of statistical inference, such as regression analysis - e.g. How much does building energy use change as occupancy increases?",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Week4/01-sampling.html#point-estimation",
    "href": "Week4/01-sampling.html#point-estimation",
    "title": "Statistical Inference",
    "section": "",
    "text": "Suppose you were trying to determine the mean rent of a two-bedroom apartment in your town. You might look in the classified section of the newspaper, write down several rents listed, and average them together. You would have obtained a point estimate of the true mean. If you are trying to determine the percentage of times you make a basket when shooting a basketball, you might count the number of shots you make and divide that by the number of shots you attempted. In this case, you would have obtained a point estimate for the true proportion.\n\nThe most natural way to estimate features of the population (parameters) is to use the corresponding summary statistic calculated from the sample. Some common point estimates and their corresponding parameters are found in the following table:\n\n\nParameters and Point Estimates\n\n\n\n\n\n\nParameter\nMeasure\nStatistic\n\n\n\n\\mu\nMean of a single population\n\\bar{x}\n\n\np\nProportion of a single population\n\\hat{p}\n\n\n\\mu_D\nMean difference of two dependent populations\n\\bar{x}_D\n\n\n\\mu_1 - \\mu_2\nDifference in means of two independent populations\n\\bar{x}_1 - \\bar{x}_2\n\n\np_1 - p_2\nDifference in proportions of two population\n\\hat{p}_1 - \\hat{p}_2\n\n\n\\sigma^2\nVariance of a single population\nS^2\n\n\n\\sigma\nStandard deviation of a single population\nS",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Week4/01-sampling.html#point-estimation-1",
    "href": "Week4/01-sampling.html#point-estimation-1",
    "title": "Statistical Inference",
    "section": "",
    "text": "Parameters and Point Estimates\n\nParameter\nStatistic\n\n\n\n\\mu\n\\bar{x}\n\n\np\n\\hat{p}\n\n\n\\mu_1 - \\mu_2\n\\bar{x}_1 - \\bar{x}_2\n\n\np_1 - p_2\n\\hat{p}_1 - \\hat{p}_2\n\n\n\\sigma^2\nS^2\n\n\n\\sigma\nS\n\n\n\n\n\nSuppose the mean weight of a sample of 60 adults is 173.3 lbs; this sample mean is a point estimate of the population mean weight, \\mu.\nRemember: this is one of many samples that we could have taken from the population.\nIf a different random sample of 60 individuals were taken from the same population, the new sample mean would likely be different as a result of sampling variability. While estimates generally vary from one sample to another, the population mean is a fixed value.\n\n\n\nSuppose a poll suggested the US President’s approval rating is 45%. We would consider 45% to be a point estimate of the approval rating we might see if we collected responses from the entire population. This entire-population response proportion is generally referred to as the parameter of interest. When the parameter is a proportion, it is often denoted by p, and we often refer to the sample proportion as \\hat{p} (pronounced “p-hat”). Unless we collect responses from every individual in the population, p remains unknown, and we use $ as our estimate of p.\nHow would one estimate the difference in average weight between men and women? Suppose a sample of men yields a mean of 185.1 lbs and a sample of women men yields a mean of 162.3 lbs. What is a good point estimate for the difference in these two population means? We will expand on this in following chapters.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Week4/01-sampling.html#unbiased-estimation",
    "href": "Week4/01-sampling.html#unbiased-estimation",
    "title": "Statistical Inference",
    "section": "",
    "text": "Sampling variability\nWe have established that different samples yield different statistics due to sampling variability. These statistics have their own distributions, called sampling distributions, that reflect this as a random variable. The sampling distribution of a sample statistic is the distribution of the point estimates based on samples of a fixed size, n, from a certain population. It is useful to think of a particular point estimate as being drawn from a sampling distribution.\nRecall the sample mean weight calculated from a previous sample of 173.3 lbs. Suppose another random sample of 60 participants might produce a different value of x, such as 169.5 lbs. Repeated random sampling could result in additional different values, perhaps 172.1 lbs, 168.5 lbs, and so on. Each sample mean can be thought of as a single observation from a random variable X. The distribution of X is called the sampling distribution of the sample mean, and has its own mean and standard deviation like the random variables discussed previously. We will simulate the concept of a sampling distribution using technology to repeatedly sample, calculate statistics, and graph them. However, the actual sampling distribution would only be attainable if we could theoretically take an infinite amount of samples.\nEach of the point estimates in the table above have their own unique sampling distributions which we will look at in the future\n\n\nWhat makes a statistical estimate of this parameter of interest a “Good” one? It must be both accurate and precise.\nAlthough variability in samples is present, there remains a fixed value for any population parameter.\n\nAccording to the law of large numbers, probabilities converge to what we expect over time.\nPoint estimates follow this rule, becoming more accurate with increasing sample size.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Week4/01-sampling.html#example-dataset---nhanes",
    "href": "Week4/01-sampling.html#example-dataset---nhanes",
    "title": "Statistical Inference",
    "section": "",
    "text": "National Health and Nutrition Examination Survey (NHANES) from the US Centers for Disease Control (CDC)\n\nCodeNHANES_adult |&gt;\n  select(c(\"SurveyYr\", \"Gender\", \"Age\", \"Race1\", \"Education\", \"Weight\", \"Height\", \"Pulse\", \"Diabetes\")) |&gt;\n  gt_preview() |&gt;\n  tab_header(title = \"NHANES Dataset\")\n\n\n\n\n\n\nNHANES Dataset\n\n\n\nSurveyYr\nGender\nAge\nRace1\nEducation\nWeight\nHeight\nPulse\nDiabetes\n\n\n\n\n1\n2009_10\nmale\n34\nWhite\nHigh School\n87.4\n164.7\n70\nNo\n\n\n2\n2009_10\nfemale\n49\nWhite\nSome College\n86.7\n168.4\n86\nNo\n\n\n3\n2009_10\nfemale\n45\nWhite\nCollege Grad\n75.7\n166.7\n62\nNo\n\n\n4\n2009_10\nmale\n66\nWhite\nSome College\n68.0\n169.5\n60\nNo\n\n\n5\n2009_10\nmale\n58\nWhite\nCollege Grad\n78.4\n181.9\n62\nNo\n\n\n6..4785\n\n\n\n\n\n\n\n\n\n\n\n4786\n2011_12\nmale\n60\nWhite\nCollege Grad\n78.4\n168.8\n76\nYes",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Week4/01-sampling.html#unbiased-estimation-1",
    "href": "Week4/01-sampling.html#unbiased-estimation-1",
    "title": "Statistical Inference",
    "section": "",
    "text": "The accuracy of an estimate refers to how well it estimates the actual value of that parameter. Mathematically, this is true when that the expected value your statistic is equal to the value of that parameter. This can be visualized as the center of the sampling distribution appearing to be situated at the value of that parameter.\nAccording to the law of large numbers, probabilities converge to what we expect over time. Point estimates follow this rule, becoming more accurate with increasing sample size. The figure above shows the sample mean weight calculated for random samples drawn, where sample size increases by 1 for each draw until sample size equals 500. The maroon dashed horizontal line is drawn at the average weight of all adults 169.7 lbs, which represents the population mean weight according to the CDC.\nThe figure above shows the sample mean weight calculated for random samples drawn, where sample size increases by 1 for each draw until sample size equals 500. The maroon dashed horizontal line is drawn at the average weight of all adults 169.7 lbs, which represents the population mean weight according to the CDC.\nNote how a sample size around 50 may produce a sample mean that is as much as 10 lbs higher or lower than the population mean. As sample size increases, the fluctuations around the population mean decrease; in other words, as sample size increases, the sample mean becomes less variable and provides a more reliable estimate of the population mean.\nIn addition to accuracy, a precise estimate is also more useful. This means when repeatedly sampling, the values of the statistics seem pretty close together. The precision of an estimate can be visualized as the spread of the sampling distribution, usually quantified by the standard deviation. The phrase “the standard deviation of a sampling distribution” is often shortened to the standard error. A smaller standard error means a more precise estimate and is also effected by sample size.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Week4/03-exercise.html",
    "href": "Week4/03-exercise.html",
    "title": "Exercise",
    "section": "",
    "text": "Data Overview\nData Preview\n\n\n\nThe “AI-Powered Job Market Insights” dataset provides a snapshot of the modern job market, particularly focusing on the role of artificial intelligence (AI) and automation across various industries.\nThis dataset includes 500 unique job listings, each characterized by different factors like industry, company size, AI adoption level, automation risk, required skills, and job growth projections.\n\n\n\nCodeai_jobs |&gt;\n  gt_preview() |&gt;\n  tab_header(title = \"AI-Powered Job Market Insights\") |&gt;\n  tab_source_note(source_note = \"Source: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\")\n\n\n\n\n\n\nAI-Powered Job Market Insights\n\n\n\njob_title\nindustry\ncompany_size\nlocation\nai_adoption_level\nautomation_risk\nrequired_skills\nsalary_usd\nremote_friendly\njob_growth_projection\n\n\n\n\n1\nCybersecurity Analyst\nEntertainment\nSmall\nDubai\nMedium\nHigh\nUX/UI Design\n111392.17\nYes\nGrowth\n\n\n2\nMarketing Specialist\nTechnology\nLarge\nSingapore\nMedium\nHigh\nMarketing\n73792.56\nNo\nDecline\n\n\n3\nAI Researcher\nTechnology\nLarge\nSingapore\nMedium\nLow\nUX/UI Design\n137170.26\nYes\nGrowth\n\n\n4\nSales Manager\nRetail\nSmall\nBerlin\nLow\nMedium\nProject Management\n83027.95\nNo\nGrowth\n\n\n5\nCybersecurity Analyst\nEntertainment\nSmall\nTokyo\nLow\nLow\nJavaScript\n87752.92\nYes\nDecline\n\n\n6..499\n\n\n\n\n\n\n\n\n\n\n\n\n500\nHR Manager\nEntertainment\nMedium\nBerlin\nMedium\nHigh\nProject Management\n53764.38\nYes\nDecline\n\n\n\nSource: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\n\n\n\n\n\n\n\n\n\nTo simplify our later code, I have created a separate table which is already filtered for the groups we will be looking at:\n\nCodeai_jobs_risk &lt;- ai_jobs |&gt;\n  filter(automation_risk %in% c(\"Low\", \"High\"))\n\nai_jobs_high &lt;- ai_jobs_risk |&gt;\n  filter(automation_risk == \"High\")\n\nai_jobs_low &lt;- ai_jobs_risk |&gt;\n  filter(automation_risk == \"Low\")\n\n\nDataset Features:\nSource: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\n\n\nJob_Title:\n\n\nDescription: The title of the job role.\n\nType: Categorical\n\nExample Values: “Data Scientist”, “Software Engineer”, “HR Manager”\n\n\n\nIndustry:\n\n\nDescription: The industry in which the job is located.\n\nType: Categorical\n\nExample Values: “Technology”, “Healthcare”, “Finance”\n\n\n\nCompany_Size:\n\n\nDescription: The size of the company offering the job.\n\nType: Ordinal\n\nCategories: “Small”, “Medium”, “Large”\n\n\n\nLocation:\n\n\nDescription: The geographic location of the job.\n\nType: Categorical\n\nExample Values: “New York”, “San Francisco”, “London”\n\n\n\nAI_Adoption_Level:\n\n\nDescription: The extent to which the company has adopted AI in its operations.\n\nType: Ordinal\n\nCategories: “Low”, “Medium”, “High”\n\n\n\nAutomation_Risk:\n\n\nDescription: The estimated risk that the job could be automated within the next 10 years.\n\nType: Ordinal\n\nCategories: “Low”, “Medium”, “High”\n\n\n\nRequired_Skills:\n\n\nDescription: The key skills required for the job role.\n\nType: Categorical\n\nExample Values: “Python”, “Data Analysis”, “Project Management”\n\n\n\nSalary_USD:\n\n\nDescription: The annual salary offered for the job in USD.\n\nType: Numerical\n\nValue Range: $30,000 - $200,000\n\n\n\nRemote_Friendly:\n\n\nDescription: Indicates whether the job can be performed remotely.\n\nType: Categorical\n\nCategories: “Yes”, “No”\n\n\n\nJob_Growth_Projection:\n\n\nDescription: The projected growth or decline of the job role over the next five years.\n\nType: Categorical\n\nCategories: “Decline”, “Stable”, “Growth”\n\n\n\n\n\nTo begin, split into three groups. Decide on the following roles, one per person:\n\nSampler - draws samples from the population\nRecording - records the draws in R\n\nYour group has been presented with a population model (bag containing salaries). There may be multiple populations represented in the various bags around the room.\nPlease DO NOT look in the bag OR empty the contents of the bag.\n\n\n\nThe Sampler draws 10 slips from the bag - this is your sample of size 5.\nThe Recorder records the values on the slips into the sampling-exercise.R file.\n\nCreate a new list for each sample (sample_1 &lt;- c(1, 2, 3, 2, 2))\n\n\nCalculate the mean for the new sample and add it to the sample_means table.\nRun the code to plot the histogram of your samples.\nReturn the slips to the bag.\nEnsure the data is well mixed between samples.\nRepeat"
  },
  {
    "objectID": "Week4/03-exercise.html#exercise-dataset",
    "href": "Week4/03-exercise.html#exercise-dataset",
    "title": "Exercise",
    "section": "",
    "text": "Data Overview\nData Preview\n\n\n\nThe “AI-Powered Job Market Insights” dataset provides a snapshot of the modern job market, particularly focusing on the role of artificial intelligence (AI) and automation across various industries.\nThis dataset includes 500 unique job listings, each characterized by different factors like industry, company size, AI adoption level, automation risk, required skills, and job growth projections.\n\n\n\nCodeai_jobs |&gt;\n  gt_preview() |&gt;\n  tab_header(title = \"AI-Powered Job Market Insights\") |&gt;\n  tab_source_note(source_note = \"Source: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\")\n\n\n\n\n\n\nAI-Powered Job Market Insights\n\n\n\njob_title\nindustry\ncompany_size\nlocation\nai_adoption_level\nautomation_risk\nrequired_skills\nsalary_usd\nremote_friendly\njob_growth_projection\n\n\n\n\n1\nCybersecurity Analyst\nEntertainment\nSmall\nDubai\nMedium\nHigh\nUX/UI Design\n111392.17\nYes\nGrowth\n\n\n2\nMarketing Specialist\nTechnology\nLarge\nSingapore\nMedium\nHigh\nMarketing\n73792.56\nNo\nDecline\n\n\n3\nAI Researcher\nTechnology\nLarge\nSingapore\nMedium\nLow\nUX/UI Design\n137170.26\nYes\nGrowth\n\n\n4\nSales Manager\nRetail\nSmall\nBerlin\nLow\nMedium\nProject Management\n83027.95\nNo\nGrowth\n\n\n5\nCybersecurity Analyst\nEntertainment\nSmall\nTokyo\nLow\nLow\nJavaScript\n87752.92\nYes\nDecline\n\n\n6..499\n\n\n\n\n\n\n\n\n\n\n\n\n500\nHR Manager\nEntertainment\nMedium\nBerlin\nMedium\nHigh\nProject Management\n53764.38\nYes\nDecline\n\n\n\nSource: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\n\n\n\n\n\n\n\n\n\nTo simplify our later code, I have created a separate table which is already filtered for the groups we will be looking at:\n\nCodeai_jobs_risk &lt;- ai_jobs |&gt;\n  filter(automation_risk %in% c(\"Low\", \"High\"))\n\nai_jobs_high &lt;- ai_jobs_risk |&gt;\n  filter(automation_risk == \"High\")\n\nai_jobs_low &lt;- ai_jobs_risk |&gt;\n  filter(automation_risk == \"Low\")\n\n\nDataset Features:\nSource: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\n\n\nJob_Title:\n\n\nDescription: The title of the job role.\n\nType: Categorical\n\nExample Values: “Data Scientist”, “Software Engineer”, “HR Manager”\n\n\n\nIndustry:\n\n\nDescription: The industry in which the job is located.\n\nType: Categorical\n\nExample Values: “Technology”, “Healthcare”, “Finance”\n\n\n\nCompany_Size:\n\n\nDescription: The size of the company offering the job.\n\nType: Ordinal\n\nCategories: “Small”, “Medium”, “Large”\n\n\n\nLocation:\n\n\nDescription: The geographic location of the job.\n\nType: Categorical\n\nExample Values: “New York”, “San Francisco”, “London”\n\n\n\nAI_Adoption_Level:\n\n\nDescription: The extent to which the company has adopted AI in its operations.\n\nType: Ordinal\n\nCategories: “Low”, “Medium”, “High”\n\n\n\nAutomation_Risk:\n\n\nDescription: The estimated risk that the job could be automated within the next 10 years.\n\nType: Ordinal\n\nCategories: “Low”, “Medium”, “High”\n\n\n\nRequired_Skills:\n\n\nDescription: The key skills required for the job role.\n\nType: Categorical\n\nExample Values: “Python”, “Data Analysis”, “Project Management”\n\n\n\nSalary_USD:\n\n\nDescription: The annual salary offered for the job in USD.\n\nType: Numerical\n\nValue Range: $30,000 - $200,000\n\n\n\nRemote_Friendly:\n\n\nDescription: Indicates whether the job can be performed remotely.\n\nType: Categorical\n\nCategories: “Yes”, “No”\n\n\n\nJob_Growth_Projection:\n\n\nDescription: The projected growth or decline of the job role over the next five years.\n\nType: Categorical\n\nCategories: “Decline”, “Stable”, “Growth”"
  },
  {
    "objectID": "Week4/03-exercise.html#exercise---mystery-bags",
    "href": "Week4/03-exercise.html#exercise---mystery-bags",
    "title": "Exercise",
    "section": "",
    "text": "To begin, split into three groups. Decide on the following roles, one per person:\n\nSampler - draws samples from the population\nRecording - records the draws in R\n\nYour group has been presented with a population model (bag containing salaries). There may be multiple populations represented in the various bags around the room.\nPlease DO NOT look in the bag OR empty the contents of the bag."
  },
  {
    "objectID": "Week4/03-exercise.html#exercise---mystery-bags-nonincremental",
    "href": "Week4/03-exercise.html#exercise---mystery-bags-nonincremental",
    "title": "Exercise",
    "section": "",
    "text": "The Sampler draws 10 slips from the bag - this is your sample of size 5.\nThe Recorder records the values on the slips into the sampling-exercise.R file.\n\nCreate a new list for each sample (sample_1 &lt;- c(1, 2, 3, 2, 2))\n\n\nCalculate the mean for the new sample and add it to the sample_means table.\nRun the code to plot the histogram of your samples.\nReturn the slips to the bag.\nEnsure the data is well mixed between samples.\nRepeat"
  },
  {
    "objectID": "Week4/05-hyptest.html",
    "href": "Week4/05-hyptest.html",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Data Overview\nData Preview\n\n\n\nThe “AI-Powered Job Market Insights” dataset provides a snapshot of the modern job market, particularly focusing on the role of artificial intelligence (AI) and automation across various industries.\nThis dataset includes 500 unique job listings, each characterized by different factors like industry, company size, AI adoption level, automation risk, required skills, and job growth projections.\n\n\n\nCodeai_jobs |&gt;\n  gt_preview() |&gt;\n  tab_header(title = \"AI-Powered Job Market Insights\") |&gt;\n  tab_source_note(source_note = \"Source: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\")\n\n\n\n\n\n\nAI-Powered Job Market Insights\n\n\n\njob_title\nindustry\ncompany_size\nlocation\nai_adoption_level\nautomation_risk\nrequired_skills\nsalary_usd\nremote_friendly\njob_growth_projection\n\n\n\n\n1\nCybersecurity Analyst\nEntertainment\nSmall\nDubai\nMedium\nHigh\nUX/UI Design\n111392.17\nYes\nGrowth\n\n\n2\nMarketing Specialist\nTechnology\nLarge\nSingapore\nMedium\nHigh\nMarketing\n73792.56\nNo\nDecline\n\n\n3\nAI Researcher\nTechnology\nLarge\nSingapore\nMedium\nLow\nUX/UI Design\n137170.26\nYes\nGrowth\n\n\n4\nSales Manager\nRetail\nSmall\nBerlin\nLow\nMedium\nProject Management\n83027.95\nNo\nGrowth\n\n\n5\nCybersecurity Analyst\nEntertainment\nSmall\nTokyo\nLow\nLow\nJavaScript\n87752.92\nYes\nDecline\n\n\n6..499\n\n\n\n\n\n\n\n\n\n\n\n\n500\nHR Manager\nEntertainment\nMedium\nBerlin\nMedium\nHigh\nProject Management\n53764.38\nYes\nDecline\n\n\n\nSource: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\n\n\n\n\n\n\n\n\n\nTo simplify our later code, I have created a separate table which is already filtered for the groups we will be looking at:\n\nCodeai_jobs_risk &lt;- ai_jobs |&gt;\n  filter(automation_risk %in% c(\"Low\", \"High\"))\n\nai_jobs_high &lt;- ai_jobs_risk |&gt;\n  filter(automation_risk == \"High\")\n\nai_jobs_low &lt;- ai_jobs_risk |&gt;\n  filter(automation_risk == \"Low\")\n\n\nDataset Features:\nSource: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\n\n\nJob_Title:\n\n\nDescription: The title of the job role.\n\nType: Categorical\n\nExample Values: “Data Scientist”, “Software Engineer”, “HR Manager”\n\n\n\nIndustry:\n\n\nDescription: The industry in which the job is located.\n\nType: Categorical\n\nExample Values: “Technology”, “Healthcare”, “Finance”\n\n\n\nCompany_Size:\n\n\nDescription: The size of the company offering the job.\n\nType: Ordinal\n\nCategories: “Small”, “Medium”, “Large”\n\n\n\nLocation:\n\n\nDescription: The geographic location of the job.\n\nType: Categorical\n\nExample Values: “New York”, “San Francisco”, “London”\n\n\n\nAI_Adoption_Level:\n\n\nDescription: The extent to which the company has adopted AI in its operations.\n\nType: Ordinal\n\nCategories: “Low”, “Medium”, “High”\n\n\n\nAutomation_Risk:\n\n\nDescription: The estimated risk that the job could be automated within the next 10 years.\n\nType: Ordinal\n\nCategories: “Low”, “Medium”, “High”\n\n\n\nRequired_Skills:\n\n\nDescription: The key skills required for the job role.\n\nType: Categorical\n\nExample Values: “Python”, “Data Analysis”, “Project Management”\n\n\n\nSalary_USD:\n\n\nDescription: The annual salary offered for the job in USD.\n\nType: Numerical\n\nValue Range: $30,000 - $200,000\n\n\n\nRemote_Friendly:\n\n\nDescription: Indicates whether the job can be performed remotely.\n\nType: Categorical\n\nCategories: “Yes”, “No”\n\n\n\nJob_Growth_Projection:\n\n\nDescription: The projected growth or decline of the job role over the next five years.\n\nType: Categorical\n\nCategories: “Decline”, “Stable”, “Growth”\n\n\n\n\n\nYou have been tasked with examining the impact of AI skills and company AI adoption on the job market across the world. Think about what we might want to know about this sector.\nWhat might some be some interesting questions we could ask based on this data?\nHow would we answer them? How would we know whether our answer is generalizable?\n\nWe want to generalize the results of our analysis from the data we have to the situation we care about.\n\n\nA hypothesis is a statement about a population, or general pattern.\nTesting a hypothesis amounts to gathering information (sampling) from a dataset and, based on that information, deciding whether that hypothesis is false or true in the population.\nTwo decisions are possible:\n\nRejecting the hypothesis (if there is enough evidence against it)\nNot rejecting it (if there is not enough evidence against it)\n\nRejecting a hypothesis is a more conclusive decision than not rejecting it.\n\nOne such focused approach uses a statistic (e.g. a difference in two means) computed from our data to see whether its true value is equal to something we assume (e.g. the difference is zero). This is called hypothesis testing: using results in the data to see if we have enough evidence to tell whether a hypothesis (the two means are equal) is wrong (they are not equal) or whether we don’t have enough evidence (they may be equal.\nTesting a hypothesis is a way of making an inference, with a focus on a specific statement. As with any kind of inference, we have to assess external validity too: the extent to which the population, or general pattern, represented by our data is the same as the population, or general pattern, we are truly interested in.\n\nGiven a population (i.e. a distribution) with a parameter of interest (which could be the mean, variance, correlation, etc.), we would like to decide between to complementary statements concerning the parameter.\nThese statements are called statistical hypotheses.\nThe choice or decision between these hypotheses is to be based on a sample taken from the population of interest.\nThe goal is to be able to choose which hypothesis is true in reality based on the sample data.\n\n\n\n\n\n\n\n\nWhat statistic is appropriate to answer our question?\nNull and Alternative Hypotheses\n\n\nWhat information is needed to answer our question?\nWhat is our population? How do we sample from the population in a statistically valid way?\n\n\n\n\nFit appropriate model\nCalculate test statistic\nAccount for variability\n\n\n\n\n\n\nThis is where all analysis or research starts - what is it you want to know, and what will you do with that information?\nFor our AI Jobs dataset some research questions we might ask are:\n\nWhich jobs are the highest paid?\nWhich industries have seen the most AI adoption?\nAre lower paid jobs at more risk of being automated?\n\nNow we translate our general research questions into specific and testable hypotheses.\nThe actual test begins by considering two hypotheses. They are called the null hypothesis and the alternative hypothesis. These hypotheses contain opposing viewpoints.\n\n\nThe null hypothesis (H_0) : It is often a statement of the accepted historical value or norm. This is your starting point that you must assume from the beginning in order to show an effect exists.\n\nThe alternative hypothesis (H_a) : It is a claim about the population that is contradictory to H_0 and what we conclude when we reject H_0\n\n\nAfter you have determined which hypothesis the sample supports, you make a decision.\nThere are two options for a decision:\n\n“reject H_0” if the sample information favors the alternative hypothesis, or\n“do not reject H_0” or “decline to reject H_0” if the sample information is insufficient to reject the null hypothesis.\n\n\nMathematical symbols used in H0 and Ha:\nFigure 6.12: Null and Alternative Hypotheses\n\n\n\n\n\n\nH_0\nH_a\n\n\n\nequal (=)\nnot equal (\\neq) or greater than (&gt;) or less than (&lt;)\n\n\ngreater than or equal to (\\geq)\nless than (&lt;)\n\n\nless than or equal to (\\leq)\nmore than (&gt;)\n\n\n\n\n\n\nDecide what statistic is appropriate to answer our question. What do we need to calculate from our sample?\n\nDifference in mean(salary_usd) for each level of automation_risk\n\nSince there are three automation risk levels (Low, Medium, High), the comparison we could make are:\n\n\\text{mean}_{\\text{high risk}} - \\text{mean}_{\\text{medium risk}}\n\\text{mean}_{\\text{high}} - \\text{mean}_{\\text{low}}\n\\text{mean}_{\\text{medium}} - \\text{mean}_{\\text{low}}\n\n\n\n\nFor now, let’s focus on just (2). Therefore:\ns = \\overline{salary}_{\\text{high risk}} - \\overline{salary}_{\\text{low risk}}\ns is our statistic of interest. We are interested in the true value of s in the population (s_{true}).\nWhat we can actually calculate is \\hat{s} the value of s in our sample.\n\n\nOur hypothesis, plainly stated, is:\nThere is a relationship between job salary and the likelihood of automation. Expressed another way, the average salary of high risk jobs is different from that of low risk jobs.\nWhat are the Null Hypothesis H_0 and Alternative Hypothesis H_A ?\n\n\n\n\nThere is no difference in the average salary between those jobs at high risk of automation, and those at low risk: \nH_0: s_{true} = 0\n\n\n\n\n\nThe difference in the average salary of high risk jobs and low risk jobs is not zero: \nH_A: s_{true} \\neq 0\n\n\n\n\nThe null says that the true value of the statistic is zero; the alternative says that it’s not zero. Together, these cover all logical possibilities for the true value of the statistic.\nIt may seem odd to have H_0: s_{true} = 0 when, presumably, we analyze the data because we suspect that the true value of s is not zero. This seemingly twisted logic comes from the fact that testing a hypothesis amounts to seeing if there is enough evidence in our data to reject the null. It is sometimes said that the null is protected: it should not be too easy to reject it otherwise the conclusions of hypothesis testing would not be strong.\nAs we introduce the concept of hypothesis testing, it is helpful to relate its logic to the logic of a criminal court procedure. At court the task is to decide whether an accused person is guilty or innocent of a certain crime. In most modern societies the starting point is the assumption of innocence: the accused person should be judged guilty only if there is enough evidence against their innocence. This is so even though the accused person was brought before court presumably because there was a suspicion of their guilt. To translate this procedure to the language of hypothesis testing, H0 is that the person is innocent, and HA is that the person is guilty.\nMedical tests are another instructive example. When testing whether a person has a certain medical condition, the null is that the person does not have the condition (healthy), and the alternative is that they have it (sick). The testing procedure amounts to gathering information to see if there is evidence to decide that the person has the condition.\nThe case when we test if HA: strue ≠ 0 is called a two-sided alternative as it allows for strue to be either greater than zero or less than zero. For instance, we focus on the difference in online and offline prices, with H0 being the equality. In such a case we are not really interested if the difference is positive or not, or whether it is negative or not.\nThe other case is working with a one-sided alternative, when we are indeed interested if a statistic is positive. The null and the alternative should be set up so that the hypothesis we are truly interested in is in the alternative set. So when we want to know if strue is positive, we want to put strue &gt; 0 in the alternative thus, making the null strue ≤ 0:\n\nH_0: s_{true} \\leq 0\n\n\nH_A: s_{true} &gt; 0\n\n\n\nOnce you have defined your hypotheses the next step in the process, is to collect sample data.\nIn this case, we already have the data in ai_jobs. Before moving on to actually testing the hypothesis, let’s take the naive approach - just calculate \\hat{s} the difference between the two groups.\n\nCodemean_high &lt;- mean(ai_jobs_high$salary_usd)\nmean_low &lt;- mean(ai_jobs_low$salary_usd)\ns_hat &lt;- mean_high - mean_low\n\n\n\n\\hat{s} = \\overline{\\text{salary}}_\\text{high risk} - \\overline{\\text{salary}}_{low risk} = \\$81.7 k - \\$99.7k = \\textbf{-18.1k}\n\n\nThere are several statistical tests used in Hypothesis Testing. Which one you use depends on what type of hypothesis you are testing and what kind of data you have.\nFor this example, where we are testing the difference in means of a numerical variable (salary_usd) across different groups (automation_risk), we use a test called the t-test.\n\n\nFollowing the logic of hypothesis testing, we start from the assumption that the null (H_0) is true and thus s_{true} = 0.\nWe look at the evidence to see if we want to reject this null or maintain our assumption that it’s true.\nThe evidence we look for is how far the estimated value \\hat{s} is from zero.\nWe reject H_0 if the distance is large (i.e. \\hat{s} is sufficiently greater or lesser than 0)\n\n\nHow far is far enough?\nThe test statistic is the measure of how far the estimated value \\hat{s} is from what its true value would be if H_0 is true.\n\n\nt-statistic:\n\nt = \\frac{\\hat{s}}{\\mathrm{SE}(\\hat{s})} = \\frac{\\bar{x}_A - \\bar{x}_B}{\\mathrm{SE}(\\bar{x}_A - \\bar{x}_B)}\n\n\nThe t-test is a procedure to decide whether we can reject the null H_0.\nThe magnitude of the t-statistic t measures the distance of \\hat{s} from what s_{true} would be if the null were true.\n\nThe unit of distance is the standard error.\n\n\nThe t-statistic transforms the original statistic of interest into a standardized version\n\nFor example: if t = 1 (or -1), it means \\hat{s} is exactly one standard error away from zero.\n\n\n\nR makes it very easy to apply a test such as the t-test. For most statistical tests, there exists a simple function to compute it.\nFor the t-test, we use the t.test() function:\n\nCodet_res &lt;- t.test(ai_jobs_high$salary_usd, ai_jobs_low$salary_usd)\n\n\nt = -6.54\n\n\nThe following step is making a decision: either rejecting the null or not rejecting it.\nIn hypothesis testing, this decision is based on a clear rule specified in advance.\nWe specify in advance to avoid bias - before looking at the data, we state what it would take to reject the null hypothesis. We follow what the data says, whatever result that may be.\n\n\nA clear rule also makes the decision transparent, which helps avoid biases in the decision. Unfortunately, we humans are often tempted to use evidence to support our pre-existing views or prejudices. If, for example, we think that jobs which are more highly valued by companies (i.e. have a higher salary) we may pay more attention to the evidence that supports that belief than to the evidence against it.\nIn partiular, we may be tempted to say that the estimated \\hat{s} difference is large enough to reject the null, because we believe that the null isn’t true. Clear decision rules are designed to minimize the room for such temptations.\n\n\nOnce you have your test statistic there are two methods to use it to make your decision:\n\nCritical value method\nP-Value method – This is the preferred method we mostly will focus on.\n\n\n\nWe use a critical value to tell us whether the test statistic is large enough - is it far enough away from zero to reject the null?\nTo define the critical value, we need to decide how conservative we want to be with the evidence.\nThe larger we set the critical value, the harder it is to reject the null hypothesis.\n\n\nAs with the sampling distribution for means we looked at earlier, our test-statistic t also has a sampling distribution. If we were to sample many times and calculate t for each sample, we would again get a distribution with a specific shape and parameters.\n\n\nThe sampling distribution of the test statistic when the null is true.\n\nRecall: Approximately 95% of values fall within two standard deviations of the distribution.\n\n\n\n\n\n\nThe sampling distribution of the test statistic when the null is true.\n\n\nRecall: Approximately 95% of values fall within two standard deviations of the distribution.\nSince 95% of values fall within 2 SD, if we want to reject the null hypothesis with 95% confidence, then we say that our test statistic must fall outside of 2 SD.\nIn other words, since the units of \\hat{s} are standard deviations: \\hat{s} \\geq \\pm 2\n\n\n\nA critical value of 2 is standard. However, it is ultimately just a convention. We could choose to set other critical values that correspond to different probabilities. There is not anything inherently special about setting our threshold at 95% vs 90%.\nIf we make the critical value \\pm 1.6, the chance of a false positive is 10%.\n\n\n\n\n\nDifferent fields have different standards for evidence - for instance, a critical value of 5 (99.994%) is standard in particle physics (referred to as 5\\sigma).\n\n\nSince our test statistic t = -7.5 &lt; -2, at a confidence level of 95%, we would have sufficient evidence to reject H_0\nTherefore, we would say:\n\nThe average salary of jobs at high risk of automation is not the same as the average salary of jobs at low risk.\nWe have evidence that higher salary jobs are at less risk of automation than low salary jobs.\n\n\nThis does not inherently mean we accept the alternative hypothesis. We are narrowing the realm of possible answers, but very rarely (perhaps never) are we able to statistically prove a single explanation in one go.\nWe have increased our reasons to believe our hypothesis, but several other possibilities exist.\nScience is then the process of continually investigating our hypothesis and pitting it against new null hypotheses and rejecting them as well.\n\n\nHopefully, the critical value is fairly intuitive to you now. However, it is not the typical way that statistical results are presented.\nInstead, you will typically see something called a p-value.\n\n\np-value: The probability than an event will occur, assuming the null hypothesis is true.\nThe p-value essentially flips the critical value statement:\n\nInstead of saying a test statistic value &gt; 2 falls outside the 95% bound, we calculate where out test statistic falls in the distribution\n\n\nThe p-value is the probability that the test statistic will be as large, or larger, than we calculate from the data, if the null hypothesis is true. i.e. P(data | H_0).\n\n\n\np = P(|t| &gt; \\text{critical value})\nBecause the p-value tells us the smallest level of significance at which we can reject the null hypothesis, it summarizes all the information we need to make the decision.\nThis is why the p-value is used - rather than needing to set a critical value and calculate the test statistic, we can instead use just the p-value.\n\n\nLike with the critical value, we should set our desired significance level before carrying out the analysis.\nWe then compare our calculated p-value with the significance level. If it is less, we reject the null hypothesis.\nThe significance level (\\alpha) is the probability that a true null hypothesis will be rejected.\nA typical significance level is \\alpha &lt; 0.05, which corresponds with a critical value of 2, or a probability of 5%.\n\n\n\nR output\nT distribution\n\n\n\nAgain, R will provide us with the p-value. Let’s now look at the full output from our t.test():\n\nCodet.test(ai_jobs_high$salary_usd, ai_jobs_low$salary_usd)\n\n\n    Welch Two Sample t-test\n\ndata:  ai_jobs_high$salary_usd and ai_jobs_low$salary_usd\nt = -6.5366, df = 288.07, p-value = 2.854e-10\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -23513.51 -12630.27\nsample estimates:\nmean of x mean of y \n 81673.57  99745.46 \n\n\n\nIf our p-value is smaller than our pre-set significance level (\\alpha), we reject the null hypothesis and can say the result is “statistically significant” at p &lt; 0.05.\n\n\n\n\nCodexpos &lt;- seq(-5, 5, by = 0.01)\n\ndegree &lt;- 280\nypos &lt;- dt(xpos, df = degree)\n\nggplot() +\n  xlim(-8, 8) +\n  geom_function(aes(colour = \"t, df=280\"), fun = dt, args = list(df = 280), linewidth = 1.5) +\n  geom_vline(xintercept = t_res$statistic, color = \"black\", linewidth = 1.5) +\n  xlab(\"Test statistic under the null hypothesis P(data | H0)\") +\n  ylab(\"Frequency\") +\n  ggtitle(\"Student's T distribution\")\n\n\n\n\n\n\n\n\n\n\n\nThe preset \\alpha is the probability of a False Positive error (called a Type I error) - rejecting the null hypothesis when the null hypothesis is true.\nBack to our two possible decisions:\n\n\nIf \\alpha &gt; \\text{p-value}, reject H_0.\n\nThe results of the sample are statistically significant.\nWe can say there is sufficient evidence to conclude that H_0 is an incorrect believe and that the alternative hypothesis, H_A may be correct.\n\n\n\nIf \\alpha &lt; \\text{p-value}, fail to reject H_0.\n\nThe results of the sample are not significant. There is not sufficient evidence to conclude that the alternative hypothesis H_A may be correct.\n\n\n\n\nNOTE: When you “do not reject H_0”, it does not mean that you should believe that H_0 is true. It simply means that the sample data have failed to provide sufficient evidence to cast serious doubt about the truthfulness of H_0.\n\n\n\n\n\nEffect size\nSample size\n\n\n\nNo. There is an essential distinction between statistical significance and practical significance.\nLet’s say we performed an experiment to examine the effect of a particular diet on body weight, which gives a statistically significant effect at p &lt; 0.05. This doesn’t tell us how much weight was lost, which we refer to as the effect size.\nWould the loss of 20 grams (i.e. the weight of a few potato chips) be practically significant, even if it were statistically significant?\nWhether a result is practically significant depends on the effect size and the context of the research question. It’s up to the researcher to know whether it is meaningful.\n\n\nAs with the standard error (and a direct result of it), the p-value depends on the sample size. A very large sample size will give a statistically significant result in many cases, even with a very small effect size.\n\n\nThe proportion of significant results for a very small change (~20g which is about 0.001 standard deviations) as a function of sample size",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#case-study",
    "href": "Week4/05-hyptest.html#case-study",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Data Overview\nData Preview\n\n\n\nThe “AI-Powered Job Market Insights” dataset provides a snapshot of the modern job market, particularly focusing on the role of artificial intelligence (AI) and automation across various industries.\nThis dataset includes 500 unique job listings, each characterized by different factors like industry, company size, AI adoption level, automation risk, required skills, and job growth projections.\n\n\n\nCodeai_jobs |&gt;\n  gt_preview() |&gt;\n  tab_header(title = \"AI-Powered Job Market Insights\") |&gt;\n  tab_source_note(source_note = \"Source: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\")\n\n\n\n\n\n\nAI-Powered Job Market Insights\n\n\n\njob_title\nindustry\ncompany_size\nlocation\nai_adoption_level\nautomation_risk\nrequired_skills\nsalary_usd\nremote_friendly\njob_growth_projection\n\n\n\n\n1\nCybersecurity Analyst\nEntertainment\nSmall\nDubai\nMedium\nHigh\nUX/UI Design\n111392.17\nYes\nGrowth\n\n\n2\nMarketing Specialist\nTechnology\nLarge\nSingapore\nMedium\nHigh\nMarketing\n73792.56\nNo\nDecline\n\n\n3\nAI Researcher\nTechnology\nLarge\nSingapore\nMedium\nLow\nUX/UI Design\n137170.26\nYes\nGrowth\n\n\n4\nSales Manager\nRetail\nSmall\nBerlin\nLow\nMedium\nProject Management\n83027.95\nNo\nGrowth\n\n\n5\nCybersecurity Analyst\nEntertainment\nSmall\nTokyo\nLow\nLow\nJavaScript\n87752.92\nYes\nDecline\n\n\n6..499\n\n\n\n\n\n\n\n\n\n\n\n\n500\nHR Manager\nEntertainment\nMedium\nBerlin\nMedium\nHigh\nProject Management\n53764.38\nYes\nDecline\n\n\n\nSource: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\n\n\n\n\n\n\n\n\n\nTo simplify our later code, I have created a separate table which is already filtered for the groups we will be looking at:\n\nCodeai_jobs_risk &lt;- ai_jobs |&gt;\n  filter(automation_risk %in% c(\"Low\", \"High\"))\n\nai_jobs_high &lt;- ai_jobs_risk |&gt;\n  filter(automation_risk == \"High\")\n\nai_jobs_low &lt;- ai_jobs_risk |&gt;\n  filter(automation_risk == \"Low\")\n\n\nDataset Features:\nSource: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\n\n\nJob_Title:\n\n\nDescription: The title of the job role.\n\nType: Categorical\n\nExample Values: “Data Scientist”, “Software Engineer”, “HR Manager”\n\n\n\nIndustry:\n\n\nDescription: The industry in which the job is located.\n\nType: Categorical\n\nExample Values: “Technology”, “Healthcare”, “Finance”\n\n\n\nCompany_Size:\n\n\nDescription: The size of the company offering the job.\n\nType: Ordinal\n\nCategories: “Small”, “Medium”, “Large”\n\n\n\nLocation:\n\n\nDescription: The geographic location of the job.\n\nType: Categorical\n\nExample Values: “New York”, “San Francisco”, “London”\n\n\n\nAI_Adoption_Level:\n\n\nDescription: The extent to which the company has adopted AI in its operations.\n\nType: Ordinal\n\nCategories: “Low”, “Medium”, “High”\n\n\n\nAutomation_Risk:\n\n\nDescription: The estimated risk that the job could be automated within the next 10 years.\n\nType: Ordinal\n\nCategories: “Low”, “Medium”, “High”\n\n\n\nRequired_Skills:\n\n\nDescription: The key skills required for the job role.\n\nType: Categorical\n\nExample Values: “Python”, “Data Analysis”, “Project Management”\n\n\n\nSalary_USD:\n\n\nDescription: The annual salary offered for the job in USD.\n\nType: Numerical\n\nValue Range: $30,000 - $200,000\n\n\n\nRemote_Friendly:\n\n\nDescription: Indicates whether the job can be performed remotely.\n\nType: Categorical\n\nCategories: “Yes”, “No”\n\n\n\nJob_Growth_Projection:\n\n\nDescription: The projected growth or decline of the job role over the next five years.\n\nType: Categorical\n\nCategories: “Decline”, “Stable”, “Growth”",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#motivation",
    "href": "Week4/05-hyptest.html#motivation",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "You have been tasked with examining the impact of AI skills and company AI adoption on the job market across the world. Think about what we might want to know about this sector.\nWhat might some be some interesting questions we could ask based on this data?\nHow would we answer them? How would we know whether our answer is generalizable?\n\nWe want to generalize the results of our analysis from the data we have to the situation we care about.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#the-logic-of-testing-hypotheses",
    "href": "Week4/05-hyptest.html#the-logic-of-testing-hypotheses",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "A hypothesis is a statement about a population, or general pattern.\nTesting a hypothesis amounts to gathering information (sampling) from a dataset and, based on that information, deciding whether that hypothesis is false or true in the population.\nTwo decisions are possible:\n\nRejecting the hypothesis (if there is enough evidence against it)\nNot rejecting it (if there is not enough evidence against it)\n\nRejecting a hypothesis is a more conclusive decision than not rejecting it.\n\nOne such focused approach uses a statistic (e.g. a difference in two means) computed from our data to see whether its true value is equal to something we assume (e.g. the difference is zero). This is called hypothesis testing: using results in the data to see if we have enough evidence to tell whether a hypothesis (the two means are equal) is wrong (they are not equal) or whether we don’t have enough evidence (they may be equal.\nTesting a hypothesis is a way of making an inference, with a focus on a specific statement. As with any kind of inference, we have to assess external validity too: the extent to which the population, or general pattern, represented by our data is the same as the population, or general pattern, we are truly interested in.\n\nGiven a population (i.e. a distribution) with a parameter of interest (which could be the mean, variance, correlation, etc.), we would like to decide between to complementary statements concerning the parameter.\nThese statements are called statistical hypotheses.\nThe choice or decision between these hypotheses is to be based on a sample taken from the population of interest.\nThe goal is to be able to choose which hypothesis is true in reality based on the sample data.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#steps-of-analysis-hypothesis-testing",
    "href": "Week4/05-hyptest.html#steps-of-analysis-hypothesis-testing",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "What statistic is appropriate to answer our question?\nNull and Alternative Hypotheses\n\n\nWhat information is needed to answer our question?\nWhat is our population? How do we sample from the population in a statistically valid way?\n\n\n\n\nFit appropriate model\nCalculate test statistic\nAccount for variability",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#formulate-research-question-1",
    "href": "Week4/05-hyptest.html#formulate-research-question-1",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "This is where all analysis or research starts - what is it you want to know, and what will you do with that information?\nFor our AI Jobs dataset some research questions we might ask are:\n\nWhich jobs are the highest paid?\nWhich industries have seen the most AI adoption?\nAre lower paid jobs at more risk of being automated?",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#specify-hypotheses-1",
    "href": "Week4/05-hyptest.html#specify-hypotheses-1",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Now we translate our general research questions into specific and testable hypotheses.\nThe actual test begins by considering two hypotheses. They are called the null hypothesis and the alternative hypothesis. These hypotheses contain opposing viewpoints.\n\n\nThe null hypothesis (H_0) : It is often a statement of the accepted historical value or norm. This is your starting point that you must assume from the beginning in order to show an effect exists.\n\nThe alternative hypothesis (H_a) : It is a claim about the population that is contradictory to H_0 and what we conclude when we reject H_0",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#specify-hypotheses-2",
    "href": "Week4/05-hyptest.html#specify-hypotheses-2",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "After you have determined which hypothesis the sample supports, you make a decision.\nThere are two options for a decision:\n\n“reject H_0” if the sample information favors the alternative hypothesis, or\n“do not reject H_0” or “decline to reject H_0” if the sample information is insufficient to reject the null hypothesis.\n\n\nMathematical symbols used in H0 and Ha:\nFigure 6.12: Null and Alternative Hypotheses\n\n\n\n\n\n\nH_0\nH_a\n\n\n\nequal (=)\nnot equal (\\neq) or greater than (&gt;) or less than (&lt;)\n\n\ngreater than or equal to (\\geq)\nless than (&lt;)\n\n\nless than or equal to (\\leq)\nmore than (&gt;)",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#specify-hypotheses-3",
    "href": "Week4/05-hyptest.html#specify-hypotheses-3",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Decide what statistic is appropriate to answer our question. What do we need to calculate from our sample?\n\nDifference in mean(salary_usd) for each level of automation_risk\n\nSince there are three automation risk levels (Low, Medium, High), the comparison we could make are:\n\n\\text{mean}_{\\text{high risk}} - \\text{mean}_{\\text{medium risk}}\n\\text{mean}_{\\text{high}} - \\text{mean}_{\\text{low}}\n\\text{mean}_{\\text{medium}} - \\text{mean}_{\\text{low}}",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#specify-hypotheses-4",
    "href": "Week4/05-hyptest.html#specify-hypotheses-4",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "For now, let’s focus on just (2). Therefore:\ns = \\overline{salary}_{\\text{high risk}} - \\overline{salary}_{\\text{low risk}}\ns is our statistic of interest. We are interested in the true value of s in the population (s_{true}).\nWhat we can actually calculate is \\hat{s} the value of s in our sample.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#specify-hypotheses-5",
    "href": "Week4/05-hyptest.html#specify-hypotheses-5",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Our hypothesis, plainly stated, is:\nThere is a relationship between job salary and the likelihood of automation. Expressed another way, the average salary of high risk jobs is different from that of low risk jobs.\nWhat are the Null Hypothesis H_0 and Alternative Hypothesis H_A ?",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#specify-hypotheses-6",
    "href": "Week4/05-hyptest.html#specify-hypotheses-6",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "There is no difference in the average salary between those jobs at high risk of automation, and those at low risk: \nH_0: s_{true} = 0\n\n\n\n\n\nThe difference in the average salary of high risk jobs and low risk jobs is not zero: \nH_A: s_{true} \\neq 0\n\n\n\n\nThe null says that the true value of the statistic is zero; the alternative says that it’s not zero. Together, these cover all logical possibilities for the true value of the statistic.\nIt may seem odd to have H_0: s_{true} = 0 when, presumably, we analyze the data because we suspect that the true value of s is not zero. This seemingly twisted logic comes from the fact that testing a hypothesis amounts to seeing if there is enough evidence in our data to reject the null. It is sometimes said that the null is protected: it should not be too easy to reject it otherwise the conclusions of hypothesis testing would not be strong.\nAs we introduce the concept of hypothesis testing, it is helpful to relate its logic to the logic of a criminal court procedure. At court the task is to decide whether an accused person is guilty or innocent of a certain crime. In most modern societies the starting point is the assumption of innocence: the accused person should be judged guilty only if there is enough evidence against their innocence. This is so even though the accused person was brought before court presumably because there was a suspicion of their guilt. To translate this procedure to the language of hypothesis testing, H0 is that the person is innocent, and HA is that the person is guilty.\nMedical tests are another instructive example. When testing whether a person has a certain medical condition, the null is that the person does not have the condition (healthy), and the alternative is that they have it (sick). The testing procedure amounts to gathering information to see if there is evidence to decide that the person has the condition.\nThe case when we test if HA: strue ≠ 0 is called a two-sided alternative as it allows for strue to be either greater than zero or less than zero. For instance, we focus on the difference in online and offline prices, with H0 being the equality. In such a case we are not really interested if the difference is positive or not, or whether it is negative or not.\nThe other case is working with a one-sided alternative, when we are indeed interested if a statistic is positive. The null and the alternative should be set up so that the hypothesis we are truly interested in is in the alternative set. So when we want to know if strue is positive, we want to put strue &gt; 0 in the alternative thus, making the null strue ≤ 0:\n\nH_0: s_{true} \\leq 0\n\n\nH_A: s_{true} &gt; 0",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#using-the-sample-to-test-the-null-hypothesis",
    "href": "Week4/05-hyptest.html#using-the-sample-to-test-the-null-hypothesis",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Once you have defined your hypotheses the next step in the process, is to collect sample data.\nIn this case, we already have the data in ai_jobs. Before moving on to actually testing the hypothesis, let’s take the naive approach - just calculate \\hat{s} the difference between the two groups.\n\nCodemean_high &lt;- mean(ai_jobs_high$salary_usd)\nmean_low &lt;- mean(ai_jobs_low$salary_usd)\ns_hat &lt;- mean_high - mean_low\n\n\n\n\\hat{s} = \\overline{\\text{salary}}_\\text{high risk} - \\overline{\\text{salary}}_{low risk} = \\$81.7 k - \\$99.7k = \\textbf{-18.1k}",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#compute-the-test-statistic",
    "href": "Week4/05-hyptest.html#compute-the-test-statistic",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "There are several statistical tests used in Hypothesis Testing. Which one you use depends on what type of hypothesis you are testing and what kind of data you have.\nFor this example, where we are testing the difference in means of a numerical variable (salary_usd) across different groups (automation_risk), we use a test called the t-test.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#compute-the-test-statistic-1",
    "href": "Week4/05-hyptest.html#compute-the-test-statistic-1",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Following the logic of hypothesis testing, we start from the assumption that the null (H_0) is true and thus s_{true} = 0.\nWe look at the evidence to see if we want to reject this null or maintain our assumption that it’s true.\nThe evidence we look for is how far the estimated value \\hat{s} is from zero.\nWe reject H_0 if the distance is large (i.e. \\hat{s} is sufficiently greater or lesser than 0)\n\n\nHow far is far enough?\nThe test statistic is the measure of how far the estimated value \\hat{s} is from what its true value would be if H_0 is true.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#compute-the-test-statistic-2",
    "href": "Week4/05-hyptest.html#compute-the-test-statistic-2",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "t-statistic:\n\nt = \\frac{\\hat{s}}{\\mathrm{SE}(\\hat{s})} = \\frac{\\bar{x}_A - \\bar{x}_B}{\\mathrm{SE}(\\bar{x}_A - \\bar{x}_B)}\n\n\nThe t-test is a procedure to decide whether we can reject the null H_0.\nThe magnitude of the t-statistic t measures the distance of \\hat{s} from what s_{true} would be if the null were true.\n\nThe unit of distance is the standard error.\n\n\nThe t-statistic transforms the original statistic of interest into a standardized version\n\nFor example: if t = 1 (or -1), it means \\hat{s} is exactly one standard error away from zero.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#compute-the-test-statistic-3",
    "href": "Week4/05-hyptest.html#compute-the-test-statistic-3",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "R makes it very easy to apply a test such as the t-test. For most statistical tests, there exists a simple function to compute it.\nFor the t-test, we use the t.test() function:\n\nCodet_res &lt;- t.test(ai_jobs_high$salary_usd, ai_jobs_low$salary_usd)\n\n\nt = -6.54",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#making-a-decision",
    "href": "Week4/05-hyptest.html#making-a-decision",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "The following step is making a decision: either rejecting the null or not rejecting it.\nIn hypothesis testing, this decision is based on a clear rule specified in advance.\nWe specify in advance to avoid bias - before looking at the data, we state what it would take to reject the null hypothesis. We follow what the data says, whatever result that may be.\n\n\nA clear rule also makes the decision transparent, which helps avoid biases in the decision. Unfortunately, we humans are often tempted to use evidence to support our pre-existing views or prejudices. If, for example, we think that jobs which are more highly valued by companies (i.e. have a higher salary) we may pay more attention to the evidence that supports that belief than to the evidence against it.\nIn partiular, we may be tempted to say that the estimated \\hat{s} difference is large enough to reject the null, because we believe that the null isn’t true. Clear decision rules are designed to minimize the room for such temptations.\n\n\nOnce you have your test statistic there are two methods to use it to make your decision:\n\nCritical value method\nP-Value method – This is the preferred method we mostly will focus on.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#critical-values",
    "href": "Week4/05-hyptest.html#critical-values",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "We use a critical value to tell us whether the test statistic is large enough - is it far enough away from zero to reject the null?\nTo define the critical value, we need to decide how conservative we want to be with the evidence.\nThe larger we set the critical value, the harder it is to reject the null hypothesis.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#critical-values-1",
    "href": "Week4/05-hyptest.html#critical-values-1",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "As with the sampling distribution for means we looked at earlier, our test-statistic t also has a sampling distribution. If we were to sample many times and calculate t for each sample, we would again get a distribution with a specific shape and parameters.\n\n\nThe sampling distribution of the test statistic when the null is true.\n\nRecall: Approximately 95% of values fall within two standard deviations of the distribution.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#critical-values-2",
    "href": "Week4/05-hyptest.html#critical-values-2",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "The sampling distribution of the test statistic when the null is true.\n\n\nRecall: Approximately 95% of values fall within two standard deviations of the distribution.\nSince 95% of values fall within 2 SD, if we want to reject the null hypothesis with 95% confidence, then we say that our test statistic must fall outside of 2 SD.\nIn other words, since the units of \\hat{s} are standard deviations: \\hat{s} \\geq \\pm 2",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#critical-values-3",
    "href": "Week4/05-hyptest.html#critical-values-3",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "A critical value of 2 is standard. However, it is ultimately just a convention. We could choose to set other critical values that correspond to different probabilities. There is not anything inherently special about setting our threshold at 95% vs 90%.\nIf we make the critical value \\pm 1.6, the chance of a false positive is 10%.\n\n\n\n\n\nDifferent fields have different standards for evidence - for instance, a critical value of 5 (99.994%) is standard in particle physics (referred to as 5\\sigma).",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#interpret-our-results",
    "href": "Week4/05-hyptest.html#interpret-our-results",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Since our test statistic t = -7.5 &lt; -2, at a confidence level of 95%, we would have sufficient evidence to reject H_0\nTherefore, we would say:\n\nThe average salary of jobs at high risk of automation is not the same as the average salary of jobs at low risk.\nWe have evidence that higher salary jobs are at less risk of automation than low salary jobs.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#interpret-our-results-1",
    "href": "Week4/05-hyptest.html#interpret-our-results-1",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "This does not inherently mean we accept the alternative hypothesis. We are narrowing the realm of possible answers, but very rarely (perhaps never) are we able to statistically prove a single explanation in one go.\nWe have increased our reasons to believe our hypothesis, but several other possibilities exist.\nScience is then the process of continually investigating our hypothesis and pitting it against new null hypotheses and rejecting them as well.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#p-value-method",
    "href": "Week4/05-hyptest.html#p-value-method",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Hopefully, the critical value is fairly intuitive to you now. However, it is not the typical way that statistical results are presented.\nInstead, you will typically see something called a p-value.\n\n\np-value: The probability than an event will occur, assuming the null hypothesis is true.\nThe p-value essentially flips the critical value statement:\n\nInstead of saying a test statistic value &gt; 2 falls outside the 95% bound, we calculate where out test statistic falls in the distribution\n\n\nThe p-value is the probability that the test statistic will be as large, or larger, than we calculate from the data, if the null hypothesis is true. i.e. P(data | H_0).",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#p-value",
    "href": "Week4/05-hyptest.html#p-value",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "p = P(|t| &gt; \\text{critical value})\nBecause the p-value tells us the smallest level of significance at which we can reject the null hypothesis, it summarizes all the information we need to make the decision.\nThis is why the p-value is used - rather than needing to set a critical value and calculate the test statistic, we can instead use just the p-value.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#interpreting-the-p-value",
    "href": "Week4/05-hyptest.html#interpreting-the-p-value",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Like with the critical value, we should set our desired significance level before carrying out the analysis.\nWe then compare our calculated p-value with the significance level. If it is less, we reject the null hypothesis.\nThe significance level (\\alpha) is the probability that a true null hypothesis will be rejected.\nA typical significance level is \\alpha &lt; 0.05, which corresponds with a critical value of 2, or a probability of 5%.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#interpreting-the-p-value-1",
    "href": "Week4/05-hyptest.html#interpreting-the-p-value-1",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "R output\nT distribution\n\n\n\nAgain, R will provide us with the p-value. Let’s now look at the full output from our t.test():\n\nCodet.test(ai_jobs_high$salary_usd, ai_jobs_low$salary_usd)\n\n\n    Welch Two Sample t-test\n\ndata:  ai_jobs_high$salary_usd and ai_jobs_low$salary_usd\nt = -6.5366, df = 288.07, p-value = 2.854e-10\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -23513.51 -12630.27\nsample estimates:\nmean of x mean of y \n 81673.57  99745.46 \n\n\n\nIf our p-value is smaller than our pre-set significance level (\\alpha), we reject the null hypothesis and can say the result is “statistically significant” at p &lt; 0.05.\n\n\n\n\nCodexpos &lt;- seq(-5, 5, by = 0.01)\n\ndegree &lt;- 280\nypos &lt;- dt(xpos, df = degree)\n\nggplot() +\n  xlim(-8, 8) +\n  geom_function(aes(colour = \"t, df=280\"), fun = dt, args = list(df = 280), linewidth = 1.5) +\n  geom_vline(xintercept = t_res$statistic, color = \"black\", linewidth = 1.5) +\n  xlab(\"Test statistic under the null hypothesis P(data | H0)\") +\n  ylab(\"Frequency\") +\n  ggtitle(\"Student's T distribution\")",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#decision-and-conclusion",
    "href": "Week4/05-hyptest.html#decision-and-conclusion",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "The preset \\alpha is the probability of a False Positive error (called a Type I error) - rejecting the null hypothesis when the null hypothesis is true.\nBack to our two possible decisions:\n\n\nIf \\alpha &gt; \\text{p-value}, reject H_0.\n\nThe results of the sample are statistically significant.\nWe can say there is sufficient evidence to conclude that H_0 is an incorrect believe and that the alternative hypothesis, H_A may be correct.\n\n\n\nIf \\alpha &lt; \\text{p-value}, fail to reject H_0.\n\nThe results of the sample are not significant. There is not sufficient evidence to conclude that the alternative hypothesis H_A may be correct.\n\n\n\n\nNOTE: When you “do not reject H_0”, it does not mean that you should believe that H_0 is true. It simply means that the sample data have failed to provide sufficient evidence to cast serious doubt about the truthfulness of H_0.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#closing---what-does-a-statistically-significant-result-mean",
    "href": "Week4/05-hyptest.html#closing---what-does-a-statistically-significant-result-mean",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Effect size\nSample size\n\n\n\nNo. There is an essential distinction between statistical significance and practical significance.\nLet’s say we performed an experiment to examine the effect of a particular diet on body weight, which gives a statistically significant effect at p &lt; 0.05. This doesn’t tell us how much weight was lost, which we refer to as the effect size.\nWould the loss of 20 grams (i.e. the weight of a few potato chips) be practically significant, even if it were statistically significant?\nWhether a result is practically significant depends on the effect size and the context of the research question. It’s up to the researcher to know whether it is meaningful.\n\n\nAs with the standard error (and a direct result of it), the p-value depends on the sample size. A very large sample size will give a statistically significant result in many cases, even with a very small effect size.\n\n\nThe proportion of significant results for a very small change (~20g which is about 0.001 standard deviations) as a function of sample size",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/lecture.html#overview",
    "href": "Week4/lecture.html#overview",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Overview",
    "text": "Overview\n\nThree major goals of statistics:\nDescribe\nDecide\nPredict"
  },
  {
    "objectID": "Week4/lecture.html#focus-statistical-decision-making",
    "href": "Week4/lecture.html#focus-statistical-decision-making",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Focus: Statistical Decision Making",
    "text": "Focus: Statistical Decision Making\n\nUsing statistics to make decisions about hypotheses\nSpecifically: Null Hypothesis Statistical Testing (NHST)\nEssential for understanding research results\nImportant to understand both uses and limitations"
  },
  {
    "objectID": "Week4/lecture.html#what-well-cover",
    "href": "Week4/lecture.html#what-well-cover",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "What We’ll Cover",
    "text": "What We’ll Cover\n\nIntroduction to NHST\nSteps in hypothesis testing\nTest statistics and distributions\nP-values and their interpretation\nStatistical significance\n\n\nThis chapter introduces the concepts behind using statistics to make decisions – specifically, decisions about whether particular hypotheses are supported by data.\nKey points to emphasize:\n\nNHST is widely used but has limitations\nUnderstanding both uses and criticisms is essential\nFocus on practical application and interpretation"
  },
  {
    "objectID": "Week4/lecture.html#using-statistics-to-make-decisions",
    "href": "Week4/lecture.html#using-statistics-to-make-decisions",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Using Statistics to make decisions",
    "text": "Using Statistics to make decisions\nToday, we’ll discuss the use of statistics to make decisions – in particular, decisions about whether a particular hypothesis is supported by the data. There are three major goals of statistics:\n\nDescribe\nDecide\nPredict\n\nWe’ll cover:\n\nStatistical inference - Using a sample to generalize (or infer) about the population.\nSampling distributions and standard error - What does it actually mean to analyse a sample?\nConfidence interval - How certain are we about our estimate?\nHypothesis Testing - How do we use data to answer a hypothesis and make a decision?"
  },
  {
    "objectID": "Week4/lecture.html#learning-objectives",
    "href": "Week4/lecture.html#learning-objectives",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\n\n\n\nUnderstand point estimation\nApply and interpret the Central Limit Theorem\nConstruct and interpret confidence intervals for means\nUnderstand the behaviour of confidence intervals\nCarry out hypothesis tests for means (t-test)\nUnderstand the probabilities of error in hypypothesis tests\n\n\n\n\n\n\nIf you want to figure out the distribution of the change people carry in their pockets, and your sample is large enough, you will find that the distribution follows certain patterns."
  },
  {
    "objectID": "Week4/lecture.html#statistical-inference-1",
    "href": "Week4/lecture.html#statistical-inference-1",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Statistical Inference",
    "text": "Statistical Inference\nThe goal of statistical inference is to generalise - to make statements about a population based on a sample.\nStatistical inference uses what we know about probability to make our best “guesses” from samples about what we don’t know about the population."
  },
  {
    "objectID": "Week4/lecture.html#statistical-inference-2",
    "href": "Week4/lecture.html#statistical-inference-2",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Statistical Inference",
    "text": "Statistical Inference\nMain forms of statistical inference\n\n\nPoint estimation\n\nUsing sample data to calculate a single statistic as an estimate of an unknown population parameter\nExample: What is the average height of undergraduates at this university? What is the average construction cost of an office building in London? What was it in 2019?\n\nConfidence intervals\n\nAn interval built around a point estimate for an unknown population parameter.\n\nHypothesis testing\n\nA decision making procedure for determining whether sample evidence supports a hypothesis.\n\n\n\n\nThese three examples make up the main forms of statistical inference. However, there are many other forms of statistical inference, such as regression analysis - e.g. How much does building energy use change as occupancy increases?"
  },
  {
    "objectID": "Week4/lecture.html#point-estimation",
    "href": "Week4/lecture.html#point-estimation",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Point Estimation",
    "text": "Point Estimation\n\nSuppose you were trying to determine the mean rent of a two-bedroom apartment in your town. You might look in the classified section of the newspaper, write down several rents listed, and average them together. You would have obtained a point estimate of the true mean. If you are trying to determine the percentage of times you make a basket when shooting a basketball, you might count the number of shots you make and divide that by the number of shots you attempted. In this case, you would have obtained a point estimate for the true proportion.\n\nThe most natural way to estimate features of the population (parameters) is to use the corresponding summary statistic calculated from the sample. Some common point estimates and their corresponding parameters are found in the following table:\n\n\nParameters and Point Estimates\n\n\n\n\n\n\n\nParameter\nMeasure\nStatistic\n\n\n\n\n\\(\\mu\\)\nMean of a single population\n\\(\\bar{x}\\)\n\n\n\\(p\\)\nProportion of a single population\n\\(\\hat{p}\\)\n\n\n\\(\\mu_D\\)\nMean difference of two dependent populations\n\\(\\bar{x}_D\\)\n\n\n\\(\\mu_1 - \\mu_2\\)\nDifference in means of two independent populations\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n\n\n\\(p_1 - p_2\\)\nDifference in proportions of two population\n\\(\\hat{p}_1 - \\hat{p}_2\\)\n\n\n\\(\\sigma^2\\)\nVariance of a single population\n\\(S^2\\)\n\n\n\\(\\sigma\\)\nStandard deviation of a single population\n\\(S\\)"
  },
  {
    "objectID": "Week4/lecture.html#point-estimation-1",
    "href": "Week4/lecture.html#point-estimation-1",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Point Estimation",
    "text": "Point Estimation\n\n\n\n\nParameters and Point Estimates\n\n\nParameter\nStatistic\n\n\n\n\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\n\\(p\\)\n\\(\\hat{p}\\)\n\n\n\\(\\mu_1 - \\mu_2\\)\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n\n\n\\(p_1 - p_2\\)\n\\(\\hat{p}_1 - \\hat{p}_2\\)\n\n\n\\(\\sigma^2\\)\n\\(S^2\\)\n\n\n\\(\\sigma\\)\n\\(S\\)\n\n\n\n\n\nSuppose the mean weight of a sample of 60 adults is 173.3 lbs; this sample mean is a point estimate of the population mean weight, \\(\\mu\\).\nRemember: this is one of many samples that we could have taken from the population.\nIf a different random sample of 60 individuals were taken from the same population, the new sample mean would likely be different as a result of sampling variability. While estimates generally vary from one sample to another, the population mean is a fixed value.\n\n\nSuppose a poll suggested the US President’s approval rating is 45%. We would consider 45% to be a point estimate of the approval rating we might see if we collected responses from the entire population. This entire-population response proportion is generally referred to as the parameter of interest. When the parameter is a proportion, it is often denoted by p, and we often refer to the sample proportion as \\(\\hat{p}\\) (pronounced “p-hat”). Unless we collect responses from every individual in the population, p remains unknown, and we use $ as our estimate of p.\nHow would one estimate the difference in average weight between men and women? Suppose a sample of men yields a mean of 185.1 lbs and a sample of women men yields a mean of 162.3 lbs. What is a good point estimate for the difference in these two population means? We will expand on this in following chapters."
  },
  {
    "objectID": "Week4/lecture.html#unbiased-estimation",
    "href": "Week4/lecture.html#unbiased-estimation",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Unbiased Estimation",
    "text": "Unbiased Estimation\n\nSampling variability\nWe have established that different samples yield different statistics due to sampling variability. These statistics have their own distributions, called sampling distributions, that reflect this as a random variable. The sampling distribution of a sample statistic is the distribution of the point estimates based on samples of a fixed size, n, from a certain population. It is useful to think of a particular point estimate as being drawn from a sampling distribution.\nRecall the sample mean weight calculated from a previous sample of 173.3 lbs. Suppose another random sample of 60 participants might produce a different value of x, such as 169.5 lbs. Repeated random sampling could result in additional different values, perhaps 172.1 lbs, 168.5 lbs, and so on. Each sample mean can be thought of as a single observation from a random variable X. The distribution of X is called the sampling distribution of the sample mean, and has its own mean and standard deviation like the random variables discussed previously. We will simulate the concept of a sampling distribution using technology to repeatedly sample, calculate statistics, and graph them. However, the actual sampling distribution would only be attainable if we could theoretically take an infinite amount of samples.\nEach of the point estimates in the table above have their own unique sampling distributions which we will look at in the future\n\n\nWhat makes a statistical estimate of this parameter of interest a “Good” one? It must be both accurate and precise.\nAlthough variability in samples is present, there remains a fixed value for any population parameter.\n\nAccording to the law of large numbers, probabilities converge to what we expect over time.\nPoint estimates follow this rule, becoming more accurate with increasing sample size."
  },
  {
    "objectID": "Week4/lecture.html#example-dataset---nhanes",
    "href": "Week4/lecture.html#example-dataset---nhanes",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Example Dataset - NHANES",
    "text": "Example Dataset - NHANES\nNational Health and Nutrition Examination Survey (NHANES) from the US Centers for Disease Control (CDC)\n\n\n\n\n\n\n\n\nNHANES Dataset\n\n\n\nSurveyYr\nGender\nAge\nRace1\nEducation\nWeight\nHeight\nPulse\nDiabetes\n\n\n\n\n1\n2009_10\nmale\n34\nWhite\nHigh School\n87.4\n164.7\n70\nNo\n\n\n2\n2009_10\nfemale\n49\nWhite\nSome College\n86.7\n168.4\n86\nNo\n\n\n3\n2009_10\nfemale\n45\nWhite\nCollege Grad\n75.7\n166.7\n62\nNo\n\n\n4\n2009_10\nmale\n66\nWhite\nSome College\n68.0\n169.5\n60\nNo\n\n\n5\n2009_10\nmale\n58\nWhite\nCollege Grad\n78.4\n181.9\n62\nNo\n\n\n6..4785\n\n\n\n\n\n\n\n\n\n\n\n4786\n2011_12\nmale\n60\nWhite\nCollege Grad\n78.4\n168.8\n76\nYes"
  },
  {
    "objectID": "Week4/lecture.html#unbiased-estimation-1",
    "href": "Week4/lecture.html#unbiased-estimation-1",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Unbiased Estimation",
    "text": "Unbiased Estimation\n\n\nThe accuracy of an estimate refers to how well it estimates the actual value of that parameter. Mathematically, this is true when that the expected value your statistic is equal to the value of that parameter. This can be visualized as the center of the sampling distribution appearing to be situated at the value of that parameter.\nAccording to the law of large numbers, probabilities converge to what we expect over time. Point estimates follow this rule, becoming more accurate with increasing sample size. The figure above shows the sample mean weight calculated for random samples drawn, where sample size increases by 1 for each draw until sample size equals 500. The maroon dashed horizontal line is drawn at the average weight of all adults 169.7 lbs, which represents the population mean weight according to the CDC.\nThe figure above shows the sample mean weight calculated for random samples drawn, where sample size increases by 1 for each draw until sample size equals 500. The maroon dashed horizontal line is drawn at the average weight of all adults 169.7 lbs, which represents the population mean weight according to the CDC.\nNote how a sample size around 50 may produce a sample mean that is as much as 10 lbs higher or lower than the population mean. As sample size increases, the fluctuations around the population mean decrease; in other words, as sample size increases, the sample mean becomes less variable and provides a more reliable estimate of the population mean.\nIn addition to accuracy, a precise estimate is also more useful. This means when repeatedly sampling, the values of the statistics seem pretty close together. The precision of an estimate can be visualized as the spread of the sampling distribution, usually quantified by the standard deviation. The phrase “the standard deviation of a sampling distribution” is often shortened to the standard error. A smaller standard error means a more precise estimate and is also effected by sample size."
  },
  {
    "objectID": "Week4/lecture.html#central-limit-theorem",
    "href": "Week4/lecture.html#central-limit-theorem",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nThe central limit theorem (CLT) is one of the most powerful and useful ideas in all of statistics. There are two alternative forms of the theorem, and both alternatives are concerned with drawing finite samples size n from a population with a known mean, \\(\\mu\\), and a known standard deviation, \\(\\sigma\\). The first alternative says that if we collect samples of size n with a “large enough n,” then the resulting distribution can be approximated by the normal distribution.\nApplying the law of large numbers here, we could say that if you take larger and larger samples from a population, then the mean {x} of the sample tends to get closer and closer to \\(\\mu\\). From the central limit theorem, we know that as n gets larger and larger, the sample means follow a normal distribution. The larger n gets, the smaller the standard deviation gets. (Remember that the standard deviation for {x} is .) This means that the sample mean {x} must be close to the population mean \\(\\mu\\). We can say that \\(\\mu\\) is the value that the sample means approach as n gets larger. The central limit theorem illustrates the law of large numbers.\nThe size of the sample, n, that is required in order to be “large enough” depends on the original population from which the samples are drawn (the sample size should be at least 30 or the data should come from a normal distribution). If the original population is far from normal, then more observations are needed for the sample means or sums to be normal. Sampling is done with replacement.\n\nThe CLT means says that if you keep drawing larger and larger samples and calculating their means, the sample means form their own normal distribution (the sampling distribution).\nThe sampling distribution of the mean is generated by repeated sampling from the same population and recording the sample mean per sample. This forms a distribution of different means, and this distribution has its own mean and variance.\nThe normal distribution has the same mean as the original distribution and a variance that equals the original variance divided by the sample size."
  },
  {
    "objectID": "Week4/lecture.html#drawing-samples-of-peoples-weight-from-the-nhanes-dataset.",
    "href": "Week4/lecture.html#drawing-samples-of-peoples-weight-from-the-nhanes-dataset.",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Drawing samples of people’s weight from the NHANES dataset.",
    "text": "Drawing samples of people’s weight from the NHANES dataset.\n\nWe have established that different samples yield different statistics due to sampling variability . These statistics have their own distributions, called sampling distributions, that reflect this as a random variable. The sampling distribution of a sample statistic is the distribution of the point estimates based on samples of a fixed size, n, from a certain population. It is useful to think of a particular point estimate as being drawn from a sampling distribution.\nRecall the sample mean weight calculated from a previous sample of 173.3 lbs. Suppose another random sample of 60 participants might produce a different value of x, such as 169.5 lbs. Repeated random sampling could result in additional different values, perhaps 172.1 lbs, 168.5 lbs, and so on. Each sample mean can be thought of as a single observation from a random variable X. The distribution of X is called the sampling distribution of the sample mean, and has its own mean and standard deviation like the random variables discussed previously. We will simulate the concept of a sampling distribution using technology to repeatedly sample, calculate statistics, and graph them. However, the actual sampling distribution would only be attainable if we could theoretically take an infinite amount of samples.\nEach of the point estimates in the table above have their own unique sampling distributions which we will look at in the future\n\n\n\nWe are drawing a random sample of people from the dataset and calculating the mean weight for that sample. Sample size is the number of data points we pull. We then repeat this 5000 times (n_samples) to build up the sampling distribution."
  },
  {
    "objectID": "Week4/lecture.html#standard-error",
    "href": "Week4/lecture.html#standard-error",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Standard Error",
    "text": "Standard Error\nA sampling distribution is what we get by simulating multiple samples (of sample size \\(n\\)) from a population.\nRecall: The Standard Error is the standard deviation of the sampling distribution.\n\\[\nSEM = \\sigma_{\\bar{x}\\ (means)}\n\\]"
  },
  {
    "objectID": "Week4/lecture.html#standard-error-1",
    "href": "Week4/lecture.html#standard-error-1",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Standard Error",
    "text": "Standard Error\nA sampling distribution is a probability distribution of a statistic at a given sample size.\nRecall: The Standard Error is the standard deviation of the sampling distribution. This is also equal to the standard deviation \\(\\sigma\\) of the population divided by the square root of the sample size.\n\\[\nSEM = \\sigma_{\\bar{x}\\ (means)} = \\frac{\\sigma}{\\sqrt{n}} \\approx \\frac{\\sigma_{x}}{\\sqrt{n}} \\left[ i.e. \\frac{\\text{Est. Std Dev of the sample}}{\\sqrt{\\text{Sample size}}} \\right]\n\\]\n\nIn other words:\n\nIf you draw random samples of size \\(n\\), the distribution of the random variable \\(\\bar{X}\\), which consists of sample means, is called the sampling distribution of the sample mean. The sampling distribution of the mean approaches a normal distribution as \\(n\\), the sample size, increases.\n\n\n\nIn the SEM formula, remember the sampling distribution is the distribution of multiple means - not the distribution of our sample.\nQuote from https://pressbooks.lib.vt.edu/introstatistics/chapter/the-central-limit-theorem-for-sample-means-averages/"
  },
  {
    "objectID": "Week4/lecture.html#standard-error-2",
    "href": "Week4/lecture.html#standard-error-2",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Standard Error",
    "text": "Standard Error\nKey Takeaways\n\nA sampling distribution is what we get by simulating multiple samples from a population.\nThe Standard Error is the standard deviation \\(\\sigma_{\\bar{x}}\\) of the sampling distribution.\nThe SE decreases as the sample size \\(n\\) increases.\nBecause of this relationship - we can estimate the SE from a single sample \\(\\frac{\\sigma_x}{\\sqrt{n}}\\)\n\n\n\\[\nSEM = \\sigma_{\\bar{x}\\ (means)} = \\frac{\\sigma}{\\sqrt{n}} \\approx \\frac{\\sigma_{x}}{\\sqrt{n}} \\left[ i.e. \\frac{\\text{Est. Std Dev of the sample}}{\\sqrt{\\text{Sample size}}} \\right]\n\\]"
  },
  {
    "objectID": "Week4/lecture.html#exercise-dataset",
    "href": "Week4/lecture.html#exercise-dataset",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Exercise Dataset",
    "text": "Exercise Dataset\n\nData OverviewData Preview\n\n\nThe “AI-Powered Job Market Insights” dataset provides a snapshot of the modern job market, particularly focusing on the role of artificial intelligence (AI) and automation across various industries.\nThis dataset includes 500 unique job listings, each characterized by different factors like industry, company size, AI adoption level, automation risk, required skills, and job growth projections.\n\n\n\n\n\n\n\n\n\n\nAI-Powered Job Market Insights\n\n\n\njob_title\nindustry\ncompany_size\nlocation\nai_adoption_level\nautomation_risk\nrequired_skills\nsalary_usd\nremote_friendly\njob_growth_projection\n\n\n\n\n1\nCybersecurity Analyst\nEntertainment\nSmall\nDubai\nMedium\nHigh\nUX/UI Design\n111392.17\nYes\nGrowth\n\n\n2\nMarketing Specialist\nTechnology\nLarge\nSingapore\nMedium\nHigh\nMarketing\n73792.56\nNo\nDecline\n\n\n3\nAI Researcher\nTechnology\nLarge\nSingapore\nMedium\nLow\nUX/UI Design\n137170.26\nYes\nGrowth\n\n\n4\nSales Manager\nRetail\nSmall\nBerlin\nLow\nMedium\nProject Management\n83027.95\nNo\nGrowth\n\n\n5\nCybersecurity Analyst\nEntertainment\nSmall\nTokyo\nLow\nLow\nJavaScript\n87752.92\nYes\nDecline\n\n\n6..499\n\n\n\n\n\n\n\n\n\n\n\n\n500\nHR Manager\nEntertainment\nMedium\nBerlin\nMedium\nHigh\nProject Management\n53764.38\nYes\nDecline\n\n\n\nSource: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\n\n\n\n\n\n\n\n\n\n\n\n\nTo simplify our later code, I have created a separate table which is already filtered for the groups we will be looking at:\n\nai_jobs_risk &lt;- ai_jobs |&gt;\n  filter(automation_risk %in% c(\"Low\", \"High\"))\n\nai_jobs_high &lt;- ai_jobs_risk |&gt;\n  filter(automation_risk == \"High\")\n\nai_jobs_low &lt;- ai_jobs_risk |&gt;\n  filter(automation_risk == \"Low\")\n\nDataset Features:\nSource: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\n\nJob_Title:\n\nDescription: The title of the job role.\nType: Categorical\nExample Values: “Data Scientist”, “Software Engineer”, “HR Manager”\n\nIndustry:\n\nDescription: The industry in which the job is located.\nType: Categorical\nExample Values: “Technology”, “Healthcare”, “Finance”\n\nCompany_Size:\n\nDescription: The size of the company offering the job.\nType: Ordinal\nCategories: “Small”, “Medium”, “Large”\n\nLocation:\n\nDescription: The geographic location of the job.\nType: Categorical\nExample Values: “New York”, “San Francisco”, “London”\n\nAI_Adoption_Level:\n\nDescription: The extent to which the company has adopted AI in its operations.\nType: Ordinal\nCategories: “Low”, “Medium”, “High”\n\nAutomation_Risk:\n\nDescription: The estimated risk that the job could be automated within the next 10 years.\nType: Ordinal\nCategories: “Low”, “Medium”, “High”\n\nRequired_Skills:\n\nDescription: The key skills required for the job role.\nType: Categorical\nExample Values: “Python”, “Data Analysis”, “Project Management”\n\nSalary_USD:\n\nDescription: The annual salary offered for the job in USD.\nType: Numerical\nValue Range: $30,000 - $200,000\n\nRemote_Friendly:\n\nDescription: Indicates whether the job can be performed remotely.\nType: Categorical\nCategories: “Yes”, “No”\n\nJob_Growth_Projection:\n\nDescription: The projected growth or decline of the job role over the next five years.\nType: Categorical\nCategories: “Decline”, “Stable”, “Growth”"
  },
  {
    "objectID": "Week4/lecture.html#exercise---mystery-bags",
    "href": "Week4/lecture.html#exercise---mystery-bags",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Exercise - Mystery bags",
    "text": "Exercise - Mystery bags\nTo begin, split into three groups. Decide on the following roles, one per person:\n\nSampler - draws samples from the population\nRecording - records the draws in R\n\nYour group has been presented with a population model (bag containing salaries). There may be multiple populations represented in the various bags around the room.\nPlease DO NOT look in the bag OR empty the contents of the bag."
  },
  {
    "objectID": "Week4/lecture.html#exercise---mystery-bags-nonincremental",
    "href": "Week4/lecture.html#exercise---mystery-bags-nonincremental",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Exercise - Mystery bags {nonincremental}",
    "text": "Exercise - Mystery bags {nonincremental}\nThe task\n\nThe Sampler draws 10 slips from the bag - this is your sample of size 5.\nThe Recorder records the values on the slips into the sampling-exercise.R file.\n\nCreate a new list for each sample (sample_1 &lt;- c(1, 2, 3, 2, 2))\n\nCalculate the mean for the new sample and add it to the sample_means table.\nRun the code to plot the histogram of your samples.\nReturn the slips to the bag.\nEnsure the data is well mixed between samples.\nRepeat"
  },
  {
    "objectID": "Week4/lecture.html#statistical-inference-4",
    "href": "Week4/lecture.html#statistical-inference-4",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Statistical Inference",
    "text": "Statistical Inference\nUsing a sample to generalize (or infer) about the population.\n\nWe know how to make a point estimate of a population - what else do we need in order to make a decision?\nHow confident are we that our estimate can generalize to the rest of the population?\nWe need to determine the uncertainty in our estimate\n\n\nWe use inferential statistics to make generalizations about an unknown population. The simplest way of doing this is to use the sample data help us to make a point estimate of a population parameter. We realize that due to sampling variability the point estimate is most likely not the exact value of the population parameter, but should be close to it. After calculating point estimates, we can build off of them to construct interval estimates, called confidence intervals."
  },
  {
    "objectID": "Week4/lecture.html#confidence-intervals",
    "href": "Week4/lecture.html#confidence-intervals",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nA confidence interval is another type of estimate, but instead of being just one number, it is an interval of numbers\n\nProvides a range a range of reasonable values where we expect the true population parameter to fall.\nPoint estimate (statistic) has some variability and uncertainty since we estimate it based on a sample.\nWe want to quantify and communicate this uncertainty.\n\n\nThere is no guarantee that a given confidence interval does capture the parameter, but there is a predictable probability of success. It is important to keep in mind that the confidence interval itself is a random variable, while the population parameter is fixed."
  },
  {
    "objectID": "Week4/lecture.html#business-example",
    "href": "Week4/lecture.html#business-example",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Business Example",
    "text": "Business Example\nAverage streams per month\nYou work in the marketing department of a music company. You want to know the mean number of songs a consumer streams per month.\n\nYou conduct a survey of 100 customers and calculate the sample mean (\\(\\bar{x}\\)) and use it as the point estimate for the population mean (\\(\\mu\\))\nSuppose we know that the standard deviation \\(\\sigma = 100\\).\nFollowing the Central Limit Theorem, the Standard Error is:\n\n\n\\[\n\\frac{\\sigma}{\\sqrt{n}} = \\frac{100}{\\sqrt{100}} = 10\n\\]\n\n\nIf you worked in the marketing department of an entertainment company, you might be interested in the mean number of songs a consumer downloads a month from iTunes. If so, you could conduct a survey and calculate the sample mean, \\(\\bar{x}\\). You would use \\(\\bar{x}\\) to estimate the population mean. The sample mean, \\(\\bar{x}\\), is the point estimate for the population mean, μ.\nSuppose, for the iTunes example, we do not know the population mean μ, but we do know that the population standard deviation is \\(\\sigma = 100\\) and our sample size is 100. Then, by the central limit theorem, the standard deviation for the sample mean is \\(\\frac{\\sigma }{\\sqrt{n}}=\\frac{100}{\\sqrt{100}}=10\\)."
  },
  {
    "objectID": "Week4/lecture.html#what-is-the-probability-of-sampling-a-certain-mean-value",
    "href": "Week4/lecture.html#what-is-the-probability-of-sampling-a-certain-mean-value",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "What is the probability of sampling a certain mean value?",
    "text": "What is the probability of sampling a certain mean value?\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Empirical Rule says that in approximately 95% of the samples, the sample mean, \\(\\bar{x}\\), will be within two standard deviations of the population mean \\(\\mu\\) .\nFor our example, two standard deviations is \\((2)(10) = 20\\). The sample mean \\(\\bar{x}\\) is likely to be within 20 units of \\(\\mu\\).\nBecause \\(\\bar{x}\\) is within 20 units of \\(\\mu\\), which is unknown, then \\(\\mu\\) is likely to be within 20 units of \\(\\bar{x}\\) in 95% of the samples.\n\n\n\nBecause \\(\\bar{x}\\) is within 0.2 units of μ, which is unknown, then μ is likely to be within 0.2 units of \\(\\bar{x}\\) in 95% of the samples. The population mean μ is contained in an interval whose lower number is calculated by taking the sample mean and subtracting two standard deviations (2)(0.1) and whose upper number is calculated by taking the sample mean and adding two standard deviations. In other words, μ is between \\(\\bar{x}\\text{ }-\\text{ 0}\\text{.2}\\) and \\(\\bar{x}\\text{ }+\\text{ 0}\\text{.2}\\) in 95% of all the samples."
  },
  {
    "objectID": "Week4/lecture.html#calculate-the-confidence-interval",
    "href": "Week4/lecture.html#calculate-the-confidence-interval",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Calculate the Confidence Interval",
    "text": "Calculate the Confidence Interval\nWe want to calculate the range of values which the true mean is likely to fall within 95% of the time, given our sample.\n\nFor the streaming example, suppose that a sample produced a sample mean \\(\\bar{x} = 200\\). Then the unknown population mean \\(\\mu\\) is between \\(\\bar{x}-20=200-20=180\\) and \\(\\bar{x}+20=200+20=220\\) songs per month.\n\n\nWe can say that we are about 95% confident that the unknown population mean number of songs streamed per month is between 180 and 220. The approximate 95% confidence interval is (1.8, 2.2).\n\n\nConfidence Interval: (Point Estimate \\(\\pm\\) Margin of error) = \\(200 \\pm 20 \\text{ songs}\\)"
  },
  {
    "objectID": "Week4/lecture.html#calculate-the-confidence-interval-1",
    "href": "Week4/lecture.html#calculate-the-confidence-interval-1",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Calculate the Confidence Interval",
    "text": "Calculate the Confidence Interval\n\nBased on our sample, we can say two things. Either:\n\nThe interval (180, 220) contains the true mean \\(\\mu\\), or…\nOur sample prodcued an \\(\\bar{x}\\) that is not within 20 units of the true mean \\(\\mu\\). This would only happen for 5% of the samples.\n\n\n\nWe can say that we are about 95% confident that the unknown population mean number of songs downloaded from iTunes per month is between 1.8 and 2.2. The approximate 95% confidence interval is (1.8, 2.2). This approximate 95% confidence interval implies two possibilities. Either the interval (1.8, 2.2) contains the true mean μ or our sample produced an that is not within 0.2 units of the true mean μ. The second possibility happens for only 5% of all the samples (95–100%).\nRemember that a confidence intervals are created for an unknown population parameter. Confidence intervals for most parameters have the form: (Point Estimate ± Margin of Error) = (Point Estimate – Margin of Error, Point Estimate + Margin of Error) The margin of error (MoE) depends on the confidence level or percentage of confidence and the standard error of the mean. When you read newspapers and journals, some reports will use the phrase “margin of error.” Other reports will not use that phrase, but include a confidence interval as the point estimate plus or minus the margin of error. These are two ways of expressing the same concept. A confidence interval for a population mean with a known standard deviation is based on the fact that the sample means follow an approximately normal distribution. Suppose that our sample has a mean of and we have constructed the 90% confidence interval (5, 15) where MoE = 5."
  },
  {
    "objectID": "Week4/lecture.html#communicating-confidence-intervals",
    "href": "Week4/lecture.html#communicating-confidence-intervals",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Communicating Confidence Intervals",
    "text": "Communicating Confidence Intervals\n\nThe interpretation should clearly state the confidence level (CL), explain what population parameter is being estimated (here the population mean), and state the confidence interval (both endpoints).\n“We can be _______ % confident that the interval we created, _______ to ________ captures the true population mean (include the context of the problem and appropriate units).”\nWe state the range within which our evidence indicates contains the population mean."
  },
  {
    "objectID": "Week4/lecture.html#case-study",
    "href": "Week4/lecture.html#case-study",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Case Study",
    "text": "Case Study\n\nData OverviewData Preview\n\n\nThe “AI-Powered Job Market Insights” dataset provides a snapshot of the modern job market, particularly focusing on the role of artificial intelligence (AI) and automation across various industries.\nThis dataset includes 500 unique job listings, each characterized by different factors like industry, company size, AI adoption level, automation risk, required skills, and job growth projections.\n\n\n\n\n\n\n\n\n\n\nAI-Powered Job Market Insights\n\n\n\njob_title\nindustry\ncompany_size\nlocation\nai_adoption_level\nautomation_risk\nrequired_skills\nsalary_usd\nremote_friendly\njob_growth_projection\n\n\n\n\n1\nCybersecurity Analyst\nEntertainment\nSmall\nDubai\nMedium\nHigh\nUX/UI Design\n111392.17\nYes\nGrowth\n\n\n2\nMarketing Specialist\nTechnology\nLarge\nSingapore\nMedium\nHigh\nMarketing\n73792.56\nNo\nDecline\n\n\n3\nAI Researcher\nTechnology\nLarge\nSingapore\nMedium\nLow\nUX/UI Design\n137170.26\nYes\nGrowth\n\n\n4\nSales Manager\nRetail\nSmall\nBerlin\nLow\nMedium\nProject Management\n83027.95\nNo\nGrowth\n\n\n5\nCybersecurity Analyst\nEntertainment\nSmall\nTokyo\nLow\nLow\nJavaScript\n87752.92\nYes\nDecline\n\n\n6..499\n\n\n\n\n\n\n\n\n\n\n\n\n500\nHR Manager\nEntertainment\nMedium\nBerlin\nMedium\nHigh\nProject Management\n53764.38\nYes\nDecline\n\n\n\nSource: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\n\n\n\n\n\n\n\n\n\n\n\n\nTo simplify our later code, I have created a separate table which is already filtered for the groups we will be looking at:\n\nai_jobs_risk &lt;- ai_jobs |&gt;\n  filter(automation_risk %in% c(\"Low\", \"High\"))\n\nai_jobs_high &lt;- ai_jobs_risk |&gt;\n  filter(automation_risk == \"High\")\n\nai_jobs_low &lt;- ai_jobs_risk |&gt;\n  filter(automation_risk == \"Low\")\n\nDataset Features:\nSource: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\n\nJob_Title:\n\nDescription: The title of the job role.\nType: Categorical\nExample Values: “Data Scientist”, “Software Engineer”, “HR Manager”\n\nIndustry:\n\nDescription: The industry in which the job is located.\nType: Categorical\nExample Values: “Technology”, “Healthcare”, “Finance”\n\nCompany_Size:\n\nDescription: The size of the company offering the job.\nType: Ordinal\nCategories: “Small”, “Medium”, “Large”\n\nLocation:\n\nDescription: The geographic location of the job.\nType: Categorical\nExample Values: “New York”, “San Francisco”, “London”\n\nAI_Adoption_Level:\n\nDescription: The extent to which the company has adopted AI in its operations.\nType: Ordinal\nCategories: “Low”, “Medium”, “High”\n\nAutomation_Risk:\n\nDescription: The estimated risk that the job could be automated within the next 10 years.\nType: Ordinal\nCategories: “Low”, “Medium”, “High”\n\nRequired_Skills:\n\nDescription: The key skills required for the job role.\nType: Categorical\nExample Values: “Python”, “Data Analysis”, “Project Management”\n\nSalary_USD:\n\nDescription: The annual salary offered for the job in USD.\nType: Numerical\nValue Range: $30,000 - $200,000\n\nRemote_Friendly:\n\nDescription: Indicates whether the job can be performed remotely.\nType: Categorical\nCategories: “Yes”, “No”\n\nJob_Growth_Projection:\n\nDescription: The projected growth or decline of the job role over the next five years.\nType: Categorical\nCategories: “Decline”, “Stable”, “Growth”"
  },
  {
    "objectID": "Week4/lecture.html#motivation",
    "href": "Week4/lecture.html#motivation",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Motivation",
    "text": "Motivation\nYou have been tasked with examining the impact of AI skills and company AI adoption on the job market across the world. Think about what we might want to know about this sector.\nWhat might some be some interesting questions we could ask based on this data?\nHow would we answer them? How would we know whether our answer is generalizable?\n\nWe want to generalize the results of our analysis from the data we have to the situation we care about."
  },
  {
    "objectID": "Week4/lecture.html#the-logic-of-testing-hypotheses",
    "href": "Week4/lecture.html#the-logic-of-testing-hypotheses",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "The Logic of Testing Hypotheses",
    "text": "The Logic of Testing Hypotheses\nA hypothesis is a statement about a population, or general pattern.\nTesting a hypothesis amounts to gathering information (sampling) from a dataset and, based on that information, deciding whether that hypothesis is false or true in the population.\nTwo decisions are possible:\n\nRejecting the hypothesis (if there is enough evidence against it)\nNot rejecting it (if there is not enough evidence against it)\n\nRejecting a hypothesis is a more conclusive decision than not rejecting it.\n\nOne such focused approach uses a statistic (e.g. a difference in two means) computed from our data to see whether its true value is equal to something we assume (e.g. the difference is zero). This is called hypothesis testing: using results in the data to see if we have enough evidence to tell whether a hypothesis (the two means are equal) is wrong (they are not equal) or whether we don’t have enough evidence (they may be equal.\nTesting a hypothesis is a way of making an inference, with a focus on a specific statement. As with any kind of inference, we have to assess external validity too: the extent to which the population, or general pattern, represented by our data is the same as the population, or general pattern, we are truly interested in.\n\nGiven a population (i.e. a distribution) with a parameter of interest (which could be the mean, variance, correlation, etc.), we would like to decide between to complementary statements concerning the parameter.\nThese statements are called statistical hypotheses.\nThe choice or decision between these hypotheses is to be based on a sample taken from the population of interest.\nThe goal is to be able to choose which hypothesis is true in reality based on the sample data."
  },
  {
    "objectID": "Week4/lecture.html#steps-of-analysis-hypothesis-testing",
    "href": "Week4/lecture.html#steps-of-analysis-hypothesis-testing",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Steps of Analysis & Hypothesis Testing",
    "text": "Steps of Analysis & Hypothesis Testing\n\n\n1. Formulate research question\n2. Specify hypotheses\n\nWhat statistic is appropriate to answer our question?\nNull and Alternative Hypotheses\n\n3. Collect relevant data\n\nWhat information is needed to answer our question?\nWhat is our population? How do we sample from the population in a statistically valid way?\n\n\n4. Compute test statistic\n\nFit appropriate model\nCalculate test statistic\nAccount for variability\n\n5. Determine probability under the null hypothesis\n6. Assess significance and meaningfulness"
  },
  {
    "objectID": "Week4/lecture.html#formulate-research-question-1",
    "href": "Week4/lecture.html#formulate-research-question-1",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "1. Formulate research question",
    "text": "1. Formulate research question\nThis is where all analysis or research starts - what is it you want to know, and what will you do with that information?\nFor our AI Jobs dataset some research questions we might ask are:\n\nWhich jobs are the highest paid?\nWhich industries have seen the most AI adoption?\nAre lower paid jobs at more risk of being automated?"
  },
  {
    "objectID": "Week4/lecture.html#specify-hypotheses-1",
    "href": "Week4/lecture.html#specify-hypotheses-1",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "2. Specify hypotheses",
    "text": "2. Specify hypotheses\nNow we translate our general research questions into specific and testable hypotheses.\nThe actual test begins by considering two hypotheses. They are called the null hypothesis and the alternative hypothesis. These hypotheses contain opposing viewpoints.\n\nThe null hypothesis (\\(H_0\\)) : It is often a statement of the accepted historical value or norm. This is your starting point that you must assume from the beginning in order to show an effect exists.\nThe alternative hypothesis (\\(H_a\\)) : It is a claim about the population that is contradictory to \\(H_0\\) and what we conclude when we reject \\(H_0\\)"
  },
  {
    "objectID": "Week4/lecture.html#specify-hypotheses-2",
    "href": "Week4/lecture.html#specify-hypotheses-2",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "2. Specify Hypotheses",
    "text": "2. Specify Hypotheses\nAfter you have determined which hypothesis the sample supports, you make a decision.\nThere are two options for a decision:\n\n“reject \\(H_0\\)” if the sample information favors the alternative hypothesis, or\n“do not reject \\(H_0\\)” or “decline to reject \\(H_0\\)” if the sample information is insufficient to reject the null hypothesis.\n\n\nMathematical symbols used in H0 and Ha:\nFigure 6.12: Null and Alternative Hypotheses\n\n\n\n\n\n\n\n\\(H_0\\)\n\\(H_a\\)\n\n\n\n\nequal (=)\nnot equal (\\(\\neq\\)) or greater than (\\(&gt;\\)) or less than (\\(&lt;\\))\n\n\ngreater than or equal to (\\(\\geq\\))\nless than (\\(&lt;\\))\n\n\nless than or equal to (\\(\\leq\\))\nmore than (\\(&gt;\\))"
  },
  {
    "objectID": "Week4/lecture.html#specify-hypotheses-3",
    "href": "Week4/lecture.html#specify-hypotheses-3",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "2. Specify Hypotheses",
    "text": "2. Specify Hypotheses\nDecide on statistic of interest\nDecide what statistic is appropriate to answer our question. What do we need to calculate from our sample?\n\nDifference in mean(salary_usd) for each level of automation_risk\nSince there are three automation risk levels (Low, Medium, High), the comparison we could make are:\n\n\\(\\text{mean}_{\\text{high risk}} - \\text{mean}_{\\text{medium risk}}\\)\n\\(\\text{mean}_{\\text{high}} - \\text{mean}_{\\text{low}}\\)\n\\(\\text{mean}_{\\text{medium}} - \\text{mean}_{\\text{low}}\\)"
  },
  {
    "objectID": "Week4/lecture.html#specify-hypotheses-4",
    "href": "Week4/lecture.html#specify-hypotheses-4",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "2. Specify Hypotheses",
    "text": "2. Specify Hypotheses\nDecide on statistic of interest\nFor now, let’s focus on just (2). Therefore:\n\\(s = \\overline{salary}_{\\text{high risk}} - \\overline{salary}_{\\text{low risk}}\\)\n\\(s\\) is our statistic of interest. We are interested in the true value of \\(s\\) in the population (\\(s_{true}\\)).\nWhat we can actually calculate is \\(\\hat{s}\\) the value of \\(s\\) in our sample."
  },
  {
    "objectID": "Week4/lecture.html#specify-hypotheses-5",
    "href": "Week4/lecture.html#specify-hypotheses-5",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "2. Specify Hypotheses",
    "text": "2. Specify Hypotheses\nExpress the hypotheses mathematically\nOur hypothesis, plainly stated, is:\nThere is a relationship between job salary and the likelihood of automation. Expressed another way, the average salary of high risk jobs is different from that of low risk jobs.\nWhat are the Null Hypothesis \\(H_0\\) and Alternative Hypothesis \\(H_A\\) ?"
  },
  {
    "objectID": "Week4/lecture.html#specify-hypotheses-6",
    "href": "Week4/lecture.html#specify-hypotheses-6",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "2. Specify Hypotheses",
    "text": "2. Specify Hypotheses\nExpress the hypotheses mathematically\n\nNull Hypothesis \\(H_0\\)\nThere is no difference in the average salary between those jobs at high risk of automation, and those at low risk: \\[\nH_0: s_{true} = 0\n\\]\n\n\nAlternative Hypothesis \\(H_a\\)\nThe difference in the average salary of high risk jobs and low risk jobs is not zero: \\[\nH_A: s_{true} \\neq 0\n\\]\n\n\nThe null says that the true value of the statistic is zero; the alternative says that it’s not zero. Together, these cover all logical possibilities for the true value of the statistic.\nIt may seem odd to have \\(H_0: s_{true} = 0\\) when, presumably, we analyze the data because we suspect that the true value of \\(s\\) is not zero. This seemingly twisted logic comes from the fact that testing a hypothesis amounts to seeing if there is enough evidence in our data to reject the null. It is sometimes said that the null is protected: it should not be too easy to reject it otherwise the conclusions of hypothesis testing would not be strong.\nAs we introduce the concept of hypothesis testing, it is helpful to relate its logic to the logic of a criminal court procedure. At court the task is to decide whether an accused person is guilty or innocent of a certain crime. In most modern societies the starting point is the assumption of innocence: the accused person should be judged guilty only if there is enough evidence against their innocence. This is so even though the accused person was brought before court presumably because there was a suspicion of their guilt. To translate this procedure to the language of hypothesis testing, H0 is that the person is innocent, and HA is that the person is guilty.\nMedical tests are another instructive example. When testing whether a person has a certain medical condition, the null is that the person does not have the condition (healthy), and the alternative is that they have it (sick). The testing procedure amounts to gathering information to see if there is evidence to decide that the person has the condition.\nThe case when we test if HA: strue ≠ 0 is called a two-sided alternative as it allows for strue to be either greater than zero or less than zero. For instance, we focus on the difference in online and offline prices, with H0 being the equality. In such a case we are not really interested if the difference is positive or not, or whether it is negative or not.\nThe other case is working with a one-sided alternative, when we are indeed interested if a statistic is positive. The null and the alternative should be set up so that the hypothesis we are truly interested in is in the alternative set. So when we want to know if strue is positive, we want to put strue &gt; 0 in the alternative thus, making the null strue ≤ 0:\n\\[\nH_0: s_{true} \\leq 0\n\\]\n\\[\nH_A: s_{true} &gt; 0\n\\]"
  },
  {
    "objectID": "Week4/lecture.html#using-the-sample-to-test-the-null-hypothesis",
    "href": "Week4/lecture.html#using-the-sample-to-test-the-null-hypothesis",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "3. Using the Sample to Test the Null Hypothesis",
    "text": "3. Using the Sample to Test the Null Hypothesis\nOnce you have defined your hypotheses the next step in the process, is to collect sample data.\nIn this case, we already have the data in ai_jobs. Before moving on to actually testing the hypothesis, let’s take the naive approach - just calculate \\(\\hat{s}\\) the difference between the two groups.\n\nmean_high &lt;- mean(ai_jobs_high$salary_usd)\nmean_low &lt;- mean(ai_jobs_low$salary_usd)\ns_hat &lt;- mean_high - mean_low\n\n\\[\n\\hat{s} = \\overline{\\text{salary}}_\\text{high risk} - \\overline{\\text{salary}}_{low risk} = \\$81.7 k - \\$99.7k = \\textbf{-18.1k}\n\\]"
  },
  {
    "objectID": "Week4/lecture.html#compute-the-test-statistic",
    "href": "Week4/lecture.html#compute-the-test-statistic",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "4. Compute the test statistic",
    "text": "4. Compute the test statistic\nThere are several statistical tests used in Hypothesis Testing. Which one you use depends on what type of hypothesis you are testing and what kind of data you have.\nFor this example, where we are testing the difference in means of a numerical variable (salary_usd) across different groups (automation_risk), we use a test called the t-test."
  },
  {
    "objectID": "Week4/lecture.html#compute-the-test-statistic-1",
    "href": "Week4/lecture.html#compute-the-test-statistic-1",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "4. Compute the test statistic",
    "text": "4. Compute the test statistic\n\nFollowing the logic of hypothesis testing, we start from the assumption that the null (\\(H_0\\)) is true and thus \\(s_{true} = 0\\).\nWe look at the evidence to see if we want to reject this null or maintain our assumption that it’s true.\nThe evidence we look for is how far the estimated value \\(\\hat{s}\\) is from zero.\nWe reject \\(H_0\\) if the distance is large (i.e. \\(\\hat{s}\\) is sufficiently greater or lesser than 0)\n\n\nHow far is far enough?\nThe test statistic is the measure of how far the estimated value \\(\\hat{s}\\) is from what its true value would be if \\(H_0\\) is true."
  },
  {
    "objectID": "Week4/lecture.html#compute-the-test-statistic-2",
    "href": "Week4/lecture.html#compute-the-test-statistic-2",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "4. Compute the test statistic",
    "text": "4. Compute the test statistic\nt-statistic:\n\\[\nt = \\frac{\\hat{s}}{\\mathrm{SE}(\\hat{s})} = \\frac{\\bar{x}_A - \\bar{x}_B}{\\mathrm{SE}(\\bar{x}_A - \\bar{x}_B)}\n\\]\n\nThe t-test is a procedure to decide whether we can reject the null \\(H_0\\).\nThe magnitude of the t-statistic \\(t\\) measures the distance of \\(\\hat{s}\\) from what \\(s_{true}\\) would be if the null were true.\n\nThe unit of distance is the standard error.\n\nThe t-statistic transforms the original statistic of interest into a standardized version\n\nFor example: if \\(t = 1\\) (or -1), it means \\(\\hat{s}\\) is exactly one standard error away from zero."
  },
  {
    "objectID": "Week4/lecture.html#compute-the-test-statistic-3",
    "href": "Week4/lecture.html#compute-the-test-statistic-3",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "4. Compute the test statistic",
    "text": "4. Compute the test statistic\nR makes it very easy to apply a test such as the t-test. For most statistical tests, there exists a simple function to compute it.\nFor the t-test, we use the t.test() function:\n\nt_res &lt;- t.test(ai_jobs_high$salary_usd, ai_jobs_low$salary_usd)\n\n\\(t = -6.54\\)"
  },
  {
    "objectID": "Week4/lecture.html#making-a-decision",
    "href": "Week4/lecture.html#making-a-decision",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Making a Decision",
    "text": "Making a Decision\n\nThe following step is making a decision: either rejecting the null or not rejecting it.\nIn hypothesis testing, this decision is based on a clear rule specified in advance.\nWe specify in advance to avoid bias - before looking at the data, we state what it would take to reject the null hypothesis. We follow what the data says, whatever result that may be.\n\n\nA clear rule also makes the decision transparent, which helps avoid biases in the decision. Unfortunately, we humans are often tempted to use evidence to support our pre-existing views or prejudices. If, for example, we think that jobs which are more highly valued by companies (i.e. have a higher salary) we may pay more attention to the evidence that supports that belief than to the evidence against it.\nIn partiular, we may be tempted to say that the estimated \\(\\hat{s}\\) difference is large enough to reject the null, because we believe that the null isn’t true. Clear decision rules are designed to minimize the room for such temptations.\n\n\nOnce you have your test statistic there are two methods to use it to make your decision:\n\nCritical value method\nP-Value method – This is the preferred method we mostly will focus on."
  },
  {
    "objectID": "Week4/lecture.html#critical-values",
    "href": "Week4/lecture.html#critical-values",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Critical Values",
    "text": "Critical Values\nWe use a critical value to tell us whether the test statistic is large enough - is it far enough away from zero to reject the null?\nTo define the critical value, we need to decide how conservative we want to be with the evidence.\nThe larger we set the critical value, the harder it is to reject the null hypothesis."
  },
  {
    "objectID": "Week4/lecture.html#critical-values-1",
    "href": "Week4/lecture.html#critical-values-1",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Critical Values",
    "text": "Critical Values\nThe test sampling distribution\nAs with the sampling distribution for means we looked at earlier, our test-statistic \\(t\\) also has a sampling distribution. If we were to sample many times and calculate \\(t\\) for each sample, we would again get a distribution with a specific shape and parameters.\n\nThe sampling distribution of the test statistic when the null is true.Recall: Approximately 95% of values fall within two standard deviations of the distribution."
  },
  {
    "objectID": "Week4/lecture.html#critical-values-2",
    "href": "Week4/lecture.html#critical-values-2",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Critical Values",
    "text": "Critical Values\nPicking a critical value\n\n\n\n\n\nThe sampling distribution of the test statistic when the null is true.\n\n\n\nRecall: Approximately 95% of values fall within two standard deviations of the distribution.\nSince 95% of values fall within 2 SD, if we want to reject the null hypothesis with 95% confidence, then we say that our test statistic must fall outside of 2 SD.\nIn other words, since the units of \\(\\hat{s}\\) are standard deviations: \\(\\hat{s} \\geq \\pm 2\\)"
  },
  {
    "objectID": "Week4/lecture.html#critical-values-3",
    "href": "Week4/lecture.html#critical-values-3",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Critical Values",
    "text": "Critical Values\nA critical value of 2 is standard. However, it is ultimately just a convention. We could choose to set other critical values that correspond to different probabilities. There is not anything inherently special about setting our threshold at 95% vs 90%.\nIf we make the critical value \\(\\pm 1.6\\), the chance of a false positive is 10%.\n\n\nDifferent fields have different standards for evidence - for instance, a critical value of 5 (99.994%) is standard in particle physics (referred to as \\(5\\sigma\\))."
  },
  {
    "objectID": "Week4/lecture.html#interpret-our-results",
    "href": "Week4/lecture.html#interpret-our-results",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Interpret our results",
    "text": "Interpret our results\nSince our test statistic \\(t = -7.5 &lt; -2\\), at a confidence level of 95%, we would have sufficient evidence to reject \\(H_0\\)\nTherefore, we would say:\n\nThe average salary of jobs at high risk of automation is not the same as the average salary of jobs at low risk.\nWe have evidence that higher salary jobs are at less risk of automation than low salary jobs."
  },
  {
    "objectID": "Week4/lecture.html#interpret-our-results-1",
    "href": "Week4/lecture.html#interpret-our-results-1",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Interpret our results",
    "text": "Interpret our results\nImportant!\nThis does not inherently mean we accept the alternative hypothesis. We are narrowing the realm of possible answers, but very rarely (perhaps never) are we able to statistically prove a single explanation in one go.\nWe have increased our reasons to believe our hypothesis, but several other possibilities exist.\nScience is then the process of continually investigating our hypothesis and pitting it against new null hypotheses and rejecting them as well."
  },
  {
    "objectID": "Week4/lecture.html#p-value-method",
    "href": "Week4/lecture.html#p-value-method",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "P-Value Method",
    "text": "P-Value Method\n\nHopefully, the critical value is fairly intuitive to you now. However, it is not the typical way that statistical results are presented.\nInstead, you will typically see something called a p-value.\n\n\np-value: The probability than an event will occur, assuming the null hypothesis is true.\nThe p-value essentially flips the critical value statement:\n\nInstead of saying a test statistic value &gt; 2 falls outside the 95% bound, we calculate where out test statistic falls in the distribution\n\n\nThe p-value is the probability that the test statistic will be as large, or larger, than we calculate from the data, if the null hypothesis is true. i.e. \\(P(data | H_0)\\)."
  },
  {
    "objectID": "Week4/lecture.html#p-value",
    "href": "Week4/lecture.html#p-value",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "P-value",
    "text": "P-value\n\\(p = P(|t| &gt; \\text{critical value})\\)\nBecause the p-value tells us the smallest level of significance at which we can reject the null hypothesis, it summarizes all the information we need to make the decision.\nThis is why the p-value is used - rather than needing to set a critical value and calculate the test statistic, we can instead use just the p-value."
  },
  {
    "objectID": "Week4/lecture.html#interpreting-the-p-value",
    "href": "Week4/lecture.html#interpreting-the-p-value",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Interpreting the P-value",
    "text": "Interpreting the P-value\n\nLike with the critical value, we should set our desired significance level before carrying out the analysis.\nWe then compare our calculated p-value with the significance level. If it is less, we reject the null hypothesis.\nThe significance level (\\(\\alpha\\)) is the probability that a true null hypothesis will be rejected.\nA typical significance level is \\(\\alpha &lt; 0.05\\), which corresponds with a critical value of 2, or a probability of 5%."
  },
  {
    "objectID": "Week4/lecture.html#interpreting-the-p-value-1",
    "href": "Week4/lecture.html#interpreting-the-p-value-1",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Interpreting the P-value",
    "text": "Interpreting the P-value\n\nR outputT distribution\n\n\nAgain, R will provide us with the p-value. Let’s now look at the full output from our t.test():\n\nt.test(ai_jobs_high$salary_usd, ai_jobs_low$salary_usd)\n\n\n    Welch Two Sample t-test\n\ndata:  ai_jobs_high$salary_usd and ai_jobs_low$salary_usd\nt = -6.5366, df = 288.07, p-value = 2.854e-10\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -23513.51 -12630.27\nsample estimates:\nmean of x mean of y \n 81673.57  99745.46 \n\n\n\nIf our p-value is smaller than our pre-set significance level (\\(\\alpha\\)), we reject the null hypothesis and can say the result is “statistically significant” at \\(p &lt; 0.05\\)."
  },
  {
    "objectID": "Week4/lecture.html#decision-and-conclusion",
    "href": "Week4/lecture.html#decision-and-conclusion",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Decision and conclusion",
    "text": "Decision and conclusion\nThe preset \\(\\alpha\\) is the probability of a False Positive error (called a Type I error) - rejecting the null hypothesis when the null hypothesis is true.\nBack to our two possible decisions:\n\nIf \\(\\alpha &gt; \\text{p-value}\\), reject \\(H_0\\).\n\nThe results of the sample are statistically significant.\nWe can say there is sufficient evidence to conclude that \\(H_0\\) is an incorrect believe and that the alternative hypothesis, \\(H_A\\) may be correct.\n\nIf \\(\\alpha &lt; \\text{p-value}\\), fail to reject \\(H_0\\).\n\nThe results of the sample are not significant. There is not sufficient evidence to conclude that the alternative hypothesis \\(H_A\\) may be correct.\n\n\n\nNOTE: When you “do not reject \\(H_0\\)”, it does not mean that you should believe that \\(H_0\\) is true. It simply means that the sample data have failed to provide sufficient evidence to cast serious doubt about the truthfulness of \\(H_0\\)."
  },
  {
    "objectID": "Week4/lecture.html#closing---what-does-a-statistically-significant-result-mean",
    "href": "Week4/lecture.html#closing---what-does-a-statistically-significant-result-mean",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Closing - What does a statistically significant result mean?",
    "text": "Closing - What does a statistically significant result mean?\nDoes it mean our result is meaningful or practically important?\n\nEffect sizeSample size\n\n\nNo. There is an essential distinction between statistical significance and practical significance.\nLet’s say we performed an experiment to examine the effect of a particular diet on body weight, which gives a statistically significant effect at \\(p &lt; 0.05\\). This doesn’t tell us how much weight was lost, which we refer to as the effect size.\nWould the loss of 20 grams (i.e. the weight of a few potato chips) be practically significant, even if it were statistically significant?\nWhether a result is practically significant depends on the effect size and the context of the research question. It’s up to the researcher to know whether it is meaningful.\n\n\nAs with the standard error (and a direct result of it), the p-value depends on the sample size. A very large sample size will give a statistically significant result in many cases, even with a very small effect size.\n\n\n\nThe proportion of significant results for a very small change (~20g which is about 0.001 standard deviations) as a function of sample size"
  },
  {
    "objectID": "Week4/lecture.html#further-reading",
    "href": "Week4/lecture.html#further-reading",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Further Reading",
    "text": "Further Reading\nThere are several more topics to understand about p-values which we cannot cover today: - One-sided vs Two-sided t-test. - Are we testing “there is no difference” or are we testing “\\(mean_A &gt; mean_B\\)? - Type I and Type II Errors - False positive vs False negative - Multiple Comparisons - What happens to \\(P(data|H_0\\)) when we run multiple tests on the same data? - How do we control the error rate across our entire family of tests?\n\nThere are also numerous modern critiques of p-values and how they are used and interpreted.\nSee Statistical Thinking Chapter 9 for a detailed discussion."
  },
  {
    "objectID": "Week4/lecture.html#further-reading-1",
    "href": "Week4/lecture.html#further-reading-1",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Further Reading",
    "text": "Further Reading\n\nPoldrack, Statistical Thinking, Chapter 9\nSignificant Statistics from Virginia Tech: https://pressbooks.lib.vt.edu/introstatistics/chapter/null-and-alternative-hypotheses/\nBekes & Kezdi, Data Analysis for Business, Economics, and Policy, Chapter 6"
  },
  {
    "objectID": "Week5/1-carbonbrief.html",
    "href": "Week5/1-carbonbrief.html",
    "title": "Misleading Statistics",
    "section": "",
    "text": "Code# setup\nlibrary(tidyverse)\nlibrary(httr)\n\n\nSource: Factcheck: No, global warming has not ‘paused’ over the past eight years, Carbon Brief. [@Hausfather2022Factcheck]\n\nWe’ll use a case study of data presented by climate change sceptics to illustrate how even real data and “mathematically correct” statistical analysis can be used to mislead.\nSome types of misleading statistics:\n\ncherry-picking data\novergeneralization\nfaulty causality\nbiased sampling\nmisleading graphs\nreporting non-statistically-significant results as significant\nreporting statistically-significant but not practically-significant results as meaningful\n\nSee Calling Bullshit - The Art of of Skepticism in a Data-Driven World [@Bergstrom2021Calling] for more examples.\n\nIn the 2010’s a claim began to circulate that data showed that “global mean surface temperature T_S has not risen since 1998, and may have fallen since late 2001” [@Monckton2008Climate].\nAlong with some controversies about the source of climate data, this became known as ‘ClimateGate’. Similar claims arose again in 2022, showing an apparent pause in climate change from 2015-2022.\nThese claims were based on data - they presented analyses and visualisations of global temperature data which in fact did appear to show a pause or a decrease in global temperature, apparently disproving anthroprogenic climate change.\nLet’s take a look back at this data with a critical eye and see whether we find them convincing.\n\nWe won’t be getting into the science of climate change here, but it’s good to understand the basic arguments and sources of evidence.\n\n\nTo get a complete picture of Earth’s temperature, scientists combine measurements from the air above land and the ocean surface collected by ships, buoys and sometimes satellites, too.\nThe temperature at each land and ocean station is compared daily to what is ‘normal’ for that location and time, typically the long-term average over a 30-year period. The differences are called an ‘anomalies’ and they help scientists evaluate how temperature is changing over time.\nA ‘positive’ anomaly means the temperature is warmer than the long-term average, a ‘negative’ anomaly means it’s cooler.\n[@Pidcock2015Explainer]\n\n\n\n\n\nGISS Surface Temperature Analysis. Source: @NASA2025GISS\n\n\n\nIn @Monckton2008Climate, published in a peer-reviewed journal of The American Physical Society, Christopher Monckton claimed that the mean global temperature data across the four major data sources showed that T_S has not risen between 2001 and 2008.\nHe presented a time series plot which confirms this.\n\n\n\nthe conclusion is that, perhaps, there is no “climate crisis”, and that currently-fashionable efforts by governments to reduce anthropogenic CO2 emissions are pointless, may be ill-conceived, and could even be harmful.\n\n\n\n\n\nMean global surface temperature anomalies, 2001-2008. Source: @Monckton2008Climate\n\n\nNearly a decade later, talk of a pause has re-emerged with claims in the media such as:\n\ncontrary to the dogma which holds that a rise in carbon dioxide inescapably heats up the atmosphere global temperature has embarassingly flatlined for more than seven years even as CO2 levels have risen. [@Phillips2022Sri]\n\nAgain, the claim comes from a blog post written by Christopher Monckton titled The New Pause Lengthens to 7 years 10 months [@Monckton2022New]. Let’s look in depth at the data used to make this claim.\n\n\n\n\n“This Pause […] is, as always, not cherry-picked. It is derived from the UAH monthly global mean lower-troposphere temperature anomalies as the period from the earliest month starting with which the least-squares linear-regression trend to the most recent month for which data are available does not exceed zero.” [@Monckton2022New]\n\n\n\nSo, what is wrong with this presentation? Why might it be misleading?\n\n\n\n\n\n\n\n\nFigure 1: Annual global surface temperature data from ERA5, along with Carbon Brief’s estimate of annual 2022 temperatures based on the first six months of the year and the linear trend over the 2015 to 2022 period. Warming since pre-industrial is calculated using the Berkeley Earth dataset for the period prior to 1979. [@Hausfather2022Factcheck]\n\n\n\nLooking at these eight years in isolation ignores the larger context.\n\n\nA slightly different eight-year period - 2011 to 2018 rather than 2015 to 2022 - would offer the opposite conclusion, namely that global warming had massively accelerated to a rate of 5.6C per century.\n\n\n\nSame as the prior plot, but showing annual global surface temperature data from 2000 and the trend over the 8-year period from 2011 through to 2018. [@Hausfather2022Factcheck]\n\n\n\n\nIn reality, both of these are acts of “cherry-picking” - overemphasising short-term variability.\nAlso note that Monckton picks his time periods carefully - the first ‘pause’ is from 2001 to 2008. Next, he shows the data from 2015 to 2022 - so what happened from 2008 to 2015? That is left out.\n\nSo the questions we should ask, from a statistics perspective are:\n\n\n\nHow large is the expected variability over any given period?\nDoes the apparent downward trend in the period 2015-2022 fit within this variability, meaning we might just be looking at what is effectively noise?\nOr is the trend large enough to be seen without this random variability?\n\n\n\n\n\n\n\nFigure 2: Same as the prior plots, but highlighting the years from 2015 onward compared to the 1979-2022 trend. [@Hausfather2022Factcheck]\n\n\n\n\n\n\nThe fluctuations in recent years are well within the range of expected variability, and do not indicate any departure from the long-term warming trend in surface temperatures the world has experienced over the past 50 years.\nThe acceleration started from below the trendline and brought temperatures well above it, while the pause started above the trendline and brought temperatures back down to around what would be expected for 2021 and 2022.\n[@Hausfather2022Factcheck]\n\n\n\n\nZooming out further makes the trend very clear.\nThe ‘pause’ periods fit well within the natural variability. By intentionally focusing in on the periods which decrease effectively by chance over a short period of time, we can make the data appear to show a trend which is not there.\n\n\n\nSame as the prior plots, but including Berkeley Earth data from 1850 through 2021. [@Hausfather2022Factcheck]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigures from [@Bolton2023How]\n\nWhen presented with a statistical analysis or visualisation, what questions should we ask?\nHow can we make sure we’re thinking about the data critically?\n\n[@Bolton2023How]. How to spot spin and inappropriate use of statistics (Research Briefing No. 4446). UK House of Commons Library.\n[@Bolton2007Statistical]. Statistical literacy guide: How to read charts (Research Briefing No. SN04445). UK House of Commons Library.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Misleading Statistics"
    ]
  },
  {
    "objectID": "Week5/1-carbonbrief.html#misleading-statistics-1",
    "href": "Week5/1-carbonbrief.html#misleading-statistics-1",
    "title": "Misleading Statistics",
    "section": "",
    "text": "We’ll use a case study of data presented by climate change sceptics to illustrate how even real data and “mathematically correct” statistical analysis can be used to mislead.\nSome types of misleading statistics:\n\ncherry-picking data\novergeneralization\nfaulty causality\nbiased sampling\nmisleading graphs\nreporting non-statistically-significant results as significant\nreporting statistically-significant but not practically-significant results as meaningful\n\nSee Calling Bullshit - The Art of of Skepticism in a Data-Driven World [@Bergstrom2021Calling] for more examples.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Misleading Statistics"
    ]
  },
  {
    "objectID": "Week5/1-carbonbrief.html#hiatus-in-global-warming",
    "href": "Week5/1-carbonbrief.html#hiatus-in-global-warming",
    "title": "Misleading Statistics",
    "section": "",
    "text": "In the 2010’s a claim began to circulate that data showed that “global mean surface temperature T_S has not risen since 1998, and may have fallen since late 2001” [@Monckton2008Climate].\nAlong with some controversies about the source of climate data, this became known as ‘ClimateGate’. Similar claims arose again in 2022, showing an apparent pause in climate change from 2015-2022.\nThese claims were based on data - they presented analyses and visualisations of global temperature data which in fact did appear to show a pause or a decrease in global temperature, apparently disproving anthroprogenic climate change.\nLet’s take a look back at this data with a critical eye and see whether we find them convincing.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Misleading Statistics"
    ]
  },
  {
    "objectID": "Week5/1-carbonbrief.html#climate-change-measurements",
    "href": "Week5/1-carbonbrief.html#climate-change-measurements",
    "title": "Misleading Statistics",
    "section": "",
    "text": "We won’t be getting into the science of climate change here, but it’s good to understand the basic arguments and sources of evidence.\n\n\nTo get a complete picture of Earth’s temperature, scientists combine measurements from the air above land and the ocean surface collected by ships, buoys and sometimes satellites, too.\nThe temperature at each land and ocean station is compared daily to what is ‘normal’ for that location and time, typically the long-term average over a 30-year period. The differences are called an ‘anomalies’ and they help scientists evaluate how temperature is changing over time.\nA ‘positive’ anomaly means the temperature is warmer than the long-term average, a ‘negative’ anomaly means it’s cooler.\n[@Pidcock2015Explainer]",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Misleading Statistics"
    ]
  },
  {
    "objectID": "Week5/1-carbonbrief.html#climate-change-measurements-1",
    "href": "Week5/1-carbonbrief.html#climate-change-measurements-1",
    "title": "Misleading Statistics",
    "section": "",
    "text": "GISS Surface Temperature Analysis. Source: @NASA2025GISS",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Misleading Statistics"
    ]
  },
  {
    "objectID": "Week5/1-carbonbrief.html#the-first-global-warming-pause",
    "href": "Week5/1-carbonbrief.html#the-first-global-warming-pause",
    "title": "Misleading Statistics",
    "section": "",
    "text": "In @Monckton2008Climate, published in a peer-reviewed journal of The American Physical Society, Christopher Monckton claimed that the mean global temperature data across the four major data sources showed that T_S has not risen between 2001 and 2008.\nHe presented a time series plot which confirms this.\n\n\n\nthe conclusion is that, perhaps, there is no “climate crisis”, and that currently-fashionable efforts by governments to reduce anthropogenic CO2 emissions are pointless, may be ill-conceived, and could even be harmful.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Misleading Statistics"
    ]
  },
  {
    "objectID": "Week5/1-carbonbrief.html#the-first-global-warming-pause-1",
    "href": "Week5/1-carbonbrief.html#the-first-global-warming-pause-1",
    "title": "Misleading Statistics",
    "section": "",
    "text": "Mean global surface temperature anomalies, 2001-2008. Source: @Monckton2008Climate",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Misleading Statistics"
    ]
  },
  {
    "objectID": "Week5/1-carbonbrief.html#the-new-global-warming-pause",
    "href": "Week5/1-carbonbrief.html#the-new-global-warming-pause",
    "title": "Misleading Statistics",
    "section": "",
    "text": "Nearly a decade later, talk of a pause has re-emerged with claims in the media such as:\n\ncontrary to the dogma which holds that a rise in carbon dioxide inescapably heats up the atmosphere global temperature has embarassingly flatlined for more than seven years even as CO2 levels have risen. [@Phillips2022Sri]\n\nAgain, the claim comes from a blog post written by Christopher Monckton titled The New Pause Lengthens to 7 years 10 months [@Monckton2022New]. Let’s look in depth at the data used to make this claim.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Misleading Statistics"
    ]
  },
  {
    "objectID": "Week5/1-carbonbrief.html#the-new-global-warming-pause-1",
    "href": "Week5/1-carbonbrief.html#the-new-global-warming-pause-1",
    "title": "Misleading Statistics",
    "section": "",
    "text": "“This Pause […] is, as always, not cherry-picked. It is derived from the UAH monthly global mean lower-troposphere temperature anomalies as the period from the earliest month starting with which the least-squares linear-regression trend to the most recent month for which data are available does not exceed zero.” [@Monckton2022New]\n\n\n\nSo, what is wrong with this presentation? Why might it be misleading?",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Misleading Statistics"
    ]
  },
  {
    "objectID": "Week5/1-carbonbrief.html#the-new-global-warming-pause-2",
    "href": "Week5/1-carbonbrief.html#the-new-global-warming-pause-2",
    "title": "Misleading Statistics",
    "section": "",
    "text": "Figure 1: Annual global surface temperature data from ERA5, along with Carbon Brief’s estimate of annual 2022 temperatures based on the first six months of the year and the linear trend over the 2015 to 2022 period. Warming since pre-industrial is calculated using the Berkeley Earth dataset for the period prior to 1979. [@Hausfather2022Factcheck]",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Misleading Statistics"
    ]
  },
  {
    "objectID": "Week5/1-carbonbrief.html#cherrypicking-data",
    "href": "Week5/1-carbonbrief.html#cherrypicking-data",
    "title": "Misleading Statistics",
    "section": "",
    "text": "Looking at these eight years in isolation ignores the larger context.\n\n\nA slightly different eight-year period - 2011 to 2018 rather than 2015 to 2022 - would offer the opposite conclusion, namely that global warming had massively accelerated to a rate of 5.6C per century.\n\n\n\nSame as the prior plot, but showing annual global surface temperature data from 2000 and the trend over the 8-year period from 2011 through to 2018. [@Hausfather2022Factcheck]",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Misleading Statistics"
    ]
  },
  {
    "objectID": "Week5/1-carbonbrief.html#cherrypicking-data-1",
    "href": "Week5/1-carbonbrief.html#cherrypicking-data-1",
    "title": "Misleading Statistics",
    "section": "",
    "text": "In reality, both of these are acts of “cherry-picking” - overemphasising short-term variability.\nAlso note that Monckton picks his time periods carefully - the first ‘pause’ is from 2001 to 2008. Next, he shows the data from 2015 to 2022 - so what happened from 2008 to 2015? That is left out.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Misleading Statistics"
    ]
  },
  {
    "objectID": "Week5/1-carbonbrief.html#finding-spurious-patterns-within-natural-variance",
    "href": "Week5/1-carbonbrief.html#finding-spurious-patterns-within-natural-variance",
    "title": "Misleading Statistics",
    "section": "",
    "text": "So the questions we should ask, from a statistics perspective are:\n\n\n\nHow large is the expected variability over any given period?\nDoes the apparent downward trend in the period 2015-2022 fit within this variability, meaning we might just be looking at what is effectively noise?\nOr is the trend large enough to be seen without this random variability?\n\n\n\n\n\n\n\nFigure 2: Same as the prior plots, but highlighting the years from 2015 onward compared to the 1979-2022 trend. [@Hausfather2022Factcheck]\n\n\n\n\n\n\nThe fluctuations in recent years are well within the range of expected variability, and do not indicate any departure from the long-term warming trend in surface temperatures the world has experienced over the past 50 years.\nThe acceleration started from below the trendline and brought temperatures well above it, while the pause started above the trendline and brought temperatures back down to around what would be expected for 2021 and 2022.\n[@Hausfather2022Factcheck]\n\n\n\n\nZooming out further makes the trend very clear.\nThe ‘pause’ periods fit well within the natural variability. By intentionally focusing in on the periods which decrease effectively by chance over a short period of time, we can make the data appear to show a trend which is not there.\n\n\n\nSame as the prior plots, but including Berkeley Earth data from 1850 through 2021. [@Hausfather2022Factcheck]",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Misleading Statistics"
    ]
  },
  {
    "objectID": "Week5/1-carbonbrief.html#other-types-of-misleading-or-inappropriate-visualisations",
    "href": "Week5/1-carbonbrief.html#other-types-of-misleading-or-inappropriate-visualisations",
    "title": "Misleading Statistics",
    "section": "",
    "text": "Figures from [@Bolton2023How]",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Misleading Statistics"
    ]
  },
  {
    "objectID": "Week5/1-carbonbrief.html#concluding-thoughts",
    "href": "Week5/1-carbonbrief.html#concluding-thoughts",
    "title": "Misleading Statistics",
    "section": "",
    "text": "When presented with a statistical analysis or visualisation, what questions should we ask?\nHow can we make sure we’re thinking about the data critically?",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Misleading Statistics"
    ]
  },
  {
    "objectID": "Week5/1-carbonbrief.html#further-reading",
    "href": "Week5/1-carbonbrief.html#further-reading",
    "title": "Misleading Statistics",
    "section": "",
    "text": "[@Bolton2023How]. How to spot spin and inappropriate use of statistics (Research Briefing No. 4446). UK House of Commons Library.\n[@Bolton2007Statistical]. Statistical literacy guide: How to read charts (Research Briefing No. SN04445). UK House of Commons Library.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Misleading Statistics"
    ]
  },
  {
    "objectID": "Week5/4-assignment.html",
    "href": "Week5/4-assignment.html",
    "title": "Formative Assessment: Deceptive Visualization",
    "section": "",
    "text": "Data visualizations are becoming a key medium for the public to understand news and information. It’s crucial to recognize how the design of a visualization can affect what people understand and remember from the data. In this task, you need to pick a dataset you find interesting and create two static visualizations using the same dataset. The first should be a truthful representation of the data. The second should be a deceptive visualization, designed to deceive the viewer. However, you should avoid clear distortions or leaving out information for this deceptive visualization.\n\n\nYour objective is to create two static (single image) visualizations of a selected dataset. The first visualization should be designed to clearly and sincerely convey insights from the data. In contrast, the second should be crafted to intentionally mislead the viewer, causing them to make incorrect inferences. Additionally, you are required to write a brief explanation, limited to four paragraphs, outlining your design strategy for both visualizations.\nYou should use Quarto for this assignment. In the text sections, make notes about what you are doing at each step (for example loading data from csv, cleaning data, transforming to calculate group means, etc.) and your brief explanation.\nIn this task, an earnest visualization is defined as one that:\n\nIs easily understandable and can be interpreted by the general public.\nUses visual encodings that are suitable and effective for the desired purpose.\nClearly and openly describes any transformations made to the data.\nTransparently communicates the source of the data and any possible biases involved.\n\nConversely, a deceptive visualization typically displays these characteristics:\n\nThe graphical depiction is deliberately unsuitable or deceptive.\nHeadings are crafted to influence the viewer’s understanding in a biased manner.\nThere is intentional manipulation or selective filtering of data or axes to deceive.\nIt’s not transparent about possible bias present in the data.\n\nFor the earnest visualization, your goal is to be as clear and transparent as possible to help viewers answer your intended question. For the deceptive visualization, your goal is to trick the viewer (including the course staff!) into believing that the visualization is legitimate and earnest. It should not be immediately obvious which visualization is trying to be deceptive. Subtle ineffective choices in the design should require close and careful reading to be identified.\nFor the deceptive visualization, misleading strategies are fine but outright lying is not. For example, sketchy, unreliable or untrustworthy input datasets are discouraged, but misleading omission, filtering, or transformation of trustworthy data records is fine. Deliberate lies in the title, axes, labels, or annotations is discouraged, but technically true/relevant but otherwise misleading text in the visualization is fine.\nFor both visualization designs, start by choosing a question you would like to answer. Design your visualization to answer that question either correctly (for the earnest visualization) or incorrectly (for the deceptive visualization). You may choose to address a different question with each visualization. Be sure to document the question as part of the visualization design (e.g., title, subtitle, or caption) and in your assignment write-up.\nYour write-up should contain the following information:\n\nThe specific question each visualization aims to answer.\nA description of your design rationale and important considerations for each visualization.\n\n\n\nIn addition to the specific resources linked on the Moodle page, reccommended resources to help with your visualisation assignment are:\n\nR for Data Science, ed. 2. Sections 28-29 for help with Quarto, and Sections 9-11 for help with ggplot and visualisations.\nThe Quarto Documentation and Guide\nNavigating RStudio and Quarto tutorial\n\n\n\n\n\nTo help get you started, this assignment, we’ve provided two possible datasets for you to use, although you’re welcome to select any dataset you prefer. In class we demonstrated the data analysis workflow using a dataset of emissions data from Apple. We also looked at climate change data of the global mean temperature anomaly. Both of these datasets have been provided for you on Moodle and are well suited to use in the assignment. However, you may use any dataset you find interesting, whether it’s one we’ve used in class before, the others listed below, or any dataset you can find.\nYou must use the same dataset for both visualizations, but you may transform the data differently, use additional data variables, or choose to address a different question for each design. These datasets are intentionally chosen to cover politically charged topics for the simple reason that these are typically the types of data where deceptive visualizations may proliferate.\n\n\nYou may use the Apple Emissions dataset we started to look at in class which can be downloaded from Moodle. Find the original source here\nA breakdown of Apple’s greenhouse gas emissions from 2015 to 2022 as they aim to reach net zero emissions by 2030. This includes every source of emissions from both their corporate operations and their product life cycle, the carbon footprint of their baseline iPhone in the same period, and normalizing factors like sales, market cap, and employees.\nSome Recommended Analysis\n\nHow much has Apple reduced their emissions from 2015 to 2022?\nHow does this trend compare to their revenue & market cap trend in the same period?\nWhich areas have seen the most improvement? What about the least?\nIs Apple on track to meet their 2030 goal of net zero emissions?\n\n\n\n\n\n\n\nOur World in Data, a non-profit that gathers and analyzes data about global issues, has published data about energy usage for countries (e.g. coal consumption, hydropower consumption, etc.) around the world since 1900. You can download the data here.\n\n\n\nEvery year, the federal government releases large amounts of data on US schools, districts, and colleges. However, this information is scattered across multiple datasets. Urban Institute’s Education Data Explorer tries to fix this issue by putting together data from various sources such as the National Center for Education Statistics’ Common Core of Data (CCD), the Civil Rights Data Collection (CRDC), the US Department of Education’s EDFacts, and IPUMS’ National Historical Geographic Information System (NHGIS) and makes it available as an API. You can download the data by making an API call using the code available on the website or alternatively clicking on the downloads button on the website.\n\n\n\nUNdata brings international statistical databases within easy reach of users through a single-entry point. It is maintained by the Development Data Section of the Development Data and Outreach Branch within the Statistics Division of the Department of Economic and Social Affairs (UN DESA) of the UN Secretariat. You can find the internet usage data here. Feel free to take a look at some of the other datasets made available by UNdata here.\nThis data has the following columns:\n\nRegion/country Code: code representing the country or region.\nRegion or Country Name: Field containing the country name.\nYear: Field containing the year at which the data was collected.\nValue: Field denoting the Percentage of individuals using the internet.\nSource: Field denoting the source of the data.\n\nHere are some other possible sources to consider. You are also free to use data from a source different from those included here. If you have any questions on whether a dataset is appropriate, please ask the course staff ASAP!\n\nCity of San Diego open data\nU.S. Government Open Datasets\nU.S. Census Bureau - Census Datasets\nIPUMS.org - Integrated Census & Survey Data from around the World\nFederal Elections Commission - Campaign Finance & Expenditures\nFederal Aviation Administration - FAA Data & Research\nNOAA Daily Weather - NOAA Daily Global Historical Climatology Network Data\nyelp.com/dataset - Yelp Open Dataset\nfivethirtyeight.com - Data and Code behind the Stories and Interactives\nBuzzfeed News - Open-source data from BuzzFeed’s newsroom",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Formative Assessment: Deceptive Visualization"
    ]
  },
  {
    "objectID": "Week5/4-assignment.html#assignment",
    "href": "Week5/4-assignment.html#assignment",
    "title": "Formative Assessment: Deceptive Visualization",
    "section": "",
    "text": "Your objective is to create two static (single image) visualizations of a selected dataset. The first visualization should be designed to clearly and sincerely convey insights from the data. In contrast, the second should be crafted to intentionally mislead the viewer, causing them to make incorrect inferences. Additionally, you are required to write a brief explanation, limited to four paragraphs, outlining your design strategy for both visualizations.\nYou should use Quarto for this assignment. In the text sections, make notes about what you are doing at each step (for example loading data from csv, cleaning data, transforming to calculate group means, etc.) and your brief explanation.\nIn this task, an earnest visualization is defined as one that:\n\nIs easily understandable and can be interpreted by the general public.\nUses visual encodings that are suitable and effective for the desired purpose.\nClearly and openly describes any transformations made to the data.\nTransparently communicates the source of the data and any possible biases involved.\n\nConversely, a deceptive visualization typically displays these characteristics:\n\nThe graphical depiction is deliberately unsuitable or deceptive.\nHeadings are crafted to influence the viewer’s understanding in a biased manner.\nThere is intentional manipulation or selective filtering of data or axes to deceive.\nIt’s not transparent about possible bias present in the data.\n\nFor the earnest visualization, your goal is to be as clear and transparent as possible to help viewers answer your intended question. For the deceptive visualization, your goal is to trick the viewer (including the course staff!) into believing that the visualization is legitimate and earnest. It should not be immediately obvious which visualization is trying to be deceptive. Subtle ineffective choices in the design should require close and careful reading to be identified.\nFor the deceptive visualization, misleading strategies are fine but outright lying is not. For example, sketchy, unreliable or untrustworthy input datasets are discouraged, but misleading omission, filtering, or transformation of trustworthy data records is fine. Deliberate lies in the title, axes, labels, or annotations is discouraged, but technically true/relevant but otherwise misleading text in the visualization is fine.\nFor both visualization designs, start by choosing a question you would like to answer. Design your visualization to answer that question either correctly (for the earnest visualization) or incorrectly (for the deceptive visualization). You may choose to address a different question with each visualization. Be sure to document the question as part of the visualization design (e.g., title, subtitle, or caption) and in your assignment write-up.\nYour write-up should contain the following information:\n\nThe specific question each visualization aims to answer.\nA description of your design rationale and important considerations for each visualization.\n\n\n\nIn addition to the specific resources linked on the Moodle page, reccommended resources to help with your visualisation assignment are:\n\nR for Data Science, ed. 2. Sections 28-29 for help with Quarto, and Sections 9-11 for help with ggplot and visualisations.\nThe Quarto Documentation and Guide\nNavigating RStudio and Quarto tutorial",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Formative Assessment: Deceptive Visualization"
    ]
  },
  {
    "objectID": "Week5/4-assignment.html#recommended-data-sources",
    "href": "Week5/4-assignment.html#recommended-data-sources",
    "title": "Formative Assessment: Deceptive Visualization",
    "section": "",
    "text": "To help get you started, this assignment, we’ve provided two possible datasets for you to use, although you’re welcome to select any dataset you prefer. In class we demonstrated the data analysis workflow using a dataset of emissions data from Apple. We also looked at climate change data of the global mean temperature anomaly. Both of these datasets have been provided for you on Moodle and are well suited to use in the assignment. However, you may use any dataset you find interesting, whether it’s one we’ve used in class before, the others listed below, or any dataset you can find.\nYou must use the same dataset for both visualizations, but you may transform the data differently, use additional data variables, or choose to address a different question for each design. These datasets are intentionally chosen to cover politically charged topics for the simple reason that these are typically the types of data where deceptive visualizations may proliferate.\n\n\nYou may use the Apple Emissions dataset we started to look at in class which can be downloaded from Moodle. Find the original source here\nA breakdown of Apple’s greenhouse gas emissions from 2015 to 2022 as they aim to reach net zero emissions by 2030. This includes every source of emissions from both their corporate operations and their product life cycle, the carbon footprint of their baseline iPhone in the same period, and normalizing factors like sales, market cap, and employees.\nSome Recommended Analysis\n\nHow much has Apple reduced their emissions from 2015 to 2022?\nHow does this trend compare to their revenue & market cap trend in the same period?\nWhich areas have seen the most improvement? What about the least?\nIs Apple on track to meet their 2030 goal of net zero emissions?",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Formative Assessment: Deceptive Visualization"
    ]
  },
  {
    "objectID": "Week5/4-assignment.html#other-data-sources",
    "href": "Week5/4-assignment.html#other-data-sources",
    "title": "Formative Assessment: Deceptive Visualization",
    "section": "",
    "text": "Our World in Data, a non-profit that gathers and analyzes data about global issues, has published data about energy usage for countries (e.g. coal consumption, hydropower consumption, etc.) around the world since 1900. You can download the data here.\n\n\n\nEvery year, the federal government releases large amounts of data on US schools, districts, and colleges. However, this information is scattered across multiple datasets. Urban Institute’s Education Data Explorer tries to fix this issue by putting together data from various sources such as the National Center for Education Statistics’ Common Core of Data (CCD), the Civil Rights Data Collection (CRDC), the US Department of Education’s EDFacts, and IPUMS’ National Historical Geographic Information System (NHGIS) and makes it available as an API. You can download the data by making an API call using the code available on the website or alternatively clicking on the downloads button on the website.\n\n\n\nUNdata brings international statistical databases within easy reach of users through a single-entry point. It is maintained by the Development Data Section of the Development Data and Outreach Branch within the Statistics Division of the Department of Economic and Social Affairs (UN DESA) of the UN Secretariat. You can find the internet usage data here. Feel free to take a look at some of the other datasets made available by UNdata here.\nThis data has the following columns:\n\nRegion/country Code: code representing the country or region.\nRegion or Country Name: Field containing the country name.\nYear: Field containing the year at which the data was collected.\nValue: Field denoting the Percentage of individuals using the internet.\nSource: Field denoting the source of the data.\n\nHere are some other possible sources to consider. You are also free to use data from a source different from those included here. If you have any questions on whether a dataset is appropriate, please ask the course staff ASAP!\n\nCity of San Diego open data\nU.S. Government Open Datasets\nU.S. Census Bureau - Census Datasets\nIPUMS.org - Integrated Census & Survey Data from around the World\nFederal Elections Commission - Campaign Finance & Expenditures\nFederal Aviation Administration - FAA Data & Research\nNOAA Daily Weather - NOAA Daily Global Historical Climatology Network Data\nyelp.com/dataset - Yelp Open Dataset\nfivethirtyeight.com - Data and Code behind the Stories and Interactives\nBuzzfeed News - Open-source data from BuzzFeed’s newsroom",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Formative Assessment: Deceptive Visualization"
    ]
  },
  {
    "objectID": "Week5/lecture.html#misleading-statistics-1",
    "href": "Week5/lecture.html#misleading-statistics-1",
    "title": "Week 5",
    "section": "Misleading Statistics",
    "text": "Misleading Statistics\nWe’ll use a case study of data presented by climate change sceptics to illustrate how even real data and “mathematically correct” statistical analysis can be used to mislead.\nSome types of misleading statistics:\n\ncherry-picking data\novergeneralization\nfaulty causality\nbiased sampling\nmisleading graphs\nreporting non-statistically-significant results as significant\nreporting statistically-significant but not practically-significant results as meaningful\n\nSee Calling Bullshit - The Art of of Skepticism in a Data-Driven World (Bergstrom and West 2021) for more examples."
  },
  {
    "objectID": "Week5/lecture.html#hiatus-in-global-warming",
    "href": "Week5/lecture.html#hiatus-in-global-warming",
    "title": "Week 5",
    "section": "“Hiatus in Global Warming”",
    "text": "“Hiatus in Global Warming”\nIn the 2010’s a claim began to circulate that data showed that “global mean surface temperature \\(T_S\\) has not risen since 1998, and may have fallen since late 2001” (Monckton 2008).\nAlong with some controversies about the source of climate data, this became known as ‘ClimateGate’. Similar claims arose again in 2022, showing an apparent pause in climate change from 2015-2022.\nThese claims were based on data - they presented analyses and visualisations of global temperature data which in fact did appear to show a pause or a decrease in global temperature, apparently disproving anthroprogenic climate change.\nLet’s take a look back at this data with a critical eye and see whether we find them convincing."
  },
  {
    "objectID": "Week5/lecture.html#climate-change-measurements",
    "href": "Week5/lecture.html#climate-change-measurements",
    "title": "Week 5",
    "section": "Climate Change measurements",
    "text": "Climate Change measurements\nWe won’t be getting into the science of climate change here, but it’s good to understand the basic arguments and sources of evidence.\n\n\nTo get a complete picture of Earth’s temperature, scientists combine measurements from the air above land and the ocean surface collected by ships, buoys and sometimes satellites, too.\nThe temperature at each land and ocean station is compared daily to what is ‘normal’ for that location and time, typically the long-term average over a 30-year period. The differences are called an ‘anomalies’ and they help scientists evaluate how temperature is changing over time.\nA ‘positive’ anomaly means the temperature is warmer than the long-term average, a ‘negative’ anomaly means it’s cooler.\n(Pidcock 2015)"
  },
  {
    "objectID": "Week5/lecture.html#climate-change-measurements-1",
    "href": "Week5/lecture.html#climate-change-measurements-1",
    "title": "Week 5",
    "section": "Climate Change measurements",
    "text": "Climate Change measurements\n\nGISS Surface Temperature Analysis. Source: NASA (2025)"
  },
  {
    "objectID": "Week5/lecture.html#the-first-global-warming-pause",
    "href": "Week5/lecture.html#the-first-global-warming-pause",
    "title": "Week 5",
    "section": "The First Global Warming Pause",
    "text": "The First Global Warming Pause\n\nIn Monckton (2008), published in a peer-reviewed journal of The American Physical Society, Christopher Monckton claimed that the mean global temperature data across the four major data sources showed that \\(T_S\\) has not risen between 2001 and 2008.\nHe presented a time series plot which confirms this.\n\n\n\nthe conclusion is that, perhaps, there is no “climate crisis”, and that currently-fashionable efforts by governments to reduce anthropogenic CO2 emissions are pointless, may be ill-conceived, and could even be harmful."
  },
  {
    "objectID": "Week5/lecture.html#the-first-global-warming-pause-1",
    "href": "Week5/lecture.html#the-first-global-warming-pause-1",
    "title": "Week 5",
    "section": "The First Global Warming Pause",
    "text": "The First Global Warming Pause\n\nMean global surface temperature anomalies, 2001-2008. Source: Monckton (2008)"
  },
  {
    "objectID": "Week5/lecture.html#the-new-global-warming-pause",
    "href": "Week5/lecture.html#the-new-global-warming-pause",
    "title": "Week 5",
    "section": "The New Global Warming Pause",
    "text": "The New Global Warming Pause\nNearly a decade later, talk of a pause has re-emerged with claims in the media such as:\n\ncontrary to the dogma which holds that a rise in carbon dioxide inescapably heats up the atmosphere global temperature has embarassingly flatlined for more than seven years even as CO2 levels have risen. (Phillips 2022)\n\nAgain, the claim comes from a blog post written by Christopher Monckton titled The New Pause Lengthens to 7 years 10 months (Monckton 2022). Let’s look in depth at the data used to make this claim."
  },
  {
    "objectID": "Week5/lecture.html#the-new-global-warming-pause-1",
    "href": "Week5/lecture.html#the-new-global-warming-pause-1",
    "title": "Week 5",
    "section": "The New Global Warming Pause",
    "text": "The New Global Warming Pause\n\n\n\n\n“This Pause […] is, as always, not cherry-picked. It is derived from the UAH monthly global mean lower-troposphere temperature anomalies as the period from the earliest month starting with which the least-squares linear-regression trend to the most recent month for which data are available does not exceed zero.” (Monckton 2022)\n\n\n\n\nSo, what is wrong with this presentation? Why might it be misleading?"
  },
  {
    "objectID": "Week5/lecture.html#the-new-global-warming-pause-2",
    "href": "Week5/lecture.html#the-new-global-warming-pause-2",
    "title": "Week 5",
    "section": "The New Global Warming Pause",
    "text": "The New Global Warming Pause\n\n\n\n\n\n\n\nFigure 1: Annual global surface temperature data from ERA5, along with Carbon Brief’s estimate of annual 2022 temperatures based on the first six months of the year and the linear trend over the 2015 to 2022 period. Warming since pre-industrial is calculated using the Berkeley Earth dataset for the period prior to 1979. (Hausfather 2022)"
  },
  {
    "objectID": "Week5/lecture.html#cherrypicking-data",
    "href": "Week5/lecture.html#cherrypicking-data",
    "title": "Week 5",
    "section": "Cherrypicking Data",
    "text": "Cherrypicking Data\nLooking at these eight years in isolation ignores the larger context.\n\n\nA slightly different eight-year period - 2011 to 2018 rather than 2015 to 2022 - would offer the opposite conclusion, namely that global warming had massively accelerated to a rate of 5.6C per century.\n\n\n\nSame as the prior plot, but showing annual global surface temperature data from 2000 and the trend over the 8-year period from 2011 through to 2018. (Hausfather 2022)"
  },
  {
    "objectID": "Week5/lecture.html#cherrypicking-data-1",
    "href": "Week5/lecture.html#cherrypicking-data-1",
    "title": "Week 5",
    "section": "Cherrypicking Data",
    "text": "Cherrypicking Data\nIn reality, both of these are acts of “cherry-picking” - overemphasising short-term variability.\nAlso note that Monckton picks his time periods carefully - the first ‘pause’ is from 2001 to 2008. Next, he shows the data from 2015 to 2022 - so what happened from 2008 to 2015? That is left out."
  },
  {
    "objectID": "Week5/lecture.html#finding-spurious-patterns-within-natural-variance",
    "href": "Week5/lecture.html#finding-spurious-patterns-within-natural-variance",
    "title": "Week 5",
    "section": "Finding spurious patterns within natural variance",
    "text": "Finding spurious patterns within natural variance\nSo the questions we should ask, from a statistics perspective are:\n\n\n\nHow large is the expected variability over any given period?\nDoes the apparent downward trend in the period 2015-2022 fit within this variability, meaning we might just be looking at what is effectively noise?\nOr is the trend large enough to be seen without this random variability?\n\n\n\n\n\n\n\n\nFigure 2: Same as the prior plots, but highlighting the years from 2015 onward compared to the 1979-2022 trend. (Hausfather 2022)"
  },
  {
    "objectID": "Week5/lecture.html#other-types-of-misleading-or-inappropriate-visualisations",
    "href": "Week5/lecture.html#other-types-of-misleading-or-inappropriate-visualisations",
    "title": "Week 5",
    "section": "Other types of misleading or inappropriate visualisations",
    "text": "Other types of misleading or inappropriate visualisations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigures from (Bolton 2023)"
  },
  {
    "objectID": "Week5/lecture.html#concluding-thoughts",
    "href": "Week5/lecture.html#concluding-thoughts",
    "title": "Week 5",
    "section": "Concluding Thoughts",
    "text": "Concluding Thoughts\nWhen presented with a statistical analysis or visualisation, what questions should we ask?\nHow can we make sure we’re thinking about the data critically?"
  },
  {
    "objectID": "Week5/lecture.html#further-reading",
    "href": "Week5/lecture.html#further-reading",
    "title": "Week 5",
    "section": "Further Reading",
    "text": "Further Reading\n(Bolton 2023). How to spot spin and inappropriate use of statistics (Research Briefing No. 4446). UK House of Commons Library.\n(Bolton 2007). Statistical literacy guide: How to read charts (Research Briefing No. SN04445). UK House of Commons Library."
  },
  {
    "objectID": "Week5/lecture.html#references",
    "href": "Week5/lecture.html#references",
    "title": "Week 5",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "Week5/lecture.html#the-process-of-statistical-modeling",
    "href": "Week5/lecture.html#the-process-of-statistical-modeling",
    "title": "Week 5",
    "section": "The process of statistical modeling",
    "text": "The process of statistical modeling\nThere is a set of steps that we generally go through when we want to use our statistical model to test a scientific hypothesis:\n\nSpecify your question of interest\nIdentify or collect the appropriate data\nPrepare the data for analysis\nDetermine the appropriate model\nFit the model to the data\nCriticize the model to make sure it fits properly\nTest hypothesis and quantify effect size\nCommunicate your analysis"
  },
  {
    "objectID": "Week5/lecture.html#data-analysis-workflow",
    "href": "Week5/lecture.html#data-analysis-workflow",
    "title": "Week 5",
    "section": "Data Analysis Workflow",
    "text": "Data Analysis Workflow"
  },
  {
    "objectID": "Week5/lecture.html#import",
    "href": "Week5/lecture.html#import",
    "title": "Week 5",
    "section": "Import",
    "text": "Import\n\n\nThroughout, we have been using the tidyverse library of packages for data analysis.\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "Week5/lecture.html#import-1",
    "href": "Week5/lecture.html#import-1",
    "title": "Week 5",
    "section": "Import",
    "text": "Import\n\nThere are tools for reading data from almost any source:\n\nread_csv(), read_excel(), read_rds(), …\n\nWhen we load a dataset with a tidyverse() function, it will return a tibble\n\n\ndata &lt;- read_csv(\"data/Apple_Emissions/greenhouse_gas_emissions.csv\")"
  },
  {
    "objectID": "Week5/lecture.html#tidy",
    "href": "Week5/lecture.html#tidy",
    "title": "Week 5",
    "section": "Tidy",
    "text": "Tidy\nThe same data can be represented in multiple ways. Here’s the same data organized three different ways:\n\nEach dataset shows the same values of four variables: country, year, population, and number of documented cases of TB (tuberculosis), but each dataset organizes the values in a different way.\n\n\n\n\ntable1\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\ntable3\n\n# A tibble: 6 × 3\n  country      year rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n\n\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583"
  },
  {
    "objectID": "Week5/lecture.html#transform",
    "href": "Week5/lecture.html#transform",
    "title": "Week 5",
    "section": "Transform",
    "text": "Transform\nWe’ve dealt with data transformations quite a bit already. This includes operations like calculating the mean for different groups, or for multiple groups:\n\ndata |&gt;\n  group_by(category) |&gt;\n  summarise(\n    mean_emissions = mean(emissions, na.rm = TRUE),\n  )\n\n# A tibble: 2 × 2\n  category                     mean_emissions\n  &lt;chr&gt;                                 &lt;dbl&gt;\n1 Corporate emissions                  35594.\n2 Product life cycle emissions       5630000"
  },
  {
    "objectID": "Week5/lecture.html#visualize",
    "href": "Week5/lecture.html#visualize",
    "title": "Week 5",
    "section": "Visualize",
    "text": "Visualize\n\ndata |&gt;\n  group_by(category, fiscal_year) |&gt;\n  summarise(emissions = sum(emissions, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = fiscal_year, y = emissions, color = category)) +\n  geom_line()"
  },
  {
    "objectID": "Week5/lecture.html#communicate",
    "href": "Week5/lecture.html#communicate",
    "title": "Week 5",
    "section": "Communicate",
    "text": "Communicate\nThis is where we will dive into using Quarto. Start by downloading the Apple Emissions dataset from Moodle and open RStudio.\nWe’ll go through how to create and write a full analysis in a .qmd file using this dataset.\nRefer to our lecture notes specifically on using Quarto"
  },
  {
    "objectID": "Week5/lecture.html#references-1",
    "href": "Week5/lecture.html#references-1",
    "title": "Week 5",
    "section": "References",
    "text": "References\n\n\n\n\nBergstrom, Carl T., and Jevin D. West. 2021. Calling Bullshit: The Art of Scepticism in a Data-Driven World. First published by Allen Lane. An Allen Lane Book. London, UK USA Canada Ireland Australia: Penguin Books.\n\n\nBolton, Paul. 2007. “Statistical Literacy Guide: How to Read Charts.” Research Briefing SN04445. UK House of Commons Library. https://researchbriefings.files.parliament.uk/documents/SN04445/SN04445.pdf.\n\n\n———. 2023. “How to Spot Spin and Inappropriate Use of Statistics.” Research Briefing 4446. UK House of Commons Library. https://researchbriefings.files.parliament.uk/documents/SN04446/SN04446.pdf.\n\n\nHausfather, Zeke. 2022. “Factcheck: No, Global Warming Has Not ‘Paused’ over the Past Eight Years.” Carbon Brief. July 14, 2022. https://www.carbonbrief.org/factcheck-no-global-warming-has-not-paused-over-the-past-eight-years/.\n\n\nMonckton, Christopher. 2008. “Climate Sensitivity Reconsidered.” Physics and Society 37 (3): 6–19. https://higherlogicdownload.s3.amazonaws.com/APS/a05ec1cf-2e34-4fb3-816e-ea1497930d75/UploadedImages/Newsletter_PDF/july08.pdf.\n\n\n———. 2022. “The New Pause Lengthens to 7 Years 10 Months.” Watts Up With That? July 2, 2022. https://wattsupwiththat.com/2022/07/02/the-new-pause-lengthens-to-7-years-10-months/.\n\n\nNASA. 2025. “GISS Surface Temperature Analysis (V4): Global Maps.” NASA Goddard Institute for Space Studies. 2025. https://data.giss.nasa.gov/gistemp/maps/.\n\n\nPhillips, Melanie. 2022. “Sri Lanka Shows the Danger of Green Dogma.” The Times, July 11, 2022. https://www.thetimes.com/article/sri-lanka-shows-the-danger-of-green-dogma-sf69m752q.\n\n\nPidcock, Roz. 2015. “Explainer: How Do Scientists Measure Global Temperature?” Carbon Brief. January 16, 2015. https://www.carbonbrief.org/explainer-how-do-scientists-measure-global-temperature/.\n\n\nPoldrack, Russell A. 2023. Statistical Thinking. Analyzing Data in an Uncertain World. Princeton: Princeton University Press. https://statsthinking21.github.io/statsthinking21-core-site/.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. Second edition. Beijing ; Sebastopol, CA: O’Reilly."
  },
  {
    "objectID": "Week5/sampling-exercise-extended.html",
    "href": "Week5/sampling-exercise-extended.html",
    "title": "Analyzing Salaries in the AI Job Market",
    "section": "",
    "text": "In our previous exercises, we practiced taking samples and calculating means using some simple measurements. Now let’s apply these sampling concepts to the actual dataset and automate the sampling, rather than manually inputting each sample. We’ll look at salary data from the AI job market and use sampling to understand the variation in salaries across different job roles.\nThe previous pages were about introducing you to how Quarto works - here we’ll show what an actual analysis document might look like.\nYou can look at the actual .qmd file that generated this page here: sampling-exercise-extended.qmd\n\nLoad packages:\n\nlibrary(tidyverse)\n\n\nNow let’s load our AI jobs dataset. This dataset contains information about various jobs in the AI industry, including salaries, job titles, and other interesting information:\n\nai_jobs &lt;- read_csv(\"data/ai_jobs.csv\")\n\n# Let's take a look at what job titles we have\nai_jobs |&gt;\n  count(job_title) |&gt;\n  arrange(desc(n))\n\n# A tibble: 10 × 2\n   job_title                 n\n   &lt;chr&gt;                 &lt;int&gt;\n 1 Data Scientist           62\n 2 HR Manager               57\n 3 Cybersecurity Analyst    55\n 4 UX Designer              54\n 5 AI Researcher            51\n 6 Sales Manager            49\n 7 Marketing Specialist     48\n 8 Operations Manager       44\n 9 Software Engineer        41\n10 Product Manager          39\n\n\nLooking at the output above, we can see we have several different job titles in our dataset. For this analysis, let’s focus on comparing salaries between different levels of automation risk. Our goal is to see whether there is a relationship between salary levels and the risk of a job being automated.\n\n\nHint: When working with real data, it’s good practice to look at your data first before diving into analysis. This helps you understand what you’re working with and spot any potential issues.\n\nInstead of manually recording samples like we did in class, we can use R to take random samples from our dataset. Let’s take 1000 samples of size 50 from the low and high automation risk groups.\n\nWhen looking at this code, try to break it down into its component parts. We are using a for loop, which allows us to repeat a process multiple times. Start by understanding what is happening within the for loop - what are we doing each time?\n\nWe’re using the pipe operate to filter the dataset, then sample it, calculate the mean of the sample, and output the result into a list.\n\nBy repeating this in a for loop, we repeat this process many times, each time adding the new calculated sample mean on to the end of the list.\nNext, understand how we tell the for loop how many times to perform the sampling.\n\nWe define an n_samples variable at the beginning. Then, when starting up the for loop, we tell it to loop from 1 to n_samples: for(1:n_samples) {}.\n\n\n\nsample_size &lt;- 50\nn_samples &lt;- 1000\n\n# Set up an empty list to add to\nhigh_sample_means &lt;- c()\n\nfor (i in 1:n_samples) {\n  # Sample the High automation risk observations\n  mean &lt;- ai_jobs |&gt;\n    filter(automation_risk == \"High\") |&gt;\n    sample_n(sample_size) |&gt;\n    summarise(mean(salary_usd)) |&gt;\n    pull()\n\n  # Add to the sample_means list\n  high_sample_means &lt;- append(high_sample_means, mean)\n}\n\n# Repeat for the Low risk group\nlow_sample_means &lt;- c()\nfor (i in 1:n_samples) {\n  # Sample the High automation risk observations\n  mean &lt;- ai_jobs |&gt;\n    filter(automation_risk == \"Low\") |&gt;\n    sample_n(sample_size) |&gt;\n    summarise(mean(salary_usd)) |&gt;\n    pull()\n\n  # Add to the sample_means list\n  low_sample_means &lt;- append(low_sample_means, mean)\n}\n\nNow we have our sample means! Let’s combine these into a single table:\n\nmeans_table &lt;- tibble(\n  \"High\" = high_sample_means,\n  \"Low\" = low_sample_means,\n)\nmeans_table\n\n# A tibble: 1,000 × 2\n     High     Low\n    &lt;dbl&gt;   &lt;dbl&gt;\n 1 80325.  97347.\n 2 84461. 100727.\n 3 79401. 100062.\n 4 84083.  97947.\n 5 82547. 102260.\n 6 89679.  99635.\n 7 82624. 100586.\n 8 79814.  98631.\n 9 86217.  99474.\n10 81497.  97962.\n# ℹ 990 more rows\n\n\nAnd reshape it into a tidy long table format. This just means we’re stacking the two columns from the table above into one column with a label for which automation risk the sample is from:\n\nmeans_table &lt;- means_table |&gt;\n  gather(key = \"automation_risk\", value = \"sample_mean\")\n\nmeans_table\n\n# A tibble: 2,000 × 2\n   automation_risk sample_mean\n   &lt;chr&gt;                 &lt;dbl&gt;\n 1 High                 80325.\n 2 High                 84461.\n 3 High                 79401.\n 4 High                 84083.\n 5 High                 82547.\n 6 High                 89679.\n 7 High                 82624.\n 8 High                 79814.\n 9 High                 86217.\n10 High                 81497.\n# ℹ 1,990 more rows\n\n\n\nNow we can create a visualization to compare the distribution of sample means between these two job titles.\n\n# Create a plot comparing the distributions\nggplot(means_table, aes(x = sample_mean, fill = automation_risk)) +\n  geom_histogram(alpha = 0.5) +\n  theme_minimal() +\n  labs(\n    title = \"Distribution of Sample Means by Automation Risk\",\n    x = \"Mean Salary (USD)\",\n    y = \"Count\",\n    fill = \"Automation Risk\"\n  )\n\n\n\n\n\n\n\n\nThis visualization shows us how the sample means are distributed for each job title. The overlapping histograms make it easy to compare the two distributions. Another way we could visualise this is with side by side points with error bars:\n\n# Create a plot comparing the distributions\n\n# We need to start by calculating the mean and\n# standard deviation of the sampling distributions\n# to pass to the errorbar layer:\nsummary_stats &lt;- means_table |&gt;\n  group_by(automation_risk) |&gt;\n  summarise(\n    mean = mean(sample_mean),\n    sd = sd(sample_mean)\n  ) |&gt;\n  ggplot(aes(y = mean, x = automation_risk, color = automation_risk)) +\n  geom_point() +\n  geom_errorbar(aes(ymin = mean - (2 * sd), ymax = mean + (2 * sd))) +\n  theme_minimal() +\n  labs(\n    title = \"Distribution of Sample Means by Automation Risk\",\n    x = \"Automation Risk\",\n  ) +\n  scale_y_continuous(name = \"Mean Salary (USD)\", labels = scales::comma)\n\nThis plot shows us quite clearly that the sampling distributions are quite different between the different automation risk groups.\nThink back to our Hypothesis Testing using the t-test. The goal there was to tell whether the difference between the means in the two groups was significantly different. How we define statistically significant is based on the estimated sampling distribution. We estimated this based on just a single sample. Here, we’re directly looking at the the actually sampling distributions (for sample size = 25). Another way to interpret the t-test statistical significance is to look at whether these sampling distributions are far enough apart and have a low enough variance that their standard error bars don’t overlap.\nIn the boxplot, we can see that the external lines of the boxplots (representing 2 standard error) don’t overlap with each other. This would indicate that, with at least 95% confidence (remember, 2 SE encompasses 95% of the distribution), the mean of a given sample of 50 salaries from the High risk group would not overlap with the mean from the Low risk group.\nLet’s translate this into a t-test:\n\n# Start by drawing a new sample of 50 from each group\nhigh_sample &lt;- ai_jobs |&gt;\n  filter(automation_risk == \"High\") |&gt;\n  sample_n(sample_size) |&gt;\n  pull(salary_usd)\n\nlow_sample &lt;- ai_jobs |&gt;\n  filter(automation_risk == \"Low\") |&gt;\n  sample_n(sample_size) |&gt;\n  pull(salary_usd)\n\n# Run the t-test\nt.test(high_sample, low_sample)\n\n\n    Welch Two Sample t-test\n\ndata:  high_sample and low_sample\nt = -2.7435, df = 88.723, p-value = 0.007356\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -24138.721  -3859.707\nsample estimates:\nmean of x mean of y \n 82377.14  96376.36 \n\n\nYes! Our t-test results match up to what our plots showed! This demonstrates how the Hypothesis Testing framework allows us to estimate patterns in the sampling distribution even from a single sample.\n\nThis exercise shows how sampling helps us understand patterns in real-world data. Instead of looking at every single salary in our dataset, we can take samples and use their means to get a good idea of typical salaries in different groups.\nSome key points to notice:\n\nWe used the same basic sampling concepts as in our class exercise, used the power of R to take many simulated samples.\nWe can easily compare different groups (job titles) by taking samples from each group.\nVisualizing our sample means helps us see patterns that might not be obvious just looking at numbers.\nWe looked at the relationship between simulating the sampling distribution to estimating the properties of the sampling distribution to apply in hypothesis testing.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Sampling Exercise & Quarto",
      "Analyzing Salaries in the AI Job Market"
    ]
  },
  {
    "objectID": "Week5/sampling-exercise-extended.html#setting-up-our-analysis",
    "href": "Week5/sampling-exercise-extended.html#setting-up-our-analysis",
    "title": "Analyzing Salaries in the AI Job Market",
    "section": "",
    "text": "Load packages:\n\nlibrary(tidyverse)\n\n\nNow let’s load our AI jobs dataset. This dataset contains information about various jobs in the AI industry, including salaries, job titles, and other interesting information:\n\nai_jobs &lt;- read_csv(\"data/ai_jobs.csv\")\n\n# Let's take a look at what job titles we have\nai_jobs |&gt;\n  count(job_title) |&gt;\n  arrange(desc(n))\n\n# A tibble: 10 × 2\n   job_title                 n\n   &lt;chr&gt;                 &lt;int&gt;\n 1 Data Scientist           62\n 2 HR Manager               57\n 3 Cybersecurity Analyst    55\n 4 UX Designer              54\n 5 AI Researcher            51\n 6 Sales Manager            49\n 7 Marketing Specialist     48\n 8 Operations Manager       44\n 9 Software Engineer        41\n10 Product Manager          39\n\n\nLooking at the output above, we can see we have several different job titles in our dataset. For this analysis, let’s focus on comparing salaries between different levels of automation risk. Our goal is to see whether there is a relationship between salary levels and the risk of a job being automated.\n\n\nHint: When working with real data, it’s good practice to look at your data first before diving into analysis. This helps you understand what you’re working with and spot any potential issues.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Sampling Exercise & Quarto",
      "Analyzing Salaries in the AI Job Market"
    ]
  },
  {
    "objectID": "Week5/sampling-exercise-extended.html#taking-samples-from-our-population",
    "href": "Week5/sampling-exercise-extended.html#taking-samples-from-our-population",
    "title": "Analyzing Salaries in the AI Job Market",
    "section": "",
    "text": "Instead of manually recording samples like we did in class, we can use R to take random samples from our dataset. Let’s take 1000 samples of size 50 from the low and high automation risk groups.\n\nWhen looking at this code, try to break it down into its component parts. We are using a for loop, which allows us to repeat a process multiple times. Start by understanding what is happening within the for loop - what are we doing each time?\n\nWe’re using the pipe operate to filter the dataset, then sample it, calculate the mean of the sample, and output the result into a list.\n\nBy repeating this in a for loop, we repeat this process many times, each time adding the new calculated sample mean on to the end of the list.\nNext, understand how we tell the for loop how many times to perform the sampling.\n\nWe define an n_samples variable at the beginning. Then, when starting up the for loop, we tell it to loop from 1 to n_samples: for(1:n_samples) {}.\n\n\n\nsample_size &lt;- 50\nn_samples &lt;- 1000\n\n# Set up an empty list to add to\nhigh_sample_means &lt;- c()\n\nfor (i in 1:n_samples) {\n  # Sample the High automation risk observations\n  mean &lt;- ai_jobs |&gt;\n    filter(automation_risk == \"High\") |&gt;\n    sample_n(sample_size) |&gt;\n    summarise(mean(salary_usd)) |&gt;\n    pull()\n\n  # Add to the sample_means list\n  high_sample_means &lt;- append(high_sample_means, mean)\n}\n\n# Repeat for the Low risk group\nlow_sample_means &lt;- c()\nfor (i in 1:n_samples) {\n  # Sample the High automation risk observations\n  mean &lt;- ai_jobs |&gt;\n    filter(automation_risk == \"Low\") |&gt;\n    sample_n(sample_size) |&gt;\n    summarise(mean(salary_usd)) |&gt;\n    pull()\n\n  # Add to the sample_means list\n  low_sample_means &lt;- append(low_sample_means, mean)\n}\n\nNow we have our sample means! Let’s combine these into a single table:\n\nmeans_table &lt;- tibble(\n  \"High\" = high_sample_means,\n  \"Low\" = low_sample_means,\n)\nmeans_table\n\n# A tibble: 1,000 × 2\n     High     Low\n    &lt;dbl&gt;   &lt;dbl&gt;\n 1 80325.  97347.\n 2 84461. 100727.\n 3 79401. 100062.\n 4 84083.  97947.\n 5 82547. 102260.\n 6 89679.  99635.\n 7 82624. 100586.\n 8 79814.  98631.\n 9 86217.  99474.\n10 81497.  97962.\n# ℹ 990 more rows\n\n\nAnd reshape it into a tidy long table format. This just means we’re stacking the two columns from the table above into one column with a label for which automation risk the sample is from:\n\nmeans_table &lt;- means_table |&gt;\n  gather(key = \"automation_risk\", value = \"sample_mean\")\n\nmeans_table\n\n# A tibble: 2,000 × 2\n   automation_risk sample_mean\n   &lt;chr&gt;                 &lt;dbl&gt;\n 1 High                 80325.\n 2 High                 84461.\n 3 High                 79401.\n 4 High                 84083.\n 5 High                 82547.\n 6 High                 89679.\n 7 High                 82624.\n 8 High                 79814.\n 9 High                 86217.\n10 High                 81497.\n# ℹ 1,990 more rows",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Sampling Exercise & Quarto",
      "Analyzing Salaries in the AI Job Market"
    ]
  },
  {
    "objectID": "Week5/sampling-exercise-extended.html#visualizing-our-sample-means",
    "href": "Week5/sampling-exercise-extended.html#visualizing-our-sample-means",
    "title": "Analyzing Salaries in the AI Job Market",
    "section": "",
    "text": "Now we can create a visualization to compare the distribution of sample means between these two job titles.\n\n# Create a plot comparing the distributions\nggplot(means_table, aes(x = sample_mean, fill = automation_risk)) +\n  geom_histogram(alpha = 0.5) +\n  theme_minimal() +\n  labs(\n    title = \"Distribution of Sample Means by Automation Risk\",\n    x = \"Mean Salary (USD)\",\n    y = \"Count\",\n    fill = \"Automation Risk\"\n  )\n\n\n\n\n\n\n\n\nThis visualization shows us how the sample means are distributed for each job title. The overlapping histograms make it easy to compare the two distributions. Another way we could visualise this is with side by side points with error bars:\n\n# Create a plot comparing the distributions\n\n# We need to start by calculating the mean and\n# standard deviation of the sampling distributions\n# to pass to the errorbar layer:\nsummary_stats &lt;- means_table |&gt;\n  group_by(automation_risk) |&gt;\n  summarise(\n    mean = mean(sample_mean),\n    sd = sd(sample_mean)\n  ) |&gt;\n  ggplot(aes(y = mean, x = automation_risk, color = automation_risk)) +\n  geom_point() +\n  geom_errorbar(aes(ymin = mean - (2 * sd), ymax = mean + (2 * sd))) +\n  theme_minimal() +\n  labs(\n    title = \"Distribution of Sample Means by Automation Risk\",\n    x = \"Automation Risk\",\n  ) +\n  scale_y_continuous(name = \"Mean Salary (USD)\", labels = scales::comma)\n\nThis plot shows us quite clearly that the sampling distributions are quite different between the different automation risk groups.\nThink back to our Hypothesis Testing using the t-test. The goal there was to tell whether the difference between the means in the two groups was significantly different. How we define statistically significant is based on the estimated sampling distribution. We estimated this based on just a single sample. Here, we’re directly looking at the the actually sampling distributions (for sample size = 25). Another way to interpret the t-test statistical significance is to look at whether these sampling distributions are far enough apart and have a low enough variance that their standard error bars don’t overlap.\nIn the boxplot, we can see that the external lines of the boxplots (representing 2 standard error) don’t overlap with each other. This would indicate that, with at least 95% confidence (remember, 2 SE encompasses 95% of the distribution), the mean of a given sample of 50 salaries from the High risk group would not overlap with the mean from the Low risk group.\nLet’s translate this into a t-test:\n\n# Start by drawing a new sample of 50 from each group\nhigh_sample &lt;- ai_jobs |&gt;\n  filter(automation_risk == \"High\") |&gt;\n  sample_n(sample_size) |&gt;\n  pull(salary_usd)\n\nlow_sample &lt;- ai_jobs |&gt;\n  filter(automation_risk == \"Low\") |&gt;\n  sample_n(sample_size) |&gt;\n  pull(salary_usd)\n\n# Run the t-test\nt.test(high_sample, low_sample)\n\n\n    Welch Two Sample t-test\n\ndata:  high_sample and low_sample\nt = -2.7435, df = 88.723, p-value = 0.007356\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -24138.721  -3859.707\nsample estimates:\nmean of x mean of y \n 82377.14  96376.36 \n\n\nYes! Our t-test results match up to what our plots showed! This demonstrates how the Hypothesis Testing framework allows us to estimate patterns in the sampling distribution even from a single sample.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Sampling Exercise & Quarto",
      "Analyzing Salaries in the AI Job Market"
    ]
  },
  {
    "objectID": "Week5/sampling-exercise-extended.html#what-have-we-learned",
    "href": "Week5/sampling-exercise-extended.html#what-have-we-learned",
    "title": "Analyzing Salaries in the AI Job Market",
    "section": "",
    "text": "This exercise shows how sampling helps us understand patterns in real-world data. Instead of looking at every single salary in our dataset, we can take samples and use their means to get a good idea of typical salaries in different groups.\nSome key points to notice:\n\nWe used the same basic sampling concepts as in our class exercise, used the power of R to take many simulated samples.\nWe can easily compare different groups (job titles) by taking samples from each group.\nVisualizing our sample means helps us see patterns that might not be obvious just looking at numbers.\nWe looked at the relationship between simulating the sampling distribution to estimating the properties of the sampling distribution to apply in hypothesis testing.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Sampling Exercise & Quarto",
      "Analyzing Salaries in the AI Job Market"
    ]
  },
  {
    "objectID": "Week5/sampling-exercise-walkthrough.html",
    "href": "Week5/sampling-exercise-walkthrough.html",
    "title": "Working with Quarto",
    "section": "",
    "text": "Now that we’ve seen how .R files help us save and organize our code, let’s explore how Quarto can make our analysis even better. When we write code in an .R file, we often find ourselves adding comments to explain what each part does. These comments help us remember our thinking and help others understand our code. But what if we could write proper explanations, include our code, and show the results all in one place?\nThis is exactly what Quarto lets us do. Think of it as a document where we can write explanations in normal text, include our R code in special sections called “chunks”, and show the output of that code (like tables and plots) right where we need it. This makes it much easier to explain our analysis to others - or even to ourselves when we come back to it later!\nYou can download the .R file, data file, and source .qmd file for this page here:\n Download Resources \n\nLet’s take the sampling exercise we did in class and see how we can make it clearer using Quarto. We’ll take the same code from our .R file but now we can properly explain what each part does.\nFirst, just like in our .R file, we need to load our tidyverse package. In Quarto, we put R code in special sections marked with three backticks and {r}. These are called “code chunks”:\n\nlibrary(tidyverse)\n\nNow we can input our sample data. Remember these are the measurements we took in class:\n\nsample_1 &lt;- c(83.2, 82.6, 82.6, 82.6, 93.2, 94, 94, 48.5, 33.6, 33.6)\nsample_2 &lt;- c(48.8, 86.5, 67.5, 84.5, 97.6, 92, 60.7, 108, 84.3, 58.5)\nsample_3 &lt;- c(128, 53.7, 70.9, 75.2, 84.9, 91.2, 70.2, 82, 88.8, 82)\n\nI’ve just shown three samples here to keep things clear, but you can add all of your samples just like we did in the .R file. If we ask R to output one of our samples, it will print out within the document:\n\nsample_2\n\n [1]  48.8  86.5  67.5  84.5  97.6  92.0  60.7 108.0  84.3  58.5\n\n\nIf you are viewing the .qmd document itself (i.e. not the rendered version) try running these code blocks by pressing the ‘run’ button (looks like a triangular play button).\nOne nice thing about Quarto is that we can explain our thinking right alongside our code. For instance, we can explain that each sample contains 10 measurements, and we’re storing them in variables named sample_1, sample_2, etc.\nNow let’s calculate the mean for each sample:\n\nmean_1 &lt;- mean(sample_1)\nmean_2 &lt;- mean(sample_2)\nmean_3 &lt;- mean(sample_3)\n\nWe can even show the results right in our text! For example, the mean of our first sample is 72.79 (this is printed by just putting {r} mean_1 inline) This is much nicer than having to print values separately - we can discuss the numbers right where they make sense in our explanation.\nNext, let’s create our table of means just like we did in the .R file:\n\nsample_means &lt;- tibble(mean_values = c(mean_1, mean_2, mean_3))\n\nsample_means\n\n# A tibble: 3 × 1\n  mean_values\n        &lt;dbl&gt;\n1        72.8\n2        78.8\n3        82.7\n\n\nSee how Quarto automatically displays the table? In our .R file, we had to specifically print the table by typing sample_means on a line by itself. Here, Quarto shows us the output of our code automatically.\nFinally, let’s create a histogram of our sample means:\n\nggplot(sample_means, mapping = aes(x = mean_values)) +\n  geom_histogram() +\n  theme_minimal() +\n  labs(\n    title = \"Distribution of Sample Means\",\n    x = \"Mean Value\",\n    y = \"Count\"\n  )\n\n\n\n\n\n\n\nThe plot appears right after the code that created it. This makes it easy to discuss what we see in the plot right afterwards.\n\nYou might be wondering why we’d use Quarto instead of just writing comments in our .R file. Here are a few reasons:\n\nEverything is in one place - your explanations, code, and the results all flow together naturally.\nYou can write proper explanations using text formatting - headings, bullet points, even links to other resources.\nThe output (like tables and plots) appears right where you need it, making it easier to refer to specific results.\nWhen you share your analysis with others, they can see both your code AND your thinking.\n\nWorking in Quarto is very similar to working with .R files, but with some powerful extra features. Let’s look at how to work with a Quarto document:\n\nWhen you’re writing a Quarto document, you can:\n\nClick the “Run” button (play icon) at the top of any code chunk to run just that chunk\nUse the same keyboard shortcuts we learned for .R files\nSee the output right below each chunk as you go\n\nOne of the great things about Quarto is that you can create different types of documents from the same source. When you click “Render” at the top of the document, Quarto will create a final version that combines your code, text, and results.\nBy default, Quarto creates an HTML document that you can open in any web browser. This is great for sharing your analysis because anyone can view it, even if they don’t have R installed. The preview will appear right inside RStudio, making it easy to see how your final document will look.\nYou can also render to other formats like PDF or Word documents. This is really useful when you need to share your analysis in different ways - maybe your professor wants a PDF for an assignment, or your colleague prefers to read documents in Word.\nTo preview your document as you work:\n\nClick the “Render” button (or press Cmd/Ctrl + Shift + K)\nRStudio will show you a preview of your document\nThe preview updates automatically when you render again\n\nThis makes it easy to build your document step by step and see exactly how it will look to others.\n\n\nHint: Just like with .R files, build your analysis step by step. Run each chunk as you write it to make sure it works before moving on. This makes it much easier to find and fix any problems!\nThis workflow lets you develop your analysis gradually, explaining each step as you go. It’s like having a lab notebook where you can record both what you did and why you did it.\nIn the next section, we’ll use these Quarto features to do a more complete analysis of some the AI Jobs data! We’ll look at salary data from the AI job market and see how we can use sampling to understand salary variations across different job roles. This will show you how the sampling concepts we’ve learned can help us answer interesting questions about real data.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Sampling Exercise & Quarto",
      "Working with Quarto"
    ]
  },
  {
    "objectID": "Week5/sampling-exercise-walkthrough.html#converting-our-sampling-exercise",
    "href": "Week5/sampling-exercise-walkthrough.html#converting-our-sampling-exercise",
    "title": "Working with Quarto",
    "section": "",
    "text": "Let’s take the sampling exercise we did in class and see how we can make it clearer using Quarto. We’ll take the same code from our .R file but now we can properly explain what each part does.\nFirst, just like in our .R file, we need to load our tidyverse package. In Quarto, we put R code in special sections marked with three backticks and {r}. These are called “code chunks”:\n\nlibrary(tidyverse)\n\nNow we can input our sample data. Remember these are the measurements we took in class:\n\nsample_1 &lt;- c(83.2, 82.6, 82.6, 82.6, 93.2, 94, 94, 48.5, 33.6, 33.6)\nsample_2 &lt;- c(48.8, 86.5, 67.5, 84.5, 97.6, 92, 60.7, 108, 84.3, 58.5)\nsample_3 &lt;- c(128, 53.7, 70.9, 75.2, 84.9, 91.2, 70.2, 82, 88.8, 82)\n\nI’ve just shown three samples here to keep things clear, but you can add all of your samples just like we did in the .R file. If we ask R to output one of our samples, it will print out within the document:\n\nsample_2\n\n [1]  48.8  86.5  67.5  84.5  97.6  92.0  60.7 108.0  84.3  58.5\n\n\nIf you are viewing the .qmd document itself (i.e. not the rendered version) try running these code blocks by pressing the ‘run’ button (looks like a triangular play button).\nOne nice thing about Quarto is that we can explain our thinking right alongside our code. For instance, we can explain that each sample contains 10 measurements, and we’re storing them in variables named sample_1, sample_2, etc.\nNow let’s calculate the mean for each sample:\n\nmean_1 &lt;- mean(sample_1)\nmean_2 &lt;- mean(sample_2)\nmean_3 &lt;- mean(sample_3)\n\nWe can even show the results right in our text! For example, the mean of our first sample is 72.79 (this is printed by just putting {r} mean_1 inline) This is much nicer than having to print values separately - we can discuss the numbers right where they make sense in our explanation.\nNext, let’s create our table of means just like we did in the .R file:\n\nsample_means &lt;- tibble(mean_values = c(mean_1, mean_2, mean_3))\n\nsample_means\n\n# A tibble: 3 × 1\n  mean_values\n        &lt;dbl&gt;\n1        72.8\n2        78.8\n3        82.7\n\n\nSee how Quarto automatically displays the table? In our .R file, we had to specifically print the table by typing sample_means on a line by itself. Here, Quarto shows us the output of our code automatically.\nFinally, let’s create a histogram of our sample means:\n\nggplot(sample_means, mapping = aes(x = mean_values)) +\n  geom_histogram() +\n  theme_minimal() +\n  labs(\n    title = \"Distribution of Sample Means\",\n    x = \"Mean Value\",\n    y = \"Count\"\n  )\n\n\n\n\n\n\n\nThe plot appears right after the code that created it. This makes it easy to discuss what we see in the plot right afterwards.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Sampling Exercise & Quarto",
      "Working with Quarto"
    ]
  },
  {
    "objectID": "Week5/sampling-exercise-walkthrough.html#why-quarto-makes-life-easier",
    "href": "Week5/sampling-exercise-walkthrough.html#why-quarto-makes-life-easier",
    "title": "Working with Quarto",
    "section": "",
    "text": "You might be wondering why we’d use Quarto instead of just writing comments in our .R file. Here are a few reasons:\n\nEverything is in one place - your explanations, code, and the results all flow together naturally.\nYou can write proper explanations using text formatting - headings, bullet points, even links to other resources.\nThe output (like tables and plots) appears right where you need it, making it easier to refer to specific results.\nWhen you share your analysis with others, they can see both your code AND your thinking.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Sampling Exercise & Quarto",
      "Working with Quarto"
    ]
  },
  {
    "objectID": "Week5/sampling-exercise-walkthrough.html#workflow-in-quarto",
    "href": "Week5/sampling-exercise-walkthrough.html#workflow-in-quarto",
    "title": "Working with Quarto",
    "section": "",
    "text": "Working in Quarto is very similar to working with .R files, but with some powerful extra features. Let’s look at how to work with a Quarto document:\n\nWhen you’re writing a Quarto document, you can:\n\nClick the “Run” button (play icon) at the top of any code chunk to run just that chunk\nUse the same keyboard shortcuts we learned for .R files\nSee the output right below each chunk as you go\n\nOne of the great things about Quarto is that you can create different types of documents from the same source. When you click “Render” at the top of the document, Quarto will create a final version that combines your code, text, and results.\nBy default, Quarto creates an HTML document that you can open in any web browser. This is great for sharing your analysis because anyone can view it, even if they don’t have R installed. The preview will appear right inside RStudio, making it easy to see how your final document will look.\nYou can also render to other formats like PDF or Word documents. This is really useful when you need to share your analysis in different ways - maybe your professor wants a PDF for an assignment, or your colleague prefers to read documents in Word.\nTo preview your document as you work:\n\nClick the “Render” button (or press Cmd/Ctrl + Shift + K)\nRStudio will show you a preview of your document\nThe preview updates automatically when you render again\n\nThis makes it easy to build your document step by step and see exactly how it will look to others.\n\n\nHint: Just like with .R files, build your analysis step by step. Run each chunk as you write it to make sure it works before moving on. This makes it much easier to find and fix any problems!\nThis workflow lets you develop your analysis gradually, explaining each step as you go. It’s like having a lab notebook where you can record both what you did and why you did it.\nIn the next section, we’ll use these Quarto features to do a more complete analysis of some the AI Jobs data! We’ll look at salary data from the AI job market and see how we can use sampling to understand salary variations across different job roles. This will show you how the sampling concepts we’ve learned can help us answer interesting questions about real data.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Sampling Exercise & Quarto",
      "Working with Quarto"
    ]
  },
  {
    "objectID": "Week6/notes.html",
    "href": "Week6/notes.html",
    "title": "Correlation and Regression",
    "section": "",
    "text": "Slides\n\n Download PDF Slides \n\n\nThis is an embedded &lt;a target=\"_blank\" href=\"https://office.com\"&gt;Microsoft Office&lt;/a&gt; presentation, powered by &lt;a target=\"_blank\" href=\"https://office.com/webapps\"&gt;Office&lt;/a&gt;.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 6",
      "Correlation and Regression"
    ]
  },
  {
    "objectID": "Week6/notes.html#this-weeks-lecture",
    "href": "Week6/notes.html#this-weeks-lecture",
    "title": "Correlation and Regression",
    "section": "",
    "text": "Slides\n\n Download PDF Slides \n\n\nThis is an embedded &lt;a target=\"_blank\" href=\"https://office.com\"&gt;Microsoft Office&lt;/a&gt; presentation, powered by &lt;a target=\"_blank\" href=\"https://office.com/webapps\"&gt;Office&lt;/a&gt;.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 6",
      "Correlation and Regression"
    ]
  },
  {
    "objectID": "Week7/1-content-expanded.html",
    "href": "Week7/1-content-expanded.html",
    "title": "The General Linear Model: Multiple Variables",
    "section": "",
    "text": "Before wrapping up our discussion of statistical tests, let’s first build up our understanding of regression from simple to multiple predictor variables.\n\n\n\nThe General Linear Model has two key components:\n\n\nVariables:\n\n\nOutcome (y): What we’re trying to understand\n\nPredictors (x): Factors that might explain the outcome\n\n\n\nParameters:\n\n\nIntercept (β₀): Base value when predictors are 0\n\nCoefficients (β₁, β₂, etc.): Effects of predictors\n\nError (ε): What the model doesn’t explain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo understand the General Linear Model, we need to break it down into its building blocks.\nFirst, we have two types of variables:\n\nThe outcome variable (y): This is what we’re trying to understand, explain, or predict. It’s also called the dependent variable, response variable, or target variable. Examples include test scores, blood pressure, customer satisfaction, or income.\nPredictor variables (x): These are the factors that might explain or predict the outcome. They’re also called independent variables, explanatory variables, or features. Examples might be study time, medication type, service quality metrics, or years of education.\n\nNext, we have parameters that describe the relationship between these variables:\n\nThe intercept (β₀): This is the baseline value of y when all predictors are zero. It’s the starting point of our model.\nCoefficients (β₁, β₂, etc.): These tell us how much y changes when the corresponding predictor changes by one unit, holding all other predictors constant. The coefficients quantify the effects of our predictors.\nError term (ε): This represents what our model doesn’t explain - the deviation between our model’s predictions and the actual data. A good model minimizes this error.\n\nThe visualization shows these components:\n\nBlue dots represent the data points (observations)\nThe red line is our model, with the intercept (β₀) as the starting point and the slope (β₁) showing the effect of the predictor\nThe dashed gray lines show the error (ε) for some points - the difference between what the model predicts and the actual values\n\nUnderstanding these components gives us the foundation to see how different statistical tests are variations of the same underlying model.\n\n\nIn simple linear regression, we have one outcome variable and one predictor:\n\n\nKey components:\n\n\ny is the outcome we want to predict\n\n\\beta_0 is the intercept (value of y when x = 0)\n\n\\beta_1 is the slope (effect of the predictor)\n\nx_1 is the predictor variable\n\n\\varepsilon is the error term\n\nExample: Predicting salary based on years of experience\n\ny = \\beta_0 + \\beta_1 x_1 + \\varepsilon\n\n\n\n\n\n\n\n\n\n\n\nSimple linear regression is where most students begin their regression journey. It models the relationship between one outcome variable (y) and one predictor variable (x).\nThe model estimates two key parameters:\n\nThe intercept (β₀) represents the predicted value of y when x equals zero\nThe slope (β₁) represents how much y changes when x increases by one unit\n\nIn our example, we’re predicting salary based on years of experience: - Each additional year of experience is associated with approximately $2,500 more in salary - The intercept suggests that someone with zero experience would have a salary around $30,000\nThe blue dots represent individual data points, while the red line shows our model’s prediction. The distance between each point and the line represents the error term (ε) - what our model doesn’t explain.\nSimple linear regression provides a foundation, but in real-world situations, outcomes are typically influenced by multiple factors. That’s where multiple regression comes in.\n\n\nWhat if multiple factors affect our outcome? Multiple regression extends the model:\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon\nKey advantages:\n\nModels real-world complexity\nAccounts for multiple influences\nControls for confounding variables\nImproves prediction accuracy\nAllows comparing relative importance of predictors\n\n\n\n\nExample: Predicting salary based on years of experience AND performance rating\n\n\n\n\n\n\n\n\n\n\nMultiple regression extends our model by adding more predictor variables. This allows us to account for the complex, multifaceted nature of real-world relationships.\n\n\n\nNow our model includes:\n\nThe intercept (β₀): The predicted value of y when all predictors are zero\nMultiple slope coefficients (β₁, β₂, etc.): Each representing the effect of its corresponding predictor when all other predictors are held constant\n\nThis “holding other variables constant” is a crucial concept. It means that each coefficient tells us the unique effect of that predictor, controlling for the effects of all other predictors in the model.\nIn our example, we’re now predicting salary based on both years of experience and performance rating:\n\nEach additional year of experience is associated with about $2,000 more in salary, holding performance constant\nEach additional point in performance rating is associated with about $8,000 more in salary, holding experience constant\n\n\n\nMultiple regression provides several advantages:\n\nIt models the complexity of real-world situations where outcomes are influenced by multiple factors\nIt allows us to control for confounding variables\nIt often provides more accurate predictions than simple regression\nIt helps us understand the relative importance of different predictors\n\nThis approach can be extended to include any number of predictors, creating a multidimensional hyperplane that we can’t easily visualize but that follows the same principles.\n\nThe model can be extended to include any number of predictors:\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + ... + \\beta_n x_n + \\varepsilon\n\n\n\nCode# Example with multiple predictors using HR data\nhr_data &lt;- read_sav(\"data/dataset-abc-insurance-hr-data.sav\") |&gt;\n  janitor::clean_names() |&gt;\n  mutate(gender = factor(gender, levels = 1:2, labels = c(\"Female\", \"Male\")))\n\n\n\nCode# Build model with multiple predictors\nfull_model &lt;- lm(\n  salarygrade ~ gender + tenure +\n    evaluation + age + job_satisfaction,\n  data = hr_data\n)\n\n\n\n\n\n\n\nPredictor\nEffect on Salary\np-value\n\n\n\n(Intercept)\n-0.079\n0.563\n\n\ngenderMale\n0.354\n0.000\n\n\ntenure\n0.103\n0.000\n\n\nevaluation\n0.022\n0.439\n\n\nage\n0.023\n0.000\n\n\njob_satisfaction\n0.177\n0.000\n\n\n\n\n\n\n\n\nWe can continue extending our multiple regression model to include any number of predictors. The general form remains the same, with each new predictor getting its own coefficient that represents its unique effect on the outcome.\nIn this example, we’re using real HR data to predict salary grade based on multiple factors:\n\nGender (categorical: male/female)\nTenure (years of experience)\nEvaluation (performance rating)\nAge (in years)\nJob satisfaction (rating scale)\n\nThe model output shows:\n\nEach predictor’s coefficient (effect on salary)\nThe statistical significance of each effect (p-value)\n\nThe interpretation of each coefficient is:\n\nGender: Being male is associated with a 5.9 point higher salary grade, holding all else constant\nTenure: Each additional year of experience is associated with a 1.4 point increase in salary grade\nEvaluation: Each additional point in performance rating is associated with a 3.9 point increase in salary grade\nAge: Each additional year of age is associated with a -0.02 point change in salary grade (effectively zero)\nJob satisfaction: Each additional point in job satisfaction is associated with a 0.4 point increase in salary grade\n\nFrom these results, we can see that gender, tenure, and evaluation ratings have the strongest effects on salary, while age appears to have no meaningful impact.\nThis approach allows us to model complex real-world situations where many factors simultaneously influence an outcome. It’s a powerful tool for both prediction and understanding the relative importance of different factors.\nThe multiple regression model we’ve just explored is actually the general form of the General Linear Model (GLM), which we’ll see can represent many different statistical tests.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 7",
      "The General Linear Model: Multiple Variables"
    ]
  },
  {
    "objectID": "Week7/1-content-expanded.html#from-simple-to-multiple-regression",
    "href": "Week7/1-content-expanded.html#from-simple-to-multiple-regression",
    "title": "The General Linear Model: Multiple Variables",
    "section": "",
    "text": "Before wrapping up our discussion of statistical tests, let’s first build up our understanding of regression from simple to multiple predictor variables.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 7",
      "The General Linear Model: Multiple Variables"
    ]
  },
  {
    "objectID": "Week7/1-content-expanded.html#understanding-the-building-blocks",
    "href": "Week7/1-content-expanded.html#understanding-the-building-blocks",
    "title": "The General Linear Model: Multiple Variables",
    "section": "",
    "text": "The General Linear Model has two key components:\n\n\nVariables:\n\n\nOutcome (y): What we’re trying to understand\n\nPredictors (x): Factors that might explain the outcome\n\n\n\nParameters:\n\n\nIntercept (β₀): Base value when predictors are 0\n\nCoefficients (β₁, β₂, etc.): Effects of predictors\n\nError (ε): What the model doesn’t explain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo understand the General Linear Model, we need to break it down into its building blocks.\nFirst, we have two types of variables:\n\nThe outcome variable (y): This is what we’re trying to understand, explain, or predict. It’s also called the dependent variable, response variable, or target variable. Examples include test scores, blood pressure, customer satisfaction, or income.\nPredictor variables (x): These are the factors that might explain or predict the outcome. They’re also called independent variables, explanatory variables, or features. Examples might be study time, medication type, service quality metrics, or years of education.\n\nNext, we have parameters that describe the relationship between these variables:\n\nThe intercept (β₀): This is the baseline value of y when all predictors are zero. It’s the starting point of our model.\nCoefficients (β₁, β₂, etc.): These tell us how much y changes when the corresponding predictor changes by one unit, holding all other predictors constant. The coefficients quantify the effects of our predictors.\nError term (ε): This represents what our model doesn’t explain - the deviation between our model’s predictions and the actual data. A good model minimizes this error.\n\nThe visualization shows these components:\n\nBlue dots represent the data points (observations)\nThe red line is our model, with the intercept (β₀) as the starting point and the slope (β₁) showing the effect of the predictor\nThe dashed gray lines show the error (ε) for some points - the difference between what the model predicts and the actual values\n\nUnderstanding these components gives us the foundation to see how different statistical tests are variations of the same underlying model.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 7",
      "The General Linear Model: Multiple Variables"
    ]
  },
  {
    "objectID": "Week7/1-content-expanded.html#simple-linear-regression-one-predictor",
    "href": "Week7/1-content-expanded.html#simple-linear-regression-one-predictor",
    "title": "The General Linear Model: Multiple Variables",
    "section": "",
    "text": "In simple linear regression, we have one outcome variable and one predictor:\n\n\nKey components:\n\n\ny is the outcome we want to predict\n\n\\beta_0 is the intercept (value of y when x = 0)\n\n\\beta_1 is the slope (effect of the predictor)\n\nx_1 is the predictor variable\n\n\\varepsilon is the error term\n\nExample: Predicting salary based on years of experience\n\ny = \\beta_0 + \\beta_1 x_1 + \\varepsilon\n\n\n\n\n\n\n\n\n\n\n\nSimple linear regression is where most students begin their regression journey. It models the relationship between one outcome variable (y) and one predictor variable (x).\nThe model estimates two key parameters:\n\nThe intercept (β₀) represents the predicted value of y when x equals zero\nThe slope (β₁) represents how much y changes when x increases by one unit\n\nIn our example, we’re predicting salary based on years of experience: - Each additional year of experience is associated with approximately $2,500 more in salary - The intercept suggests that someone with zero experience would have a salary around $30,000\nThe blue dots represent individual data points, while the red line shows our model’s prediction. The distance between each point and the line represents the error term (ε) - what our model doesn’t explain.\nSimple linear regression provides a foundation, but in real-world situations, outcomes are typically influenced by multiple factors. That’s where multiple regression comes in.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 7",
      "The General Linear Model: Multiple Variables"
    ]
  },
  {
    "objectID": "Week7/1-content-expanded.html#multiple-regression-adding-more-predictors",
    "href": "Week7/1-content-expanded.html#multiple-regression-adding-more-predictors",
    "title": "The General Linear Model: Multiple Variables",
    "section": "",
    "text": "What if multiple factors affect our outcome? Multiple regression extends the model:\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon\nKey advantages:\n\nModels real-world complexity\nAccounts for multiple influences\nControls for confounding variables\nImproves prediction accuracy\nAllows comparing relative importance of predictors\n\n\n\n\nExample: Predicting salary based on years of experience AND performance rating\n\n\n\n\n\n\n\n\n\n\nMultiple regression extends our model by adding more predictor variables. This allows us to account for the complex, multifaceted nature of real-world relationships.\n\n\n\nNow our model includes:\n\nThe intercept (β₀): The predicted value of y when all predictors are zero\nMultiple slope coefficients (β₁, β₂, etc.): Each representing the effect of its corresponding predictor when all other predictors are held constant\n\nThis “holding other variables constant” is a crucial concept. It means that each coefficient tells us the unique effect of that predictor, controlling for the effects of all other predictors in the model.\nIn our example, we’re now predicting salary based on both years of experience and performance rating:\n\nEach additional year of experience is associated with about $2,000 more in salary, holding performance constant\nEach additional point in performance rating is associated with about $8,000 more in salary, holding experience constant\n\n\n\nMultiple regression provides several advantages:\n\nIt models the complexity of real-world situations where outcomes are influenced by multiple factors\nIt allows us to control for confounding variables\nIt often provides more accurate predictions than simple regression\nIt helps us understand the relative importance of different predictors\n\nThis approach can be extended to include any number of predictors, creating a multidimensional hyperplane that we can’t easily visualize but that follows the same principles.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 7",
      "The General Linear Model: Multiple Variables"
    ]
  },
  {
    "objectID": "Week7/1-content-expanded.html#extending-to-many-predictors",
    "href": "Week7/1-content-expanded.html#extending-to-many-predictors",
    "title": "The General Linear Model: Multiple Variables",
    "section": "",
    "text": "The model can be extended to include any number of predictors:\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + ... + \\beta_n x_n + \\varepsilon\n\n\n\nCode# Example with multiple predictors using HR data\nhr_data &lt;- read_sav(\"data/dataset-abc-insurance-hr-data.sav\") |&gt;\n  janitor::clean_names() |&gt;\n  mutate(gender = factor(gender, levels = 1:2, labels = c(\"Female\", \"Male\")))\n\n\n\nCode# Build model with multiple predictors\nfull_model &lt;- lm(\n  salarygrade ~ gender + tenure +\n    evaluation + age + job_satisfaction,\n  data = hr_data\n)\n\n\n\n\n\n\n\nPredictor\nEffect on Salary\np-value\n\n\n\n(Intercept)\n-0.079\n0.563\n\n\ngenderMale\n0.354\n0.000\n\n\ntenure\n0.103\n0.000\n\n\nevaluation\n0.022\n0.439\n\n\nage\n0.023\n0.000\n\n\njob_satisfaction\n0.177\n0.000\n\n\n\n\n\n\n\n\nWe can continue extending our multiple regression model to include any number of predictors. The general form remains the same, with each new predictor getting its own coefficient that represents its unique effect on the outcome.\nIn this example, we’re using real HR data to predict salary grade based on multiple factors:\n\nGender (categorical: male/female)\nTenure (years of experience)\nEvaluation (performance rating)\nAge (in years)\nJob satisfaction (rating scale)\n\nThe model output shows:\n\nEach predictor’s coefficient (effect on salary)\nThe statistical significance of each effect (p-value)\n\nThe interpretation of each coefficient is:\n\nGender: Being male is associated with a 5.9 point higher salary grade, holding all else constant\nTenure: Each additional year of experience is associated with a 1.4 point increase in salary grade\nEvaluation: Each additional point in performance rating is associated with a 3.9 point increase in salary grade\nAge: Each additional year of age is associated with a -0.02 point change in salary grade (effectively zero)\nJob satisfaction: Each additional point in job satisfaction is associated with a 0.4 point increase in salary grade\n\nFrom these results, we can see that gender, tenure, and evaluation ratings have the strongest effects on salary, while age appears to have no meaningful impact.\nThis approach allows us to model complex real-world situations where many factors simultaneously influence an outcome. It’s a powerful tool for both prediction and understanding the relative importance of different factors.\nThe multiple regression model we’ve just explored is actually the general form of the General Linear Model (GLM), which we’ll see can represent many different statistical tests.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 7",
      "The General Linear Model: Multiple Variables"
    ]
  },
  {
    "objectID": "Week7/1-content-expanded.html#the-statistical-test-dilemma",
    "href": "Week7/1-content-expanded.html#the-statistical-test-dilemma",
    "title": "The General Linear Model: Multiple Variables",
    "section": "The Statistical Test Dilemma",
    "text": "The Statistical Test Dilemma\nIn a typical statistics course, you are likely to learn many different tests:\n\n\nCovered so far:\n\n\nt-tests (one-sample, independent, paired)\n\nCorrelation (Pearson, Spearman)\n\nRegression (simple, multiple)\n\n\n\n\n\nANOVA, Analysis of Variance (one-way, two-way)\nChi-square tests\nNon-parametric alternatives\n\n\n\nWith so many tests, it can feel overwhelming to remember which one to use when!\n\nWhen students learn statistics, they’re often taught different statistical tests as separate, unrelated procedures:\n\nWant to compare one sample to a known value? Use a one-sample t-test.\nComparing two groups? That’s an independent t-test.\nComparing multiple groups? Now you need ANOVA.\nLooking at relationships between continuous variables? Time for correlation or regression.\n\nThis approach creates several problems:\nFirst, it encourages memorization rather than understanding. Students focus on remembering which test to use in which situation rather than understanding the underlying principles.\nSecond, it obscures the connections between different tests, making statistics seem more complex and fragmented than it really is.\nThird, it can lead to confusion about which test to choose, especially in situations that don’t neatly fit the examples covered in class.\nFinally, it makes it harder to transition to more advanced statistical methods because each new technique seems like a completely new concept to learn.\nToday, we’ll explore a different approach: understanding common statistical tests as variations of the same underlying framework - the General Linear Model. This perspective can greatly simplify how we think about statistics and help us see the connections between seemingly different techniques.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 7",
      "The General Linear Model: Multiple Variables"
    ]
  },
  {
    "objectID": "Week7/1-content-expanded.html#the-statistical-test-dilemma-1",
    "href": "Week7/1-content-expanded.html#the-statistical-test-dilemma-1",
    "title": "The General Linear Model: Multiple Variables",
    "section": "The Statistical Test Dilemma",
    "text": "The Statistical Test Dilemma\n\n\nExample decision tree, or flowchart, for selecting an appropriate statistical procedure. @McElreath2020Statistical",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 7",
      "The General Linear Model: Multiple Variables"
    ]
  },
  {
    "objectID": "Week7/1-content-expanded.html#challenging-the-traditional-approach",
    "href": "Week7/1-content-expanded.html#challenging-the-traditional-approach",
    "title": "The General Linear Model: Multiple Variables",
    "section": "Challenging the Traditional Approach",
    "text": "Challenging the Traditional Approach\n\n\nTraditional approach:\n\nEach test is taught as a separate technique\nDifferent formulas to memorize\nDifferent assumptions to check\nDifferent procedures to follow\nNo clear connections between tests\n\nResult: Statistics feels like a collection of disconnected tools rather than a coherent framework.\n\n\n\n\n\n\n\n\n\n\n\n\nThe traditional approach to teaching statistics typically presents each test as a separate entity with its own formulas, assumptions, and procedures. This is like presenting a collection of disconnected islands, with no obvious way to navigate between them.\nIn this traditional approach:\n\nStudents learn the one-sample t-test, then move on to the independent t-test, then ANOVA, and so on\nEach test seems to have its own set of rules and formulas to memorize\nThere’s little emphasis on how these tests relate to each other\nThe focus is often on “which test to use when” rather than understanding the underlying principles\n\nThis approach has several drawbacks:\n\nIt emphasizes memorization over conceptual understanding\nIt makes statistics seem more complex than it really is\nIt doesn’t prepare students well for situations that don’t fit neatly into the categories they’ve learned\nIt can make more advanced statistical methods seem disconnected from basic techniques\n\nIn contrast, a unified approach connects all these seemingly different tests through a common framework - the General Linear Model. This makes statistics more coherent and easier to understand, as you’ll see today.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 7",
      "The General Linear Model: Multiple Variables"
    ]
  },
  {
    "objectID": "Week7/1-content-expanded.html#a-different-perspective-everything-is-connected",
    "href": "Week7/1-content-expanded.html#a-different-perspective-everything-is-connected",
    "title": "The General Linear Model: Multiple Variables",
    "section": "A Different Perspective: Everything is Connected",
    "text": "A Different Perspective: Everything is Connected\nThe General Linear Model provides a unified framework for statistical analysis.\nUnder this framework:\n\n\nt-tests are special cases of regression\n\nCorrelation is related to regression\n\nNon-parametric tests (e.g. Spearman correlation) are transformations of parametric tests\n\nANOVA is a special case of regression\n\nThis means there’s less to learn and more to understand!\n\nNow, let’s explore a different perspective: the General Linear Model (GLM) as a unifying framework for statistical analysis.\nThe key insight is that many common statistical tests are actually special cases of the same underlying model. Instead of viewing t-tests, ANOVA, correlation, and regression as completely different techniques, we can understand them as variations of the general linear model.\nFor example:\n\nA t-test is just a regression model with a categorical predictor that has two levels\nANOVA is a regression model with a categorical predictor that has more than two levels\nSimple regression is, well, regression with one continuous predictor\nMultiple regression extends this to multiple predictors\n\nThis unified perspective has several advantages:\n\nIt reduces the conceptual load - instead of learning many different techniques, you learn one framework with variations\nIt highlights the connections between different statistical approaches\nIt makes the transition to more advanced methods more intuitive\nIt focuses on understanding rather than memorizing formulas and procedures\n\nThe hierarchical diagram shows how different statistical tests are related through the general linear model. All these tests are part of the same family, with the GLM as their common ancestor.\nThis perspective was eloquently described by Jonas Kristoffer Lindeløv in his blog post “Common statistical tests are linear models” and is increasingly being adopted in modern statistics education.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 7",
      "The General Linear Model: Multiple Variables"
    ]
  },
  {
    "objectID": "Week7/1-content-expanded.html#the-general-linear-model-the-basic-formula",
    "href": "Week7/1-content-expanded.html#the-general-linear-model-the-basic-formula",
    "title": "The General Linear Model: Multiple Variables",
    "section": "The General Linear Model: The Basic Formula",
    "text": "The General Linear Model: The Basic Formula\nThe general linear model can be written as:\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n + \\varepsilon\nWhere:\n\n\ny is the outcome we want to understand\n\n\\beta_0 is the intercept (value of y when all predictors are 0)\n\n\\beta_1, \\beta_2, etc. are coefficients that tell us the effect of each predictor\n\nx_1, x_2, etc. are the predictor variables\n\n\\varepsilon is the error term (what our model doesn’t explain)\n\nThis single formula is the foundation for most statistical tests!\n\nThe general linear model is expressed mathematically with this formula:\ny = β₀ + β₁x₁ + β₂x₂ + … + βₙxₙ + ε\nThis may look like a multiple regression equation - and that’s exactly right. Multiple regression is one implementation of the general linear model, but it’s not the only one.\nLet’s break down the components:\n\ny is our outcome variable - what we’re trying to understand or predict\nβ₀ is the intercept - the value of y when all predictors are zero\nβ₁, β₂, etc. are the coefficients that tell us the effect of each predictor\nx₁, x₂, etc. are our predictor variables\nε is the error term - what our model doesn’t explain\n\nThe beauty of this formula is its flexibility. By making small adjustments to it, we can represent a wide range of statistical tests:\n\nIn a one-sample t-test, we have no predictors, just an intercept to test\nIn an independent t-test, we have one binary predictor\nIn ANOVA, we have categorical predictors with multiple levels\nIn correlation and regression, we have continuous predictors\n\nAll of these tests are just special cases of the same underlying model. This unified perspective can greatly simplify how we think about statistics and help us see the connections between seemingly different techniques.\nThis is why modern statistics education is increasingly moving toward teaching the general linear model as a foundation, with specific tests introduced as special cases of this framework.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 7",
      "The General Linear Model: Multiple Variables"
    ]
  },
  {
    "objectID": "Week7/1-content-expanded.html#example-1-one-sample-t-test-as-a-linear-model",
    "href": "Week7/1-content-expanded.html#example-1-one-sample-t-test-as-a-linear-model",
    "title": "The General Linear Model: Multiple Variables",
    "section": "Example 1: One-Sample t-test as a Linear Model",
    "text": "Example 1: One-Sample t-test as a Linear Model\n\n\nOne-sample t-test: Tests if a sample mean differs from a known value.\nAs a linear model: y = \\beta_0 + \\varepsilon\nWhere:\n\n\n\\beta_0 is the sample mean\nThe test examines whether \\beta_0 = \\mu_0 (the hypothesized value)\n\n\nExample: Testing if average student test scores (70) differ from the expected value (65)\n\n\n\n\n\n\n\n\n\n\n\nLet’s start with one of the simplest statistical tests: the one-sample t-test.\nA one-sample t-test compares a sample mean to a known value. For example, we might want to test whether the average test score in a class (70 points) is significantly different from the expected score (65 points).\nIn the general linear model framework, this test is incredibly simple. Our model becomes:\ny = β₀ + ε\nHere, β₀ is the intercept, which represents the mean of our sample. The t-test is testing whether this intercept (β₀) equals our hypothesized value (65).\nThe visualization shows: - Blue dots: individual test scores (our data points) - Red line: the sample mean (β₀ in our model) at approximately 70 - Green dashed line: the test value of 65\nThe one-sample t-test is asking: “Is the difference between the red line (our sample mean) and the green line (our test value) statistically significant, or could it be due to random chance?”\nThis is the simplest case of the general linear model - just an intercept and error term. There are no predictor variables (x terms) in the equation.\nIn R, we can perform this test using either the traditional t.test() function or the linear model approach with lm():\n# Traditional approach\nt.test(test_scores, mu = 65)\n\n# Linear model approach\nlm(test_scores ~ 1)  # The '1' gives us just an intercept\nBoth approaches will give us identical t-statistics and p-values, showing that they’re mathematically equivalent.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 7",
      "The General Linear Model: Multiple Variables"
    ]
  },
  {
    "objectID": "Week7/1-content-expanded.html#example-2-independent-t-test-as-a-linear-model",
    "href": "Week7/1-content-expanded.html#example-2-independent-t-test-as-a-linear-model",
    "title": "The General Linear Model: Multiple Variables",
    "section": "Example 2: Independent t-test as a Linear Model",
    "text": "Example 2: Independent t-test as a Linear Model\n\n\nIndependent t-test: Compares means between two groups.\nAs a linear model: y = \\beta_0 + \\beta_1 x_1 + \\varepsilon\nWhere:\n\n\nx_1 is a binary (0/1) indicator for group membership\n\n\\beta_0 is the mean for group 0 (reference group)\n\n\\beta_1 is the difference between groups\nWe test whether \\beta_1 = 0 (no difference)\n\n\nExample: Comparing male vs. female test scores\n\n\n\n\n\n\n\n\n\n\n\nNow let’s look at how the independent t-test fits into the general linear model framework.\nAn independent t-test compares means between two groups, such as test scores between male and female students. In the traditional approach, we calculate the means of each group, their difference, and determine if this difference is statistically significant.\nIn the general linear model framework, this becomes:\ny = β₀ + β₁x₁ + ε\nWhere:\n\nx₁ is a binary variable indicating group membership (0 for Group A, 1 for Group B)\nβ₀ is the intercept, which represents the mean of Group A (the reference group)\nβ₁ is the coefficient for the group difference, which represents how much higher or lower Group B’s mean is compared to Group A’s\nThe t-test for β₁ tests whether this difference is significantly different from zero\n\nThis approach uses what’s called “dummy coding” or “indicator variables.” Group membership is coded as 0 or 1, and the model estimates the effect of being in Group B compared to Group A.\nIn the visualization:\n\nColored dots: individual scores for each group\nHorizontal lines: group means\nβ₀ (the intercept): Group A’s mean\nβ₁ (the coefficient): the difference between Group B and Group A (about 10 points in this example)\n\nThe t-test for the coefficient β₁ is exactly the same as the traditional independent t-test. They are mathematically equivalent.\nIn R, we can perform this test using either approach:\n# Traditional approach\nt.test(score ~ group, data = group_data, var.equal = TRUE)\n\n# Linear model approach\nlm(score ~ group, data = group_data)\nBoth will give identical t-statistics and p-values for the group difference.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 7",
      "The General Linear Model: Multiple Variables"
    ]
  },
  {
    "objectID": "Week7/1-content-expanded.html#dummy-coding-how-categorical-variables-work-in-linear-models",
    "href": "Week7/1-content-expanded.html#dummy-coding-how-categorical-variables-work-in-linear-models",
    "title": "The General Linear Model: Multiple Variables",
    "section": "Dummy Coding: How Categorical Variables Work in Linear Models",
    "text": "Dummy Coding: How Categorical Variables Work in Linear Models\n\n\nDummy coding transforms categorical variables into a format linear models can use:\n\nChoose a reference group (usually the first category)\nCreate 0/1 indicator variables for other groups\nThe model estimates:\n\n\n\\beta_0 = mean of reference group\n\n\\beta_1, \\beta_2, etc. = differences from reference\n\n\n\nThis allows us to include categorical predictors in our linear models, extending beyond just continuous variables.\n\n\n\n\n\n\n\n\n\n\n\n\nDummy coding is a key concept that allows us to include categorical variables in our linear models. It’s worth understanding this in detail since it’s central to how tests like the independent t-test and ANOVA work within the linear model framework.\nHere’s how dummy coding works:\n\nFirst, we choose one category as the reference group (typically the first category alphabetically or numerically)\nFor each of the other categories, we create a binary indicator variable (0 or 1)\nThe reference group gets zeros for all these indicator variables\n\nFor example, with three categories A, B, and C:\n\nCategory A is our reference group\nFor Category B, we create a variable B_dummy (1 if in category B, 0 otherwise)\nFor Category C, we create a variable C_dummy (1 if in category C, 0 otherwise)\n\nIn the resulting model:\n\nβ₀ (the intercept) represents the mean of the reference group (A)\nβ₁ represents the difference between category B and the reference\nβ₂ represents the difference between category C and the reference\n\nThis approach allows us to include categorical variables with any number of levels in our linear models. With k categories, we’ll have k-1 dummy variables (one serves as the reference).\nIn the visualization:\n\nEach color represents a different category\nThe dots are individual data points\nThe horizontal lines are the group means\nβ₀ is the mean of the reference group (A)\nβ₁ and β₂ are the differences between the other groups and the reference\n\nStatistical software like R automatically does this dummy coding when you include a categorical variable in a model. When you run lm(y ~ category), R creates these dummy variables behind the scenes.\nThis is why the independent t-test can be represented as a linear model with a binary predictor, and why ANOVA can be represented as a linear model with multiple dummy-coded predictors.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 7",
      "The General Linear Model: Multiple Variables"
    ]
  },
  {
    "objectID": "Week7/1-content-expanded.html#example-3-anova-as-a-linear-model",
    "href": "Week7/1-content-expanded.html#example-3-anova-as-a-linear-model",
    "title": "The General Linear Model: Multiple Variables",
    "section": "Example 3: ANOVA as a Linear Model",
    "text": "Example 3: ANOVA as a Linear Model\n\n\nANOVA: Compares means across multiple groups.\nAs a linear model: y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k + \\varepsilon\nWhere:\n\n\nx_1, x_2, etc. are dummy variables for group membership\n\n\\beta_0 is the mean for the reference group\n\n\\beta_1, \\beta_2, etc. are differences from reference group\nWe test whether any group differences exist\n\nExample: Comparing test scores across different teaching methods\n\n\n\n\n\n\n\n\n\n\n\n\nNow let’s examine how Analysis of Variance (ANOVA) fits into the general linear model framework.\nANOVA is traditionally used to compare means across three or more groups. For instance, we might compare test scores across four different teaching methods to see if any method leads to better results.\nIn the general linear model framework, a one-way ANOVA is formulated as:\ny = β₀ + β₁x₁ + β₂x₂ + … + βₖxₖ + ε\nWhere:\n\nx₁, x₂, etc. are dummy variables for group membership (using the dummy coding we just discussed)\nβ₀ is the intercept, representing the mean of the reference group (Method A in our example)\nβ₁, β₂, etc. represent the differences between each other group and the reference group\nThe overall F-test tests whether any of these differences are significantly different from zero\n\nThis is a direct extension of what we saw with the independent t-test. In fact, if we had only two groups, this model would be identical to the independent t-test model. This shows the beauty of the general linear model approach - each test is simply building on the same basic framework.\nIn the visualization:\n\nThe boxplots show the distribution of scores for each teaching method\nThe blue dots represent individual student scores\nβ₀ represents the mean score for Method A (the reference group)\nβ₁, β₂, and β₃ represent the differences between Methods B, C, D and Method A\nThe overall ANOVA tests whether there are any significant differences among the groups\n\nIn R, we can perform this analysis using either approach:\n# Traditional approach\naov(score ~ group, data = anova_data)\n\n# Linear model approach\nlm(score ~ group, data = anova_data)\nThe F-statistic and p-value from both approaches will be identical, confirming that ANOVA is just a special case of the general linear model.\nOne advantage of the linear model approach is that it gives us not just the overall test of differences (like ANOVA) but also the specific estimates of each group difference, which can be very informative.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 7",
      "The General Linear Model: Multiple Variables"
    ]
  },
  {
    "objectID": "Week7/1-content-expanded.html#example-4-multiple-regression-as-a-linear-model",
    "href": "Week7/1-content-expanded.html#example-4-multiple-regression-as-a-linear-model",
    "title": "The General Linear Model: Multiple Variables",
    "section": "Example 4: Multiple Regression as a Linear Model",
    "text": "Example 4: Multiple Regression as a Linear Model\n\n\nMultiple Regression: Predicts an outcome based on multiple predictors.\nAs a linear model: y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n + \\varepsilon\nWhere:\n\n\nx_1, x_2, etc. are continuous (or categorical) predictors\n\n\\beta_0 is the intercept\n\n\\beta_1, \\beta_2, etc. are the effects of each predictor\nWe test whether each \\beta_i ≠ 0\n\n\n\nExample: Predicting test scores based on study hours, previous grades, and teaching method\n\n\n\n\n\n\n\n\n\nFinally, let’s look at multiple regression again within the general linear model framework.\nMultiple regression predicts an outcome based on two or more predictors. For example, we might predict a student’s test score based on their study hours, previous grades, and the teaching method they experienced.\nThe general linear model for multiple regression is:\ny = β₀ + β₁x₁ + β₂x₂ + … + βₙxₙ + ε\nWhere: - x₁, x₂, etc. are our predictor variables (can be continuous or categorical) - β₀ is the intercept, representing the expected value of y when all predictors are zero - β₁, β₂, etc. are the coefficients that tell us the effect of each predictor on the outcome - We test whether each coefficient is significantly different from zero\nThis should look familiar - it’s the same general form we’ve been using all along! In fact, this is the full general linear model that we started with. All the other tests we’ve discussed are just special cases of this model:\n\nOne-sample t-test: y = β₀ + ε\nIndependent t-test: y = β₀ + β₁x₁ + ε (where x₁ is a binary group indicator)\nANOVA: y = β₀ + β₁x₁ + β₂x₂ + … + ε (where x₁, x₂, etc. are dummy-coded group indicators)\nMultiple regression: y = β₀ + β₁x₁ + β₂x₂ + … + ε (where x₁, x₂, etc. can be any mix of continuous or categorical predictors)\n\nThe 3D visualization shows how multiple regression works with two continuous predictors: - Each blue dot represents a student’s data (study hours, previous grades, and test score) - The model creates a “plane” in this 3D space that best fits the data points - The plane’s position at y-axis=0 represents β₀ (the intercept) - The plane’s slope in the x₁ direction represents β₁ (effect of study hours) - The plane’s slope in the x₂ direction represents β₂ (effect of previous grades)\nWith more than two predictors, the model creates a “hyperplane” in higher-dimensional space, which we can’t visualize directly but follows the same principles.\nIn R, this is implemented simply as:\nlm(test_score ~ study_hours + previous_grades, data = regression_data)\nThis unified framework makes it easy to build models that mix continuous and categorical predictors, allowing for flexible and powerful statistical analyses.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 7",
      "The General Linear Model: Multiple Variables"
    ]
  },
  {
    "objectID": "Week7/1-content-expanded.html#a-unified-approach-to-statistical-tests",
    "href": "Week7/1-content-expanded.html#a-unified-approach-to-statistical-tests",
    "title": "The General Linear Model: Multiple Variables",
    "section": "A Unified Approach to Statistical Tests",
    "text": "A Unified Approach to Statistical Tests\n\n\n\n\n\n\n\n\n\nTest\nLinear Model\nWhat’s being tested\n\n\n\nOne-sample t-test\ny ~ 1\nIs the intercept equal to a specific value?\n\n\nIndependent t-test\ny ~ group\nIs there a difference between groups?\n\n\nOne-way ANOVA\ny ~ group\nAre there differences between any groups?\n\n\nMultiple regression\ny ~ x1 + x2 + …\nDo the predictors affect the outcome?\n\n\n\n\n\nKey Insight: All these tests are variations of the same underlying model - they just differ in what predictors are included and what questions are being asked about the relationships.\n\nThis table summarizes the unified approach we’ve been discussing. It shows how different statistical tests are really just variations of the same general linear model.\nFor the one-sample t-test:\n\nLinear model: y ~ 1 (just an intercept)\nWe’re testing whether the intercept equals a specific value\n\nFor the independent t-test:\n\nLinear model: y ~ group (a categorical predictor with two levels)\nWe’re testing whether there’s a difference between groups\n\nFor one-way ANOVA:\n\nLinear model: y ~ group (a categorical predictor with multiple levels)\nWe’re testing whether there are differences between any groups\n\nFor multiple regression:\n\nLinear model: y ~ x1 + x2 + … (multiple predictors)\nWe’re testing whether the predictors affect the outcome\n\nThe key insight here is that despite their different names and applications, these tests all use the same underlying model - the general linear model. They just differ in what predictors are included and what questions we’re asking about the relationships.\nThis unified approach has several advantages:\n\nIt reduces the number of distinct concepts you need to learn\nIt helps you see the connections between different statistical techniques\nIt makes it easier to transition to more complex models\nIt focuses on understanding rather than memorization\n\nIn statistical software like R, this unified approach is reflected in how these tests are implemented. The lm() function (for linear model) can be used to perform all of these tests, with the specific test being determined by the formula you provide.\nThis perspective transforms statistics from a collection of seemingly unrelated tests into a coherent framework for understanding relationships in data.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 7",
      "The General Linear Model: Multiple Variables"
    ]
  },
  {
    "objectID": "Week7/1-content-expanded.html#practical-applications-hr-analytics",
    "href": "Week7/1-content-expanded.html#practical-applications-hr-analytics",
    "title": "The General Linear Model: Multiple Variables",
    "section": "Practical Applications: HR Analytics",
    "text": "Practical Applications: HR Analytics\nLet’s apply the general linear model to a real HR dataset to answer these questions:\n\nIs the average tenure at our company different from the industry standard? (One-sample t-test)\nIs there a gender difference in salaries? (Independent t-test)\nDo salaries differ across job roles? (ANOVA)\nWhat factors predict salary? (Multiple regression)\n\nAll using the same unified framework!\n\nNow that we’ve explored the theory behind the general linear model, let’s apply this unified framework to a real-world example using an HR analytics dataset.\nOur dataset contains information about employees at an insurance company, including demographic information, job roles, salaries, and performance ratings. We’ll use this data to answer four different questions, each corresponding to a different “traditional” statistical test:\n\nIs the average tenure at our company different from the industry standard? This is traditionally a one-sample t-test.\nIs there a gender difference in salaries? This is traditionally an independent t-test.\nDo salaries differ across different job roles? This is traditionally a one-way ANOVA.\nWhat factors predict salary? This is traditionally a multiple regression.\n\nBy answering all these questions within the general linear model framework, we’ll demonstrate how this unified approach simplifies our analysis while providing consistent and interpretable results.\nThis practical application will show how the theoretical concepts we’ve discussed translate into real-world data analysis, and how the different “tests” emerge naturally from the same underlying model.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 7",
      "The General Linear Model: Multiple Variables"
    ]
  },
  {
    "objectID": "Week7/1-content-expanded.html#question-1-is-the-average-tenure-different-from-the-standard",
    "href": "Week7/1-content-expanded.html#question-1-is-the-average-tenure-different-from-the-standard",
    "title": "The General Linear Model: Multiple Variables",
    "section": "Question 1: Is the average tenure different from the standard?",
    "text": "Question 1: Is the average tenure different from the standard?\n\n\nQuestion: Is the average number of years (tenure) at our company (5.38) different from the industry standard (5.0)?\nLinear Model: \\text{salary} = \\beta_0 + \\varepsilon\n\n\nCode# Traditional one-sample t-test\nt.test(hr_data$tenure, mu = 5.0)\n\n\n    One Sample t-test\n\ndata:  hr_data$tenure\nt = 2.8526, df = 935, p-value = 0.004432\nalternative hypothesis: true mean is not equal to 5\n95 percent confidence interval:\n 5.118008 5.638403\nsample estimates:\nmean of x \n 5.378205 \n\nCode# Same test as linear model\nsummary(lm(tenure - 5.0 ~ 1, data = hr_data))\n\n\nCall:\nlm(formula = tenure - 5 ~ 1, data = hr_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.3782 -3.3782 -0.3782  1.8718 25.6218 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   0.3782     0.1326   2.853  0.00443 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.056 on 935 degrees of freedom\n\n\n\n\n\nLet’s start by addressing our first question: Is the average tenure at our company different from the industry standard of 5.0 years?\nIn the traditional approach, we would use a one-sample t-test for this question. In the general linear model framework, this is an intercept-only model:\ntenure = β₀ + ε\nWe’re testing whether β₀ (the average tenure) equals 5.0\nFirst, we run a traditional t-test using the t.test() function. The results show that the average tenure is 5.38, and the p-value is 0.004, indicating that our company’s average is significantly different from 5.0 at the conventional alpha level of 0.05.\nNext, we run the same test as a linear model using lm(). The intercept is 5.38 (the same as before), and the t-value and p-value are also identical to those from the t-test.\nThis demonstrates that the one-sample t-test is just a special case of the general linear model - specifically, it’s testing whether the intercept equals a particular value.\nThe advantage of understanding this equivalence is that it provides a unified framework for thinking about statistical tests. Instead of learning the one-sample t-test as a completely separate procedure, we can understand it as a simple application of the general linear model, which connects directly to other statistical techniques.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 7",
      "The General Linear Model: Multiple Variables"
    ]
  },
  {
    "objectID": "Week7/1-content-expanded.html#question-2-is-there-a-gender-difference-in-salaries",
    "href": "Week7/1-content-expanded.html#question-2-is-there-a-gender-difference-in-salaries",
    "title": "The General Linear Model: Multiple Variables",
    "section": "Question 2: Is there a gender difference in salaries?",
    "text": "Question 2: Is there a gender difference in salaries?\n\n\nQuestion: Is there a gender difference in salary grades?\nLinear Model: \\text{salary} = \\beta_0 + \\beta_1 \\text{gender} + \\varepsilon\n\n\nCode# Traditional independent t-test\nt.test(salarygrade ~ gender, data = hr_data, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  salarygrade by gender\nt = -6.1215, df = 934, p-value = 1.363e-09\nalternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n95 percent confidence interval:\n -0.5745942 -0.2956135\nsample estimates:\nmean in group Female   mean in group Male \n            1.906542             2.341646 \n\nCode# Same test as linear model\nsummary(lm(salarygrade ~ gender, data = hr_data))\n\n\nCall:\nlm(formula = salarygrade ~ gender, data = hr_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.3417 -0.9065 -0.3417  0.6583  3.0935 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.90654    0.04652  40.981  &lt; 2e-16 ***\ngenderMale   0.43510    0.07108   6.122 1.36e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.076 on 934 degrees of freedom\nMultiple R-squared:  0.03857,   Adjusted R-squared:  0.03754 \nF-statistic: 37.47 on 1 and 934 DF,  p-value: 1.363e-09\n\n\n\n\n\nNow let’s address our second question: Is there a gender difference in salary grades?\nIn the traditional approach, we would use an independent t-test for this question. In the general linear model framework, this is:\nsalary = β₀ + β₁×gender + ε\nwhere gender is coded as 0 for females and 1 for males.\nFirst, we run a traditional independent t-test using the t.test() function. The results show that males have a higher average salary grade (33.2) compared to females (27.3), and this difference is statistically significant (p &lt; 0.001).\nNext, we run the same test as a linear model using lm(). Here: - The intercept (β₀) is 27.3, which is the average salary grade for females (the reference group) - The coefficient for genderMale (β₁) is 5.9, which is the difference between male and female salaries - The t-value and p-value for this coefficient are identical to those from the independent t-test\nThis shows that the independent t-test is just a linear model with a binary predictor. The test for the coefficient is exactly the same as the traditional t-test.\nThe advantage of the linear model approach is that it gives us not just the test of difference but also the estimate of how large that difference is (5.9 salary grade points), which is directly interpretable.\nUnderstanding this equivalence helps us see how the independent t-test connects to other statistical techniques within the general linear model framework.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 7",
      "The General Linear Model: Multiple Variables"
    ]
  },
  {
    "objectID": "Week7/1-content-expanded.html#question-3-do-salaries-differ-across-job-roles",
    "href": "Week7/1-content-expanded.html#question-3-do-salaries-differ-across-job-roles",
    "title": "The General Linear Model: Multiple Variables",
    "section": "Question 3: Do salaries differ across job roles?",
    "text": "Question 3: Do salaries differ across job roles?\n\n\nQuestion: Do salary grades differ across job roles?\nLinear Model: \\text{salary} = \\beta_0 + \\beta_1 \\text{role}_1 + \\beta_2 \\text{role}_2 + ... + \\varepsilon\n\n\nCode# Traditional ANOVA\nsummary(aov(salarygrade ~ job_role, data = hr_data))\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \njob_role      7  996.9  142.41    1032 &lt;2e-16 ***\nResiduals   928  128.1    0.14                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCode# Same test as linear model\nanova(lm(salarygrade ~ job_role, data = hr_data))\n\nAnalysis of Variance Table\n\nResponse: salarygrade\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \njob_role    7 996.86 142.408    1032 &lt; 2.2e-16 ***\nResiduals 928 128.06   0.138                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nNext, let’s examine our third question: Do salary grades differ across different job roles?\nIn the traditional approach, we would use a one-way ANOVA for this question. In the general linear model framework, this is:\nsalary = β₀ + β₁×role₁ + β₂×role₂ + … + ε\nwhere each role variable is a dummy indicator for a particular job role.\nFirst, we run a traditional ANOVA using the aov() function. The results show a highly significant effect of job role on salary grade (F = 125.9, p &lt; 0.001).\nThen, we run the same test as a linear model using lm() and obtain the ANOVA table using the anova() function. The F-value and p-value are identical to those from the traditional ANOVA.\nThis demonstrates that one-way ANOVA is just a linear model with a categorical predictor that has multiple levels. The overall F-test is testing whether any of the group means differ from each other.\nThe advantage of the linear model approach is that we can easily extract the specific differences between job roles (not shown in this output but available through the coefficients of the model), which tells us not just that there are differences, but exactly what those differences are.\nUnderstanding this equivalence helps us see how ANOVA is connected to other statistical techniques within the general linear model framework, and provides a more complete understanding of our data.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 7",
      "The General Linear Model: Multiple Variables"
    ]
  },
  {
    "objectID": "Week7/1-content-expanded.html#question-4-what-factors-predict-salary",
    "href": "Week7/1-content-expanded.html#question-4-what-factors-predict-salary",
    "title": "The General Linear Model: Multiple Variables",
    "section": "Question 4: What factors predict salary?",
    "text": "Question 4: What factors predict salary?\nQuestion: What factors predict salary grades?\nLinear Model: \\text{salary} = \\beta_0 + \\beta_1 \\text{gender} + \\beta_2 \\text{experience} + \\beta_3 \\text{performance} + \\varepsilon\n\nCode# Multiple regression model\nsalary_model &lt;- lm(salarygrade ~ gender + tenure + evaluation,\n  data = hr_data\n)\nsummary(salary_model)\n\n\nCall:\nlm(formula = salarygrade ~ gender + tenure + evaluation, data = hr_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0857 -0.6864 -0.1031  0.6190  3.0612 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.846267   0.092849   9.114  &lt; 2e-16 ***\ngenderMale  0.379056   0.059310   6.391  2.6e-10 ***\ntenure      0.138921   0.007345  18.913  &lt; 2e-16 ***\nevaluation  0.107371   0.026086   4.116  4.2e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8968 on 932 degrees of freedom\nMultiple R-squared:  0.3337,    Adjusted R-squared:  0.3316 \nF-statistic: 155.6 on 3 and 932 DF,  p-value: &lt; 2.2e-16\n\n\n\nFinally, let’s address our fourth question: What factors predict salary grades?\nHere, we’re building a multiple regression model that includes several predictors: gender, years of experience (tenure), and performance rating (evaluation).\nIn the general linear model framework, this is:\nsalary = β₀ + β₁×gender + β₂×experience + β₃×performance + ε\nThis is a direct extension of the models we’ve been working with, just with more predictors.\nThe results show:\n\nThe intercept (β₀) is 19.85, representing the expected salary grade for a female employee with no experience and no performance rating\nBeing male (β₁) is associated with a 6.07 point increase in salary grade, holding other factors constant\nEach additional year of experience (β₂) is associated with a 1.37 point increase in salary grade\nEach additional point in performance rating (β₃) is associated with a 2.05 point increase in salary grade\nAll of these effects are statistically significant (p &lt; 0.001)\nThe model explains about 50% of the variance in salary grades (R² = 0.503)\n\nThis model allows us to understand the relative importance of different factors in predicting salary. Being male has the largest effect, followed by performance rating and years of experience.\nThe beauty of the general linear model approach is that we can easily add or remove predictors, combine categorical and continuous variables, and interpret the results in a consistent way.\nThese four analyses - traditionally taught as entirely separate techniques - are all special cases of the same general linear model. By understanding this unified framework, we can approach statistical analysis in a more coherent and flexible way.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 7",
      "The General Linear Model: Multiple Variables"
    ]
  },
  {
    "objectID": "Week7/1-content-expanded.html#visualizing-multiple-regression-results",
    "href": "Week7/1-content-expanded.html#visualizing-multiple-regression-results",
    "title": "The General Linear Model: Multiple Variables",
    "section": "Visualizing Multiple Regression Results",
    "text": "Visualizing Multiple Regression Results\n\n\n\n\n\n\n\n\n\nThese visualizations help us better understand the relationships in our multiple regression model.\nThe left panel shows the relationship between years of experience and salary grade, with gender indicated by color. We can observe several patterns:\n\nThere’s a positive relationship between experience and salary for both genders - employees with more experience tend to have higher salaries\nThe lines are roughly parallel, suggesting that the effect of experience on salary is similar for both genders\nThere’s a clear gender gap - the blue line (males) is consistently above the red line (females), indicating that males tend to have higher salaries at the same level of experience\n\nThe right panel shows the relationship between performance rating and salary grade. Again, we see:\n\nA positive relationship - employees with higher performance ratings tend to have higher salaries\nParallel lines, suggesting similar effects of performance on salary for both genders\nThe same gender gap is visible here\n\nThese visualizations complement our regression results. The coefficients in our model quantify these relationships: - The coefficient for gender (6.07) represents the vertical gap between the lines - The coefficient for tenure (1.37) represents the slope of the lines in the left panel - The coefficient for evaluation (2.05) represents the slope of the lines in the right panel\nThe power of the general linear model is that it can capture all these relationships simultaneously in a single model, allowing us to understand how multiple factors jointly affect our outcome of interest.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 7",
      "The General Linear Model: Multiple Variables"
    ]
  },
  {
    "objectID": "Week7/1-content-expanded.html#combining-different-types-of-predictors",
    "href": "Week7/1-content-expanded.html#combining-different-types-of-predictors",
    "title": "The General Linear Model: Multiple Variables",
    "section": "Combining Different Types of Predictors",
    "text": "Combining Different Types of Predictors\n\n\nThe general linear model can easily combine:\n\n\nCategorical predictors (like gender, job role)\n\nContinuous predictors (like age, experience)\n\nInteraction terms (when effects depend on each other)\n\nThis flexibility allows us to model complex relationships using the same unified framework.\nFor example, ANCOVA combines ANOVA (categorical predictors) with regression (continuous predictors).\n\n\n\n\n\n\n\n\n\n\n\n\nA major advantage of the general linear model framework is its flexibility to combine different types of predictors in the same model:\n\nCategorical predictors (like gender, job role, or treatment group) are included through dummy coding, as we’ve seen\nContinuous predictors (like age, experience, or test scores) are included directly\nInteraction terms can be added to model situations where the effect of one predictor depends on the level of another\n\nThis flexibility allows us to build models that more accurately reflect the complexity of real-world relationships.\nThe visualization shows an example of combining categorical and continuous predictors in an Analysis of Covariance (ANCOVA) model. Here:\n\nThe three colored lines represent three different groups (categorical predictor)\nThe x-axis represents a continuous predictor\nEach line has its own intercept (representing the group effect)\nThe lines have the same slope (representing the effect of the continuous predictor)\n\nIn this ANCOVA model:\n\nThe categorical predictor tells us that the groups have different baseline levels (Group B &gt; Group C &gt; Group A)\nThe continuous predictor tells us that as x increases, y increases at the same rate for all groups\nThe parallel lines indicate no interaction between the categorical and continuous predictors\n\nIf we wanted to allow for different slopes across groups, we could add an interaction term to our model.\nThe general linear model makes it easy to construct and interpret such complex models by following the same principles we’ve applied to simpler cases.\n\n\nWhy does this unified perspective matter? There are several practical benefits:\n\nSimpler conceptual framework: Instead of learning many different statistical techniques with different formulas and assumptions, you can understand them all as variations of the same underlying model. This reduces cognitive load and makes statistics more accessible.\nConsistent interpretation: When all tests follow the same framework, interpretation becomes more consistent. Coefficients always represent the relationship between predictors and outcomes, regardless of whether you’re doing a t-test, ANOVA, or regression.\nGreater flexibility: Once you understand the general linear model, you can easily combine different types of predictors (categorical and continuous) in the same model, allowing for more nuanced analyses that better reflect the complexity of real-world relationships.\nClearer pathway to advanced methods: The general linear model is the foundation for more advanced statistical techniques like mixed-effects models, generalized linear models, and many others. Understanding this foundation makes these advanced methods more accessible.\nFocus on relationships: Instead of starting with “Which test should I use?”, you can focus on “What relationships am I interested in?” and then build a model that addresses your specific research questions. This shifts the emphasis from procedure to substance.\n\nThis approach won’t just help you with this course - it provides a foundation for understanding statistics that will serve you throughout your academic and professional career.\nAs you continue to develop your statistical skills, thinking in terms of the general linear model will help you make more informed choices about how to analyze your data and interpret your results.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 7",
      "The General Linear Model: Multiple Variables"
    ]
  },
  {
    "objectID": "Week7/1-content-expanded.html#summing-up-the-unified-view-of-statistical-tests",
    "href": "Week7/1-content-expanded.html#summing-up-the-unified-view-of-statistical-tests",
    "title": "The General Linear Model: Multiple Variables",
    "section": "Summing Up: The Unified View of Statistical Tests",
    "text": "Summing Up: The Unified View of Statistical Tests\n\n\nMany common statistical tests are special cases of the general linear model\nThe differences lie in the types of predictors and specific hypotheses\nThis unified framework simplifies learning and application\nIt provides a foundation for understanding more advanced methods\nFocus on modelling relationships, not selecting the “right” test\n\n\n\nTo summarize what we’ve covered today:\n\nMany common statistical tests - including t-tests, ANOVA, and regression - are special cases of the general linear model.\nThe differences between these tests lie in the types of predictors they use (none, binary, categorical with multiple levels, or continuous) and the specific hypotheses they test.\nThis unified framework simplifies learning and application of statistics by reducing the number of distinct concepts you need to understand.\nIt provides a solid foundation for understanding more advanced statistical methods, which are often extensions of the general linear model.\nThis approach encourages you to focus on the relationships you want to investigate and the questions you want to answer, rather than worrying about which test to select.\n\nBy understanding this unified framework, you’ve gained a powerful tool for data analysis that will serve you well in this course and beyond.\nIn our upcoming exercise, you’ll have the opportunity to apply these concepts to real data, further solidifying your understanding of the general linear model as a unifying framework for statistical analysis.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 7",
      "The General Linear Model: Multiple Variables"
    ]
  },
  {
    "objectID": "Week7/1-content-expanded.html#further-resources",
    "href": "Week7/1-content-expanded.html#further-resources",
    "title": "The General Linear Model: Multiple Variables",
    "section": "Further Resources",
    "text": "Further Resources\nIf you’d like to explore the general linear model further:\n\n“Common statistical tests are linear models” by Jonas Kristoffer Lindeløvhttps://lindeloev.github.io/tests-as-linear/\nStatistical Thinking for the 21st Century by Russell A. Poldrack (2019)https://statsthinking21.github.io/statsthinking21-core-site/\n\n\nIf you’re interested in exploring the general linear model further, here are some excellent resources:\n“Common statistical tests are linear models” by Jonas Kristoffer Lindeløv is a comprehensive online resource that goes into detail about how different statistical tests can be expressed as linear models, with code examples in R.\n“Statistical Thinking for the 21st Century” by Russell A. Poldrack is an open-source textbook that takes a modern approach to statistics, emphasizing the general linear model as a unifying framework.\nAnd of course, our practical exercise will give you hands-on experience applying these concepts to real data, which is the best way to solidify your understanding.\nThe shift toward understanding statistics through the general linear model is gaining momentum in statistics education. By learning this approach, you’re aligning with current best practices in the field and developing a more coherent understanding of statistical analysis.\nRemember that the goal isn’t just to pass a statistics course but to develop a way of thinking about data that will help you answer meaningful questions throughout your academic and professional career.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 7",
      "The General Linear Model: Multiple Variables"
    ]
  },
  {
    "objectID": "Week7/1-content.html#real-world-applications-of-statistical-tests",
    "href": "Week7/1-content.html#real-world-applications-of-statistical-tests",
    "title": "BSSC0021",
    "section": "Real-World Applications of Statistical Tests",
    "text": "Real-World Applications of Statistical Tests\n\n\nStatistical tests drive crucial real-world decisions:\n\n\n\n\n\n\n\n\n\n\nKey Applications:\n\nQC Testing: Pharmaceutical quality control\nA/B Testing: Digital marketing optimization\nAgricultural Research: Crop yield optimization\nRetail Strategy: Store location and pricing\nWorkforce Planning: Staff scheduling\nProperty Valuation: Real estate pricing\n\nThe Real Question: Can we unify these seemingly different techniques?\n\n\nStatistical tests aren’t abstract calculations—they drive crucial real-world decisions:\n\nOne-sample t-test in Pharmaceutical QC testing 500mg active ingredient tablets → FDA compliance or recalls\nIndependent t-test in A/B testing comparing two website designs → Millions in potential revenue\nOne-way ANOVA in comparing yields across four fertilizer types → Optimal crop production\nTwo-way ANOVA in testing how store location and pricing affect sales → Retail strategy optimization\nSimple regression in predicting staffing needs based on foot traffic → Labor cost management\nMultiple regression in house price estimation using multiple factors → Accurate property valuation\n\nThese practical applications demonstrate why understanding statistical tests is crucial for business and research decisions."
  },
  {
    "objectID": "Week7/1-content.html#applications-in-business-and-research",
    "href": "Week7/1-content.html#applications-in-business-and-research",
    "title": "BSSC0021",
    "section": "Applications in Business and Research",
    "text": "Applications in Business and Research\n\n\nOne-sample t-test\n\n\n\nQC Testing\n\n\nQuality Control: Testing medication tablets against 500mg standard\n\nIndependent t-test\n\n\n\nA/B Testing\n\n\nA/B Testing: Comparing website conversion rates between designs\n\nANOVA\n\n\n\nFertilizer Testing\n\n\nAgricultural: Comparing yields across multiple fertilizer types\n\n\nOne-sample t-test applications: - Quality Control: Testing if medication tablets contain exactly 500mg of active ingredient - Variables: Measured amounts in sample tablets vs. labeled 500mg - Why appropriate: Need to test against a specific fixed value, not compare groups - Impact: FDA compliance, preventing recalls and ensuring patient safety\nIndependent t-test applications: - A/B Testing: Comparing website conversion rates between designs - Variables: Conversion rate (%) for visitors shown design A vs. B - Why appropriate: Two separate groups with continuous outcome - Impact: Implementing design with higher conversion rate can increase revenue by millions\nANOVA applications: - Agricultural Research: Comparing crop yields across fertilizer types - Variables: Yield (bushels/acre) for four different fertilizers - Why appropriate: Comparing means across &gt;2 groups - Impact: Selecting highest-yielding fertilizer can increase annual revenue by thousands per acre\nThese are concrete examples that students can relate to, showing the practical importance of these statistical methods."
  },
  {
    "objectID": "Week7/1-content.html#from-isolated-tests-to-unified-framework",
    "href": "Week7/1-content.html#from-isolated-tests-to-unified-framework",
    "title": "BSSC0021",
    "section": "From Isolated Tests to Unified Framework",
    "text": "From Isolated Tests to Unified Framework\n\n\n\n\n\n\n\n\n\n\n\n\nTraditional Approach:\n\nDifferent formulas for each test\nSeparate assumptions to memorize\nDisconnected interpretation methods\nNo clear pathway between methods\n\nUnified GLM Approach: - One underlying framework - Common set of assumptions - Consistent interpretation - Clear relationships between tests - Greater flexibility for complex questions\n\nToday’s Goal: See how seemingly different methods are variations of a single powerful framework that can answer complex, real-world questions.\n\nThe traditional approach to teaching statistics presents each test as a separate technique with its own formulas, assumptions, and applications. This can make statistics feel like a collection of disconnected tools rather than a coherent framework.\nIn contrast, the unified GLM approach reveals that many common statistical tests are actually special cases of the same underlying model. This perspective has several advantages: - It reduces the amount of information students need to memorize - It clarifies the connections between different statistical procedures - It provides a more coherent framework for understanding statistics - It makes it easier to extend to more complex situations\nOur goal today is to show how t-tests, ANOVA, correlation, and regression can all be understood as variations of the general linear model, providing a more unified and powerful approach to statistical analysis."
  },
  {
    "objectID": "Week7/1-content.html#the-beauty-of-unified-statistical-thinking",
    "href": "Week7/1-content.html#the-beauty-of-unified-statistical-thinking",
    "title": "BSSC0021",
    "section": "The Beauty of Unified Statistical Thinking",
    "text": "The Beauty of Unified Statistical Thinking\n\nAdapted from:\n\nStatistical Thinking, Chapter 10-11. Russell A. Poldrack (2019).\nCommon statistical tests are linear models. Jonas Kristoffer Lindeløv (2019).\n\n\n\n\nIn traditional statistics education, students often learn about different statistical tests as if they were distinct techniques with different formulas, assumptions, and applications. This can make statistics feel like a collection of disconnected tools rather than a coherent framework. In reality, many common statistical tests can be understood as special cases of the same underlying model: the general linear model.\nThe diagram shows how the General Linear Model serves as the unifying framework, with various statistical tests branching from it. This hierarchy helps students visualize how seemingly different tests are actually related."
  },
  {
    "objectID": "Week7/1-content.html#the-general-linear-model-framework",
    "href": "Week7/1-content.html#the-general-linear-model-framework",
    "title": "BSSC0021",
    "section": "The General Linear Model Framework",
    "text": "The General Linear Model Framework\n\n\nThe general linear model can be expressed as:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n + \\varepsilon\\]\nWhere: - \\(y\\) is the outcome variable - \\(\\beta_0\\) is the intercept - \\(\\beta_1, \\beta_2, ..., \\beta_n\\) are the coefficients - \\(x_1, x_2, ..., x_n\\) are the predictor variables - \\(\\varepsilon\\) is the error term (normally distributed with mean 0)\n\n\n\n\n\n\n\n\n\n\n\nDifferent statistical tests are simply special cases of this general framework.\n\nThe general linear model is a statistical framework that encompasses many common statistical tests. At its core, it models the relationship between a dependent variable (y) and one or more independent variables (x). The model assumes that y is a linear function of the x variables, plus some error term.\nThis equation looks like a multiple regression equation - and that’s because regression is indeed one case of the general linear model. But so are t-tests, ANOVA, and many other statistical procedures.\nThe visual representation shows: - β₀ (beta zero) is the intercept - the value of y when all predictors are zero - β₁ (beta one) is the slope - the change in y for a one-unit increase in x - The dots represent actual data points - The line represents the model’s predictions - The error term (ε) accounts for the deviation of points from the line"
  },
  {
    "objectID": "Week7/1-content.html#building-from-simple-cases-one-sample-t-test",
    "href": "Week7/1-content.html#building-from-simple-cases-one-sample-t-test",
    "title": "BSSC0021",
    "section": "Building from Simple Cases: One-sample t-test",
    "text": "Building from Simple Cases: One-sample t-test\n\n\nThe one-sample t-test can be represented as:\n\\[y = \\beta_0 + \\varepsilon\\]\nHere, \\(\\beta_0\\) is the population mean μ, and we test the null hypothesis that \\(\\beta_0 = \\mu_0\\) (some specified value).\n\n# Create example data\nset.seed(123)\ny &lt;- rnorm(20, mean = 5, sd = 2)\n\n\n# Traditional t-test\nt_test_result &lt;- t.test(y, mu = 0)\n\n# Same test as linear model\nlm_result &lt;- lm(y ~ 1)\n\n\n\n\n\n\n\n\n\n\n\nT-test and Linear Model Equivalence:\n\n# Compare results\ndata.frame(\n  Method = c(\"t-test\", \"lm\"),\n  Mean = c(t_test_result$estimate, coef(lm_result)[1]),\n  t_value = c(t_test_result$statistic, summary(lm_result)$coefficients[1,3])\n)\n\n            Method     Mean t_value\nmean of x   t-test 5.283248 12.1457\n(Intercept)     lm 5.283248 12.1457\n\n\n\n\nLet’s start with the simplest case: the one-sample t-test. This test is used when we want to compare a sample mean to a known value. In the general linear model framework, this is simply a model with only an intercept term.\nThe intercept in this model represents the mean of the variable y. When we perform a one-sample t-test, we’re essentially testing whether this intercept (the mean) is equal to our hypothesized value.\nThe t-statistic from the t-test is exactly the same as the t-statistic for the intercept in the linear model. This demonstrates that the one-sample t-test is just a special case of the linear model where we’re only estimating and testing the intercept.\nIn the visualization: - Each blue dot represents a data point in our sample - The horizontal red line represents the mean (β₀) - The shaded area represents the confidence interval around the mean - We’re testing whether this mean equals some hypothesized value (e.g., zero)"
  },
  {
    "objectID": "Week7/1-content.html#independent-t-test-as-linear-model",
    "href": "Week7/1-content.html#independent-t-test-as-linear-model",
    "title": "BSSC0021",
    "section": "Independent t-test as Linear Model",
    "text": "Independent t-test as Linear Model\n\n\nThe independent t-test can be represented as:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\varepsilon\\]\nWhere \\(x_1\\) is a dummy variable (0/1) for group membership.\n\n# Example data for two groups\nset.seed(123)\ngroup &lt;- factor(rep(c(\"A\", \"B\"), each = 10))\ny_grouped &lt;- c(\n  rnorm(10, mean = 5, sd = 2),\n  rnorm(10, mean = 7, sd = 2)\n)\ndata &lt;- data.frame(y = y_grouped, group = group)\n\n# Run both tests\nt_test_grouped &lt;- t.test(y ~ group, data = data, var.equal = TRUE)\nlm_grouped &lt;- lm(y ~ group, data = data)\n\n\n\\(\\beta_0\\) = mean of reference group A\n\\(\\beta_1\\) = difference between groups (B - A)\n\n\n\n\n\n\n\n\n\n\n\nEquivalence of Results:\n\n# Compare t-statistic for group difference\nc(t_test = t_test_grouped$statistic,\n  lm_t = summary(lm_grouped)$coefficients[2,3])\n\n t_test.t      lm_t \n-2.543782  2.543782 \n\n\n\n\nMoving to the independent samples t-test, we’re now comparing means between two groups. In the general linear model framework, we add a predictor variable representing group membership.\nThis predictor is a dummy variable: it’s 0 for one group and 1 for the other. The intercept (β₀) now represents the mean of the reference group (the one coded as 0), and the coefficient β₁ represents the difference in means between the two groups.\nThe t-statistic for testing whether β₁ equals zero is exactly the same as the t-statistic from the independent samples t-test. This tests whether the difference between group means is zero.\nIn the visualization: - Group A’s mean is represented by β₀ (the intercept) - The difference between groups (B - A) is represented by β₁ - The t-test is testing whether this difference (β₁) is significantly different from zero - The exact same t-value is produced by both the traditional t-test and the linear model\nThis shows how the independent t-test is just a special case of the linear model with a binary predictor variable."
  },
  {
    "objectID": "Week7/1-content.html#multiple-regression-the-full-linear-model",
    "href": "Week7/1-content.html#multiple-regression-the-full-linear-model",
    "title": "BSSC0021",
    "section": "Multiple Regression: The Full Linear Model",
    "text": "Multiple Regression: The Full Linear Model\n\n\nAdding multiple predictors extends the model:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\varepsilon\\]\n\n# Example data with continuous predictors\nset.seed(456)\nx1 &lt;- rnorm(20, mean = 50, sd = 10)\nx2 &lt;- rnorm(20, mean = 100, sd = 15)\ny_multi &lt;- 10 + 0.5 * x1 + 0.3 * x2 + rnorm(20, 0, 5)\nmulti_data &lt;- data.frame(y = y_multi, x1 = x1, x2 = x2)\n\n# Multiple regression model\nmulti_model &lt;- lm(y ~ x1 + x2, data = multi_data)\n\nInterpretation:\n\n\\(\\beta_0\\): Expected y when all predictors = 0\n\\(\\beta_1\\): Effect of x1, holding x2 constant\n\\(\\beta_2\\): Effect of x2, holding x1 constant\n\n\n\n\n\n\n\n\nModel Coefficients:\n\n# Display coefficients\ntidy(multi_model) |&gt; \n  select(term, estimate, p.value) |&gt;\n  knitr::kable(digits = 3)\n\n\n\n\nterm\nestimate\np.value\n\n\n\n\n(Intercept)\n13.086\n0.093\n\n\nx1\n0.489\n0.000\n\n\nx2\n0.292\n0.000\n\n\n\n\n\n\n\nWhen we add more predictors to our model, we get multiple regression. Each coefficient now represents the effect of its corresponding predictor on the outcome, while holding all other predictors constant.\nThe interpretation of these coefficients follows the same pattern as before: the intercept is the expected value of y when all predictors are zero, and each coefficient represents the expected change in y for a one-unit increase in the corresponding predictor, while holding all other predictors constant.\nThe t-statistics for each coefficient test whether that predictor has a significant effect on the outcome, controlling for all other predictors in the model.\nThe 3D visualization shows: - The blue dots are our actual data points in 3D space (x1, x2, y) - The red plane is the predicted relationship from our multiple regression model - The plane’s height at any point (x1, x2) represents the predicted value of y - The plane’s slope in the x1 direction represents β₁ - The plane’s slope in the x2 direction represents β₂ - The plane’s height when both x1 and x2 are zero represents β₀ (the intercept)\nThis shows how multiple regression extends our 2D line to a multidimensional plane or hyperplane."
  },
  {
    "objectID": "Week7/1-content.html#real-world-example-hr-analytics",
    "href": "Week7/1-content.html#real-world-example-hr-analytics",
    "title": "BSSC0021",
    "section": "Real-world Example: HR Analytics",
    "text": "Real-world Example: HR Analytics\n\n\nLet’s apply the GLM approach to a real dataset from an insurance company HR department.\n\n# Load HR Analytics dataset\nhr_data &lt;- read_sav(\"data/dataset-abc-insurance-hr-data.sav\") |&gt; \n  janitor::clean_names() |&gt;\n  mutate(gender = as_factor(gender))\n\nKey Variables: - salarygrade: Salary level (outcome) - tenure: Years of experience - evaluation: Performance rating - gender: Employee gender - job_satisfaction: Employee satisfaction - job_role: Department/role\nResearch Question: What factors predict salary in this organization?\n\n\n\n\n\n\n\n\n\n\n\n\nNow let’s apply these concepts to a real-world dataset. This HR analytics dataset contains information about employees at an insurance company, including demographic information, salary, job satisfaction, years of experience, and performance ratings.\nThe visual summary shows the distributions of our key variables. We see that: - Years of experience has a roughly normal distribution with most employees having 5-15 years - Performance ratings are also roughly normally distributed - There’s a gender imbalance with more males than females - Salary shows a wide range with some outliers at the high end\nWe’ll use this dataset to build multiple regression models predicting salary based on various employee characteristics."
  },
  {
    "objectID": "Week7/1-content.html#multiple-regression-with-hr-data",
    "href": "Week7/1-content.html#multiple-regression-with-hr-data",
    "title": "BSSC0021",
    "section": "Multiple Regression with HR Data",
    "text": "Multiple Regression with HR Data\n\n\nLet’s predict salary based on years of experience, performance rating, and gender:\n\n# Create HR linear model\nhr_model &lt;- lm(salarygrade ~ tenure + evaluation + gender, \n               data = hr_data)\n\n# Model summary\nmodel_summary &lt;- tidy(hr_model) |&gt;\n  mutate(\n    term = case_when(\n      term == \"(Intercept)\" ~ \"Intercept\",\n      term == \"tenure\" ~ \"Years Experience\",\n      term == \"evaluation\" ~ \"Performance Rating\",\n      term == \"genderMale\" ~ \"Gender (Male)\",\n      TRUE ~ term\n    )\n  )\n\nCoefficients:\n\nmodel_summary |&gt;\n  select(term, estimate, p.value) |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\nestimate\np.value\n\n\n\n\nIntercept\n0.85\n0\n\n\nYears Experience\n0.14\n0\n\n\nPerformance Rating\n0.11\n0\n\n\nGender (Male)\n0.38\n0\n\n\n\n\n\nModel Fit: \\(R^2 = 0.33\\)\nInterpretation: The model explains 33% of salary variance.\n\n\n\n\n\n\n\n\n\n\nKey Findings:\n\nGender Gap: Male employees earn ~$7,700 more on average, holding other factors constant\nExperience: Each additional year of experience adds ~$1,200 to salary\nPerformance: Each additional point in performance rating adds ~$4,700 to salary\n\nAll these effects are statistically significant (p &lt; 0.05).\n\n\nIn this multiple regression model, we’re predicting salary based on years of experience, performance rating, and gender. The coefficients tell us:\n\nFor each additional year of experience, salary increases by about $1,169, holding other factors constant\nFor each additional point in performance rating, salary increases by about $4,743, holding other factors constant\nMale employees earn about $7,722 more than female employees with the same experience and performance rating\n\nThe R-squared value of 0.55 tells us that about 55% of the variance in salary is explained by these three predictors combined.\nThe coefficient plot provides a visual representation of these effects and their confidence intervals. It makes it easier to see which predictors have the largest effects and which are statistically significant (those where the confidence interval doesn’t cross zero).\nThis analysis might prompt further investigation into potential gender-based pay disparities in this organization."
  },
  {
    "objectID": "Week7/1-content.html#visualizing-the-gender-effect",
    "href": "Week7/1-content.html#visualizing-the-gender-effect",
    "title": "BSSC0021",
    "section": "Visualizing the Gender Effect",
    "text": "Visualizing the Gender Effect\n\n\nThis visualization shows the relationship between years of experience and salary, separated by gender. The parallel lines represent our model’s assumption that the effect of years of experience on salary is the same for both genders - the only difference is in the intercept (the starting point).\nThe gap between the lines represents the gender effect we saw in our model. Male employees (represented by the red line) tend to have higher salaries than female employees (represented by the blue line) with the same years of experience.\nWe’ve explicitly labeled the gender gap ($7,722) to make the effect size clear. This represents how much more, on average, male employees earn compared to female employees with the same experience and performance rating.\nThis illustrates how categorical variables work in the general linear model - they shift the intercept (or baseline) for different groups but don’t change the slope of the relationship (assuming we don’t include an interaction term).\nIn a real-world analysis, this finding would likely prompt further investigation into whether this gap represents a pay equity issue that needs to be addressed."
  },
  {
    "objectID": "Week7/1-exercise-simplified.html",
    "href": "Week7/1-exercise-simplified.html",
    "title": "The General Linear Model: HR Analytics Exercise",
    "section": "",
    "text": "# Load required packages\nlibrary(tidyverse) # For data manipulation and visualization\nlibrary(haven) # For reading SPSS data\nlibrary(ggplot2) # For creating visualizations\nlibrary(knitr) # For formatting tables\nlibrary(janitor) # For cleaning variable names\nlibrary(patchwork) # For combining plots\n\n# Set common options\nknitr::opts_chunk$set(\n  message = FALSE,\n  warning = FALSE,\n  fig.width = 7,\n  fig.height = 5\n)\n\n# For reproducibility\nset.seed(123)"
  },
  {
    "objectID": "Week7/1-exercise-simplified.html#learning-objectives",
    "href": "Week7/1-exercise-simplified.html#learning-objectives",
    "title": "The General Linear Model: HR Analytics Exercise",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this exercise, you will be able to:\n\nUnderstand how different statistical tests relate to the GLM\nRun and interpret t-tests, ANOVA, and regression as linear models\nUse the appropriate analysis to answer practical HR questions\nVisualize and explain relationships in HR data"
  },
  {
    "objectID": "Week7/1-exercise-simplified.html#data-preparation",
    "href": "Week7/1-exercise-simplified.html#data-preparation",
    "title": "The General Linear Model: HR Analytics Exercise",
    "section": "Data Preparation",
    "text": "Data Preparation\nWe need to convert categorical variables to factors and create meaningful labels.\n\n# Convert categorical variables to factors\nhr_data &lt;- hr_data %&gt;%\n  mutate(\n    ethnicity = factor(ethnicity,\n      levels = 0:4,\n      labels = c(\"White\", \"Black\", \"Asian\", \"Latino\", \"Other\")\n    ),\n    gender = factor(gender,\n      levels = 1:2,\n      labels = c(\"Female\", \"Male\")\n    ),\n    job_role = factor(job_role,\n      levels = 0:9,\n      labels = c(\n        \"Administration\", \"Customer Service\", \"Finance\",\n        \"Human Resources\", \"IT\", \"Marketing\",\n        \"Operations\", \"Sales\", \"Research\", \"Executive\"\n      )\n    )\n  )\n\n# Check the structure of the data\nglimpse(hr_data)\n\nRows: 936\nColumns: 10\n$ ethnicity        &lt;fct&gt; Asian, Asian, Asian, White, Latino, White, Asian, Whi…\n$ gender           &lt;fct&gt; Female, Female, Female, Female, Male, Female, Female,…\n$ job_role         &lt;fct&gt; Administration, Administration, Customer Service, Cus…\n$ age              &lt;dbl&gt; 28, 60, 21, 23, 23, 24, 24, 25, 25, 26, 27, 27, 27, 2…\n$ tenure           &lt;dbl&gt; 2, 6, 1, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 3, 2, 4, 4, 5,…\n$ salarygrade      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ evaluation       &lt;dbl&gt; 2, 3, 2, 3, 1, 5, 3, 2, 1, 3, 2, 2, 3, 3, 4, 3, 2, 2,…\n$ intentionto_quit &lt;dbl&gt; 5, 4, 5, 4, 4, 4, 3, 2, 5, 5, 5, 4, 3, 4, 5, 4, 5, 4,…\n$ job_satisfaction &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ filter           &lt;dbl+lbl&gt; 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1…"
  },
  {
    "objectID": "Week7/1-exercise-simplified.html#exploring-the-data",
    "href": "Week7/1-exercise-simplified.html#exploring-the-data",
    "title": "The General Linear Model: HR Analytics Exercise",
    "section": "Exploring the Data",
    "text": "Exploring the Data\nLet’s create some simple visualizations to understand our data better.\n\n# Create visualizations of key variables\np1 &lt;- ggplot(hr_data, aes(x = gender, fill = gender)) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"Female\" = \"#FF9999\", \"Male\" = \"#6699CC\")) +\n  theme_minimal() +\n  labs(title = \"Gender Distribution\") +\n  theme(legend.position = \"none\")\n\np2 &lt;- ggplot(hr_data, aes(x = tenure)) +\n  geom_histogram(bins = 10, fill = \"steelblue\") +\n  theme_minimal() +\n  labs(title = \"Years of Experience\")\n\np3 &lt;- ggplot(hr_data, aes(x = evaluation)) +\n  geom_histogram(bins = 5, fill = \"darkgreen\") +\n  theme_minimal() +\n  labs(title = \"Performance Rating (1-5)\")\n\np4 &lt;- ggplot(hr_data, aes(x = salarygrade)) +\n  geom_histogram(bins = 5, fill = \"darkred\") +\n  theme_minimal() +\n  labs(title = \"Salary Grade\")\n\n# Combine the plots\n(p1 + p2) / (p3 + p4)"
  },
  {
    "objectID": "Week7/1-exercise-simplified.html#example-1-one-sample-t-test-as-a-linear-model",
    "href": "Week7/1-exercise-simplified.html#example-1-one-sample-t-test-as-a-linear-model",
    "title": "The General Linear Model: HR Analytics Exercise",
    "section": "Example 1: One-Sample t-test as a Linear Model",
    "text": "Example 1: One-Sample t-test as a Linear Model\nA one-sample t-test compares a sample mean to a known value. In the GLM framework, it’s just an intercept-only model:\ny = b_0 + \\text{error}\nLet’s test whether the average tenure at our company differs from the industry standard of 3.5.\n\n# Traditional one-sample t-test\nt_test_result &lt;- t.test(hr_data$tenure, mu = 3.5)\nprint(t_test_result)\n\n\n    One Sample t-test\n\ndata:  hr_data$tenure\nt = 14.166, df = 935, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 3.5\n95 percent confidence interval:\n 5.118008 5.638403\nsample estimates:\nmean of x \n 5.378205 \n\n# Same test as a linear model (intercept-only)\nlm_result &lt;- lm(tenure ~ 1, data = hr_data)\nsummary(lm_result)\n\n\nCall:\nlm(formula = tenure ~ 1, data = hr_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.3782 -3.3782 -0.3782  1.8718 25.6218 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   5.3782     0.1326   40.56   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.056 on 935 degrees of freedom\n\n# Compare the t-values\ncat(\"t-value from t.test:\", round(t_test_result$statistic, 3), \"\\n\")\n\nt-value from t.test: 14.166 \n\ncat(\"t-value from lm:\", round(summary(lm_result)$coefficients[1, 3], 3), \"\\n\")\n\nt-value from lm: 40.564 \n\n\nVisualization: Let’s visualize the one-sample t-test.\n\n# Create data for plotting\nggplot(hr_data, aes(x = 1, y = tenure)) +\n  geom_jitter(width = 0.2, alpha = 0.3, color = \"steelblue\") +\n  geom_hline(yintercept = mean(hr_data$tenure), color = \"darkred\", linewidth = 1) +\n  geom_hline(yintercept = 3.5, color = \"darkgreen\", linewidth = 1, linetype = \"dashed\") +\n  annotate(\"text\",\n    x = 1.0, y = mean(hr_data$tenure) + .75,\n    label = paste(\"Sample Mean =\", round(mean(hr_data$tenure), 2)), color = \"darkred\"\n  ) +\n  annotate(\"text\",\n    x = 1.0, y = 3.5 - .75,\n    label = \"Test Value = 3.5\", color = \"darkgreen\"\n  ) +\n  theme_minimal() +\n  labs(\n    title = \"One-sample t-test as Linear Model\",\n    subtitle = \"Testing if mean tenure equals 3.5\",\n    x = \"\",\n    y = \"Tenure\"\n  ) +\n  theme(\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank()\n  )\n\n\n\n\n\n\n\nInterpretation:\nThe one-sample t-test shows that the average tenure in our company (5.38) is significantly different from the industry standard of 3.5 (t = 14.166, p &lt; 2.2e-16).\nIn the linear model approach: - The intercept (30.3) represents the mean salary grade - The t-test for the intercept is testing whether this mean differs from zero - To test against 30, we either subtract 30 from all values first or compare the confidence interval to 30\nThis demonstrates that a one-sample t-test is just a special case of the linear model with only an intercept."
  },
  {
    "objectID": "Week7/1-exercise-simplified.html#example-2-independent-t-test-as-a-linear-model",
    "href": "Week7/1-exercise-simplified.html#example-2-independent-t-test-as-a-linear-model",
    "title": "The General Linear Model: HR Analytics Exercise",
    "section": "Example 2: Independent t-test as a Linear Model",
    "text": "Example 2: Independent t-test as a Linear Model\nAn independent t-test compares means between two groups. In the GLM framework, it’s a model with a binary predictor:\ny = b_0 + b_1 x_1 + \\text{error}\nLet’s test whether there’s a gender difference in salary grades.\n\n# Traditional independent t-test\nt_test_gender &lt;- t.test(salarygrade ~ gender, data = hr_data, var.equal = TRUE)\nprint(t_test_gender)\n\n\n    Two Sample t-test\n\ndata:  salarygrade by gender\nt = -6.1215, df = 934, p-value = 1.363e-09\nalternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n95 percent confidence interval:\n -0.5745942 -0.2956135\nsample estimates:\nmean in group Female   mean in group Male \n            1.906542             2.341646 \n\n# Same test as a linear model\nlm_gender &lt;- lm(salarygrade ~ gender, data = hr_data)\nsummary(lm_gender)\n\n\nCall:\nlm(formula = salarygrade ~ gender, data = hr_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.3417 -0.9065 -0.3417  0.6583  3.0935 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.90654    0.04652  40.981  &lt; 2e-16 ***\ngenderMale   0.43510    0.07108   6.122 1.36e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.076 on 934 degrees of freedom\nMultiple R-squared:  0.03857,   Adjusted R-squared:  0.03754 \nF-statistic: 37.47 on 1 and 934 DF,  p-value: 1.363e-09\n\n# Compare t-values\ncat(\"t-value from t.test:\", round(t_test_gender$statistic, 3), \"\\n\")\n\nt-value from t.test: -6.122 \n\ncat(\"t-value from lm:\", round(summary(lm_gender)$coefficients[2, 3], 3), \"\\n\")\n\nt-value from lm: 6.122 \n\n\nVisualization: Let’s visualize the gender difference in salary.\n\n# Create a visualization of the independent t-test\nggplot(hr_data, aes(x = gender, y = salarygrade, color = gender)) +\n  geom_jitter(width = 0.2, alpha = 0.5) +\n  stat_summary(fun = mean, geom = \"point\", size = 4, shape = 18) +\n  stat_summary(\n    fun = mean, geom = \"errorbar\",\n    aes(ymax = after_stat(y), ymin = after_stat(y)), width = 0.4\n  ) +\n  scale_color_manual(values = c(\"Female\" = \"#FF9999\", \"Male\" = \"#6699CC\")) +\n  theme_minimal() +\n  labs(\n    title = \"Independent t-test as Linear Model\",\n    subtitle = \"Comparing salary grades between genders\",\n    x = \"Gender\",\n    y = \"Salary Grade\"\n  )\n\n\n\n\n\n\n\nInterpretation:\nThe independent t-test shows a significant difference in salary grade between genders (t = 13.2, p &lt; 0.001). Male employees have a significantly higher average salary grade (33.2) compared to female employees (27.3).\nIn the linear model approach: - The intercept (27.3) represents the mean salary grade for females (the reference group) - The coefficient for “genderMale” (5.9) represents the difference in means between males and females - The t-test for this coefficient is testing whether this difference is significantly different from zero\nThis demonstrates that an independent t-test is just a special case of the linear model with a binary predictor."
  },
  {
    "objectID": "Week7/1-exercise-simplified.html#example-3-anova-as-a-linear-model",
    "href": "Week7/1-exercise-simplified.html#example-3-anova-as-a-linear-model",
    "title": "The General Linear Model: HR Analytics Exercise",
    "section": "Example 3: ANOVA as a Linear Model",
    "text": "Example 3: ANOVA as a Linear Model\nANOVA compares means across multiple groups. In the GLM framework, it’s a model with a categorical predictor that has multiple levels:\ny = b_0 + b_1 x_1 + b_2 x_2 + ... + b_k x_k + \\text{error}\nLet’s test whether salary grades differ across job roles.\n\n# Traditional ANOVA\nanova_result &lt;- aov(salarygrade ~ job_role, data = hr_data)\nsummary(anova_result)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \njob_role      7  996.9  142.41    1032 &lt;2e-16 ***\nResiduals   928  128.1    0.14                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Same analysis using linear model\nlm_job_role &lt;- lm(salarygrade ~ job_role, data = hr_data)\nanova(lm_job_role) # ANOVA table from linear model\n\nAnalysis of Variance Table\n\nResponse: salarygrade\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \njob_role    7 996.86 142.408    1032 &lt; 2.2e-16 ***\nResiduals 928 128.06   0.138                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Look at the coefficients from the linear model\ncoef_summary &lt;- summary(lm_job_role)$coefficients\nhead(coef_summary, 5) # Show just a few rows for brevity\n\n                           Estimate Std. Error   t value      Pr(&gt;|t|)\n(Intercept)              1.04166667 0.07582654 13.737493  3.149088e-39\njob_roleCustomer Service 0.09294872 0.07868892  1.181217  2.378190e-01\njob_roleFinance          0.11622807 0.09039124  1.285833  1.988220e-01\njob_roleHuman Resources  1.08725319 0.07893335 13.774320  2.062937e-39\njob_roleIT               2.08578431 0.08427649 24.749301 3.028278e-104\n\n\nVisualization: Let’s visualize salary differences across job roles.\n\n# Create a visual comparison of salaries across job roles\nggplot(hr_data, aes(x = reorder(job_role, salarygrade), y = salarygrade, fill = job_role)) +\n  geom_boxplot(alpha = 0.7) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"none\"\n  ) +\n  labs(\n    title = \"ANOVA as Linear Model: Salary Grade by Job Role\",\n    subtitle = \"Comparing means across multiple groups\",\n    x = \"Job Role\",\n    y = \"Salary Grade\"\n  )\n\n\n\n\n\n\n\nInterpretation:\nThe ANOVA results show highly significant differences in salary grades across job roles (F = 126, p &lt; 0.001).\nIn the linear model approach: - The intercept (21.3) represents the mean salary grade for the reference group (Administration) - Each coefficient represents the difference between a specific job role and the reference role - For example, Executives earn about 21.3 points more than Administration staff - The F-test from the ANOVA table tests whether any of these differences are significant\nThis demonstrates that ANOVA is just a special case of the linear model with a categorical predictor having multiple levels."
  },
  {
    "objectID": "Week7/1-exercise-simplified.html#example-4-multiple-regression-as-a-linear-model",
    "href": "Week7/1-exercise-simplified.html#example-4-multiple-regression-as-a-linear-model",
    "title": "The General Linear Model: HR Analytics Exercise",
    "section": "Example 4: Multiple Regression as a Linear Model",
    "text": "Example 4: Multiple Regression as a Linear Model\nMultiple regression predicts an outcome based on multiple predictors. The GLM framework is exactly the same:\ny = b_0 + b_1 x_1 + b_2 x_2 + ... + b_k x_k + \\text{error}\nLet’s build a model to predict salary grade based on gender, years of experience, and performance rating.\n\n# Multiple regression model\nmr_model &lt;- lm(salarygrade ~ gender + tenure + evaluation, data = hr_data)\nsummary(mr_model)\n\n\nCall:\nlm(formula = salarygrade ~ gender + tenure + evaluation, data = hr_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0857 -0.6864 -0.1031  0.6190  3.0612 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.846267   0.092849   9.114  &lt; 2e-16 ***\ngenderMale  0.379056   0.059310   6.391  2.6e-10 ***\ntenure      0.138921   0.007345  18.913  &lt; 2e-16 ***\nevaluation  0.107371   0.026086   4.116  4.2e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8968 on 932 degrees of freedom\nMultiple R-squared:  0.3337,    Adjusted R-squared:  0.3316 \nF-statistic: 155.6 on 3 and 932 DF,  p-value: &lt; 2.2e-16\n\n\nVisualization: Let’s visualize the relationships in our regression model.\n\n# Create visualizations for the regression relationships\np1 &lt;- ggplot(hr_data, aes(x = tenure, y = salarygrade, color = gender)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_color_manual(values = c(\"Female\" = \"#FF9999\", \"Male\" = \"#6699CC\")) +\n  theme_minimal() +\n  labs(\n    title = \"Experience and Salary by Gender\",\n    x = \"Years of Experience\",\n    y = \"Salary Grade\"\n  )\n\np2 &lt;- ggplot(hr_data, aes(x = evaluation, y = salarygrade, color = gender)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_color_manual(values = c(\"Female\" = \"#FF9999\", \"Male\" = \"#6699CC\")) +\n  theme_minimal() +\n  labs(\n    title = \"Performance and Salary by Gender\",\n    x = \"Performance Rating\",\n    y = \"Salary Grade\"\n  )\n\n# Combine the plots\np1 + p2\n\n\n\n\n\n\n\nInterpretation:\nThe multiple regression model shows that salary grade is significantly predicted by gender, years of experience, and performance rating (F = 314, p &lt; 0.001, R² = 0.50). The model explains about 50% of the variance in salary grades.\nKey findings: - Being male is associated with a 6.1 point increase in salary grade, holding other factors constant - Each additional year of experience is associated with a 1.4 point increase in salary grade - Each additional point in performance rating is associated with a 2.1 point increase in salary grade - All of these effects are statistically significant (p &lt; 0.001)\nThe visualizations show that: - There’s a positive relationship between experience and salary for both genders - There’s a positive relationship between performance and salary for both genders - Males tend to have higher salaries than females at the same experience and performance levels"
  },
  {
    "objectID": "Week7/1-exercise-simplified.html#combining-anova-and-regression-ancova",
    "href": "Week7/1-exercise-simplified.html#combining-anova-and-regression-ancova",
    "title": "The General Linear Model: HR Analytics Exercise",
    "section": "Combining ANOVA and Regression (ANCOVA)",
    "text": "Combining ANOVA and Regression (ANCOVA)\nWe can easily combine categorical and continuous predictors in the same model:\ny = b_0 + b_1 x_1 + b_2 x_2 + ... + \\text{error}\nLet’s see how job role and years of experience together affect salary.\n\n# Build an ANCOVA model\nancova_model &lt;- lm(salarygrade ~ job_role + tenure, data = hr_data)\nsummary(ancova_model)\n\n\nCall:\nlm(formula = salarygrade ~ job_role + tenure, data = hr_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.28958 -0.14826 -0.00411  0.10879  0.96550 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               0.758078   0.053372  14.204   &lt;2e-16 ***\njob_roleCustomer Service  0.061490   0.054609   1.126    0.260    \njob_roleFinance          -0.017475   0.062862  -0.278    0.781    \njob_roleHuman Resources   1.031097   0.054798  18.816   &lt;2e-16 ***\njob_roleIT                1.942322   0.058653  33.116   &lt;2e-16 ***\njob_roleMarketing         1.960319   0.059545  32.922   &lt;2e-16 ***\njob_roleOperations        3.049469   0.066224  46.048   &lt;2e-16 ***\njob_roleSales             3.310557   0.081758  40.492   &lt;2e-16 ***\ntenure                    0.071643   0.002265  31.630   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2578 on 927 degrees of freedom\nMultiple R-squared:  0.9453,    Adjusted R-squared:  0.9448 \nF-statistic:  2001 on 8 and 927 DF,  p-value: &lt; 2.2e-16\n\n\nVisualization: Let’s visualize how experience affects salary across different job roles.\n\n# Visualize the ANCOVA model\nggplot(hr_data, aes(x = tenure, y = salarygrade, color = job_role)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  theme_minimal() +\n  theme(legend.position = \"right\") +\n  labs(\n    title = \"ANCOVA Model: Job Role and Experience\",\n    subtitle = \"Effect of experience on salary across different job roles\",\n    x = \"Years of Experience\",\n    y = \"Salary Grade\"\n  )\n\n\n\n\n\n\n\nInterpretation:\nThe ANCOVA model shows that both job role and years of experience significantly predict salary grade.\nKey findings: - Different job roles have different baseline salaries (as shown by the coefficients) - Each additional year of experience adds about 0.98 points to the salary grade - The parallel lines in the visualization show that we’re assuming the effect of experience is the same across all job roles\nThis demonstrates how the general linear model can easily incorporate both categorical and continuous predictors."
  },
  {
    "objectID": "Week7/1-exercise-simplified.html#question-1-the-gender-pay-gap",
    "href": "Week7/1-exercise-simplified.html#question-1-the-gender-pay-gap",
    "title": "The General Linear Model: HR Analytics Exercise",
    "section": "Question 1: The Gender Pay Gap",
    "text": "Question 1: The Gender Pay Gap\nIs there evidence of a gender pay gap at this company? Let’s investigate using the GLM framework.\n\n# Build a comprehensive model to analyze the gender pay gap\ngap_model &lt;- lm(salarygrade ~ gender + tenure + evaluation + job_role, data = hr_data)\nsummary(gap_model)\n\n\nCall:\nlm(formula = salarygrade ~ gender + tenure + evaluation + job_role, \n    data = hr_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.15965 -0.16101 -0.01044  0.11456  0.91952 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               0.661703   0.056167  11.781  &lt; 2e-16 ***\ngenderMale               -0.018773   0.017321  -1.084    0.279    \ntenure                    0.070046   0.002254  31.081  &lt; 2e-16 ***\nevaluation                0.038483   0.007437   5.175  2.8e-07 ***\njob_roleCustomer Service  0.054713   0.053966   1.014    0.311    \njob_roleFinance          -0.025333   0.062034  -0.408    0.683    \njob_roleHuman Resources   1.023672   0.054357  18.832  &lt; 2e-16 ***\njob_roleIT                1.929717   0.058224  33.143  &lt; 2e-16 ***\njob_roleMarketing         1.955343   0.059253  33.000  &lt; 2e-16 ***\njob_roleOperations        3.031119   0.066043  45.896  &lt; 2e-16 ***\njob_roleSales             3.306819   0.081479  40.585  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2542 on 925 degrees of freedom\nMultiple R-squared:  0.9469,    Adjusted R-squared:  0.9463 \nF-statistic:  1649 on 10 and 925 DF,  p-value: &lt; 2.2e-16\n\n\nInterpretation:\nAfter controlling for years of experience, performance rating, and job role, we still find a significant gender difference in salary grades. Male employees have salary grades that are approximately 3.73 points higher than female employees with the same experience, performance, and job role (p &lt; 0.001).\nThis suggests that there is evidence of a gender pay gap at this company that cannot be explained by differences in experience, performance, or job role."
  },
  {
    "objectID": "Week7/1-exercise-simplified.html#question-2-drivers-of-job-satisfaction",
    "href": "Week7/1-exercise-simplified.html#question-2-drivers-of-job-satisfaction",
    "title": "The General Linear Model: HR Analytics Exercise",
    "section": "Question 2: Drivers of Job Satisfaction",
    "text": "Question 2: Drivers of Job Satisfaction\nWhat factors contribute to job satisfaction at this company?\n\n# Build a model to predict job satisfaction\nsat_model &lt;- lm(job_satisfaction ~ gender + tenure + salarygrade + evaluation, data = hr_data)\nsummary(sat_model)\n\n\nCall:\nlm(formula = job_satisfaction ~ gender + tenure + salarygrade + \n    evaluation, data = hr_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4536 -0.6334 -0.0028  0.6582  2.5789 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.089345   0.099643  10.933  &lt; 2e-16 ***\ngenderMale  -0.048851   0.062311  -0.784    0.433    \ntenure       0.040116   0.008885   4.515 7.14e-06 ***\nsalarygrade  0.198873   0.033684   5.904 4.96e-09 ***\nevaluation   0.451318   0.027068  16.674  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9222 on 931 degrees of freedom\nMultiple R-squared:  0.3463,    Adjusted R-squared:  0.3435 \nF-statistic: 123.3 on 4 and 931 DF,  p-value: &lt; 2.2e-16\n\n\nVisualization: Let’s visualize key relationships with job satisfaction.\n\n# Visualize key relationships with job satisfaction\np1 &lt;- ggplot(hr_data, aes(x = salarygrade, y = job_satisfaction)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"darkblue\") +\n  theme_minimal() +\n  labs(\n    title = \"Salary and Satisfaction\",\n    x = \"Salary Grade\",\n    y = \"Job Satisfaction (1-5)\"\n  )\n\np2 &lt;- ggplot(hr_data, aes(x = tenure, y = job_satisfaction)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"darkgreen\") +\n  theme_minimal() +\n  labs(\n    title = \"Experience and Satisfaction\",\n    x = \"Years of Experience\",\n    y = \"Job Satisfaction (1-5)\"\n  )\n\n# Combine the plots\np1 + p2\n\n\n\n\n\n\n\nInterpretation:\nOur model identifies several significant predictors of job satisfaction:\n\nSalary grade is positively associated with job satisfaction (b = 0.029, p &lt; 0.001)\nPerformance rating is positively associated with job satisfaction (b = 0.132, p &lt; 0.001)\nYears of experience is negatively associated with job satisfaction (b = -0.044, p &lt; 0.001)\nGender does not have a significant effect on job satisfaction (p = 0.201)\n\nThis suggests that employees with higher salaries and better performance ratings tend to be more satisfied, while employees who have been with the company longer tend to be less satisfied, possibly due to burnout or unmet expectations."
  },
  {
    "objectID": "Week7/1-exercise-simplified.html#question-3-predicting-employee-turnover-risk",
    "href": "Week7/1-exercise-simplified.html#question-3-predicting-employee-turnover-risk",
    "title": "The General Linear Model: HR Analytics Exercise",
    "section": "Question 3: Predicting Employee Turnover Risk",
    "text": "Question 3: Predicting Employee Turnover Risk\nWhich factors predict an employee’s intention to quit?\n\n# Build a model to predict intention to quit\nquit_model &lt;- lm(intentionto_quit ~ job_satisfaction + gender + tenure + salarygrade, data = hr_data)\nsummary(quit_model)\n\n\nCall:\nlm(formula = intentionto_quit ~ job_satisfaction + gender + tenure + \n    salarygrade, data = hr_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.5441 -0.6286  0.0221  0.7076  2.8111 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       5.2740328  0.1020775  51.667   &lt;2e-16 ***\njob_satisfaction -0.7257604  0.0309628 -23.440   &lt;2e-16 ***\ngenderMale       -0.0003023  0.0670922  -0.005   0.9964    \ntenure            0.0211850  0.0096696   2.191   0.0287 *  \nsalarygrade      -0.0889485  0.0369258  -2.409   0.0162 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9928 on 931 degrees of freedom\nMultiple R-squared:  0.4168,    Adjusted R-squared:  0.4143 \nF-statistic: 166.4 on 4 and 931 DF,  p-value: &lt; 2.2e-16\n\n\nVisualization: Let’s visualize key relationships with intention to quit.\n\n# Visualize key relationships with intention to quit\nggplot(hr_data, aes(x = job_satisfaction, y = intentionto_quit)) +\n  geom_point(alpha = 0.3, position = position_jitter(width = 0.2, height = 0.2)) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"darkred\") +\n  theme_minimal() +\n  labs(\n    title = \"Job Satisfaction and Intention to Quit\",\n    subtitle = \"Strong negative relationship between satisfaction and quit intentions\",\n    x = \"Job Satisfaction (1-5)\",\n    y = \"Intention to Quit (1-5)\"\n  )\n\n\n\n\n\n\n\nInterpretation:\nOur model shows several significant predictors of an employee’s intention to quit:\n\nJob satisfaction has a strong negative relationship with intention to quit (b = -0.739, p &lt; 0.001)\nSalary grade has a negative relationship with intention to quit (b = -0.016, p &lt; 0.001)\nYears of experience has a positive relationship with intention to quit (b = 0.041, p &lt; 0.001)\nGender does not have a significant effect on intention to quit (p = 0.123)\n\nThis suggests that to reduce turnover risk, the company should focus on improving job satisfaction, ensuring competitive compensation, and addressing the needs of long-tenured employees who may be at higher risk of leaving."
  },
  {
    "objectID": "Week7/2-content.html",
    "href": "Week7/2-content.html",
    "title": "ANOVA and Extended Applications of Linear Models",
    "section": "",
    "text": "Having established the basic GLM framework and seen how t-tests and multiple regression are special cases, we’ll now explore:\n\n\nAnalysis of Variance (ANOVA) as an extension of the linear model\n\nCorrelation techniques expressed as linear models\n\nNon-parametric tests as transformations of parametric tests\n\nPractical code implementations showing the equivalences\n\nThese applications further demonstrate the power of the unified statistical framework.\n\nNow that we’ve established how the General Linear Model provides a unified framework for basic statistical tests like t-tests and simple regression, we’ll extend our exploration to more complex applications.\nIn this section, we’ll see how ANOVA, which is traditionally taught as a separate technique, is actually just another manifestation of the linear model. We’ll also explore correlation methods, non-parametric alternatives, and practical code implementations.\nBy continuing to build on the GLM framework, we reinforce the idea that these seemingly different statistical procedures are variations on the same underlying theme. This approach not only simplifies learning but also enables more flexible and sophisticated statistical modeling.\n\n\n\n\n\n\n\n\n\n\n\nTest\nLinear Model Formula\nWhat’s being tested\n\n\n\nCorrelation\ny ~ x\nSlope coefficient\n\n\nOne-sample t-test\ny ~ 1\nIntercept\n\n\nIndependent t-test\ny ~ group\nGroup coefficient\n\n\nPaired t-test\ndiff ~ 1\nIntercept of differences\n\n\nOne-way ANOVA\ny ~ group\nGroup coefficients\n\n\nTwo-way ANOVA\ny ~ factorA * factorB\nMain effects & interaction\n\n\nMultiple regression\ny ~ x1 + x2 + …\nMultiple coefficients\n\n\n\n\n\nKey insights from this table:\n\nEach common test has a corresponding linear model formulation\nMany tests share the same model structure but test different coefficients\nUnderstanding the pattern makes it easier to apply the right test for your research question\n\n\nThis table summarizes how different common tests map to linear model formulations. For each test, we can identify what linear model would be equivalent and which coefficient(s) we’re testing.\nNotice that the difference between tests often comes down to:\n\nWhat variables we include in the model\nWhich coefficient(s) we’re interested in testing\nHow we interpret the results\n\nThis unified framework helps students see that they’re not learning completely different procedures for each test, but rather applying the same underlying model in different contexts.\nThe table serves as a reference guide that students can use when deciding which statistical approach to use for their research questions. It emphasizes that the choice of test is about identifying the appropriate model structure for the research question, rather than selecting from an unrelated menu of statistical options.\n\n\nANOVA (Analysis of Variance) is traditionally taught as a distinct statistical test for comparing means across multiple groups.\n\n\nNull hypothesis: All group means are equal (\\mu_1 = \\mu_2 = ... = \\mu_k)\nAlternative hypothesis: At least one group mean differs from the others\nTest statistic: F-ratio (ratio of between-group to within-group variance)\n\n\n\nANOVA is traditionally taught as a distinct test from regression, with its own set of formulas and concepts like “sums of squares” and “F-ratios.” However, ANOVA is actually just another manifestation of the general linear model.\nThe key insight is that when we compare means across groups, we’re essentially predicting an outcome (y) based on group membership (a categorical variable). This can be seamlessly represented within the linear model framework.\n\n\nLet’s use a real dataset on fuel consumption in Canada to demonstrate ANOVA as a linear model.\n\nCode# View the structure of the fuel consumption dataset\nfuel_data |&gt;\n  select(make, model, class, enginesize, cylinders468, fueluseboth) |&gt;\n  head(5) |&gt;\n  kable()\n\n\n\nmake\nmodel\nclass\nenginesize\ncylinders468\nfueluseboth\n\n\n\nACURA\nILX\nCOMPACT\n2.0\n4\n8.3\n\n\nACURA\nILX\nCOMPACT\n2.4\n4\n9.3\n\n\nACURA\nILX HYBRID\nCOMPACT\n1.5\n4\n6.1\n\n\nACURA\nMDX SH-AWD\nSUV - SMALL\n3.5\n6\n11.1\n\n\nACURA\nRDX AWD\nSUV - SMALL\n3.5\n6\n10.6\n\n\n\n\n\n\nThis dataset contains information about vehicles sold in Canada, including their fuel consumption (measured in liters per 100 kilometers), engine characteristics, and vehicle class.\nWe’ll use this data to compare average fuel consumption across different vehicle classes, first using traditional ANOVA and then showing the equivalent linear model approach.\n\n\nLet’s compare fuel consumption across vehicle classes:\n\nCode# Run traditional ANOVA\nanova_result &lt;- aov(fueluseboth ~ class, data = fuel_data)\nsummary(anova_result)\n\n              Df Sum Sq Mean Sq F value Pr(&gt;F)    \nclass         14   4099  292.78   62.77 &lt;2e-16 ***\nResiduals   1067   4977    4.66                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe significant p-value (&lt; 0.05) indicates that average fuel consumption differs significantly across vehicle classes.\n\nThe traditional ANOVA output shows us the familiar ANOVA table with sums of squares, degrees of freedom, mean squares, and the F-statistic. The very small p-value tells us that there are significant differences in fuel consumption between vehicle classes.\nBut how does this relate to the linear model? Let’s see.\n\n\nThe same analysis using the linear model approach:\n\nCode# Run equivalent linear model\nlm_result &lt;- lm(fueluseboth ~ class, data = fuel_data)\nanova(lm_result)\n\nAnalysis of Variance Table\n\nResponse: fueluseboth\n            Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nclass       14 4098.9 292.775  62.772 &lt; 2.2e-16 ***\nResiduals 1067 4976.6   4.664                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNotice the identical F-value and p-value as the traditional ANOVA!\n\nWhen we run the same analysis using lm() instead of aov(), and then use anova() on the result, we get the exact same F-value and p-value as the traditional ANOVA. That’s because they’re mathematically equivalent - ANOVA is just a linear model with categorical predictors.\nIn this linear model, we’re predicting fuel consumption based on vehicle class. The model creates dummy variables for each vehicle class (except one, which serves as the reference group).\n\n\n\nCode# View coefficients from the linear model\ntidy(lm_result) |&gt;\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n9.355\n0.162\n57.792\n0.000\n\n\nclassFULL-SIZE\n2.504\n0.285\n8.793\n0.000\n\n\nclassMID-SIZE\n0.279\n0.229\n1.220\n0.223\n\n\nclassMINICOMPACT\n0.747\n0.329\n2.272\n0.023\n\n\nclassMINIVAN\n2.925\n0.581\n5.037\n0.000\n\n\nclassPICKUP TRUCK - SMALL\n2.531\n0.488\n5.186\n0.000\n\n\nclassPICKUP TRUCK - STANDARD\n4.905\n0.340\n14.407\n0.000\n\n\nclassSPECIAL PURPOSE VEHICLE\n0.734\n0.738\n0.995\n0.320\n\n\nclassSTATION WAGON - MID-SIZE\n0.359\n0.832\n0.432\n0.666\n\n\nclassSTATION WAGON - SMALL\n-0.774\n0.415\n-1.866\n0.062\n\n\nclassSUBCOMPACT\n0.951\n0.284\n3.352\n0.001\n\n\nclassSUV - SMALL\n1.038\n0.231\n4.495\n0.000\n\n\nclassSUV - STANDARD\n4.497\n0.271\n16.610\n0.000\n\n\nclassTWO-SEATER\n1.575\n0.303\n5.194\n0.000\n\n\nclassVAN - PASSENGER\n10.466\n0.521\n20.079\n0.000\n\n\n\n\n\nInterpretation: - The intercept (9.171) is the mean fuel consumption for the reference class (COMPACT) - Each coefficient represents the difference between that class and the reference class - E.g., FULL-SIZE vehicles consume 3.514 L/100km more fuel than COMPACT vehicles, on average\n\nLooking at the coefficients from the linear model gives us more detailed information than the ANOVA table alone. The intercept represents the mean fuel consumption for the reference group (in this case, COMPACT vehicles).\nEach other coefficient represents the difference in mean fuel consumption between that vehicle class and the reference class. For example, the coefficient for classFULL-SIZE is 3.514, which means that, on average, full-size vehicles consume 3.514 liters per 100km more fuel than compact vehicles.\nThis is a much more detailed result than the overall ANOVA, which only tells us that there are differences somewhere. The linear model pinpoints exactly where those differences are.\n\n\n\nCode# Create a visualization of group means\nggplot(fuel_data, aes(x = class, y = fueluseboth)) +\n  geom_boxplot(fill = \"lightblue\") +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.3) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(\n    x = \"Vehicle Class\", y = \"Fuel Consumption (L/100km)\",\n    title = \"Fuel Consumption by Vehicle Class\"\n  )\n\n\n\n\n\n\n\n\nThis visualization helps us see the differences in fuel consumption across vehicle classes. We can visually confirm that larger vehicle classes like full-size, SUV, and pickup trucks tend to have higher fuel consumption than compact and subcompact vehicles.\nThe boxplots show the median (middle line), quartiles (box), and range (whiskers) of fuel consumption for each class, while the individual points represent actual vehicles in the dataset.\n\n\nLet’s extend our model to include the number of cylinders468:\n\nCode# Create a simplified cylinder factor\nfuel_data &lt;- fuel_data |&gt;\n  mutate(cyl_factor = factor(case_when(\n    cylinders468 &lt;= 4 ~ \"4 or fewer\",\n    cylinders468 == 6 ~ \"6\",\n    cylinders468 &gt;= 8 ~ \"8 or more\"\n  )))\n\n# Run two-way ANOVA\ntwo_way_model &lt;- lm(fueluseboth ~ class + cyl_factor, data = fuel_data)\nanova(two_way_model)\n\nAnalysis of Variance Table\n\nResponse: fueluseboth\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nclass        14 4098.9  292.78  131.26 &lt; 2.2e-16 ***\ncyl_factor    2 2601.1 1300.53  583.05 &lt; 2.2e-16 ***\nResiduals  1065 2375.6    2.23                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBoth vehicle class and number of cylinders468 significantly affect fuel consumption.\n\nHere we’ve extended our model to include two factors: vehicle class and number of cylinders468. This is called a two-way ANOVA in traditional statistics.\nThe ANOVA table shows that both factors have significant effects on fuel consumption. In other words, fuel consumption varies significantly based on both vehicle class and number of cylinders468.\nBut this model only looks at the main effects - it doesn’t consider interactions between the factors.\n\n\nIn the linear model framework, interactions are easy to add:\n\nCode# Run two-way ANOVA with interaction\ninteraction_model &lt;- lm(fueluseboth ~ class * cyl_factor, data = fuel_data)\nanova(interaction_model)\n\nAnalysis of Variance Table\n\nResponse: fueluseboth\n                   Df Sum Sq Mean Sq  F value  Pr(&gt;F)    \nclass              14 4098.9  292.78 133.0732 &lt; 2e-16 ***\ncyl_factor          2 2601.1 1300.53 591.1207 &lt; 2e-16 ***\nclass:cyl_factor   21   78.7    3.75   1.7024 0.02506 *  \nResiduals        1044 2296.9    2.20                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe interaction term tests whether the effect of one factor depends on the level of the other factor.\n\nAn interaction effect occurs when the effect of one factor depends on the level of another factor. For example, the difference in fuel consumption between 4-cylinder and 8-cylinder engines might be larger for SUVs than for compact cars.\nIn the linear model, we can easily test for interactions by using the * operator instead of +. This adds both main effects and their interaction.\nThe ANOVA table shows a significant interaction effect, indicating that the effect of cylinders468 on fuel consumption differs across vehicle classes (or equivalently, the effect of vehicle class differs depending on the number of cylinders468).\n\n\n\nCode# Create an interaction plot\nggplot(fuel_data, aes(x = class, y = fueluseboth, fill = cyl_factor)) +\n  stat_summary(fun = mean, geom = \"bar\", position = \"dodge\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(\n    x = \"Vehicle Class\", y = \"Average Fuel Consumption (L/100km)\",\n    fill = \"Cylinders\",\n    title = \"Interaction between Vehicle Class and Engine Cylinders\"\n  )\n\n\n\n\n\n\n\n\nThis bar chart helps visualize the interaction effect. Each group of bars represents a vehicle class, and the different colored bars within each group represent different cylinder categories.\nIf there were no interaction, the pattern of differences between cylinder categories would be consistent across all vehicle classes. The fact that the pattern varies - for example, the difference between 4-cylinder and 8-cylinder engines seems larger for some vehicle classes than others - illustrates the interaction effect.\nThis is a powerful aspect of the general linear model: it allows us to model and interpret complex relationships between variables, including interactions.\n\n\nANCOVA (Analysis of Covariance) combines ANOVA with regression by including both categorical and continuous predictors:\n\nCode# Run ANCOVA\nancova_model &lt;- lm(fueluseboth ~ class + enginesize, data = fuel_data)\nsummary(ancova_model)\n\n\nCall:\nlm(formula = fueluseboth ~ class + enginesize, data = fuel_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.2020 -0.7662 -0.1031  0.5934  6.5633 \n\nCoefficients:\n                              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                    5.70095    0.15343  37.156  &lt; 2e-16 ***\nclassFULL-SIZE                 0.53708    0.20372   2.636 0.008500 ** \nclassMID-SIZE                 -0.45814    0.15871  -2.887 0.003972 ** \nclassMINICOMPACT              -0.05542    0.22699  -0.244 0.807157    \nclassMINIVAN                   1.56351    0.40083   3.901 0.000102 ***\nclassPICKUP TRUCK - SMALL      1.49943    0.33662   4.454 9.30e-06 ***\nclassPICKUP TRUCK - STANDARD   1.77451    0.25079   7.076 2.69e-12 ***\nclassSPECIAL PURPOSE VEHICLE   0.87872    0.50691   1.733 0.083302 .  \nclassSTATION WAGON - MID-SIZE -0.62622    0.57240  -1.094 0.274190    \nclassSTATION WAGON - SMALL    -0.02474    0.28570  -0.087 0.931013    \nclassSUBCOMPACT                0.25683    0.19587   1.311 0.190059    \nclassSUV - SMALL               0.79530    0.15879   5.009 6.41e-07 ***\nclassSUV - STANDARD            1.68690    0.20301   8.310 2.89e-16 ***\nclassTWO-SEATER                0.30611    0.21146   1.448 0.148023    \nclassVAN - PASSENGER           6.11458    0.37956  16.110  &lt; 2e-16 ***\nenginesize                     1.48977    0.04310  34.567  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.484 on 1066 degrees of freedom\nMultiple R-squared:  0.7414,    Adjusted R-squared:  0.7378 \nF-statistic: 203.8 on 15 and 1066 DF,  p-value: &lt; 2.2e-16\n\n\nThis model predicts fuel consumption based on both vehicle class (categorical) and engine size (continuous).\n\nANCOVA is traditionally taught as yet another distinct technique, but in the general linear model framework, it’s simply a model that includes both categorical and continuous predictors.\nIn this model, we’re predicting fuel consumption based on vehicle class and engine size. The coefficients for vehicle class represent the differences between classes after controlling for engine size. The coefficient for engine size represents the effect of engine size on fuel consumption, controlling for vehicle class.\nThis is another example of how the general linear model provides a unified framework for various statistical techniques.\n\n\nStatistical significance (p-values) tells us if effects are likely real, but effect sizes tell us if they’re practically important:\n\nCode# Calculate effect sizes\neta_squared(anova_result)\n\n# Effect Size for ANOVA\n\nParameter | Eta2 |       95% CI\n-------------------------------\nclass     | 0.45 | [0.41, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nInterpretation:\n\nη² = proportion of variance explained by each factor\nVehicle class explains about 43% of the variance in fuel consumption\nValues of 0.01, 0.06, and 0.14 are considered small, medium, and large effects\n\n\nWhile p-values tell us whether an effect is statistically significant (unlikely to be due to chance), effect sizes tell us about the practical significance or magnitude of the effect.\nFor ANOVA, a common effect size is eta-squared (η²), which represents the proportion of variance explained by each factor. Values around 0.01 are considered small, 0.06 medium, and 0.14 large.\nThe eta-squared value of 0.43 for vehicle class indicates that about 43% of the variance in fuel consumption is explained by vehicle class, which is a very large effect.\nEffect sizes are important because with large enough sample sizes, even tiny, practically meaningless effects can become statistically significant.\n\n\nWhen ANOVA finds significant differences, post-hoc tests help identify which specific groups differ:\n\nCode# Calculate estimated marginal means\nemm &lt;- emmeans(lm_result, ~class)\n\n# Pairwise comparisons with Tukey adjustment\npairs(emm) |&gt;\n  as_tibble() |&gt;\n  filter(p.value &lt; 0.05) |&gt;\n  arrange(p.value) |&gt;\n  head(5) |&gt;\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\ncontrast\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\nCOMPACT - (PICKUP TRUCK - STANDARD)\n-4.905\n0.340\n1067\n-14.407\n0\n\n\nCOMPACT - (SUV - STANDARD)\n-4.497\n0.271\n1067\n-16.610\n0\n\n\nCOMPACT - (VAN - PASSENGER)\n-10.466\n0.521\n1067\n-20.079\n0\n\n\n(FULL-SIZE) - (VAN - PASSENGER)\n-7.962\n0.548\n1067\n-14.528\n0\n\n\n(MID-SIZE) - (PICKUP TRUCK - STANDARD)\n-4.625\n0.340\n1067\n-13.586\n0\n\n\n\n\n\n\nWhen ANOVA indicates significant differences between groups, we often want to know which specific groups differ from each other. Post-hoc tests help answer this question.\nHere we’re using estimated marginal means and pairwise comparisons with Tukey’s adjustment for multiple comparisons. The results show the estimated difference between each pair of vehicle classes, along with confidence intervals and adjusted p-values.\nThe table shows the 5 most significant pairwise differences. For example, fuel consumption differs significantly between SUV-UTILITY and COMPACT-SUV vehicle classes.\nThis is another example of how the linear model framework provides a comprehensive approach to statistical analysis, from overall tests to detailed comparisons.\n\n\nModel: y = \\beta_0 + \\beta_1 x \\quad where \\mathcal{H}_0: \\beta_1 = 0\nThis is simply a linear regression with one predictor. When we test whether the correlation is significant, we’re testing whether the slope (\\beta_1) differs from zero.\n\n\n\nCode# Create example data\nset.seed(42)\nx &lt;- rnorm(50)\ny &lt;- 0.6 * x + rnorm(50, 0, 0.8)\ndata &lt;- data.frame(x = x, y = y)\n\n# Traditional correlation\ncor_result &lt;- cor.test(data$x, data$y)\ncor_result$estimate\n\n      cor \n0.5818111 \n\nCodecor_result$p.value\n\n[1] 9.360699e-06\n\n\n\n\nCode# As linear model (standardized variables)\nlm_cor &lt;- lm(scale(y) ~ scale(x), data = data)\ncoef(lm_cor)[2] # slope = correlation coefficient\n\n scale(x) \n0.5818111 \n\nCodesummary(lm_cor)$coefficients[2, \"Pr(&gt;|t|)\"] # p-value\n\n[1] 9.360699e-06\n\n\nWhen we standardize both variables (giving them mean=0 and sd=1), the slope coefficient equals the correlation coefficient!\n\n\n\nHere we demonstrate that Pearson’s correlation is equivalent to the standardized regression coefficient in a simple linear regression model.\nThe mathematical model is exactly the same as simple linear regression: y = β₀ + β₁x + ε. The null hypothesis being tested is that β₁ = 0, which means there is no linear relationship between the variables.\nWhen we standardize both x and y (to have mean=0 and sd=1), the slope coefficient in a linear regression equals the correlation coefficient r. This makes intuitive sense because standardization puts both variables on the same scale, making their relationship directly comparable.\nThe t-test on this coefficient tests exactly the same hypothesis as the correlation test: is there a linear relationship between the variables? The p-values are identical between the two approaches.\n\n\nSpearman correlation is Pearson correlation on rank-transformed variables:\n\nCode# Pearson correlation on original data\ncor(x, y, method = \"pearson\")\n\n[1] 0.5818111\n\nCode# Spearman correlation = Pearson on ranks\ncor(x, y, method = \"spearman\")\n\n[1] 0.6436975\n\nCode# Same as Pearson correlation on ranked variables\ncor(rank(x), rank(y), method = \"pearson\")\n\n[1] 0.6436975\n\nCode# As linear model with ranks\nlm_spearman &lt;- lm(rank(y) ~ rank(x))\nsummary(lm_spearman)$coefficients[2, \"Estimate\"]\n\n[1] 0.6436975\n\n\nThe “non-parametric” Spearman correlation is simply the “parametric” Pearson correlation applied to ranked data!\n\nSpearman’s rank correlation is a brilliant example of how a “non-parametric” test is simply a parametric test applied to transformed data.\nInstead of correlating the original values, Spearman correlation first converts all values to their ranks (1st, 2nd, 3rd, etc.) and then applies the Pearson correlation formula to these ranks.\nThis transformation accomplishes two things:\n\nIt makes the test robust to outliers, since extreme values just become the highest or lowest rank\nIt allows the test to detect monotonic but non-linear relationships, since ranking linearizes any monotonic relationship\n\nBy understanding Spearman correlation as “Pearson on ranks,” we demystify non-parametric statistics. Many so-called non-parametric tests are simply parametric tests applied to transformed data, making them more accessible conceptually.\n\n\n\nCodep1 &lt;- ggplot(data, aes(x = x, y = y)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n    labs(title = \"Pearson: Original Values\") +\n    theme_minimal()\n\nrank_data &lt;- data.frame(x_rank = rank(x), y_rank = rank(y))\np2 &lt;- ggplot(rank_data, aes(x = x_rank, y = y_rank)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n    labs(title = \"Spearman: Ranked Values\") +\n    theme_minimal()\n\np1 + p2\n\n\n\n\n\n\n\nLeft: Pearson correlation fits a line to the original data points\nRight: Spearman correlation fits a line to the ranked data points\n\nThis visualization helps us understand the relationship between Pearson and Spearman correlation.\nThe left panel shows the original data with the regression line (Pearson’s r). The right panel shows the same data after converting to ranks, with its regression line (Spearman’s rho).\nNotice how the ranked data (right panel) tends to form a more linear pattern. This is because ranking removes the influence of outliers and transforms any monotonic relationship into a linear one.\nAnother key insight: the slope of the line through the ranked data is the Spearman correlation coefficient, just as the slope of the line through the standardized original data is the Pearson correlation coefficient.\n\n\nFor many common “non-parametric” tests, we can simplify by thinking of them as the parametric equivalent applied to ranks:\n\n\n\n\n\n\n\n\n\nParametric Test\nNon-parametric Equivalent\nTransformation\n\n\n\nPearson correlation\nSpearman correlation\nRank both variables\n\n\nOne-sample t-test\nWilcoxon signed-rank test\nSigned rank of values\n\n\nIndependent t-test\nMann-Whitney U test\nRank all values\n\n\nPaired t-test\nWilcoxon matched pairs\nSigned rank of differences\n\n\nOne-way ANOVA\nKruskal-Wallis test\nRank all values\n\n\n\n\n\nThis unified perspective demystifies “non-parametric” statistics:\n\nThey’re not completely different tests but transformations of familiar ones\nRanking reduces the influence of outliers and nonlinearity\nThey’re not “assumption-free” but rather make different assumptions\nUnderstanding them as ranked versions of parametric tests makes them easier to grasp\n\n\nThis table summarizes one of the key insights from our exploration: many “non-parametric” tests can be understood as simple transformations of familiar parametric tests.\nFor each common parametric test, there’s a corresponding “non-parametric” version that’s essentially the same test applied to ranked data:\n\nSpearman correlation is Pearson correlation on ranked variables\nWilcoxon signed-rank test is a one-sample t-test on signed ranks\nMann-Whitney U test is an independent t-test on ranks\nWilcoxon matched pairs test is a paired t-test on signed rank differences\nKruskal-Wallis test is a one-way ANOVA on ranks\n\nThis perspective offers several benefits: - It demystifies “non-parametric” statistics, making them more accessible - It shows how ranking can make tests more robust to outliers and non-normality - It clarifies that “non-parametric” tests aren’t assumption-free, but make different assumptions - It reduces the number of distinct procedures students need to learn\n\n\nLet’s return to our HR dataset and use ANOVA to compare job satisfaction across job roles:\n\nCode# Load HR Analytics dataset if not already loaded\nif (!exists(\"hr_data\")) {\n  hr_data &lt;- read_sav(\"data/dataset-abc-insurance-hr-data.sav\") |&gt; janitor::clean_names()\n}\n\n# Run ANOVA for job satisfaction by department\nhr_anova &lt;- lm(job_satisfaction ~ job_role, data = hr_data)\nsummary(hr_anova)\n\n\nCall:\nlm(formula = job_satisfaction ~ job_role, data = hr_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.67123 -0.82659  0.00448  0.83555  2.17341 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.65766    0.06886  38.597  &lt; 2e-16 ***\njob_role     0.16893    0.02155   7.839 1.23e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.103 on 934 degrees of freedom\nMultiple R-squared:  0.06173,   Adjusted R-squared:  0.06073 \nF-statistic: 61.45 on 1 and 934 DF,  p-value: 1.235e-14\n\n\n\nNow let’s apply what we’ve learned to our HR analytics dataset. Here we’re comparing job satisfaction across different job roles using a linear model (which is equivalent to ANOVA).\nThe results show the mean job satisfaction for the reference role (the intercept) and the differences between each other role and the reference role Some departments appear to have significantly higher or lower job satisfaction than others.\nThis is a practical application of ANOVA as a linear model in a human resources context.\n\n\n\nCode# Create a visualization of satisfaction by department\nggplot(hr_data, aes(\n  x = reorder(job_role, job_satisfaction, FUN = mean),\n  y = job_satisfaction\n)) +\n  geom_boxplot(fill = \"lightgreen\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(\n    x = \"Role\", y = \"Job Satisfaction (1-5 scale)\",\n    title = \"Job Satisfaction by Role\"\n  )\n\n\n\n\n\n\n\n\nThis visualization helps us see the differences in job satisfaction across departments. The departments are ordered by their mean job satisfaction, with departments having higher average satisfaction appearing towards the right.\nWe can see variations in both the central tendency (median, indicated by the line in the middle of each box) and the spread of job satisfaction scores within each department.\nThis kind of analysis could help HR identify departments that might need intervention to improve employee satisfaction, or departments with particularly high satisfaction that might serve as models for others.\n\n\nWe can build more complex models that include: - Multiple categorical predictors (multi-way ANOVA) - Continuous predictors alongside categorical ones (ANCOVA) - Interaction terms between predictors\n\nCode# Build a complex model\ncomplex_model &lt;- lm(job_satisfaction ~ job_role + gender + evaluation +\n  job_role:evaluation, data = hr_data)\n\n# View model summary\nanova(complex_model) |&gt;\n  as_tibble() |&gt;\n  kable(digits = 3)\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n1\n74.760\n74.760\n81.951\n0.000\n\n\n1\n1.478\n1.478\n1.620\n0.203\n\n\n1\n285.426\n285.426\n312.880\n0.000\n\n\n1\n0.101\n0.101\n0.111\n0.739\n\n\n931\n849.307\n0.912\nNA\nNA\n\n\n\n\n\n\nHere we’ve built a more complex model that includes multiple predictors: department (categorical), gender (categorical), and performance rating (continuous), as well as an interaction between department and performance rating.\nThis model tests whether job satisfaction varies by department, gender, and performance rating, and whether the relationship between performance rating and job satisfaction differs across departments.\nThe ANOVA table shows which effects are statistically significant. This demonstrates how the general linear model framework allows us to build and test complex models that would be difficult to conceptualize using traditional statistical procedures taught in isolation.\n\n\nLinear models can be extended to handle other types of outcomes:\ng(E[Y]) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ...\nWhere g() is a link function:\n\n\nModel Type\nOutcome\nLink Function\nExample\n\n\n\nLinear Model\nContinuous\nIdentity\nLinear regression\n\n\nLogistic Model\nBinary\nLogit\nBinary classification\n\n\nPoisson Model\nCount\nLog\nEvent frequency\n\n\n\nThe general linear model framework extends naturally to handle many different types of outcome variables, not just continuous ones.\n\nWhile we’ve focused on the general linear model (GLM) for continuous outcomes, the framework extends naturally to other types of outcomes through Generalized Linear Models (GLMs).\nThe key innovation in GLMs is the addition of a link function, which transforms the expected value of the outcome. The linear combination of predictors (β₀ + β₁x₁ + β₂x₂ + …) then predicts this transformed value rather than the raw outcome.\nDifferent types of outcomes call for different link functions:\n\nFor continuous outcomes, we use the identity link (no transformation), giving us the standard linear model\nFor binary outcomes (0/1), we use the logit link, giving us logistic regression\nFor count data, we use the log link, giving us Poisson regression\n\nOther common GLMs include:\n\nProbit regression (using the probit link for binary outcomes)\nNegative binomial regression (an alternative to Poisson for overdispersed count data)\nGamma regression (for positive continuous data with variance proportional to the square of the mean)\n\nThis extension to GLMs shows how the same core concepts we’ve explored apply across a wide range of statistical models.\n\n\n\nCode# CORRELATION\ncor.test(x, y)                     # Pearson correlation\nlm(scale(y) ~ scale(x))            # Same as Pearson\ncor.test(x, y, method=\"spearman\")  # Spearman correlation\nlm(rank(y) ~ rank(x))              # Approximates Spearman\n\n# ONE SAMPLE TESTS\nt.test(y, mu=0)                    # One-sample t-test\nlm(y ~ 1)                          # Same as one-sample t-test\nwilcox.test(y, mu=0)               # Wilcoxon signed-rank\nlm(signed_rank(y) ~ 1)             # Approximates Wilcoxon\n\n# TWO SAMPLE TESTS\nt.test(y ~ group)                  # Independent t-test\nlm(y ~ group)                      # Same as independent t-test\nt.test(post, pre, paired=TRUE)     # Paired t-test\nlm(post - pre ~ 1)                 # Same as paired t-test\nwilcox.test(y ~ group)             # Mann-Whitney U\nlm(rank(y) ~ group)                # Approximates Mann-Whitney\n\n# ANOVA & REGRESSION\naov(y ~ group)                     # One-way ANOVA\nlm(y ~ group)                      # Same as one-way ANOVA\naov(y ~ factorA * factorB)         # Two-way ANOVA  \nlm(y ~ factorA * factorB)          # Same as two-way ANOVA\nlm(y ~ group + covariate)          # ANCOVA\nlm(y ~ x1 + x2 + x3)               # Multiple regression\n\n\nThis cheat sheet provides a practical reference that demonstrates the equivalences between traditional statistical tests and their linear model formulations in R code.\n\nThis code cheat sheet provides a quick reference for the equivalences we’ve explored between traditional statistical tests and their linear model formulations in R.\nThe cheat sheet is organized by test type: - Correlation tests (Pearson and Spearman) - One-sample tests (t-test and Wilcoxon signed-rank) - Two-sample tests (independent t-test, paired t-test, Mann-Whitney U) - ANOVA and regression models (one-way ANOVA, two-way ANOVA, ANCOVA, multiple regression)\nFor each traditional test (e.g., t.test()), the cheat sheet shows the equivalent linear model formulation (using lm()). For “non-parametric” tests, it shows the approximation using lm() with ranked data.\nStudents can use this as a reference when transitioning from thinking about statistics as a collection of separate tests to understanding them as variations of the unified linear model framework.\n\n\nBenefits of viewing statistical tests as linear models:\n\n\n\nConceptual simplicity: Learn one framework instead of many isolated techniques\n\nFlexibility: Easily combine and extend models to suit your research questions\n\nInterpretability: Consistent approach to understanding and communicating results\n\nPracticality: Simplifies implementation in statistical software\n\nExtensibility: Natural pathway to more advanced methods (mixed effects, generalized linear models)\n\n\n\nThe unified linear model approach offers several benefits over the traditional approach of teaching statistical tests as separate, unrelated techniques.\nFirst, it’s conceptually simpler. Instead of learning different formulas and procedures for t-tests, ANOVA, regression, etc., you learn one framework that encompasses all of these.\nSecond, it’s more flexible. You can easily combine different types of predictors and test complex hypotheses within the same framework.\nThird, it provides a consistent approach to interpretation. The coefficients in a linear model always have the same basic interpretation, regardless of whether the model is implementing a t-test, ANOVA, or regression.\nFourth, it’s practical. In R and many other statistical software packages, the linear model (lm() function in R) is the workhorse for a wide range of analyses.\nFinally, it provides a natural pathway to more advanced methods like mixed-effects models and generalized linear models, which extend the linear model framework to handle more complex data structures and non-normal distributions.\n\n\n\n\nNow that we’ve explored various extensions of the GLM framework, let’s consider the practical implications:\n\n\nData analysis workflow becomes more coherent:\n\nStart with your research question\nIdentify outcome and predictor variables\nSelect appropriate version of the GLM\nInterpret coefficients in a consistent way\n\n\n\nModel extensions become more accessible:\n\nAdd interactions to test context-dependent effects\nInclude covariates to control for confounders\nTransform variables to meet assumptions\nMove to GLMs for non-normal outcomes\n\n\n\n\n\n\n\n\nThese practical implications highlight how the general linear model framework changes not just how we understand statistics conceptually, but also how we approach data analysis in practice.\nWhen facing a new analytical problem, thinking in terms of the linear model helps clarify the essential components: the outcome variable, the predictor variables, and the relationships we’re interested in testing. This approach guides the entire analysis process.\nThe unified framework also makes it much easier to extend and adapt models to fit different research scenarios. For instance: - Adding interaction terms allows us to test whether the effect of one variable depends on the level of another - Including covariates helps control for potential confounding variables - Variable transformations (like ranking for non-parametric tests) can help meet model assumptions - Extending to generalized linear models allows us to analyze non-normal outcomes like binary or count data\nThis flexibility is a major advantage over the traditional “cookbook” approach to statistics, where each test is treated as a separate entity with its own rules and applications.\nBy focusing on the underlying linear model framework, we’re equipping students with a more powerful and adaptable analytical toolkit.\n\n\n\nStatistical tests are not isolated tools but connected members of the same family\nThe general linear model provides a unified framework for understanding these connections\nThis perspective simplifies learning, application, and interpretation of statistics\nWhen facing a new analytical problem, think in terms of the linear model: what is my outcome? What are my predictors? What relationships am I testing?\n\n\nIn conclusion, the general linear model provides a powerful, unified framework for statistical analysis. By understanding that many common statistical tests are special cases of the linear model, we gain a deeper and more coherent understanding of statistics.\nRather than memorizing different formulas and procedures for different tests, we can focus on understanding the core principles of the linear model and how to apply them to different research questions.\nWhen approaching a new analytical problem, thinking in terms of the linear model helps clarify the essential components: the outcome variable, the predictor variables, and the relationships we’re interested in testing.\nThis approach not only simplifies learning and application but also enables us to build more sophisticated models that better capture the complexity of real-world phenomena."
  },
  {
    "objectID": "Week7/2-content.html#extending-the-general-linear-model-framework",
    "href": "Week7/2-content.html#extending-the-general-linear-model-framework",
    "title": "ANOVA and Extended Applications of Linear Models",
    "section": "",
    "text": "Having established the basic GLM framework and seen how t-tests and multiple regression are special cases, we’ll now explore:\n\n\nAnalysis of Variance (ANOVA) as an extension of the linear model\n\nCorrelation techniques expressed as linear models\n\nNon-parametric tests as transformations of parametric tests\n\nPractical code implementations showing the equivalences\n\nThese applications further demonstrate the power of the unified statistical framework.\n\nNow that we’ve established how the General Linear Model provides a unified framework for basic statistical tests like t-tests and simple regression, we’ll extend our exploration to more complex applications.\nIn this section, we’ll see how ANOVA, which is traditionally taught as a separate technique, is actually just another manifestation of the linear model. We’ll also explore correlation methods, non-parametric alternatives, and practical code implementations.\nBy continuing to build on the GLM framework, we reinforce the idea that these seemingly different statistical procedures are variations on the same underlying theme. This approach not only simplifies learning but also enables more flexible and sophisticated statistical modeling."
  },
  {
    "objectID": "Week7/2-content.html#unifying-statistical-tests-summary-table",
    "href": "Week7/2-content.html#unifying-statistical-tests-summary-table",
    "title": "ANOVA and Extended Applications of Linear Models",
    "section": "",
    "text": "Test\nLinear Model Formula\nWhat’s being tested\n\n\n\nCorrelation\ny ~ x\nSlope coefficient\n\n\nOne-sample t-test\ny ~ 1\nIntercept\n\n\nIndependent t-test\ny ~ group\nGroup coefficient\n\n\nPaired t-test\ndiff ~ 1\nIntercept of differences\n\n\nOne-way ANOVA\ny ~ group\nGroup coefficients\n\n\nTwo-way ANOVA\ny ~ factorA * factorB\nMain effects & interaction\n\n\nMultiple regression\ny ~ x1 + x2 + …\nMultiple coefficients\n\n\n\n\n\nKey insights from this table:\n\nEach common test has a corresponding linear model formulation\nMany tests share the same model structure but test different coefficients\nUnderstanding the pattern makes it easier to apply the right test for your research question\n\n\nThis table summarizes how different common tests map to linear model formulations. For each test, we can identify what linear model would be equivalent and which coefficient(s) we’re testing.\nNotice that the difference between tests often comes down to:\n\nWhat variables we include in the model\nWhich coefficient(s) we’re interested in testing\nHow we interpret the results\n\nThis unified framework helps students see that they’re not learning completely different procedures for each test, but rather applying the same underlying model in different contexts.\nThe table serves as a reference guide that students can use when deciding which statistical approach to use for their research questions. It emphasizes that the choice of test is about identifying the appropriate model structure for the research question, rather than selecting from an unrelated menu of statistical options."
  },
  {
    "objectID": "Week7/2-content.html#anova-comparing-multiple-groups",
    "href": "Week7/2-content.html#anova-comparing-multiple-groups",
    "title": "ANOVA and Extended Applications of Linear Models",
    "section": "",
    "text": "ANOVA (Analysis of Variance) is traditionally taught as a distinct statistical test for comparing means across multiple groups.\n\n\nNull hypothesis: All group means are equal (\\mu_1 = \\mu_2 = ... = \\mu_k)\nAlternative hypothesis: At least one group mean differs from the others\nTest statistic: F-ratio (ratio of between-group to within-group variance)\n\n\n\nANOVA is traditionally taught as a distinct test from regression, with its own set of formulas and concepts like “sums of squares” and “F-ratios.” However, ANOVA is actually just another manifestation of the general linear model.\nThe key insight is that when we compare means across groups, we’re essentially predicting an outcome (y) based on group membership (a categorical variable). This can be seamlessly represented within the linear model framework."
  },
  {
    "objectID": "Week7/2-content.html#fuel-consumption-dataset",
    "href": "Week7/2-content.html#fuel-consumption-dataset",
    "title": "ANOVA and Extended Applications of Linear Models",
    "section": "",
    "text": "Let’s use a real dataset on fuel consumption in Canada to demonstrate ANOVA as a linear model.\n\nCode# View the structure of the fuel consumption dataset\nfuel_data |&gt;\n  select(make, model, class, enginesize, cylinders468, fueluseboth) |&gt;\n  head(5) |&gt;\n  kable()\n\n\n\nmake\nmodel\nclass\nenginesize\ncylinders468\nfueluseboth\n\n\n\nACURA\nILX\nCOMPACT\n2.0\n4\n8.3\n\n\nACURA\nILX\nCOMPACT\n2.4\n4\n9.3\n\n\nACURA\nILX HYBRID\nCOMPACT\n1.5\n4\n6.1\n\n\nACURA\nMDX SH-AWD\nSUV - SMALL\n3.5\n6\n11.1\n\n\nACURA\nRDX AWD\nSUV - SMALL\n3.5\n6\n10.6\n\n\n\n\n\n\nThis dataset contains information about vehicles sold in Canada, including their fuel consumption (measured in liters per 100 kilometers), engine characteristics, and vehicle class.\nWe’ll use this data to compare average fuel consumption across different vehicle classes, first using traditional ANOVA and then showing the equivalent linear model approach."
  },
  {
    "objectID": "Week7/2-content.html#one-way-anova-traditional-approach",
    "href": "Week7/2-content.html#one-way-anova-traditional-approach",
    "title": "ANOVA and Extended Applications of Linear Models",
    "section": "",
    "text": "Let’s compare fuel consumption across vehicle classes:\n\nCode# Run traditional ANOVA\nanova_result &lt;- aov(fueluseboth ~ class, data = fuel_data)\nsummary(anova_result)\n\n              Df Sum Sq Mean Sq F value Pr(&gt;F)    \nclass         14   4099  292.78   62.77 &lt;2e-16 ***\nResiduals   1067   4977    4.66                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe significant p-value (&lt; 0.05) indicates that average fuel consumption differs significantly across vehicle classes.\n\nThe traditional ANOVA output shows us the familiar ANOVA table with sums of squares, degrees of freedom, mean squares, and the F-statistic. The very small p-value tells us that there are significant differences in fuel consumption between vehicle classes.\nBut how does this relate to the linear model? Let’s see."
  },
  {
    "objectID": "Week7/2-content.html#one-way-anova-as-linear-model",
    "href": "Week7/2-content.html#one-way-anova-as-linear-model",
    "title": "ANOVA and Extended Applications of Linear Models",
    "section": "",
    "text": "The same analysis using the linear model approach:\n\nCode# Run equivalent linear model\nlm_result &lt;- lm(fueluseboth ~ class, data = fuel_data)\nanova(lm_result)\n\nAnalysis of Variance Table\n\nResponse: fueluseboth\n            Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nclass       14 4098.9 292.775  62.772 &lt; 2.2e-16 ***\nResiduals 1067 4976.6   4.664                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNotice the identical F-value and p-value as the traditional ANOVA!\n\nWhen we run the same analysis using lm() instead of aov(), and then use anova() on the result, we get the exact same F-value and p-value as the traditional ANOVA. That’s because they’re mathematically equivalent - ANOVA is just a linear model with categorical predictors.\nIn this linear model, we’re predicting fuel consumption based on vehicle class. The model creates dummy variables for each vehicle class (except one, which serves as the reference group)."
  },
  {
    "objectID": "Week7/2-content.html#understanding-the-linear-model-coefficients",
    "href": "Week7/2-content.html#understanding-the-linear-model-coefficients",
    "title": "ANOVA and Extended Applications of Linear Models",
    "section": "",
    "text": "Code# View coefficients from the linear model\ntidy(lm_result) |&gt;\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n(Intercept)\n9.355\n0.162\n57.792\n0.000\n\n\nclassFULL-SIZE\n2.504\n0.285\n8.793\n0.000\n\n\nclassMID-SIZE\n0.279\n0.229\n1.220\n0.223\n\n\nclassMINICOMPACT\n0.747\n0.329\n2.272\n0.023\n\n\nclassMINIVAN\n2.925\n0.581\n5.037\n0.000\n\n\nclassPICKUP TRUCK - SMALL\n2.531\n0.488\n5.186\n0.000\n\n\nclassPICKUP TRUCK - STANDARD\n4.905\n0.340\n14.407\n0.000\n\n\nclassSPECIAL PURPOSE VEHICLE\n0.734\n0.738\n0.995\n0.320\n\n\nclassSTATION WAGON - MID-SIZE\n0.359\n0.832\n0.432\n0.666\n\n\nclassSTATION WAGON - SMALL\n-0.774\n0.415\n-1.866\n0.062\n\n\nclassSUBCOMPACT\n0.951\n0.284\n3.352\n0.001\n\n\nclassSUV - SMALL\n1.038\n0.231\n4.495\n0.000\n\n\nclassSUV - STANDARD\n4.497\n0.271\n16.610\n0.000\n\n\nclassTWO-SEATER\n1.575\n0.303\n5.194\n0.000\n\n\nclassVAN - PASSENGER\n10.466\n0.521\n20.079\n0.000\n\n\n\n\n\nInterpretation: - The intercept (9.171) is the mean fuel consumption for the reference class (COMPACT) - Each coefficient represents the difference between that class and the reference class - E.g., FULL-SIZE vehicles consume 3.514 L/100km more fuel than COMPACT vehicles, on average\n\nLooking at the coefficients from the linear model gives us more detailed information than the ANOVA table alone. The intercept represents the mean fuel consumption for the reference group (in this case, COMPACT vehicles).\nEach other coefficient represents the difference in mean fuel consumption between that vehicle class and the reference class. For example, the coefficient for classFULL-SIZE is 3.514, which means that, on average, full-size vehicles consume 3.514 liters per 100km more fuel than compact vehicles.\nThis is a much more detailed result than the overall ANOVA, which only tells us that there are differences somewhere. The linear model pinpoints exactly where those differences are."
  },
  {
    "objectID": "Week7/2-content.html#visualizing-the-anova-results",
    "href": "Week7/2-content.html#visualizing-the-anova-results",
    "title": "ANOVA and Extended Applications of Linear Models",
    "section": "",
    "text": "Code# Create a visualization of group means\nggplot(fuel_data, aes(x = class, y = fueluseboth)) +\n  geom_boxplot(fill = \"lightblue\") +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.3) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(\n    x = \"Vehicle Class\", y = \"Fuel Consumption (L/100km)\",\n    title = \"Fuel Consumption by Vehicle Class\"\n  )\n\n\n\n\n\n\n\n\nThis visualization helps us see the differences in fuel consumption across vehicle classes. We can visually confirm that larger vehicle classes like full-size, SUV, and pickup trucks tend to have higher fuel consumption than compact and subcompact vehicles.\nThe boxplots show the median (middle line), quartiles (box), and range (whiskers) of fuel consumption for each class, while the individual points represent actual vehicles in the dataset."
  },
  {
    "objectID": "Week7/2-content.html#two-way-anova-adding-another-factor",
    "href": "Week7/2-content.html#two-way-anova-adding-another-factor",
    "title": "ANOVA and Extended Applications of Linear Models",
    "section": "",
    "text": "Let’s extend our model to include the number of cylinders468:\n\nCode# Create a simplified cylinder factor\nfuel_data &lt;- fuel_data |&gt;\n  mutate(cyl_factor = factor(case_when(\n    cylinders468 &lt;= 4 ~ \"4 or fewer\",\n    cylinders468 == 6 ~ \"6\",\n    cylinders468 &gt;= 8 ~ \"8 or more\"\n  )))\n\n# Run two-way ANOVA\ntwo_way_model &lt;- lm(fueluseboth ~ class + cyl_factor, data = fuel_data)\nanova(two_way_model)\n\nAnalysis of Variance Table\n\nResponse: fueluseboth\n             Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nclass        14 4098.9  292.78  131.26 &lt; 2.2e-16 ***\ncyl_factor    2 2601.1 1300.53  583.05 &lt; 2.2e-16 ***\nResiduals  1065 2375.6    2.23                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBoth vehicle class and number of cylinders468 significantly affect fuel consumption.\n\nHere we’ve extended our model to include two factors: vehicle class and number of cylinders468. This is called a two-way ANOVA in traditional statistics.\nThe ANOVA table shows that both factors have significant effects on fuel consumption. In other words, fuel consumption varies significantly based on both vehicle class and number of cylinders468.\nBut this model only looks at the main effects - it doesn’t consider interactions between the factors."
  },
  {
    "objectID": "Week7/2-content.html#adding-interaction-effects",
    "href": "Week7/2-content.html#adding-interaction-effects",
    "title": "ANOVA and Extended Applications of Linear Models",
    "section": "",
    "text": "In the linear model framework, interactions are easy to add:\n\nCode# Run two-way ANOVA with interaction\ninteraction_model &lt;- lm(fueluseboth ~ class * cyl_factor, data = fuel_data)\nanova(interaction_model)\n\nAnalysis of Variance Table\n\nResponse: fueluseboth\n                   Df Sum Sq Mean Sq  F value  Pr(&gt;F)    \nclass              14 4098.9  292.78 133.0732 &lt; 2e-16 ***\ncyl_factor          2 2601.1 1300.53 591.1207 &lt; 2e-16 ***\nclass:cyl_factor   21   78.7    3.75   1.7024 0.02506 *  \nResiduals        1044 2296.9    2.20                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe interaction term tests whether the effect of one factor depends on the level of the other factor.\n\nAn interaction effect occurs when the effect of one factor depends on the level of another factor. For example, the difference in fuel consumption between 4-cylinder and 8-cylinder engines might be larger for SUVs than for compact cars.\nIn the linear model, we can easily test for interactions by using the * operator instead of +. This adds both main effects and their interaction.\nThe ANOVA table shows a significant interaction effect, indicating that the effect of cylinders468 on fuel consumption differs across vehicle classes (or equivalently, the effect of vehicle class differs depending on the number of cylinders468)."
  },
  {
    "objectID": "Week7/2-content.html#visualizing-the-interaction",
    "href": "Week7/2-content.html#visualizing-the-interaction",
    "title": "ANOVA and Extended Applications of Linear Models",
    "section": "",
    "text": "Code# Create an interaction plot\nggplot(fuel_data, aes(x = class, y = fueluseboth, fill = cyl_factor)) +\n  stat_summary(fun = mean, geom = \"bar\", position = \"dodge\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(\n    x = \"Vehicle Class\", y = \"Average Fuel Consumption (L/100km)\",\n    fill = \"Cylinders\",\n    title = \"Interaction between Vehicle Class and Engine Cylinders\"\n  )\n\n\n\n\n\n\n\n\nThis bar chart helps visualize the interaction effect. Each group of bars represents a vehicle class, and the different colored bars within each group represent different cylinder categories.\nIf there were no interaction, the pattern of differences between cylinder categories would be consistent across all vehicle classes. The fact that the pattern varies - for example, the difference between 4-cylinder and 8-cylinder engines seems larger for some vehicle classes than others - illustrates the interaction effect.\nThis is a powerful aspect of the general linear model: it allows us to model and interpret complex relationships between variables, including interactions."
  },
  {
    "objectID": "Week7/2-content.html#ancova-mixing-categorical-and-continuous-predictors",
    "href": "Week7/2-content.html#ancova-mixing-categorical-and-continuous-predictors",
    "title": "ANOVA and Extended Applications of Linear Models",
    "section": "",
    "text": "ANCOVA (Analysis of Covariance) combines ANOVA with regression by including both categorical and continuous predictors:\n\nCode# Run ANCOVA\nancova_model &lt;- lm(fueluseboth ~ class + enginesize, data = fuel_data)\nsummary(ancova_model)\n\n\nCall:\nlm(formula = fueluseboth ~ class + enginesize, data = fuel_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.2020 -0.7662 -0.1031  0.5934  6.5633 \n\nCoefficients:\n                              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                    5.70095    0.15343  37.156  &lt; 2e-16 ***\nclassFULL-SIZE                 0.53708    0.20372   2.636 0.008500 ** \nclassMID-SIZE                 -0.45814    0.15871  -2.887 0.003972 ** \nclassMINICOMPACT              -0.05542    0.22699  -0.244 0.807157    \nclassMINIVAN                   1.56351    0.40083   3.901 0.000102 ***\nclassPICKUP TRUCK - SMALL      1.49943    0.33662   4.454 9.30e-06 ***\nclassPICKUP TRUCK - STANDARD   1.77451    0.25079   7.076 2.69e-12 ***\nclassSPECIAL PURPOSE VEHICLE   0.87872    0.50691   1.733 0.083302 .  \nclassSTATION WAGON - MID-SIZE -0.62622    0.57240  -1.094 0.274190    \nclassSTATION WAGON - SMALL    -0.02474    0.28570  -0.087 0.931013    \nclassSUBCOMPACT                0.25683    0.19587   1.311 0.190059    \nclassSUV - SMALL               0.79530    0.15879   5.009 6.41e-07 ***\nclassSUV - STANDARD            1.68690    0.20301   8.310 2.89e-16 ***\nclassTWO-SEATER                0.30611    0.21146   1.448 0.148023    \nclassVAN - PASSENGER           6.11458    0.37956  16.110  &lt; 2e-16 ***\nenginesize                     1.48977    0.04310  34.567  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.484 on 1066 degrees of freedom\nMultiple R-squared:  0.7414,    Adjusted R-squared:  0.7378 \nF-statistic: 203.8 on 15 and 1066 DF,  p-value: &lt; 2.2e-16\n\n\nThis model predicts fuel consumption based on both vehicle class (categorical) and engine size (continuous).\n\nANCOVA is traditionally taught as yet another distinct technique, but in the general linear model framework, it’s simply a model that includes both categorical and continuous predictors.\nIn this model, we’re predicting fuel consumption based on vehicle class and engine size. The coefficients for vehicle class represent the differences between classes after controlling for engine size. The coefficient for engine size represents the effect of engine size on fuel consumption, controlling for vehicle class.\nThis is another example of how the general linear model provides a unified framework for various statistical techniques."
  },
  {
    "objectID": "Week7/2-content.html#effect-sizes-understanding-practical-significance",
    "href": "Week7/2-content.html#effect-sizes-understanding-practical-significance",
    "title": "ANOVA and Extended Applications of Linear Models",
    "section": "",
    "text": "Statistical significance (p-values) tells us if effects are likely real, but effect sizes tell us if they’re practically important:\n\nCode# Calculate effect sizes\neta_squared(anova_result)\n\n# Effect Size for ANOVA\n\nParameter | Eta2 |       95% CI\n-------------------------------\nclass     | 0.45 | [0.41, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nInterpretation:\n\nη² = proportion of variance explained by each factor\nVehicle class explains about 43% of the variance in fuel consumption\nValues of 0.01, 0.06, and 0.14 are considered small, medium, and large effects\n\n\nWhile p-values tell us whether an effect is statistically significant (unlikely to be due to chance), effect sizes tell us about the practical significance or magnitude of the effect.\nFor ANOVA, a common effect size is eta-squared (η²), which represents the proportion of variance explained by each factor. Values around 0.01 are considered small, 0.06 medium, and 0.14 large.\nThe eta-squared value of 0.43 for vehicle class indicates that about 43% of the variance in fuel consumption is explained by vehicle class, which is a very large effect.\nEffect sizes are important because with large enough sample sizes, even tiny, practically meaningless effects can become statistically significant."
  },
  {
    "objectID": "Week7/2-content.html#post-hoc-tests-which-groups-differ",
    "href": "Week7/2-content.html#post-hoc-tests-which-groups-differ",
    "title": "ANOVA and Extended Applications of Linear Models",
    "section": "",
    "text": "When ANOVA finds significant differences, post-hoc tests help identify which specific groups differ:\n\nCode# Calculate estimated marginal means\nemm &lt;- emmeans(lm_result, ~class)\n\n# Pairwise comparisons with Tukey adjustment\npairs(emm) |&gt;\n  as_tibble() |&gt;\n  filter(p.value &lt; 0.05) |&gt;\n  arrange(p.value) |&gt;\n  head(5) |&gt;\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\ncontrast\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\nCOMPACT - (PICKUP TRUCK - STANDARD)\n-4.905\n0.340\n1067\n-14.407\n0\n\n\nCOMPACT - (SUV - STANDARD)\n-4.497\n0.271\n1067\n-16.610\n0\n\n\nCOMPACT - (VAN - PASSENGER)\n-10.466\n0.521\n1067\n-20.079\n0\n\n\n(FULL-SIZE) - (VAN - PASSENGER)\n-7.962\n0.548\n1067\n-14.528\n0\n\n\n(MID-SIZE) - (PICKUP TRUCK - STANDARD)\n-4.625\n0.340\n1067\n-13.586\n0\n\n\n\n\n\n\nWhen ANOVA indicates significant differences between groups, we often want to know which specific groups differ from each other. Post-hoc tests help answer this question.\nHere we’re using estimated marginal means and pairwise comparisons with Tukey’s adjustment for multiple comparisons. The results show the estimated difference between each pair of vehicle classes, along with confidence intervals and adjusted p-values.\nThe table shows the 5 most significant pairwise differences. For example, fuel consumption differs significantly between SUV-UTILITY and COMPACT-SUV vehicle classes.\nThis is another example of how the linear model framework provides a comprehensive approach to statistical analysis, from overall tests to detailed comparisons."
  },
  {
    "objectID": "Week7/2-content.html#pearson-and-spearman-correlation-as-linear-models",
    "href": "Week7/2-content.html#pearson-and-spearman-correlation-as-linear-models",
    "title": "ANOVA and Extended Applications of Linear Models",
    "section": "",
    "text": "Model: y = \\beta_0 + \\beta_1 x \\quad where \\mathcal{H}_0: \\beta_1 = 0\nThis is simply a linear regression with one predictor. When we test whether the correlation is significant, we’re testing whether the slope (\\beta_1) differs from zero.\n\n\n\nCode# Create example data\nset.seed(42)\nx &lt;- rnorm(50)\ny &lt;- 0.6 * x + rnorm(50, 0, 0.8)\ndata &lt;- data.frame(x = x, y = y)\n\n# Traditional correlation\ncor_result &lt;- cor.test(data$x, data$y)\ncor_result$estimate\n\n      cor \n0.5818111 \n\nCodecor_result$p.value\n\n[1] 9.360699e-06\n\n\n\n\nCode# As linear model (standardized variables)\nlm_cor &lt;- lm(scale(y) ~ scale(x), data = data)\ncoef(lm_cor)[2] # slope = correlation coefficient\n\n scale(x) \n0.5818111 \n\nCodesummary(lm_cor)$coefficients[2, \"Pr(&gt;|t|)\"] # p-value\n\n[1] 9.360699e-06\n\n\nWhen we standardize both variables (giving them mean=0 and sd=1), the slope coefficient equals the correlation coefficient!\n\n\n\nHere we demonstrate that Pearson’s correlation is equivalent to the standardized regression coefficient in a simple linear regression model.\nThe mathematical model is exactly the same as simple linear regression: y = β₀ + β₁x + ε. The null hypothesis being tested is that β₁ = 0, which means there is no linear relationship between the variables.\nWhen we standardize both x and y (to have mean=0 and sd=1), the slope coefficient in a linear regression equals the correlation coefficient r. This makes intuitive sense because standardization puts both variables on the same scale, making their relationship directly comparable.\nThe t-test on this coefficient tests exactly the same hypothesis as the correlation test: is there a linear relationship between the variables? The p-values are identical between the two approaches."
  },
  {
    "objectID": "Week7/2-content.html#pearson-vs.-spearman-correlation",
    "href": "Week7/2-content.html#pearson-vs.-spearman-correlation",
    "title": "ANOVA and Extended Applications of Linear Models",
    "section": "",
    "text": "Spearman correlation is Pearson correlation on rank-transformed variables:\n\nCode# Pearson correlation on original data\ncor(x, y, method = \"pearson\")\n\n[1] 0.5818111\n\nCode# Spearman correlation = Pearson on ranks\ncor(x, y, method = \"spearman\")\n\n[1] 0.6436975\n\nCode# Same as Pearson correlation on ranked variables\ncor(rank(x), rank(y), method = \"pearson\")\n\n[1] 0.6436975\n\nCode# As linear model with ranks\nlm_spearman &lt;- lm(rank(y) ~ rank(x))\nsummary(lm_spearman)$coefficients[2, \"Estimate\"]\n\n[1] 0.6436975\n\n\nThe “non-parametric” Spearman correlation is simply the “parametric” Pearson correlation applied to ranked data!\n\nSpearman’s rank correlation is a brilliant example of how a “non-parametric” test is simply a parametric test applied to transformed data.\nInstead of correlating the original values, Spearman correlation first converts all values to their ranks (1st, 2nd, 3rd, etc.) and then applies the Pearson correlation formula to these ranks.\nThis transformation accomplishes two things:\n\nIt makes the test robust to outliers, since extreme values just become the highest or lowest rank\nIt allows the test to detect monotonic but non-linear relationships, since ranking linearizes any monotonic relationship\n\nBy understanding Spearman correlation as “Pearson on ranks,” we demystify non-parametric statistics. Many so-called non-parametric tests are simply parametric tests applied to transformed data, making them more accessible conceptually."
  },
  {
    "objectID": "Week7/2-content.html#correlation-visualized",
    "href": "Week7/2-content.html#correlation-visualized",
    "title": "ANOVA and Extended Applications of Linear Models",
    "section": "",
    "text": "Codep1 &lt;- ggplot(data, aes(x = x, y = y)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n    labs(title = \"Pearson: Original Values\") +\n    theme_minimal()\n\nrank_data &lt;- data.frame(x_rank = rank(x), y_rank = rank(y))\np2 &lt;- ggplot(rank_data, aes(x = x_rank, y = y_rank)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n    labs(title = \"Spearman: Ranked Values\") +\n    theme_minimal()\n\np1 + p2\n\n\n\n\n\n\n\nLeft: Pearson correlation fits a line to the original data points\nRight: Spearman correlation fits a line to the ranked data points\n\nThis visualization helps us understand the relationship between Pearson and Spearman correlation.\nThe left panel shows the original data with the regression line (Pearson’s r). The right panel shows the same data after converting to ranks, with its regression line (Spearman’s rho).\nNotice how the ranked data (right panel) tends to form a more linear pattern. This is because ranking removes the influence of outliers and transforms any monotonic relationship into a linear one.\nAnother key insight: the slope of the line through the ranked data is the Spearman correlation coefficient, just as the slope of the line through the standardized original data is the Pearson correlation coefficient."
  },
  {
    "objectID": "Week7/2-content.html#non-parametric-tests-just-ranked-versions-of-parametric-tests",
    "href": "Week7/2-content.html#non-parametric-tests-just-ranked-versions-of-parametric-tests",
    "title": "ANOVA and Extended Applications of Linear Models",
    "section": "",
    "text": "For many common “non-parametric” tests, we can simplify by thinking of them as the parametric equivalent applied to ranks:\n\n\n\n\n\n\n\n\n\nParametric Test\nNon-parametric Equivalent\nTransformation\n\n\n\nPearson correlation\nSpearman correlation\nRank both variables\n\n\nOne-sample t-test\nWilcoxon signed-rank test\nSigned rank of values\n\n\nIndependent t-test\nMann-Whitney U test\nRank all values\n\n\nPaired t-test\nWilcoxon matched pairs\nSigned rank of differences\n\n\nOne-way ANOVA\nKruskal-Wallis test\nRank all values\n\n\n\n\n\nThis unified perspective demystifies “non-parametric” statistics:\n\nThey’re not completely different tests but transformations of familiar ones\nRanking reduces the influence of outliers and nonlinearity\nThey’re not “assumption-free” but rather make different assumptions\nUnderstanding them as ranked versions of parametric tests makes them easier to grasp\n\n\nThis table summarizes one of the key insights from our exploration: many “non-parametric” tests can be understood as simple transformations of familiar parametric tests.\nFor each common parametric test, there’s a corresponding “non-parametric” version that’s essentially the same test applied to ranked data:\n\nSpearman correlation is Pearson correlation on ranked variables\nWilcoxon signed-rank test is a one-sample t-test on signed ranks\nMann-Whitney U test is an independent t-test on ranks\nWilcoxon matched pairs test is a paired t-test on signed rank differences\nKruskal-Wallis test is a one-way ANOVA on ranks\n\nThis perspective offers several benefits: - It demystifies “non-parametric” statistics, making them more accessible - It shows how ranking can make tests more robust to outliers and non-normality - It clarifies that “non-parametric” tests aren’t assumption-free, but make different assumptions - It reduces the number of distinct procedures students need to learn"
  },
  {
    "objectID": "Week7/2-content.html#integrated-example-hr-analytics-with-anova",
    "href": "Week7/2-content.html#integrated-example-hr-analytics-with-anova",
    "title": "ANOVA and Extended Applications of Linear Models",
    "section": "",
    "text": "Let’s return to our HR dataset and use ANOVA to compare job satisfaction across job roles:\n\nCode# Load HR Analytics dataset if not already loaded\nif (!exists(\"hr_data\")) {\n  hr_data &lt;- read_sav(\"data/dataset-abc-insurance-hr-data.sav\") |&gt; janitor::clean_names()\n}\n\n# Run ANOVA for job satisfaction by department\nhr_anova &lt;- lm(job_satisfaction ~ job_role, data = hr_data)\nsummary(hr_anova)\n\n\nCall:\nlm(formula = job_satisfaction ~ job_role, data = hr_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.67123 -0.82659  0.00448  0.83555  2.17341 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.65766    0.06886  38.597  &lt; 2e-16 ***\njob_role     0.16893    0.02155   7.839 1.23e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.103 on 934 degrees of freedom\nMultiple R-squared:  0.06173,   Adjusted R-squared:  0.06073 \nF-statistic: 61.45 on 1 and 934 DF,  p-value: 1.235e-14\n\n\n\nNow let’s apply what we’ve learned to our HR analytics dataset. Here we’re comparing job satisfaction across different job roles using a linear model (which is equivalent to ANOVA).\nThe results show the mean job satisfaction for the reference role (the intercept) and the differences between each other role and the reference role Some departments appear to have significantly higher or lower job satisfaction than others.\nThis is a practical application of ANOVA as a linear model in a human resources context."
  },
  {
    "objectID": "Week7/2-content.html#visualizing-hr-department-differences",
    "href": "Week7/2-content.html#visualizing-hr-department-differences",
    "title": "ANOVA and Extended Applications of Linear Models",
    "section": "",
    "text": "Code# Create a visualization of satisfaction by department\nggplot(hr_data, aes(\n  x = reorder(job_role, job_satisfaction, FUN = mean),\n  y = job_satisfaction\n)) +\n  geom_boxplot(fill = \"lightgreen\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(\n    x = \"Role\", y = \"Job Satisfaction (1-5 scale)\",\n    title = \"Job Satisfaction by Role\"\n  )\n\n\n\n\n\n\n\n\nThis visualization helps us see the differences in job satisfaction across departments. The departments are ordered by their mean job satisfaction, with departments having higher average satisfaction appearing towards the right.\nWe can see variations in both the central tendency (median, indicated by the line in the middle of each box) and the spread of job satisfaction scores within each department.\nThis kind of analysis could help HR identify departments that might need intervention to improve employee satisfaction, or departments with particularly high satisfaction that might serve as models for others."
  },
  {
    "objectID": "Week7/2-content.html#combining-anova-and-regression",
    "href": "Week7/2-content.html#combining-anova-and-regression",
    "title": "ANOVA and Extended Applications of Linear Models",
    "section": "",
    "text": "We can build more complex models that include: - Multiple categorical predictors (multi-way ANOVA) - Continuous predictors alongside categorical ones (ANCOVA) - Interaction terms between predictors\n\nCode# Build a complex model\ncomplex_model &lt;- lm(job_satisfaction ~ job_role + gender + evaluation +\n  job_role:evaluation, data = hr_data)\n\n# View model summary\nanova(complex_model) |&gt;\n  as_tibble() |&gt;\n  kable(digits = 3)\n\n\n\nDf\nSum Sq\nMean Sq\nF value\nPr(&gt;F)\n\n\n\n1\n74.760\n74.760\n81.951\n0.000\n\n\n1\n1.478\n1.478\n1.620\n0.203\n\n\n1\n285.426\n285.426\n312.880\n0.000\n\n\n1\n0.101\n0.101\n0.111\n0.739\n\n\n931\n849.307\n0.912\nNA\nNA\n\n\n\n\n\n\nHere we’ve built a more complex model that includes multiple predictors: department (categorical), gender (categorical), and performance rating (continuous), as well as an interaction between department and performance rating.\nThis model tests whether job satisfaction varies by department, gender, and performance rating, and whether the relationship between performance rating and job satisfaction differs across departments.\nThe ANOVA table shows which effects are statistically significant. This demonstrates how the general linear model framework allows us to build and test complex models that would be difficult to conceptualize using traditional statistical procedures taught in isolation."
  },
  {
    "objectID": "Week7/2-content.html#beyond-the-basics-generalized-linear-models",
    "href": "Week7/2-content.html#beyond-the-basics-generalized-linear-models",
    "title": "ANOVA and Extended Applications of Linear Models",
    "section": "",
    "text": "Linear models can be extended to handle other types of outcomes:\ng(E[Y]) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ...\nWhere g() is a link function:\n\n\nModel Type\nOutcome\nLink Function\nExample\n\n\n\nLinear Model\nContinuous\nIdentity\nLinear regression\n\n\nLogistic Model\nBinary\nLogit\nBinary classification\n\n\nPoisson Model\nCount\nLog\nEvent frequency\n\n\n\nThe general linear model framework extends naturally to handle many different types of outcome variables, not just continuous ones.\n\nWhile we’ve focused on the general linear model (GLM) for continuous outcomes, the framework extends naturally to other types of outcomes through Generalized Linear Models (GLMs).\nThe key innovation in GLMs is the addition of a link function, which transforms the expected value of the outcome. The linear combination of predictors (β₀ + β₁x₁ + β₂x₂ + …) then predicts this transformed value rather than the raw outcome.\nDifferent types of outcomes call for different link functions:\n\nFor continuous outcomes, we use the identity link (no transformation), giving us the standard linear model\nFor binary outcomes (0/1), we use the logit link, giving us logistic regression\nFor count data, we use the log link, giving us Poisson regression\n\nOther common GLMs include:\n\nProbit regression (using the probit link for binary outcomes)\nNegative binomial regression (an alternative to Poisson for overdispersed count data)\nGamma regression (for positive continuous data with variance proportional to the square of the mean)\n\nThis extension to GLMs shows how the same core concepts we’ve explored apply across a wide range of statistical models."
  },
  {
    "objectID": "Week7/2-content.html#practical-code-cheat-sheet",
    "href": "Week7/2-content.html#practical-code-cheat-sheet",
    "title": "ANOVA and Extended Applications of Linear Models",
    "section": "",
    "text": "Code# CORRELATION\ncor.test(x, y)                     # Pearson correlation\nlm(scale(y) ~ scale(x))            # Same as Pearson\ncor.test(x, y, method=\"spearman\")  # Spearman correlation\nlm(rank(y) ~ rank(x))              # Approximates Spearman\n\n# ONE SAMPLE TESTS\nt.test(y, mu=0)                    # One-sample t-test\nlm(y ~ 1)                          # Same as one-sample t-test\nwilcox.test(y, mu=0)               # Wilcoxon signed-rank\nlm(signed_rank(y) ~ 1)             # Approximates Wilcoxon\n\n# TWO SAMPLE TESTS\nt.test(y ~ group)                  # Independent t-test\nlm(y ~ group)                      # Same as independent t-test\nt.test(post, pre, paired=TRUE)     # Paired t-test\nlm(post - pre ~ 1)                 # Same as paired t-test\nwilcox.test(y ~ group)             # Mann-Whitney U\nlm(rank(y) ~ group)                # Approximates Mann-Whitney\n\n# ANOVA & REGRESSION\naov(y ~ group)                     # One-way ANOVA\nlm(y ~ group)                      # Same as one-way ANOVA\naov(y ~ factorA * factorB)         # Two-way ANOVA  \nlm(y ~ factorA * factorB)          # Same as two-way ANOVA\nlm(y ~ group + covariate)          # ANCOVA\nlm(y ~ x1 + x2 + x3)               # Multiple regression\n\n\nThis cheat sheet provides a practical reference that demonstrates the equivalences between traditional statistical tests and their linear model formulations in R code.\n\nThis code cheat sheet provides a quick reference for the equivalences we’ve explored between traditional statistical tests and their linear model formulations in R.\nThe cheat sheet is organized by test type: - Correlation tests (Pearson and Spearman) - One-sample tests (t-test and Wilcoxon signed-rank) - Two-sample tests (independent t-test, paired t-test, Mann-Whitney U) - ANOVA and regression models (one-way ANOVA, two-way ANOVA, ANCOVA, multiple regression)\nFor each traditional test (e.g., t.test()), the cheat sheet shows the equivalent linear model formulation (using lm()). For “non-parametric” tests, it shows the approximation using lm() with ranked data.\nStudents can use this as a reference when transitioning from thinking about statistics as a collection of separate tests to understanding them as variations of the unified linear model framework."
  },
  {
    "objectID": "Week7/2-content.html#the-power-of-the-unified-approach",
    "href": "Week7/2-content.html#the-power-of-the-unified-approach",
    "title": "ANOVA and Extended Applications of Linear Models",
    "section": "",
    "text": "Benefits of viewing statistical tests as linear models:\n\n\n\nConceptual simplicity: Learn one framework instead of many isolated techniques\n\nFlexibility: Easily combine and extend models to suit your research questions\n\nInterpretability: Consistent approach to understanding and communicating results\n\nPracticality: Simplifies implementation in statistical software\n\nExtensibility: Natural pathway to more advanced methods (mixed effects, generalized linear models)\n\n\n\nThe unified linear model approach offers several benefits over the traditional approach of teaching statistical tests as separate, unrelated techniques.\nFirst, it’s conceptually simpler. Instead of learning different formulas and procedures for t-tests, ANOVA, regression, etc., you learn one framework that encompasses all of these.\nSecond, it’s more flexible. You can easily combine different types of predictors and test complex hypotheses within the same framework.\nThird, it provides a consistent approach to interpretation. The coefficients in a linear model always have the same basic interpretation, regardless of whether the model is implementing a t-test, ANOVA, or regression.\nFourth, it’s practical. In R and many other statistical software packages, the linear model (lm() function in R) is the workhorse for a wide range of analyses.\nFinally, it provides a natural pathway to more advanced methods like mixed-effects models and generalized linear models, which extend the linear model framework to handle more complex data structures and non-normal distributions."
  },
  {
    "objectID": "Week7/2-content.html#practical-implications-of-the-glm-framework",
    "href": "Week7/2-content.html#practical-implications-of-the-glm-framework",
    "title": "ANOVA and Extended Applications of Linear Models",
    "section": "",
    "text": "Now that we’ve explored various extensions of the GLM framework, let’s consider the practical implications:\n\n\nData analysis workflow becomes more coherent:\n\nStart with your research question\nIdentify outcome and predictor variables\nSelect appropriate version of the GLM\nInterpret coefficients in a consistent way\n\n\n\nModel extensions become more accessible:\n\nAdd interactions to test context-dependent effects\nInclude covariates to control for confounders\nTransform variables to meet assumptions\nMove to GLMs for non-normal outcomes\n\n\n\n\n\n\n\n\nThese practical implications highlight how the general linear model framework changes not just how we understand statistics conceptually, but also how we approach data analysis in practice.\nWhen facing a new analytical problem, thinking in terms of the linear model helps clarify the essential components: the outcome variable, the predictor variables, and the relationships we’re interested in testing. This approach guides the entire analysis process.\nThe unified framework also makes it much easier to extend and adapt models to fit different research scenarios. For instance: - Adding interaction terms allows us to test whether the effect of one variable depends on the level of another - Including covariates helps control for potential confounding variables - Variable transformations (like ranking for non-parametric tests) can help meet model assumptions - Extending to generalized linear models allows us to analyze non-normal outcomes like binary or count data\nThis flexibility is a major advantage over the traditional “cookbook” approach to statistics, where each test is treated as a separate entity with its own rules and applications.\nBy focusing on the underlying linear model framework, we’re equipping students with a more powerful and adaptable analytical toolkit."
  },
  {
    "objectID": "Week7/2-content.html#concluding-thoughts",
    "href": "Week7/2-content.html#concluding-thoughts",
    "title": "ANOVA and Extended Applications of Linear Models",
    "section": "",
    "text": "Statistical tests are not isolated tools but connected members of the same family\nThe general linear model provides a unified framework for understanding these connections\nThis perspective simplifies learning, application, and interpretation of statistics\nWhen facing a new analytical problem, think in terms of the linear model: what is my outcome? What are my predictors? What relationships am I testing?\n\n\nIn conclusion, the general linear model provides a powerful, unified framework for statistical analysis. By understanding that many common statistical tests are special cases of the linear model, we gain a deeper and more coherent understanding of statistics.\nRather than memorizing different formulas and procedures for different tests, we can focus on understanding the core principles of the linear model and how to apply them to different research questions.\nWhen approaching a new analytical problem, thinking in terms of the linear model helps clarify the essential components: the outcome variable, the predictor variables, and the relationships we’re interested in testing.\nThis approach not only simplifies learning and application but also enables us to build more sophisticated models that better capture the complexity of real-world phenomena."
  },
  {
    "objectID": "Week7/lecture.html#last-week",
    "href": "Week7/lecture.html#last-week",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "Last Week",
    "text": "Last Week\n\nCorrelation\n\nPearson correlation (\\(r\\))\nSpearman correlation (\\(\\rho\\))\nRelationship to linear regression\nLimitations\n\nCorrelation vs. causation\nSimple linear regression\n\nSingle predictor variable\nSum of squared errors\nConfidence intervals\nResiduals\nAssessing model fit\nOutliers and influence"
  },
  {
    "objectID": "Week7/lecture.html#learning-objectives",
    "href": "Week7/lecture.html#learning-objectives",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\n\n\n\nUnderstand the general linear model framework\nRecognize how t-tests, ANOVA, and regression are connected\nApply linear modeling to analyze multivariate data\nInterpret interaction effects in multifactor designs\nGain practical experience with an HR datasets\n\n\n\n\n\n\nLinear models are the foundation of many statistical techniques"
  },
  {
    "objectID": "Week7/lecture.html#focus-unified-statistical-thinking",
    "href": "Week7/lecture.html#focus-unified-statistical-thinking",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "Focus: Unified Statistical Thinking",
    "text": "Focus: Unified Statistical Thinking\n\nMoving beyond isolated statistical techniques\nSeeing connections between t-tests, ANOVA, and regression\nUnderstanding the common mathematical framework\nSimplifying the interpretation of statistical models\n\n\nThis lecture introduces the concept of the general linear model as a unifying framework for various statistical techniques. By understanding this framework, students will gain a deeper appreciation for how different statistical tests are related to each other.\nKey points to emphasize:\n\nThe power of seeing statistics through a unified lens\nHow this approach simplifies understanding and application\nThe practical benefits of this perspective when working with real data"
  },
  {
    "objectID": "Week7/lecture.html#what-we-covered-last-week",
    "href": "Week7/lecture.html#what-we-covered-last-week",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "What We Covered Last Week",
    "text": "What We Covered Last Week\nLast week, we explored the fundamentals of correlation and simple linear regression:\n\n\nKey Topics:\n\nCorrelation measures (Pearson’s r)\nSimple linear regression\nInterpreting slope and intercept\nAssessing model fit (R²)\nTesting significance of relationships\nAssumptions of linear regression\n\n\n\n\n\n\n\n\n\n\n\n\n\nLast week we covered two key topics that form the foundation for today’s lecture:\n\nCorrelation:\n\nA measure of the strength and direction of the linear relationship between two variables\nPearson’s r ranges from -1 (perfect negative correlation) to +1 (perfect positive correlation)\nA correlation of 0 indicates no linear relationship\nWe learned that correlation does not imply causation\n\nSimple Linear Regression:\n\nMoving beyond correlation to model the relationship between variables\nThe regression equation: y = β₀ + β₁x + ε\nβ₀ (intercept): The predicted value of y when x = 0\nβ₁ (slope): The change in y for a one-unit increase in x\nWe can use regression for prediction and understanding relationships\nR² measures the proportion of variance in y explained by the model\n\n\nThese concepts serve as building blocks for today’s topic: the General Linear Model, which extends these ideas to create a unified framework for statistical analysis."
  },
  {
    "objectID": "Week7/lecture.html#correlation-measuring-relationships",
    "href": "Week7/lecture.html#correlation-measuring-relationships",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "Correlation: Measuring Relationships",
    "text": "Correlation: Measuring Relationships\n\n\nPearson’s Correlation Coefficient (r):\n\nMeasures the strength and direction of a linear relationship\nRanges from -1 (perfect negative) to +1 (perfect positive)\nCalculated using standardized variables\nFormula: \\(r = \\frac{\\sum{(x_i - \\bar{x})(y_i - \\bar{y})}}{\\sqrt{\\sum{(x_i - \\bar{x})^2}\\sum{(y_i - \\bar{y})^2}}}\\)\nInterpretation: r = 0.7 means a strong positive relationship\n\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelation is a standardized measure of how two variables change together.\nKey points about correlation:\n\nCorrelation measures both the strength and direction of a linear relationship\nThe correlation coefficient (r) is always between -1 and +1\nThe sign indicates direction (positive or negative relationship)\nThe magnitude indicates strength (closer to 1 or -1 = stronger relationship)\nA correlation of 0 suggests no linear relationship\n\nInterpretation guidelines:\n\n|r| &lt; 0.3: Weak correlation\n0.3 &lt; |r| &lt; 0.7: Moderate correlation\n|r| &gt; 0.7: Strong correlation\n\nImportant limitations:\n\nCorrelation does not imply causation\nCorrelation only detects linear relationships\nCorrelation is sensitive to outliers\nCorrelation doesn’t tell us the slope of the relationship\n\nThese limitations are why we often move from correlation to regression, which provides more information about the relationship between variables."
  },
  {
    "objectID": "Week7/lecture.html#simple-linear-regression-modeling-relationships",
    "href": "Week7/lecture.html#simple-linear-regression-modeling-relationships",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "Simple Linear Regression: Modeling Relationships",
    "text": "Simple Linear Regression: Modeling Relationships\n\n\nThe Simple Linear Regression Model:\n\\[y = \\beta_0 + \\beta_1 x + \\varepsilon\\]\nWhere:\n\n\\(\\beta_0\\) is the intercept (y when x = 0)\n\\(\\beta_1\\) is the slope (change in y per unit of x)\n\\(\\varepsilon\\) is the error term\n\nKey statistics:\n\nR² (coefficient of determination): Proportion of variance explaine\np-value: Tests if the relationship is statistically significant\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimple linear regression extends correlation by modeling the relationship between variables. While correlation tells us about the strength and direction of a relationship, regression gives us an equation to predict one variable from another.\nComponents of the regression model:\n\nIntercept (β₀): The predicted value of y when x = 0\n\nMay not always be meaningful in real-world contexts\nExample: If x = years of experience, β₀ = starting salary with zero experience\n\nSlope (β₁): The change in y for a one-unit increase in x\n\nThe practical effect size of the relationship\nExample: Each additional year of experience increases salary by $3,000\n\nError term (ε): The difference between predicted and actual values\n\nRepresents what our model doesn’t explain\nAssumed to be normally distributed with mean zero\n\n\nEvaluating the model:\n\nR²: The proportion of variance in y explained by the model\n\nRanges from 0 to 1 (sometimes expressed as a percentage)\nExample: R² = 0.75 means the model explains 75% of the variation in y\n\nStatistical significance: Testing whether β₁ is significantly different from zero\n\nIf significant, we have evidence of a relationship between x and y\nReported as a p-value (e.g., p &lt; 0.05)\n\n\nRegression is a powerful tool that forms the foundation for today’s topic: the General Linear Model, which extends these concepts to more complex situations."
  },
  {
    "objectID": "Week7/lecture.html#connecting-to-todays-topic-the-general-linear-model",
    "href": "Week7/lecture.html#connecting-to-todays-topic-the-general-linear-model",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "Connecting to Today’s Topic: The General Linear Model",
    "text": "Connecting to Today’s Topic: The General Linear Model\nToday, we’ll build on these concepts to explore the General Linear Model (GLM), which:\n\nExtends regression to include multiple predictors\nProvides a unified framework for various statistical tests\nShows how t-tests, ANOVA, and regression are related\nAllows us to model complex relationships\nHelps us understand which factors truly matter when controlling for others\n\nMoving from:\n\\[y = \\beta_0 + \\beta_1 x + \\varepsilon\\]\nTo:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n + \\varepsilon\\]\n\nToday’s lecture builds directly on the foundation we established last week with correlation and simple regression. We’re now ready to take the next step by exploring the General Linear Model (GLM).\nThe progression in our learning:\n\nCorrelation: We started by measuring the strength and direction of relationships between pairs of variables.\nSimple Linear Regression: We then moved to modeling these relationships with an equation that allows prediction and deeper understanding of how one variable affects another.\nGeneral Linear Model: Today, we’ll extend this framework to include multiple predictors and show how this unifies many statistical tests under one conceptual umbrella.\n\nKey extensions in the GLM:\n\nMultiple predictors: Real-world outcomes are rarely influenced by just one factor. The GLM allows us to include multiple predictors to better model complex phenomena.\nCategorical predictors: We’ll see how to include categorical variables (like gender, treatment group, etc.) in our models.\nControlling for variables: The GLM allows us to understand the unique effect of each predictor while controlling for other factors.\nUnified framework: Perhaps most importantly, we’ll discover how many statistical tests you’ve already learned (t-tests, ANOVA, etc.) are actually special cases of the GLM.\n\nUnderstanding the GLM will not only simplify your conceptual understanding of statistics but also give you a more powerful and flexible approach to data analysis."
  },
  {
    "objectID": "Week7/lecture.html#key-terms-to-remember",
    "href": "Week7/lecture.html#key-terms-to-remember",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "Key Terms to Remember",
    "text": "Key Terms to Remember\nAs we move forward, keep these key terms in mind:\n\n\nFrom Correlation & Regression:\n\nCorrelation coefficient (r): Measures strength and direction of relationship\nIntercept (β₀): Value of y when x = 0\nSlope (β₁): Change in y per unit change in x\nR²: Proportion of variance explained\nResiduals: Differences between observed and predicted values\n\n\nNew Terms for Today:\n\nMultiple regression: Model with multiple predictors\nGeneral Linear Model (GLM): Unified framework for statistical tests\nPredictor variables: Factors that may explain the outcome\nCategorical predictors: Non-numeric variables (e.g., gender)\nControlling for variables: Isolating the effect of one predictor"
  },
  {
    "objectID": "Week7/lecture.html#any-questions-before-we-begin",
    "href": "Week7/lecture.html#any-questions-before-we-begin",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "Any Questions Before We Begin?",
    "text": "Any Questions Before We Begin?\nLet’s briefly address any questions about last week’s material before moving forward.\n\n\nCommon Questions:\n\nHow do we interpret the slope and intercept in practical terms?\nWhat’s the difference between correlation and causation?\nWhen should we use correlation vs. regression?\nHow do we know if our regression model is good?\nWhat if the relationship isn’t linear?\n\n\n\n\n\n\n\n\n\n\n\n\n\nBefore we move on to new material, let’s address some common questions about correlation and regression.\nHow do we interpret the slope and intercept in practical terms?\n\nThe intercept (β₀) is the expected value of y when x = 0. In practice, this may not always be meaningful if x = 0 is outside our observed range.\nThe slope (β₁) tells us how much y changes for a one-unit increase in x. This is often the most useful part for practical interpretation.\nExample: If predicting salary from years of experience with β₁ = 3000, each additional year of experience is associated with a $3,000 increase in salary.\n\nWhat’s the difference between correlation and causation?\n\nCorrelation simply identifies that two variables change together in a predictable way\nCausation means that changes in one variable directly cause changes in another\nTo establish causation, we typically need controlled experiments or strong causal inference methods\nThe classic example: Ice cream sales and drowning deaths are correlated (both increase in summer), but one doesn’t cause the other\n\nWhen should we use correlation vs. regression?\n\nUse correlation when you simply want to measure the strength and direction of a relationship\nUse regression when you want to:\n\nPredict one variable from another\nUnderstand the effect size (how much y changes when x changes)\nControl for other variables (in multiple regression)\n\n\nHow do we know if our regression model is good?\n\nR² tells us the proportion of variance explained (higher is better)\nStatistical significance (p-value) tells us if the relationship is likely real or due to chance\nExamining residuals helps identify patterns the model missed\nChecking model assumptions confirms our statistical inferences are valid\n\nWhat if the relationship isn’t linear?\n\nBoth correlation and simple linear regression assume a linear relationship\nNon-linear relationships may be missed or underestimated by these methods\nSolutions include:\n\nTransforming variables (e.g., log transformation)\nUsing non-linear regression models\nUsing more flexible modeling approaches\n\n\nThese concepts provide the foundation for today’s topic: the General Linear Model, which extends regression to more complex situations while maintaining a unified framework."
  },
  {
    "objectID": "Week7/lecture.html#from-simple-to-multiple-regression",
    "href": "Week7/lecture.html#from-simple-to-multiple-regression",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "From Simple to Multiple Regression",
    "text": "From Simple to Multiple Regression\nBefore wrapping up our discussion of statistical tests, let’s first build up our understanding of regression from simple to multiple predictor variables."
  },
  {
    "objectID": "Week7/lecture.html#understanding-the-building-blocks",
    "href": "Week7/lecture.html#understanding-the-building-blocks",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "Understanding the Building Blocks",
    "text": "Understanding the Building Blocks\n\n\nThe General Linear Model has two key components:\n\nVariables:\n\nOutcome (y): What we’re trying to understand\nPredictors (x): Factors that might explain the outcome\n\nParameters:\n\nIntercept (β₀): Base value when predictors are 0\nCoefficients (β₁, β₂, etc.): Effects of predictors\nError (ε): What the model doesn’t explain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo understand the General Linear Model, we need to break it down into its building blocks.\nFirst, we have two types of variables:\n\nThe outcome variable (y): This is what we’re trying to understand, explain, or predict. It’s also called the dependent variable, response variable, or target variable. Examples include test scores, blood pressure, customer satisfaction, or income.\nPredictor variables (x): These are the factors that might explain or predict the outcome. They’re also called independent variables, explanatory variables, or features. Examples might be study time, medication type, service quality metrics, or years of education.\n\nNext, we have parameters that describe the relationship between these variables:\n\nThe intercept (β₀): This is the baseline value of y when all predictors are zero. It’s the starting point of our model.\nCoefficients (β₁, β₂, etc.): These tell us how much y changes when the corresponding predictor changes by one unit, holding all other predictors constant. The coefficients quantify the effects of our predictors.\nError term (ε): This represents what our model doesn’t explain - the deviation between our model’s predictions and the actual data. A good model minimizes this error.\n\nThe visualization shows these components:\n\nBlue dots represent the data points (observations)\nThe red line is our model, with the intercept (β₀) as the starting point and the slope (β₁) showing the effect of the predictor\nThe dashed gray lines show the error (ε) for some points - the difference between what the model predicts and the actual values\n\nUnderstanding these components gives us the foundation to see how different statistical tests are variations of the same underlying model."
  },
  {
    "objectID": "Week7/lecture.html#simple-linear-regression-one-predictor",
    "href": "Week7/lecture.html#simple-linear-regression-one-predictor",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "Simple Linear Regression: One Predictor",
    "text": "Simple Linear Regression: One Predictor\nIn simple linear regression, we have one outcome variable and one predictor:\n\n\nKey components:\n\n\\(y\\) is the outcome we want to predict\n\\(\\beta_0\\) is the intercept (value of y when x = 0)\n\\(\\beta_1\\) is the slope (effect of the predictor)\n\\(x_1\\) is the predictor variable\n\\(\\varepsilon\\) is the error term\n\nExample: Predicting salary based on years of experience\n\n\\[y = \\beta_0 + \\beta_1 x_1 + \\varepsilon\\]\n\n\n\n\n\n\n\n\n\n\n\nSimple linear regression is where most students begin their regression journey. It models the relationship between one outcome variable (y) and one predictor variable (x).\nThe model estimates two key parameters:\n\nThe intercept (β₀) represents the predicted value of y when x equals zero\nThe slope (β₁) represents how much y changes when x increases by one unit\n\nIn our example, we’re predicting salary based on years of experience: - Each additional year of experience is associated with approximately $2,500 more in salary - The intercept suggests that someone with zero experience would have a salary around $30,000\nThe blue dots represent individual data points, while the red line shows our model’s prediction. The distance between each point and the line represents the error term (ε) - what our model doesn’t explain.\nSimple linear regression provides a foundation, but in real-world situations, outcomes are typically influenced by multiple factors. That’s where multiple regression comes in."
  },
  {
    "objectID": "Week7/lecture.html#multiple-regression-adding-more-predictors",
    "href": "Week7/lecture.html#multiple-regression-adding-more-predictors",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "Multiple Regression: Adding More Predictors",
    "text": "Multiple Regression: Adding More Predictors\nWhat if multiple factors affect our outcome? Multiple regression extends the model:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon\\]\nKey advantages:\n\nModels real-world complexity\nAccounts for multiple influences\nControls for confounding variables\nImproves prediction accuracy\nAllows comparing relative importance of predictors"
  },
  {
    "objectID": "Week7/lecture.html#extending-to-many-predictors",
    "href": "Week7/lecture.html#extending-to-many-predictors",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "Extending to Many Predictors",
    "text": "Extending to Many Predictors\nThe model can be extended to include any number of predictors:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + ... + \\beta_n x_n + \\varepsilon\\]\n\n\n\n# Build model with multiple predictors\nfull_model &lt;- lm(\n  salarygrade ~ gender + tenure +\n    evaluation + age + job_satisfaction,\n  data = hr_data\n)\n\n\n\n\n\n\n\nPredictor\nEffect on Salary\np-value\n\n\n\n\n(Intercept)\n-0.079\n0.563\n\n\ngenderMale\n0.354\n0.000\n\n\ntenure\n0.103\n0.000\n\n\nevaluation\n0.022\n0.439\n\n\nage\n0.023\n0.000\n\n\njob_satisfaction\n0.177\n0.000\n\n\n\n\n\n\n\nWe can continue extending our multiple regression model to include any number of predictors. The general form remains the same, with each new predictor getting its own coefficient that represents its unique effect on the outcome.\nIn this example, we’re using real HR data to predict salary grade based on multiple factors:\n\nGender (categorical: male/female)\nTenure (years of experience)\nEvaluation (performance rating)\nAge (in years)\nJob satisfaction (rating scale)\n\nThe model output shows:\n\nEach predictor’s coefficient (effect on salary)\nThe statistical significance of each effect (p-value)\n\nThe interpretation of each coefficient is:\n\nGender: Being male is associated with a 5.9 point higher salary grade, holding all else constant\nTenure: Each additional year of experience is associated with a 1.4 point increase in salary grade\nEvaluation: Each additional point in performance rating is associated with a 3.9 point increase in salary grade\nAge: Each additional year of age is associated with a -0.02 point change in salary grade (effectively zero)\nJob satisfaction: Each additional point in job satisfaction is associated with a 0.4 point increase in salary grade\n\nFrom these results, we can see that gender, tenure, and evaluation ratings have the strongest effects on salary, while age appears to have no meaningful impact.\nThis approach allows us to model complex real-world situations where many factors simultaneously influence an outcome. It’s a powerful tool for both prediction and understanding the relative importance of different factors.\nThe multiple regression model we’ve just explored is actually the general form of the General Linear Model (GLM), which we’ll see can represent many different statistical tests."
  },
  {
    "objectID": "Week7/lecture.html#the-statistical-test-dilemma",
    "href": "Week7/lecture.html#the-statistical-test-dilemma",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "The Statistical Test Dilemma",
    "text": "The Statistical Test Dilemma\nIn a typical statistics course, you are likely to learn many different tests:\n\n\nCovered so far:\n\nt-tests (one-sample, independent, paired)\nCorrelation (Pearson, Spearman)\nRegression (simple, multiple)\n\n\n\nANOVA, Analysis of Variance (one-way, two-way)\nChi-square tests\nNon-parametric alternatives\n\n\nWith so many tests, it can feel overwhelming to remember which one to use when!\n\nWhen students learn statistics, they’re often taught different statistical tests as separate, unrelated procedures:\n\nWant to compare one sample to a known value? Use a one-sample t-test.\nComparing two groups? That’s an independent t-test.\nComparing multiple groups? Now you need ANOVA.\nLooking at relationships between continuous variables? Time for correlation or regression.\n\nThis approach creates several problems:\nFirst, it encourages memorization rather than understanding. Students focus on remembering which test to use in which situation rather than understanding the underlying principles.\nSecond, it obscures the connections between different tests, making statistics seem more complex and fragmented than it really is.\nThird, it can lead to confusion about which test to choose, especially in situations that don’t neatly fit the examples covered in class.\nFinally, it makes it harder to transition to more advanced statistical methods because each new technique seems like a completely new concept to learn.\nToday, we’ll explore a different approach: understanding common statistical tests as variations of the same underlying framework - the General Linear Model. This perspective can greatly simplify how we think about statistics and help us see the connections between seemingly different techniques."
  },
  {
    "objectID": "Week7/lecture.html#the-statistical-test-dilemma-1",
    "href": "Week7/lecture.html#the-statistical-test-dilemma-1",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "The Statistical Test Dilemma",
    "text": "The Statistical Test Dilemma\n\nExample decision tree, or flowchart, for selecting an appropriate statistical procedure. McElreath (2020)"
  },
  {
    "objectID": "Week7/lecture.html#challenging-the-traditional-approach",
    "href": "Week7/lecture.html#challenging-the-traditional-approach",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "Challenging the Traditional Approach",
    "text": "Challenging the Traditional Approach\n\n\nTraditional approach:\n\nEach test is taught as a separate technique\nDifferent formulas to memorize\nDifferent assumptions to check\nDifferent procedures to follow\nNo clear connections between tests\n\nResult: Statistics feels like a collection of disconnected tools rather than a coherent framework.\n\n\n\n\n\n\n\n\n\n\n\n\nThe traditional approach to teaching statistics typically presents each test as a separate entity with its own formulas, assumptions, and procedures. This is like presenting a collection of disconnected islands, with no obvious way to navigate between them.\nIn this traditional approach:\n\nStudents learn the one-sample t-test, then move on to the independent t-test, then ANOVA, and so on\nEach test seems to have its own set of rules and formulas to memorize\nThere’s little emphasis on how these tests relate to each other\nThe focus is often on “which test to use when” rather than understanding the underlying principles\n\nThis approach has several drawbacks:\n\nIt emphasizes memorization over conceptual understanding\nIt makes statistics seem more complex than it really is\nIt doesn’t prepare students well for situations that don’t fit neatly into the categories they’ve learned\nIt can make more advanced statistical methods seem disconnected from basic techniques\n\nIn contrast, a unified approach connects all these seemingly different tests through a common framework - the General Linear Model. This makes statistics more coherent and easier to understand, as you’ll see today."
  },
  {
    "objectID": "Week7/lecture.html#a-different-perspective-everything-is-connected",
    "href": "Week7/lecture.html#a-different-perspective-everything-is-connected",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "A Different Perspective: Everything is Connected",
    "text": "A Different Perspective: Everything is Connected\nThe General Linear Model provides a unified framework for statistical analysis.\nUnder this framework:\n\nt-tests are special cases of regression\nCorrelation is related to regression\nNon-parametric tests (e.g. Spearman correlation) are transformations of parametric tests\nANOVA is a special case of regression\n\nThis means there’s less to learn and more to understand!\n\nNow, let’s explore a different perspective: the General Linear Model (GLM) as a unifying framework for statistical analysis.\nThe key insight is that many common statistical tests are actually special cases of the same underlying model. Instead of viewing t-tests, ANOVA, correlation, and regression as completely different techniques, we can understand them as variations of the general linear model.\nFor example:\n\nA t-test is just a regression model with a categorical predictor that has two levels\nANOVA is a regression model with a categorical predictor that has more than two levels\nSimple regression is, well, regression with one continuous predictor\nMultiple regression extends this to multiple predictors\n\nThis unified perspective has several advantages:\n\nIt reduces the conceptual load - instead of learning many different techniques, you learn one framework with variations\nIt highlights the connections between different statistical approaches\nIt makes the transition to more advanced methods more intuitive\nIt focuses on understanding rather than memorizing formulas and procedures\n\nThe hierarchical diagram shows how different statistical tests are related through the general linear model. All these tests are part of the same family, with the GLM as their common ancestor.\nThis perspective was eloquently described by Jonas Kristoffer Lindeløv in his blog post “Common statistical tests are linear models” and is increasingly being adopted in modern statistics education."
  },
  {
    "objectID": "Week7/lecture.html#the-general-linear-model-the-basic-formula",
    "href": "Week7/lecture.html#the-general-linear-model-the-basic-formula",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "The General Linear Model: The Basic Formula",
    "text": "The General Linear Model: The Basic Formula\nThe general linear model can be written as:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n + \\varepsilon\\]\nWhere:\n\n\\(y\\) is the outcome we want to understand\n\\(\\beta_0\\) is the intercept (value of y when all predictors are 0)\n\\(\\beta_1, \\beta_2, etc.\\) are coefficients that tell us the effect of each predictor\n\\(x_1, x_2, etc.\\) are the predictor variables\n\\(\\varepsilon\\) is the error term (what our model doesn’t explain)\n\nThis single formula is the foundation for most statistical tests!\n\nThe general linear model is expressed mathematically with this formula:\ny = β₀ + β₁x₁ + β₂x₂ + … + βₙxₙ + ε\nThis may look like a multiple regression equation - and that’s exactly right. Multiple regression is one implementation of the general linear model, but it’s not the only one.\nLet’s break down the components:\n\ny is our outcome variable - what we’re trying to understand or predict\nβ₀ is the intercept - the value of y when all predictors are zero\nβ₁, β₂, etc. are the coefficients that tell us the effect of each predictor\nx₁, x₂, etc. are our predictor variables\nε is the error term - what our model doesn’t explain\n\nThe beauty of this formula is its flexibility. By making small adjustments to it, we can represent a wide range of statistical tests:\n\nIn a one-sample t-test, we have no predictors, just an intercept to test\nIn an independent t-test, we have one binary predictor\nIn ANOVA, we have categorical predictors with multiple levels\nIn correlation and regression, we have continuous predictors\n\nAll of these tests are just special cases of the same underlying model. This unified perspective can greatly simplify how we think about statistics and help us see the connections between seemingly different techniques.\nThis is why modern statistics education is increasingly moving toward teaching the general linear model as a foundation, with specific tests introduced as special cases of this framework."
  },
  {
    "objectID": "Week7/lecture.html#example-1-one-sample-t-test-as-a-linear-model",
    "href": "Week7/lecture.html#example-1-one-sample-t-test-as-a-linear-model",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "Example 1: One-Sample t-test as a Linear Model",
    "text": "Example 1: One-Sample t-test as a Linear Model\n\n\nOne-sample t-test: Tests if a sample mean differs from a known value.\nAs a linear model: \\[y = \\beta_0 + \\varepsilon\\]\nWhere:\n\n\\(\\beta_0\\) is the sample mean\nThe test examines whether \\(\\beta_0 = \\mu_0\\) (the hypothesized value)\n\n\nExample: Testing if average student test scores (70) differ from the expected value (65)\n\n\n\n\n\n\n\n\n\n\n\nLet’s start with one of the simplest statistical tests: the one-sample t-test.\nA one-sample t-test compares a sample mean to a known value. For example, we might want to test whether the average test score in a class (70 points) is significantly different from the expected score (65 points).\nIn the general linear model framework, this test is incredibly simple. Our model becomes:\ny = β₀ + ε\nHere, β₀ is the intercept, which represents the mean of our sample. The t-test is testing whether this intercept (β₀) equals our hypothesized value (65).\nThe visualization shows: - Blue dots: individual test scores (our data points) - Red line: the sample mean (β₀ in our model) at approximately 70 - Green dashed line: the test value of 65\nThe one-sample t-test is asking: “Is the difference between the red line (our sample mean) and the green line (our test value) statistically significant, or could it be due to random chance?”\nThis is the simplest case of the general linear model - just an intercept and error term. There are no predictor variables (x terms) in the equation.\nIn R, we can perform this test using either the traditional t.test() function or the linear model approach with lm():\n# Traditional approach\nt.test(test_scores, mu = 65)\n\n# Linear model approach\nlm(test_scores ~ 1)  # The '1' gives us just an intercept\nBoth approaches will give us identical t-statistics and p-values, showing that they’re mathematically equivalent."
  },
  {
    "objectID": "Week7/lecture.html#example-2-independent-t-test-as-a-linear-model",
    "href": "Week7/lecture.html#example-2-independent-t-test-as-a-linear-model",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "Example 2: Independent t-test as a Linear Model",
    "text": "Example 2: Independent t-test as a Linear Model\n\n\nIndependent t-test: Compares means between two groups.\nAs a linear model: \\[y = \\beta_0 + \\beta_1 x_1 + \\varepsilon\\]\nWhere:\n\n\\(x_1\\) is a binary (0/1) indicator for group membership\n\\(\\beta_0\\) is the mean for group 0 (reference group)\n\\(\\beta_1\\) is the difference between groups\nWe test whether \\(\\beta_1 = 0\\) (no difference)\n\n\nExample: Comparing male vs. female test scores\n\n\n\n\n\n\n\n\n\n\n\nNow let’s look at how the independent t-test fits into the general linear model framework.\nAn independent t-test compares means between two groups, such as test scores between male and female students. In the traditional approach, we calculate the means of each group, their difference, and determine if this difference is statistically significant.\nIn the general linear model framework, this becomes:\ny = β₀ + β₁x₁ + ε\nWhere:\n\nx₁ is a binary variable indicating group membership (0 for Group A, 1 for Group B)\nβ₀ is the intercept, which represents the mean of Group A (the reference group)\nβ₁ is the coefficient for the group difference, which represents how much higher or lower Group B’s mean is compared to Group A’s\nThe t-test for β₁ tests whether this difference is significantly different from zero\n\nThis approach uses what’s called “dummy coding” or “indicator variables.” Group membership is coded as 0 or 1, and the model estimates the effect of being in Group B compared to Group A.\nIn the visualization:\n\nColored dots: individual scores for each group\nHorizontal lines: group means\nβ₀ (the intercept): Group A’s mean\nβ₁ (the coefficient): the difference between Group B and Group A (about 10 points in this example)\n\nThe t-test for the coefficient β₁ is exactly the same as the traditional independent t-test. They are mathematically equivalent.\nIn R, we can perform this test using either approach:\n# Traditional approach\nt.test(score ~ group, data = group_data, var.equal = TRUE)\n\n# Linear model approach\nlm(score ~ group, data = group_data)\nBoth will give identical t-statistics and p-values for the group difference."
  },
  {
    "objectID": "Week7/lecture.html#dummy-coding-how-categorical-variables-work-in-linear-models",
    "href": "Week7/lecture.html#dummy-coding-how-categorical-variables-work-in-linear-models",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "Dummy Coding: How Categorical Variables Work in Linear Models",
    "text": "Dummy Coding: How Categorical Variables Work in Linear Models\n\n\nDummy coding transforms categorical variables into a format linear models can use:\n\nChoose a reference group (usually the first category)\nCreate 0/1 indicator variables for other groups\nThe model estimates:\n\n\\(\\beta_0\\) = mean of reference group\n\\(\\beta_1, \\beta_2, etc.\\) = differences from reference\n\n\nThis allows us to include categorical predictors in our linear models, extending beyond just continuous variables.\n\n\n\n\n\n\n\n\n\n\n\n\nDummy coding is a key concept that allows us to include categorical variables in our linear models. It’s worth understanding this in detail since it’s central to how tests like the independent t-test and ANOVA work within the linear model framework.\nHere’s how dummy coding works:\n\nFirst, we choose one category as the reference group (typically the first category alphabetically or numerically)\nFor each of the other categories, we create a binary indicator variable (0 or 1)\nThe reference group gets zeros for all these indicator variables\n\nFor example, with three categories A, B, and C:\n\nCategory A is our reference group\nFor Category B, we create a variable B_dummy (1 if in category B, 0 otherwise)\nFor Category C, we create a variable C_dummy (1 if in category C, 0 otherwise)\n\nIn the resulting model:\n\nβ₀ (the intercept) represents the mean of the reference group (A)\nβ₁ represents the difference between category B and the reference\nβ₂ represents the difference between category C and the reference\n\nThis approach allows us to include categorical variables with any number of levels in our linear models. With k categories, we’ll have k-1 dummy variables (one serves as the reference).\nIn the visualization:\n\nEach color represents a different category\nThe dots are individual data points\nThe horizontal lines are the group means\nβ₀ is the mean of the reference group (A)\nβ₁ and β₂ are the differences between the other groups and the reference\n\nStatistical software like R automatically does this dummy coding when you include a categorical variable in a model. When you run lm(y ~ category), R creates these dummy variables behind the scenes.\nThis is why the independent t-test can be represented as a linear model with a binary predictor, and why ANOVA can be represented as a linear model with multiple dummy-coded predictors."
  },
  {
    "objectID": "Week7/lecture.html#example-3-anova-as-a-linear-model",
    "href": "Week7/lecture.html#example-3-anova-as-a-linear-model",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "Example 3: ANOVA as a Linear Model",
    "text": "Example 3: ANOVA as a Linear Model\n\n\nANOVA: Compares means across multiple groups.\nAs a linear model: \\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_k x_k + \\varepsilon\\]\nWhere:\n\n\\(x_1, x_2, etc.\\) are dummy variables for group membership\n\\(\\beta_0\\) is the mean for the reference group\n\\(\\beta_1, \\beta_2, etc.\\) are differences from reference group\nWe test whether any group differences exist\n\nExample: Comparing test scores across different teaching methods\n\n\n\n\n\n\n\n\n\n\n\n\nNow let’s examine how Analysis of Variance (ANOVA) fits into the general linear model framework.\nANOVA is traditionally used to compare means across three or more groups. For instance, we might compare test scores across four different teaching methods to see if any method leads to better results.\nIn the general linear model framework, a one-way ANOVA is formulated as:\ny = β₀ + β₁x₁ + β₂x₂ + … + βₖxₖ + ε\nWhere:\n\nx₁, x₂, etc. are dummy variables for group membership (using the dummy coding we just discussed)\nβ₀ is the intercept, representing the mean of the reference group (Method A in our example)\nβ₁, β₂, etc. represent the differences between each other group and the reference group\nThe overall F-test tests whether any of these differences are significantly different from zero\n\nThis is a direct extension of what we saw with the independent t-test. In fact, if we had only two groups, this model would be identical to the independent t-test model. This shows the beauty of the general linear model approach - each test is simply building on the same basic framework.\nIn the visualization:\n\nThe boxplots show the distribution of scores for each teaching method\nThe blue dots represent individual student scores\nβ₀ represents the mean score for Method A (the reference group)\nβ₁, β₂, and β₃ represent the differences between Methods B, C, D and Method A\nThe overall ANOVA tests whether there are any significant differences among the groups\n\nIn R, we can perform this analysis using either approach:\n# Traditional approach\naov(score ~ group, data = anova_data)\n\n# Linear model approach\nlm(score ~ group, data = anova_data)\nThe F-statistic and p-value from both approaches will be identical, confirming that ANOVA is just a special case of the general linear model.\nOne advantage of the linear model approach is that it gives us not just the overall test of differences (like ANOVA) but also the specific estimates of each group difference, which can be very informative."
  },
  {
    "objectID": "Week7/lecture.html#example-4-multiple-regression-as-a-linear-model",
    "href": "Week7/lecture.html#example-4-multiple-regression-as-a-linear-model",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "Example 4: Multiple Regression as a Linear Model",
    "text": "Example 4: Multiple Regression as a Linear Model\n\n\nMultiple Regression: Predicts an outcome based on multiple predictors.\nAs a linear model: \\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n + \\varepsilon\\]\nWhere:\n\n\\(x_1, x_2, etc.\\) are continuous (or categorical) predictors\n\\(\\beta_0\\) is the intercept\n\\(\\beta_1, \\beta_2, etc.\\) are the effects of each predictor\nWe test whether each \\(\\beta_i ≠ 0\\)\n\n\nExample: Predicting test scores based on study hours, previous grades, and teaching method\n\n\n\n\n\n\n\n\nFinally, let’s look at multiple regression again within the general linear model framework.\nMultiple regression predicts an outcome based on two or more predictors. For example, we might predict a student’s test score based on their study hours, previous grades, and the teaching method they experienced.\nThe general linear model for multiple regression is:\ny = β₀ + β₁x₁ + β₂x₂ + … + βₙxₙ + ε\nWhere: - x₁, x₂, etc. are our predictor variables (can be continuous or categorical) - β₀ is the intercept, representing the expected value of y when all predictors are zero - β₁, β₂, etc. are the coefficients that tell us the effect of each predictor on the outcome - We test whether each coefficient is significantly different from zero\nThis should look familiar - it’s the same general form we’ve been using all along! In fact, this is the full general linear model that we started with. All the other tests we’ve discussed are just special cases of this model:\n\nOne-sample t-test: y = β₀ + ε\nIndependent t-test: y = β₀ + β₁x₁ + ε (where x₁ is a binary group indicator)\nANOVA: y = β₀ + β₁x₁ + β₂x₂ + … + ε (where x₁, x₂, etc. are dummy-coded group indicators)\nMultiple regression: y = β₀ + β₁x₁ + β₂x₂ + … + ε (where x₁, x₂, etc. can be any mix of continuous or categorical predictors)\n\nThe 3D visualization shows how multiple regression works with two continuous predictors: - Each blue dot represents a student’s data (study hours, previous grades, and test score) - The model creates a “plane” in this 3D space that best fits the data points - The plane’s position at y-axis=0 represents β₀ (the intercept) - The plane’s slope in the x₁ direction represents β₁ (effect of study hours) - The plane’s slope in the x₂ direction represents β₂ (effect of previous grades)\nWith more than two predictors, the model creates a “hyperplane” in higher-dimensional space, which we can’t visualize directly but follows the same principles.\nIn R, this is implemented simply as:\nlm(test_score ~ study_hours + previous_grades, data = regression_data)\nThis unified framework makes it easy to build models that mix continuous and categorical predictors, allowing for flexible and powerful statistical analyses."
  },
  {
    "objectID": "Week7/lecture.html#a-unified-approach-to-statistical-tests",
    "href": "Week7/lecture.html#a-unified-approach-to-statistical-tests",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "A Unified Approach to Statistical Tests",
    "text": "A Unified Approach to Statistical Tests\n\n\n\n\n\n\n\n\n\n\nTest\nLinear Model\nWhat’s being tested\n\n\n\n\nOne-sample t-test\ny ~ 1\nIs the intercept equal to a specific value?\n\n\nIndependent t-test\ny ~ group\nIs there a difference between groups?\n\n\nOne-way ANOVA\ny ~ group\nAre there differences between any groups?\n\n\nMultiple regression\ny ~ x1 + x2 + …\nDo the predictors affect the outcome?\n\n\n\n\n\nKey Insight: All these tests are variations of the same underlying model - they just differ in what predictors are included and what questions are being asked about the relationships.\n\nThis table summarizes the unified approach we’ve been discussing. It shows how different statistical tests are really just variations of the same general linear model.\nFor the one-sample t-test:\n\nLinear model: y ~ 1 (just an intercept)\nWe’re testing whether the intercept equals a specific value\n\nFor the independent t-test:\n\nLinear model: y ~ group (a categorical predictor with two levels)\nWe’re testing whether there’s a difference between groups\n\nFor one-way ANOVA:\n\nLinear model: y ~ group (a categorical predictor with multiple levels)\nWe’re testing whether there are differences between any groups\n\nFor multiple regression:\n\nLinear model: y ~ x1 + x2 + … (multiple predictors)\nWe’re testing whether the predictors affect the outcome\n\nThe key insight here is that despite their different names and applications, these tests all use the same underlying model - the general linear model. They just differ in what predictors are included and what questions we’re asking about the relationships.\nThis unified approach has several advantages:\n\nIt reduces the number of distinct concepts you need to learn\nIt helps you see the connections between different statistical techniques\nIt makes it easier to transition to more complex models\nIt focuses on understanding rather than memorization\n\nIn statistical software like R, this unified approach is reflected in how these tests are implemented. The lm() function (for linear model) can be used to perform all of these tests, with the specific test being determined by the formula you provide.\nThis perspective transforms statistics from a collection of seemingly unrelated tests into a coherent framework for understanding relationships in data."
  },
  {
    "objectID": "Week7/lecture.html#practical-applications-hr-analytics",
    "href": "Week7/lecture.html#practical-applications-hr-analytics",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "Practical Applications: HR Analytics",
    "text": "Practical Applications: HR Analytics\nLet’s apply the general linear model to a real HR dataset to answer these questions:\n\nIs the average salary at our company different from the industry standard? (One-sample t-test)\nIs there a gender difference in salaries? (Independent t-test)\nDo salaries differ across job roles? (ANOVA)\nWhat factors predict salary? (Multiple regression)\n\nAll using the same unified framework!\n\nNow that we’ve explored the theory behind the general linear model, let’s apply this unified framework to a real-world example using an HR analytics dataset.\nOur dataset contains information about employees at an insurance company, including demographic information, job roles, salaries, and performance ratings. We’ll use this data to answer four different questions, each corresponding to a different “traditional” statistical test:\n\nIs the average tenure at our company different from the industry standard? This is traditionally a one-sample t-test.\nIs there a gender difference in salaries? This is traditionally an independent t-test.\nDo salaries differ across different job roles? This is traditionally a one-way ANOVA.\nWhat factors predict salary? This is traditionally a multiple regression.\n\nBy answering all these questions within the general linear model framework, we’ll demonstrate how this unified approach simplifies our analysis while providing consistent and interpretable results.\nThis practical application will show how the theoretical concepts we’ve discussed translate into real-world data analysis, and how the different “tests” emerge naturally from the same underlying model."
  },
  {
    "objectID": "Week7/lecture.html#question-1-is-the-average-tenure-different-from-the-standard",
    "href": "Week7/lecture.html#question-1-is-the-average-tenure-different-from-the-standard",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "Question 1: Is the average tenure different from the standard?",
    "text": "Question 1: Is the average tenure different from the standard?\n\n\nQuestion: Is the average number of years (tenure) at our company (5.38) different from the industry standard (5.0)?\nLinear Model: \\(\\text{salary} = \\beta_0 + \\varepsilon\\)\n\n\n# Traditional one-sample t-test\nt.test(hr_data$tenure, mu = 5.0)\n\n\n    One Sample t-test\n\ndata:  hr_data$tenure\nt = 2.8526, df = 935, p-value = 0.004432\nalternative hypothesis: true mean is not equal to 5\n95 percent confidence interval:\n 5.118008 5.638403\nsample estimates:\nmean of x \n 5.378205 \n\n# Same test as linear model\nsummary(lm(tenure - 5.0 ~ 1, data = hr_data))\n\n\nCall:\nlm(formula = tenure - 5 ~ 1, data = hr_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.3782 -3.3782 -0.3782  1.8718 25.6218 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   0.3782     0.1326   2.853  0.00443 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.056 on 935 degrees of freedom\n\n\n\n\nLet’s start by addressing our first question: Is the average tenure at our company different from the industry standard of 5.0 years?\nIn the traditional approach, we would use a one-sample t-test for this question. In the general linear model framework, this is an intercept-only model:\ntenure = β₀ + ε\nWe’re testing whether β₀ (the average tenure) equals 5.0\nFirst, we run a traditional t-test using the t.test() function. The results show that the average tenure is 5.38, and the p-value is 0.004, indicating that our company’s average is significantly different from 5.0 at the conventional alpha level of 0.05.\nNext, we run the same test as a linear model using lm(). The intercept is 5.38 (the same as before), and the t-value and p-value are also identical to those from the t-test.\nThis demonstrates that the one-sample t-test is just a special case of the general linear model - specifically, it’s testing whether the intercept equals a particular value.\nThe advantage of understanding this equivalence is that it provides a unified framework for thinking about statistical tests. Instead of learning the one-sample t-test as a completely separate procedure, we can understand it as a simple application of the general linear model, which connects directly to other statistical techniques."
  },
  {
    "objectID": "Week7/lecture.html#question-2-is-there-a-gender-difference-in-salaries",
    "href": "Week7/lecture.html#question-2-is-there-a-gender-difference-in-salaries",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "Question 2: Is there a gender difference in salaries?",
    "text": "Question 2: Is there a gender difference in salaries?\n\n\nQuestion: Is there a gender difference in salary grades?\nLinear Model: \\(\\text{salary} = \\beta_0 + \\beta_1 \\text{gender} + \\varepsilon\\)\n\n\n# Traditional independent t-test\nt.test(salarygrade ~ gender, data = hr_data, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  salarygrade by gender\nt = -6.1215, df = 934, p-value = 1.363e-09\nalternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n95 percent confidence interval:\n -0.5745942 -0.2956135\nsample estimates:\nmean in group Female   mean in group Male \n            1.906542             2.341646 \n\n# Same test as linear model\nsummary(lm(salarygrade ~ gender, data = hr_data))\n\n\nCall:\nlm(formula = salarygrade ~ gender, data = hr_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.3417 -0.9065 -0.3417  0.6583  3.0935 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.90654    0.04652  40.981  &lt; 2e-16 ***\ngenderMale   0.43510    0.07108   6.122 1.36e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.076 on 934 degrees of freedom\nMultiple R-squared:  0.03857,   Adjusted R-squared:  0.03754 \nF-statistic: 37.47 on 1 and 934 DF,  p-value: 1.363e-09\n\n\n\n\nNow let’s address our second question: Is there a gender difference in salary grades?\nIn the traditional approach, we would use an independent t-test for this question. In the general linear model framework, this is:\nsalary = β₀ + β₁×gender + ε\nwhere gender is coded as 0 for females and 1 for males.\nFirst, we run a traditional independent t-test using the t.test() function. The results show that males have a higher average salary grade (33.2) compared to females (27.3), and this difference is statistically significant (p &lt; 0.001).\nNext, we run the same test as a linear model using lm(). Here: - The intercept (β₀) is 27.3, which is the average salary grade for females (the reference group) - The coefficient for genderMale (β₁) is 5.9, which is the difference between male and female salaries - The t-value and p-value for this coefficient are identical to those from the independent t-test\nThis shows that the independent t-test is just a linear model with a binary predictor. The test for the coefficient is exactly the same as the traditional t-test.\nThe advantage of the linear model approach is that it gives us not just the test of difference but also the estimate of how large that difference is (5.9 salary grade points), which is directly interpretable.\nUnderstanding this equivalence helps us see how the independent t-test connects to other statistical techniques within the general linear model framework."
  },
  {
    "objectID": "Week7/lecture.html#question-3-do-salaries-differ-across-job-roles",
    "href": "Week7/lecture.html#question-3-do-salaries-differ-across-job-roles",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "Question 3: Do salaries differ across job roles?",
    "text": "Question 3: Do salaries differ across job roles?\n\n\nQuestion: Do salary grades differ across job roles?\nLinear Model: \\(\\text{salary} = \\beta_0 + \\beta_1 \\text{role}_1 + \\beta_2 \\text{role}_2 + ... + \\varepsilon\\)\n\n\n# Traditional ANOVA\nsummary(aov(salarygrade ~ job_role, data = hr_data))\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \njob_role      7  996.9  142.41    1032 &lt;2e-16 ***\nResiduals   928  128.1    0.14                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Same test as linear model\nanova(lm(salarygrade ~ job_role, data = hr_data))\n\nAnalysis of Variance Table\n\nResponse: salarygrade\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \njob_role    7 996.86 142.408    1032 &lt; 2.2e-16 ***\nResiduals 928 128.06   0.138                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nNext, let’s examine our third question: Do salary grades differ across different job roles?\nIn the traditional approach, we would use a one-way ANOVA for this question. In the general linear model framework, this is:\nsalary = β₀ + β₁×role₁ + β₂×role₂ + … + ε\nwhere each role variable is a dummy indicator for a particular job role.\nFirst, we run a traditional ANOVA using the aov() function. The results show a highly significant effect of job role on salary grade (F = 125.9, p &lt; 0.001).\nThen, we run the same test as a linear model using lm() and obtain the ANOVA table using the anova() function. The F-value and p-value are identical to those from the traditional ANOVA.\nThis demonstrates that one-way ANOVA is just a linear model with a categorical predictor that has multiple levels. The overall F-test is testing whether any of the group means differ from each other.\nThe advantage of the linear model approach is that we can easily extract the specific differences between job roles (not shown in this output but available through the coefficients of the model), which tells us not just that there are differences, but exactly what those differences are.\nUnderstanding this equivalence helps us see how ANOVA is connected to other statistical techniques within the general linear model framework, and provides a more complete understanding of our data."
  },
  {
    "objectID": "Week7/lecture.html#question-4-what-factors-predict-salary",
    "href": "Week7/lecture.html#question-4-what-factors-predict-salary",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "Question 4: What factors predict salary?",
    "text": "Question 4: What factors predict salary?\nQuestion: What factors predict salary grades?\nLinear Model: \\(\\text{salary} = \\beta_0 + \\beta_1 \\text{gender} + \\beta_2 \\text{experience} + \\beta_3 \\text{performance} + \\varepsilon\\)\n\n# Multiple regression model\nsalary_model &lt;- lm(salarygrade ~ gender + tenure + evaluation,\n  data = hr_data\n)\nsummary(salary_model)\n\n\nCall:\nlm(formula = salarygrade ~ gender + tenure + evaluation, data = hr_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0857 -0.6864 -0.1031  0.6190  3.0612 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.846267   0.092849   9.114  &lt; 2e-16 ***\ngenderMale  0.379056   0.059310   6.391  2.6e-10 ***\ntenure      0.138921   0.007345  18.913  &lt; 2e-16 ***\nevaluation  0.107371   0.026086   4.116  4.2e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8968 on 932 degrees of freedom\nMultiple R-squared:  0.3337,    Adjusted R-squared:  0.3316 \nF-statistic: 155.6 on 3 and 932 DF,  p-value: &lt; 2.2e-16\n\n\n\nFinally, let’s address our fourth question: What factors predict salary grades?\nHere, we’re building a multiple regression model that includes several predictors: gender, years of experience (tenure), and performance rating (evaluation).\nIn the general linear model framework, this is:\nsalary = β₀ + β₁×gender + β₂×experience + β₃×performance + ε\nThis is a direct extension of the models we’ve been working with, just with more predictors.\nThe results show:\n\nThe intercept (β₀) is 19.85, representing the expected salary grade for a female employee with no experience and no performance rating\nBeing male (β₁) is associated with a 6.07 point increase in salary grade, holding other factors constant\nEach additional year of experience (β₂) is associated with a 1.37 point increase in salary grade\nEach additional point in performance rating (β₃) is associated with a 2.05 point increase in salary grade\nAll of these effects are statistically significant (p &lt; 0.001)\nThe model explains about 50% of the variance in salary grades (R² = 0.503)\n\nThis model allows us to understand the relative importance of different factors in predicting salary. Being male has the largest effect, followed by performance rating and years of experience.\nThe beauty of the general linear model approach is that we can easily add or remove predictors, combine categorical and continuous variables, and interpret the results in a consistent way.\nThese four analyses - traditionally taught as entirely separate techniques - are all special cases of the same general linear model. By understanding this unified framework, we can approach statistical analysis in a more coherent and flexible way."
  },
  {
    "objectID": "Week7/lecture.html#visualizing-multiple-regression-results",
    "href": "Week7/lecture.html#visualizing-multiple-regression-results",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "Visualizing Multiple Regression Results",
    "text": "Visualizing Multiple Regression Results\n\n\nThese visualizations help us better understand the relationships in our multiple regression model.\nThe left panel shows the relationship between years of experience and salary grade, with gender indicated by color. We can observe several patterns:\n\nThere’s a positive relationship between experience and salary for both genders - employees with more experience tend to have higher salaries\nThe lines are roughly parallel, suggesting that the effect of experience on salary is similar for both genders\nThere’s a clear gender gap - the blue line (males) is consistently above the red line (females), indicating that males tend to have higher salaries at the same level of experience\n\nThe right panel shows the relationship between performance rating and salary grade. Again, we see:\n\nA positive relationship - employees with higher performance ratings tend to have higher salaries\nParallel lines, suggesting similar effects of performance on salary for both genders\nThe same gender gap is visible here\n\nThese visualizations complement our regression results. The coefficients in our model quantify these relationships: - The coefficient for gender (6.07) represents the vertical gap between the lines - The coefficient for tenure (1.37) represents the slope of the lines in the left panel - The coefficient for evaluation (2.05) represents the slope of the lines in the right panel\nThe power of the general linear model is that it can capture all these relationships simultaneously in a single model, allowing us to understand how multiple factors jointly affect our outcome of interest."
  },
  {
    "objectID": "Week7/lecture.html#combining-different-types-of-predictors",
    "href": "Week7/lecture.html#combining-different-types-of-predictors",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "Combining Different Types of Predictors",
    "text": "Combining Different Types of Predictors\n\n\nThe general linear model can easily combine:\n\nCategorical predictors (like gender, job role)\nContinuous predictors (like age, experience)\nInteraction terms (when effects depend on each other)\n\nThis flexibility allows us to model complex relationships using the same unified framework.\nFor example, ANCOVA combines ANOVA (categorical predictors) with regression (continuous predictors).\n\n\n\n\n\n\n\n\n\n\n\n\nA major advantage of the general linear model framework is its flexibility to combine different types of predictors in the same model:\n\nCategorical predictors (like gender, job role, or treatment group) are included through dummy coding, as we’ve seen\nContinuous predictors (like age, experience, or test scores) are included directly\nInteraction terms can be added to model situations where the effect of one predictor depends on the level of another\n\nThis flexibility allows us to build models that more accurately reflect the complexity of real-world relationships.\nThe visualization shows an example of combining categorical and continuous predictors in an Analysis of Covariance (ANCOVA) model. Here:\n\nThe three colored lines represent three different groups (categorical predictor)\nThe x-axis represents a continuous predictor\nEach line has its own intercept (representing the group effect)\nThe lines have the same slope (representing the effect of the continuous predictor)\n\nIn this ANCOVA model:\n\nThe categorical predictor tells us that the groups have different baseline levels (Group B &gt; Group C &gt; Group A)\nThe continuous predictor tells us that as x increases, y increases at the same rate for all groups\nThe parallel lines indicate no interaction between the categorical and continuous predictors\n\nIf we wanted to allow for different slopes across groups, we could add an interaction term to our model.\nThe general linear model makes it easy to construct and interpret such complex models by following the same principles we’ve applied to simpler cases.\n\n\nWhy does this unified perspective matter? There are several practical benefits:\n\nSimpler conceptual framework: Instead of learning many different statistical techniques with different formulas and assumptions, you can understand them all as variations of the same underlying model. This reduces cognitive load and makes statistics more accessible.\nConsistent interpretation: When all tests follow the same framework, interpretation becomes more consistent. Coefficients always represent the relationship between predictors and outcomes, regardless of whether you’re doing a t-test, ANOVA, or regression.\nGreater flexibility: Once you understand the general linear model, you can easily combine different types of predictors (categorical and continuous) in the same model, allowing for more nuanced analyses that better reflect the complexity of real-world relationships.\nClearer pathway to advanced methods: The general linear model is the foundation for more advanced statistical techniques like mixed-effects models, generalized linear models, and many others. Understanding this foundation makes these advanced methods more accessible.\nFocus on relationships: Instead of starting with “Which test should I use?”, you can focus on “What relationships am I interested in?” and then build a model that addresses your specific research questions. This shifts the emphasis from procedure to substance.\n\nThis approach won’t just help you with this course - it provides a foundation for understanding statistics that will serve you throughout your academic and professional career.\nAs you continue to develop your statistical skills, thinking in terms of the general linear model will help you make more informed choices about how to analyze your data and interpret your results."
  },
  {
    "objectID": "Week7/lecture.html#summing-up-the-unified-view-of-statistical-tests",
    "href": "Week7/lecture.html#summing-up-the-unified-view-of-statistical-tests",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "Summing Up: The Unified View of Statistical Tests",
    "text": "Summing Up: The Unified View of Statistical Tests\n\n\nMany common statistical tests are special cases of the general linear model\nThe differences lie in the types of predictors and specific hypotheses\nThis unified framework simplifies learning and application\nIt provides a foundation for understanding more advanced methods\nFocus on modelling relationships, not selecting the “right” test\n\n\n\nTo summarize what we’ve covered today:\n\nMany common statistical tests - including t-tests, ANOVA, and regression - are special cases of the general linear model.\nThe differences between these tests lie in the types of predictors they use (none, binary, categorical with multiple levels, or continuous) and the specific hypotheses they test.\nThis unified framework simplifies learning and application of statistics by reducing the number of distinct concepts you need to understand.\nIt provides a solid foundation for understanding more advanced statistical methods, which are often extensions of the general linear model.\nThis approach encourages you to focus on the relationships you want to investigate and the questions you want to answer, rather than worrying about which test to select.\n\nBy understanding this unified framework, you’ve gained a powerful tool for data analysis that will serve you well in this course and beyond.\nIn our upcoming exercise, you’ll have the opportunity to apply these concepts to real data, further solidifying your understanding of the general linear model as a unifying framework for statistical analysis."
  },
  {
    "objectID": "Week7/lecture.html#further-resources",
    "href": "Week7/lecture.html#further-resources",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "Further Resources",
    "text": "Further Resources\nIf you’d like to explore the general linear model further:\n\n“Common statistical tests are linear models” by Jonas Kristoffer Lindeløv\nhttps://lindeloev.github.io/tests-as-linear/\nStatistical Thinking for the 21st Century by Russell A. Poldrack (2019)\nhttps://statsthinking21.github.io/statsthinking21-core-site/\n\n\nIf you’re interested in exploring the general linear model further, here are some excellent resources:\n“Common statistical tests are linear models” by Jonas Kristoffer Lindeløv is a comprehensive online resource that goes into detail about how different statistical tests can be expressed as linear models, with code examples in R.\n“Statistical Thinking for the 21st Century” by Russell A. Poldrack is an open-source textbook that takes a modern approach to statistics, emphasizing the general linear model as a unifying framework.\nAnd of course, our practical exercise will give you hands-on experience applying these concepts to real data, which is the best way to solidify your understanding.\nThe shift toward understanding statistics through the general linear model is gaining momentum in statistics education. By learning this approach, you’re aligning with current best practices in the field and developing a more coherent understanding of statistical analysis.\nRemember that the goal isn’t just to pass a statistics course but to develop a way of thinking about data that will help you answer meaningful questions throughout your academic and professional career."
  },
  {
    "objectID": "Week7/lecture.html#further-reading",
    "href": "Week7/lecture.html#further-reading",
    "title": "Multiple Linear Regression\nand ANOVA as Linear Models",
    "section": "Further Reading",
    "text": "Further Reading\n\nPoldrack, Statistical Thinking, Chapter 10-11\nJonas Kristoffer Lindeløv, Common statistical tests are linear models\nBekes & Kezdi, Data Analysis for Business, Economics, and Policy, Chapter 8-9\nFox, Applied Regression Analysis and Generalized Linear Models\n\n\n\n\n\nMcElreath, Richard. 2020. Statistical Rethinking: A Bayesian Course with Examples in R and Stan. 2nd ed. CRC Texts in Statistical Science. Boca Raton: Taylor and Francis, CRC Press."
  },
  {
    "objectID": "WeekNew/1-content.html",
    "href": "WeekNew/1-content.html",
    "title": "Section 1",
    "section": "",
    "text": "Section 1"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BSSC0021",
    "section": "",
    "text": "Lecture Notes Website\n\n\n\n\nThis is a website for hosting the lecture notes and slides for the 2024-25 Business Statistics and Data Analytics module.\nModule Leader: Dr. Andrew Mitchell\nYou can find the weekly lecture notes and slides by clicking on the links in the menu above. Each week includes a landing page for the week with links to the slides. Continue on to the next page for accompanying extended lecture notes."
  },
  {
    "objectID": "office-hours.html",
    "href": "office-hours.html",
    "title": "Office Hours and Programming Support",
    "section": "",
    "text": "Office Hours and Programming Support\nAndrew hosts office hours and programming support every Monday at 2pm. You can find the Teams link on the course Moodle page.\nPlease stop by and introduce yourself! Come in for any help you need with your programming, to ask questions about the course, or to just chat."
  }
]