[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BSSC0021 Professional Practice 4: Business Statistics and Data Analytics",
    "section": "",
    "text": "Welcome",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>BSSC0021 Professional Practice 4: Business Statistics and Data Analytics</span>"
    ]
  },
  {
    "objectID": "Week3/index-slides.html",
    "href": "Week3/index-slides.html",
    "title": "Week 3",
    "section": "",
    "text": "This week’s lecture:\nYou can view the full slides here:\nor continue on to the next pages for lecture notes.",
    "crumbs": [
      "Week 3"
    ]
  },
  {
    "objectID": "Week3/index-slides.html#this-weeks-lecture",
    "href": "Week3/index-slides.html#this-weeks-lecture",
    "title": "Week 3",
    "section": "",
    "text": "Slides\n\n Download PDF Slides",
    "crumbs": [
      "Week 3"
    ]
  },
  {
    "objectID": "Week3/index-slides.html#suggested-readings",
    "href": "Week3/index-slides.html#suggested-readings",
    "title": "Week 3",
    "section": "Suggested Readings",
    "text": "Suggested Readings\nThis week’s lectures are adapted from Chapter 6 and Chapter 7 of Statistical Thinking by Russell Poldrack.\nAdditional resources for further exploration:\n\nProbability Lecture Notes from the Stat20 course at Berkeley.\nA Student’s Guide to Bayesian Statistics by Ben Lambert\nThe Drunkard’s Walk: How Randomness Rules Our Lives by Leonard Mlodinow\nTen Great Ideas about Chance by Persi Diaconis and Brian Skyrms\n\n\nThese books provide excellent additional resources for understanding probability theory and its applications.",
    "crumbs": [
      "Week 3"
    ]
  },
  {
    "objectID": "Week3/prob-01-intro.html",
    "href": "Week3/prob-01-intro.html",
    "title": "Part 1: Introduction to Probability",
    "section": "",
    "text": "What is Probability Theory?",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Part 1: Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "Week3/prob-01-intro.html#what-is-probability-theory",
    "href": "Week3/prob-01-intro.html#what-is-probability-theory",
    "title": "Part 1: Introduction to Probability",
    "section": "",
    "text": "Branch of mathematics dealing with chance and uncertainty\nFoundation for statistics\nProvides tools to describe uncertain events\nHistorical origins in games of chance\nDeep questions about meaning and interpretation\n\n\nProbability theory is the branch of mathematics that deals with chance and uncertainty. It forms an important part of the foundation for statistics, because it provides us with the mathematical tools to describe uncertain events.\nThe study of probability arose in part due to interest in understanding games of chance, like cards or dice. These games provide useful examples of many statistical concepts, because when we repeat these games the likelihood of different outcomes remains (mostly) the same. However, there are deep questions about the meaning of probability that we will not address here.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Part 1: Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "Week3/prob-01-intro.html#experiment-sample-space-events",
    "href": "Week3/prob-01-intro.html#experiment-sample-space-events",
    "title": "Part 1: Introduction to Probability",
    "section": "Experiment, Sample Space, Events",
    "text": "Experiment, Sample Space, Events\n\n\nAn experiment is any activity that produces or observes an outcome. Examples are flipping a coin, rolling a 6-sided die, or trying a new route to work to see if it’s faster than the old route.\n\nCoin flip: {heads, tails}\nDie roll: {1,2,3,4,5,6}\nTravel time: (0,∞)\n\nThe sample space is the set of possible outcomes for an experiment. We represent these by listing them within a set of squiggly brackets.\nAn event is a subset of the sample space. In principle it could be one or more of possible outcomes in the sample space, but here we will focus primarily on elementary events which consist of exactly one possible outcome.\n\nSubset of sample space\nCan be elementary or compound\nExample: rolling a 4\n\n\n\n\nTo formalize probability theory, we first need to define a few terms:\n\nAn experiment is any activity that produces or observes an outcome. Examples are flipping a coin, rolling a 6-sided die, or trying a new route to work to see if it’s faster than the old route.\nThe sample space is the set of possible outcomes for an experiment. We represent these by listing them within a set of squiggly brackets.\nAn event is a subset of the sample space. In principle it could be one or more of possible outcomes in the sample space, but here we will focus primarily on elementary events which consist of exactly one possible outcome.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Part 1: Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "Week3/prob-01-intro.html#kolmogorovs-axioms",
    "href": "Week3/prob-01-intro.html#kolmogorovs-axioms",
    "title": "Part 1: Introduction to Probability",
    "section": "Kolmogorov’s Axioms",
    "text": "Kolmogorov’s Axioms\nFor events \\({E_1, E_2, ... , E_N}\\) and random variable \\(X\\):\n\n\n\nNon-negativity:\n\\(P(X=E_i) \\ge 0\\)\nNormalization:\n\\(\\sum_{i=1}^N{P(X=E_i)} = 1\\)\nBoundedness:\n\\(P(X=E_i)\\le 1\\)\n\n\n\nImplications:\n\nAll probabilities are between 0 and 1\nTotal probability must sum to 1\nIndividual probabilities ≤ 1\n\n\n\n\n\nThese are the features that a value has to have if it is going to be a probability, which were first defined by the Russian mathematician Andrei Kolmogorov.\nThe summation is interpreted as saying “Take all of the N elementary events, which we have labeled from 1 to N, and add up their probabilities. These must sum to one.”\nThe third point is implied by the previous points; since they must sum to one, and they can’t be negative, then any particular probability cannot exceed one.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Part 1: Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "Week3/prob-02-rules.html",
    "href": "Week3/prob-02-rules.html",
    "title": "Probability Rules and Classical Probability",
    "section": "",
    "text": "Basic Rules",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability Rules and Classical Probability</span>"
    ]
  },
  {
    "objectID": "Week3/prob-02-rules.html#basic-rules",
    "href": "Week3/prob-02-rules.html#basic-rules",
    "title": "Probability Rules and Classical Probability",
    "section": "",
    "text": "Rule of Subtraction:\n\\(P(\\neg A) = 1 - P(A)\\)\nExample: P(not rolling a 1) = \\(1 - \\frac{1}{6} = \\frac{5}{6}\\)\nIntersection Rule (independent events):\n\\(P(A \\cap B) = P(A) * P(B)\\)\nExample: P(six on both rolls) = \\(\\frac{1}{6} * \\frac{1}{6} = \\frac{1}{36}\\)\nAddition Rule:\n\\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)\n\n\nTo understand de Méré’s error, we need to introduce some of the rules of probability theory:\n\nThe rule of subtraction says that the probability of some event A not happening is one minus the probability of the event happening\nFor independent events, we compute the probability of both occurring by multiplying their individual probabilities\nThe addition rule tells us that to obtain the probability of either of two events occurring, we add together the individual probabilities, but then subtract the likelihood of both occurring together",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability Rules and Classical Probability</span>"
    ]
  },
  {
    "objectID": "Week3/prob-02-rules.html#classical-probability",
    "href": "Week3/prob-02-rules.html#classical-probability",
    "title": "Probability Rules and Classical Probability",
    "section": "Classical Probability",
    "text": "Classical Probability\n\n\nKey Principles:\n\nEqual likelihood assumption\nBased on counting outcomes\nNo experiments needed\nCommon in games of chance\n\nBasic Formula:\n\\(P(outcome_i) = \\frac{1}{\\text{number of possible outcomes}}\\)\n\nExamples:\n\nFair coin: P(heads) = 1/2\nFair die: P(6) = 1/6\nTwo dice: P(double-six) = 1/36\n\n\n\n\nClassical probability arose from the study of games of chance such as dice and cards. In this approach, we compute the probability directly based on our knowledge of the situation.\nWe start with the assumption that all of the elementary events in the sample space are equally likely; that is, when you roll a die, each of the possible outcomes ({1,2,3,4,5,6}) is equally likely to occur.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability Rules and Classical Probability</span>"
    ]
  },
  {
    "objectID": "Week3/prob-02-rules.html#de-mérés-problem",
    "href": "Week3/prob-02-rules.html#de-mérés-problem",
    "title": "Probability Rules and Classical Probability",
    "section": "de Méré’s Problem",
    "text": "de Méré’s Problem\n\n\nFrench gambler Chevalier de Méré played two games:\n\nBet on ≥1 six in 4 die rolls\nBet on ≥1 double-six in 24 rolls of two dice\n\nHe thought both had probability \\(\\frac{2}{3}\\) but…\n\nWon money on first bet\nLost money on second bet\n\n\nHis reasoning:\nFor first bet:\n\\(4 * \\frac{1}{6} = \\frac{2}{3}\\)\nFor second bet:\n\\(24 * \\frac{1}{36} = \\frac{2}{3}\\)\n\n\n\nA famous example arose from a problem encountered by a French gambler who went by the name of Chevalier de Méré. de Méré played two different dice games: In the first he bet on the chance of at least one six on four rolls of a six-sided die, while in the second he bet on the chance of at least one double-six on 24 rolls of two dice. He expected to win money on both of these gambles, but he found that while on average he won money on the first gamble, he actually lost money on average when he played the second gamble many times.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability Rules and Classical Probability</span>"
    ]
  },
  {
    "objectID": "Week3/prob-02-rules.html#visualizing-multiple-events",
    "href": "Week3/prob-02-rules.html#visualizing-multiple-events",
    "title": "Probability Rules and Classical Probability",
    "section": "Visualizing Multiple Events",
    "text": "Visualizing Multiple Events\n\n\nMatrix of Outcomes:\n\n\n\n\n\n\n\n\n\n\nKey Points:\n\nRed cells: six on either throw\nTotal red cells: 11\nExplains \\(\\frac{11}{36}\\) probability\nShows de Méré’s error\n\n\n\n\nThis matrix represents all possible combinations of results across two throws, and highlights the cells that involve a six on either the first or second throw. If you count up the cells in red you will see that there are 11 such cells. This shows why the addition rule gives a different answer from de Méré’s; if we were to simply add together the probabilities for the two throws as he did, then we would count (6,6) towards both, when it should really only be counted once.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability Rules and Classical Probability</span>"
    ]
  },
  {
    "objectID": "Week3/prob-02-rules.html#pascals-solution",
    "href": "Week3/prob-02-rules.html#pascals-solution",
    "title": "Probability Rules and Classical Probability",
    "section": "Pascal’s Solution",
    "text": "Pascal’s Solution\n\n\nFirst bet:\n\\(P(\\text{no sixes}) = \\bigg(\\frac{5}{6}\\bigg)^4=0.482\\)\n\\(P(\\text{≥1 six}) = 1 - 0.482 = 0.517\\)\n\nSecond bet:\n\\(P(\\text{no double six}) = \\bigg(\\frac{35}{36}\\bigg)^{24}=0.509\\)\n\\(P(\\text{≥1 double six}) = 1 - 0.509 = 0.491\\)\n\nKey Insights:\n\nEasier to compute complement\nFirst bet: P &gt; 0.5\nSecond bet: P &lt; 0.5\nExplains gambling results\n\n\n\n\nBlaise Pascal used the rules of probability to solve de Méré’s problem. First, he realized that computing the probability of at least one event out of a combination was tricky, whereas computing the probability that something does not occur across several events is relatively easy – it’s just the product of the probabilities of the individual events.\nThe first bet has probability &gt; 0.5, explaining why de Méré made money on this bet on average. The second bet has probability &lt; 0.5, explaining why de Méré lost money on average on this bet.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability Rules and Classical Probability</span>"
    ]
  },
  {
    "objectID": "Week3/prob-03-empirical.html",
    "href": "Week3/prob-03-empirical.html",
    "title": "Determining Probabilities",
    "section": "",
    "text": "Three Approaches",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Determining Probabilities</span>"
    ]
  },
  {
    "objectID": "Week3/prob-03-empirical.html#three-approaches",
    "href": "Week3/prob-03-empirical.html#three-approaches",
    "title": "Determining Probabilities",
    "section": "",
    "text": "Personal Belief\n\nSubjective assessment\nBased on knowledge/experience\nLimited scientific validity\nOften only available approach\n\nEmpirical Frequency\n\nBased on repeated experiments\nLaw of large numbers\nReal-world data collection\n\n\n\n\nClassical Probability\n\nBased on equally likely outcomes\nMathematical approach\nCommon in games of chance\nNo experiments needed\n\n\n\n\n\nNow that we know what a probability is, how do we actually figure out what the probability is for any particular event? There are three main approaches, each with their own strengths and limitations.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Determining Probabilities</span>"
    ]
  },
  {
    "objectID": "Week3/prob-03-empirical.html#personal-belief",
    "href": "Week3/prob-03-empirical.html#personal-belief",
    "title": "Determining Probabilities",
    "section": "Personal Belief",
    "text": "Personal Belief\n\n\nExample Question:\nWhat was the probability that Bernie Sanders would have won the 2016 presidential election if he had been the democratic nominee?\nKey Points:\n\nCan’t run this experiment\nPeople can still estimate based on knowledge\nNot scientifically satisfying\nOften the only available approach\n\n\nOther Examples:\n\nWeather forecasts\nSports predictions\nEconomic forecasts\nPersonal decisions\n\n\n\n\nLet’s say that I asked you what the probability was that Bernie Sanders would have won the 2016 presidential election if he had been the democratic nominee instead of Hilary Clinton? We can’t actually do the experiment to find the outcome. However, most people with knowledge of American politics would be willing to at least offer a guess at the probability of this event. In many cases personal knowledge and/or opinion is the only guide we have determining the probability of an event, but this is not very scientifically satisfying.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Determining Probabilities</span>"
    ]
  },
  {
    "objectID": "Week3/prob-03-empirical.html#empirical-frequency",
    "href": "Week3/prob-03-empirical.html#empirical-frequency",
    "title": "Determining Probabilities",
    "section": "Empirical Frequency",
    "text": "Empirical Frequency\n\n\nSan Francisco Rain Example:\n\nTotal days in 2017: 365\nRainy days: 73\nP(rain in SF) = 73/365 = 0.2\n\nKey Steps:\n\nDefine experiment clearly\nCount occurrences\nDivide by total trials\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnother way to determine the probability of an event is to do the experiment many times and count how often each event happens. From the relative frequency of the different outcomes, we can compute the probability of each outcome. For example, let’s say that we are interested in knowing the probability of rain in San Francisco. We first have to define the experiment — let’s say that we will look at the National Weather Service data for each day in 2017 and determine whether there was any rain at the downtown San Francisco weather station. According to these data, in 2017 there were 73 rainy days. To compute the probability of rain in San Francisco, we simply divide the number of rainy days by the number of days counted (365), giving P(rain in SF in 2017) = 0.2.\nThe graph shows how the empirical probability of rain converges to 0.2 as we accumulate more days of data throughout the year.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Determining Probabilities</span>"
    ]
  },
  {
    "objectID": "Week3/prob-03-empirical.html#law-of-large-numbers",
    "href": "Week3/prob-03-empirical.html#law-of-large-numbers",
    "title": "Determining Probabilities",
    "section": "Law of Large Numbers",
    "text": "Law of Large Numbers\n\n\nCoin Flip Example:\n\nTrue probability of heads = 0.5\nSmall samples vary widely\nMore flips = better estimate\nConverges to true probability\n“Law of small numbers” fallacy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe graph shows how early results from coin flips can be highly variable and unrepresentative of the true value. Even though we know a fair coin has a probability of 0.5 for heads, small samples can give very different results. This demonstrates how small samples can give misleading results.\nThis was referred to as the “law of small numbers” by psychologists Danny Kahneman and Amos Tversky, who showed that people (even trained researchers) often behave as if the law of large numbers applies even to small samples, leading them to draw strong conclusions from insufficient data.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Determining Probabilities</span>"
    ]
  },
  {
    "objectID": "Week3/prob-03-empirical.html#real-world-example-alabama-election",
    "href": "Week3/prob-03-empirical.html#real-world-example-alabama-election",
    "title": "Determining Probabilities",
    "section": "Real-World Example: Alabama Election",
    "text": "Real-World Example: Alabama Election\n\n\n2017 Senate Race:\n\nRoy Moore vs Doug Jones\nEarly results volatile\nFinal outcome different\nSmall sample warning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA real-world example of this was seen in the 2017 special election for the US Senate in Alabama. Early in the evening the vote counts were especially volatile, swinging from a large initial lead for Jones to a long period where Moore had the lead, until finally Jones took the lead to win the race.\nThis demonstrates how small samples can give misleading results. Unfortunately, many people forget this and overinterpret results from small samples.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Determining Probabilities</span>"
    ]
  },
  {
    "objectID": "Week3/prob-04-conditional.html",
    "href": "Week3/prob-04-conditional.html",
    "title": "Conditional Probability and Independence",
    "section": "",
    "text": "What is Conditional Probability?",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conditional Probability and Independence</span>"
    ]
  },
  {
    "objectID": "Week3/prob-04-conditional.html#what-is-conditional-probability",
    "href": "Week3/prob-04-conditional.html#what-is-conditional-probability",
    "title": "Conditional Probability and Independence",
    "section": "",
    "text": "Definition:\n\nProbability of A given B occurred\nWritten as \\(P(A|B)\\)\nUpdates probability based on new information\n\nFormula:\n\\(P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo far we have limited ourselves to simple probabilities - that is, the probability of a single event or combination of events. However, we often wish to determine the probability of some event given that some other event has occurred, which are known as conditional probabilities.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conditional Probability and Independence</span>"
    ]
  },
  {
    "objectID": "Week3/prob-04-conditional.html#nhanes-example-physical-activity",
    "href": "Week3/prob-04-conditional.html#nhanes-example-physical-activity",
    "title": "Conditional Probability and Independence",
    "section": "NHANES Example: Physical Activity",
    "text": "NHANES Example: Physical Activity\n\n\nQuestion:\nWhat is P(diabetes|inactive)?\n\n\n\n\n\ntotal\ninactive\ndiabetes\ndiabetes_given_inactive\n\n\n\n\n5443\n0.454\n0.101\n0.141\n\n\n\n\n\n\nJoint Probabilities:\n\n\n\nJoint probabilities\n\n\nDiabetes\nPhysActive\nn\nprob\n\n\n\n\nNo\nNo\n2123\n0.3900423\n\n\nNo\nYes\n2770\n0.5089105\n\n\nYes\nNo\n349\n0.0641191\n\n\nYes\nYes\n201\n0.0369282\n\n\n\n\n\n\n\n\nWe can compute conditional probabilities directly from data. Let’s say that we are interested in the following question: What is the probability that someone has diabetes, given that they are not physically active? The NHANES dataset includes two variables that address the two parts of this question: Diabetes and PhysActive.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conditional Probability and Independence</span>"
    ]
  },
  {
    "objectID": "Week3/prob-04-conditional.html#independence",
    "href": "Week3/prob-04-conditional.html#independence",
    "title": "Conditional Probability and Independence",
    "section": "Independence",
    "text": "Independence\n\n\nStatistical Independence:\n\\(P(A|B) = P(A)\\)\nKey Points:\n\nB tells us nothing about A\nDifferent from everyday usage\nMust check with data\n\n\nExample: Jefferson State\n\nP(Jeffersonian) = 0.014\nP(Californian) = 0.986\nNot independent!\nMutually exclusive\n\n\n\n\nThe term “independent” has a very specific meaning in statistics, which is somewhat different from the common usage of the term. Statistical independence between two variables means that knowing the value of one variable doesn’t tell us anything about the value of the other.\nFor example, there is currently a move by a small group of California citizens to declare a new independent state called Jefferson. The new states might be politically independent, but they would not be statistically independent, because if we know that a person is Jeffersonian, then we can be sure they are not Californian!",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conditional Probability and Independence</span>"
    ]
  },
  {
    "objectID": "Week3/prob-04-conditional.html#mental-health-and-physical-activity",
    "href": "Week3/prob-04-conditional.html#mental-health-and-physical-activity",
    "title": "Conditional Probability and Independence",
    "section": "Mental Health and Physical Activity",
    "text": "Mental Health and Physical Activity\n\n\nQuestion: Are physical and mental health independent?\nVariables:\n\nPhysActive: physically active?\nDaysMentHlthBad: bad mental health days\nThreshold: &gt;7 days = bad mental health\n\n\n\n\n\n\n\nPhysActive\nBad Mental Health\nGood Mental Health\nTotal\n\n\n\n\nNo\n629\n2510\n3139\n\n\nYes\n471\n3095\n3566\n\n\nTotal\n1100\n5605\n6705\n\n\n\n\n\n\n\n\nLet’s look at another example, using the NHANES data: Are physical health and mental health independent of one another? To determine whether mental health and physical activity are independent, we would compare the simple probability of bad mental health to the conditional probability of bad mental health given that one is physically active.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conditional Probability and Independence</span>"
    ]
  },
  {
    "objectID": "Week3/prob-05-bayes.html",
    "href": "Week3/prob-05-bayes.html",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "",
    "text": "The Basic Formula",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayes' Rule and Learning from Data</span>"
    ]
  },
  {
    "objectID": "Week3/prob-05-bayes.html#the-basic-formula",
    "href": "Week3/prob-05-bayes.html#the-basic-formula",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "",
    "text": "When we know \\(P(A|B)\\) but want \\(P(B|A)\\):\n\\(P(B|A) = \\frac{P(A|B)*P(B)}{P(A)}\\)\nAlternative Form:\n\\(P(B|A) = \\frac{P(A|B)*P(B)}{P(A|B)*P(B) + P(A|\\neg B)*P(\\neg B)}\\)\n\n\nComponents:\n\nPrior: \\(P(B)\\)\nLikelihood: \\(P(A|B)\\)\nMarginal likelihood: \\(P(A)\\)\nPosterior: \\(P(B|A)\\)\n\n\n\n\n\nIn many cases, we know P(A|B) but we really want to know P(B|A). This commonly occurs in medical screening, where we know P(positive test result| disease) but what we want to know is P(disease|positive test result).\nIf we have only two outcomes, we can express Bayes’ rule in a somewhat clearer way, using the sum rule to redefine P(A).",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayes' Rule and Learning from Data</span>"
    ]
  },
  {
    "objectID": "Week3/prob-05-bayes.html#putting-bayes-into-practice",
    "href": "Week3/prob-05-bayes.html#putting-bayes-into-practice",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "Putting Bayes into Practice",
    "text": "Putting Bayes into Practice\n\nConstruction company drug testing\nA major construction company conducts mandatory random drug and alcohol screening using rapid saliva tests. Consider the following scenario:\n\n\nIn the UK construction industry during 2023, the prevalence of substance use affecting workplace safety was estimated at 2.5% of the workforce\nThe rapid saliva test used has a sensitivity (true positive rate) of 85% when conducted according to protocol\nThe specificity (true negative rate) of these tests is 99.2%\n\n\n\nLet’s consider a specific example. Suppose that a worker is selected for a random drug screening. The test result is positive. What is the probability that this worker is actually positive for substances?",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayes' Rule and Learning from Data</span>"
    ]
  },
  {
    "objectID": "Week3/prob-05-bayes.html#putting-bayes-into-practice-1",
    "href": "Week3/prob-05-bayes.html#putting-bayes-into-practice-1",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "Putting Bayes into Practice",
    "text": "Putting Bayes into Practice\n\nConstruction company drug testing\nLet’s consider a specific example. Suppose that a worker is selected for a random drug screening. The test result is positive. What is the probability that this worker is actually positive for substances?\n\nContext: The company’s current policy is immediate suspension without pay following a positive test result, pending a more accurate laboratory confirmation test that takes 48 hours.\n\nMandatory screening\nRapid saliva test\nSafety-critical roles\nImmediate consequences",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayes' Rule and Learning from Data</span>"
    ]
  },
  {
    "objectID": "Week3/prob-05-bayes.html#putting-bayes-into-practice-2",
    "href": "Week3/prob-05-bayes.html#putting-bayes-into-practice-2",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "Putting Bayes into Practice",
    "text": "Putting Bayes into Practice\n\nConstruction company drug testing\nConstruction Site Testing:\n\nSensitivity: P(positive|substance) = 0.85\nSpecificity: P(negative|no substance) = 0.992\nBase rate: P(substance) = 0.025\n\n\nKey Values:\n\nP(S) = 0.025 (prevalence)\nP(P|S) = 0.85 (sensitivity)\nP(P|not S) = 0.008 (1 - specificity)\n\n\n\nA major construction company conducts mandatory random drug and alcohol screening using rapid saliva tests. In the UK construction industry during 2023, the prevalence of substance use affecting workplace safety was estimated at 2.5% of the workforce. The rapid saliva test used has a sensitivity of 85% when conducted according to protocol, and a specificity of 99.2%.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayes' Rule and Learning from Data</span>"
    ]
  },
  {
    "objectID": "Week3/prob-05-bayes.html#lets-work-through-it",
    "href": "Week3/prob-05-bayes.html#lets-work-through-it",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "Let’s Work Through It",
    "text": "Let’s Work Through It\nUsing Bayes’ Theorem, calculate the probability that this worker is actually positive for substances given their positive test result.\n\nP(S) = 0.025 (prevalence)\nP(P|S) = 0.85 (sensitivity)\nP(P|not S) = 0.008 (1 - specificity)\n\n\nA construction worker is randomly selected for testing at the start of their shift. Their saliva test comes back positive. Using Bayes’ Theorem, calculate the probability that this worker is actually positive for substances given their positive test result.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayes' Rule and Learning from Data</span>"
    ]
  },
  {
    "objectID": "Week3/prob-05-bayes.html#solution",
    "href": "Week3/prob-05-bayes.html#solution",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "Solution",
    "text": "Solution\n\n\nCalculate P(substance|positive):\n\n\\[\\begin{align*}\nP(P) &= P(P|S) \\times P(S) + P(P|not S) \\times P(not S) \\\\\n&= (0.85 \\times 0.025) + (0.008 \\times 0.975) \\\\\n&= 0.02125 + 0.0078 \\\\\n&= 0.02905\n\\end{align*}\\]\n\n\n\\[\\begin{align*}\nP(S|P) &= \\frac{P(P|S) \\times P(S)}{P(P)} \\\\\n&= \\frac{0.85 \\times 0.025}{0.02905} \\\\\n&= 0.7314974 \\text{ or } 73.1\\%\n\\end{align*}\\]\n\n\n\nInterpretation:\n\n~73.1% chance true positive\n~26.9% chance false positive\nMuch higher than 2.5% base rate\nStill significant uncertainty\n\n\n\n\n\nUsing Bayes’ Theorem, we find that given a positive test result, there is a 73.1% probability that the worker actually has substances present. This is much higher than the base rate of 2.5%, but still leaves significant uncertainty with a 26.9% false positive rate.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayes' Rule and Learning from Data</span>"
    ]
  },
  {
    "objectID": "Week3/prob-05-bayes.html#discussion-the-real-world-implications",
    "href": "Week3/prob-05-bayes.html#discussion-the-real-world-implications",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "Discussion: The Real-world Implications",
    "text": "Discussion: The Real-world Implications\nThe company’s current policy is immediate suspension without pay following a positive test result.\nWhat do these results mean for this business policy? Is it fair to immediately suspend workers without pay for a positive test result?\n\nThe company’s current policy is immediate suspension without pay following a positive test result, pending a more accurate laboratory confirmation test that takes 48 hours.\nGiven that approximately 26.9% of positive test results may be false positives, an immediate suspension without pay could unfairly penalize innocent workers; however, the high stakes of construction safety and the 73.1% probability of a true positive suggest that temporary removal from safety-critical roles is prudent while awaiting confirmation.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayes' Rule and Learning from Data</span>"
    ]
  },
  {
    "objectID": "Week3/prob-05-bayes.html#learning-from-data",
    "href": "Week3/prob-05-bayes.html#learning-from-data",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "Learning from Data",
    "text": "Learning from Data\n\n\nBayes’ Rule as Learning:\n\\(P(B|A) = \\frac{P(A|B)}{P(A)}*P(B)\\)\nComponents:\n\nPrior belief: \\(P(B)\\)\nEvidence strength: \\(\\frac{P(A|B)}{P(A)}\\)\nUpdated belief: \\(P(B|A)\\)\n\n\nKey Insights:\n\nUpdates prior knowledge\nEvidence can strengthen/weaken\nSystematic way to learn\nCombines knowledge & data\n\n\n\n\nAnother way to think of Bayes’ rule is as a way to update our beliefs on the basis of data. The different parts of Bayes’ rule have specific names, that relate to their role in using Bayes’ rule to update our beliefs.\nThe part on the left tells us how much more or less likely the data A are given B, relative to the overall likelihood of the data, while the part on the right side tells us how likely we thought B was before we knew anything about the data.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayes' Rule and Learning from Data</span>"
    ]
  },
  {
    "objectID": "Week3/prob-05-bayes.html#odds-and-odds-ratios",
    "href": "Week3/prob-05-bayes.html#odds-and-odds-ratios",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "Odds and Odds Ratios",
    "text": "Odds and Odds Ratios\n\n\nConverting to Odds:\n\\(\\text{odds of A} = \\frac{P(A)}{P(\\neg A)}\\)\nExample:\nDrug test odds:\n\nPrior: \\(\\frac{0.025}{0.975} = 0.026\\)\nPosterior: \\(\\frac{0.7314974}{0.2685026} = 2.724\\)\n\n\nOdds Ratio:\n\\(\\frac{\\text{posterior odds}}{\\text{prior odds}} = \\frac{2.724}{0.026} = 106.25\\)\nInterpretation:\n\nOdds increased 105×\nMuch stronger evidence\nShows test’s power\nDespite false positives\n\n\n\n\nWe can convert probabilities into odds which express the relative likelihood of something happening or not. An odds ratio is an example of what we will later call an effect size, which is a way of quantifying how relatively large any particular statistical effect is.\nFirst, remember the rule for computing a conditional probability. We can rearrange this to get the formula to compute the joint probability using the conditional. Using this we can compute the inverse probability.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayes' Rule and Learning from Data</span>"
    ]
  },
  {
    "objectID": "Week3/prob-06-distributions.html",
    "href": "Week3/prob-06-distributions.html",
    "title": "Probability Distributions",
    "section": "",
    "text": "What is a Probability Distribution?",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Week3/prob-06-distributions.html#what-is-a-probability-distribution",
    "href": "Week3/prob-06-distributions.html#what-is-a-probability-distribution",
    "title": "Probability Distributions",
    "section": "",
    "text": "Definition:\n\nDescribes all possible outcomes\nAssigns probability to each\nDifferent types for different data\nMathematical formulation\n\nExamples:\n\nBinomial (success/failure)\nNormal (continuous)\nPoisson (counts)\n\n\n\n\nCode\n# Create example distributions\nx &lt;- seq(-4, 4, length.out = 100)\nnormal_df &lt;- data.frame(\n  x = x,\n  y = dnorm(x),\n  type = \"Normal\"\n)\n\nx &lt;- 0:10\npoisson_df &lt;- data.frame(\n  x = x,\n  y = dpois(x, lambda = 3),\n  type = \"Poisson\"\n)\n\ncolors &lt;- c(\n  \"Normal\" = \"blue\",\n  \"Poisson\" = \"red\"\n)\n\n# Plot distributions\nggplot() +\n  geom_line(data = normal_df, aes(x = x, y = y, color = \"Normal\"), size = 1) +\n  geom_point(\n    data = poisson_df,\n    aes(x = x, y = y, color = \"Poisson\"),\n    size = 1.5\n  ) +\n  labs(\n    title = \"Example Distributions\",\n    x = \"Value\",\n    y = \"Probability\",\n    color = \"Legend\"\n  ) +\n  theme(legend.position = \"top\") +\n  scale_color_manual(values = colors) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nA probability distribution describes the probability of all of the possible outcomes in an experiment. Throughout this section we will encounter a number of these probability distributions, each of which is appropriate to describe different types of data.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Week3/prob-06-distributions.html#the-binomial-distribution",
    "href": "Week3/prob-06-distributions.html#the-binomial-distribution",
    "title": "Probability Distributions",
    "section": "The Binomial Distribution",
    "text": "The Binomial Distribution\n\n\n\nProperties:\n\nIndependent trials\nTwo outcomes\nFixed probability\nOrder doesn’t matter\n\nFormula:\n\\(P(k; n,p) = \\binom{n}{k} p^k(1-p)^{n-k}\\)\nWhere:\n\nk = successes\nn = trials\np = probability per trial\n\nBinomial Coefficient:\n\\(\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\\)\n\n\n\nCode\n# Create binomial distribution plot\nx &lt;- 0:10\nn &lt;- 10\np &lt;- 0.5\nbinom_df &lt;- data.frame(\n  x = x,\n  y = dbinom(x, size = n, prob = p)\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe binomial distribution provides a way to compute the probability of some number of successes out of a number of trials on which there is either success or failure and nothing in between (known as “Bernoulli trials”), given some known probability of success on each trial.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Week3/prob-06-distributions.html#example-steph-currys-free-throws",
    "href": "Week3/prob-06-distributions.html#example-steph-currys-free-throws",
    "title": "Probability Distributions",
    "section": "Example: Steph Curry’s Free Throws",
    "text": "Example: Steph Curry’s Free Throws\n\n\nScenario:\n\nSteph Curry hits 91% of his free throws\nIn a game in Jan, 2018, he hit only 2 out of 4 free throws\nIt seems pretty unlikely that he would hit only 50% of his free throws in a game, but exactly how unlikely is it?\n\n\nCalculation:\n\\(P(2;4,0.91) = \\binom{4}{2} 0.91^2(1-0.91)^{2}\\)\n\\(= 6 * 0.8281 * 0.0081\\)\n\\(= 0.040\\)\n\n\n\nInterpretation:\n\nVery unlikely (4%)\nYet it happened\nRare events do occur\nDon’t overinterpret\n\n\n\n\n\nOn Jan 20 2018, the basketball player Steph Curry hit only 2 out of 4 free throws in a game against the Houston Rockets. We know that Curry’s overall probability of hitting free throws across the entire season was 0.91, so it seems pretty unlikely that he would hit only 50% of his free throws in a game, but exactly how unlikely is it?",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Week3/prob-06-distributions.html#cumulative-distributions",
    "href": "Week3/prob-06-distributions.html#cumulative-distributions",
    "title": "Probability Distributions",
    "section": "Cumulative Distributions",
    "text": "Cumulative Distributions\nOften we want to know not just how likely a specific value is, but how likely it is to find a value that is as extreme or more than a particular value?\nDefinition:\n\nProbability of value ≤ x\nAccumulates probabilities\nOften more useful\nImportant for testing\n\nExample:\n\\(P(k\\le2)= P(k=2) + P(k=1) + P(k=0)\\)\n\nOften we want to know not just how likely a specific value is, but how likely it is to find a value that is as extreme or more than a particular value?",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Week3/prob-06-distributions.html#cumulative-distributions-1",
    "href": "Week3/prob-06-distributions.html#cumulative-distributions-1",
    "title": "Probability Distributions",
    "section": "Cumulative Distributions",
    "text": "Cumulative Distributions\n\n\n\n\nCode\n# curry_df &lt;- tibble(\n#   numSuccesses = seq(0, 4)\n# ) %&gt;%\n#   mutate(\n#     Probability = dbinom(numSuccesses, size = 4, prob = 0.91),\n#     CumulativeProbability = pbinom(numSuccesses, size = 4, prob = 0.91)\n#   )\n# Create data for Curry's free throw distributions\nn_throws &lt;- 4\ncurry_prob &lt;- 0.91\nx &lt;- 0:n_throws\n\ncurry_dist_df &lt;- data.frame(\n  x = x,\n  Simple = dbinom(x, size = n_throws, prob = curry_prob),\n  Cumulative = pbinom(x, size = n_throws, prob = curry_prob)\n)\n\nkable(\n  curry_dist_df,\n  caption = \"Simple and cumulative probability distributions\",\n  digits = 3\n)\n\n\n\nSimple and cumulative probability distributions\n\n\nx\nSimple\nCumulative\n\n\n\n\n0\n0.000\n0.000\n\n\n1\n0.003\n0.003\n\n\n2\n0.040\n0.043\n\n\n3\n0.271\n0.314\n\n\n4\n0.686\n1.000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe binomial distribution is a discrete probability distribution that describes the number of successes in a sequence of independent experiments, each of which has a constant probability of success. In this example, we are looking at the probability of Steph Curry making a certain number of free throws out of 4 attempts, given that his overall success rate is 91%.\nThis visualization shows both the probability of making exactly k free throws (blue bars) and the probability of making k or fewer free throws (red line) for Curry’s specific scenario of 4 attempts with a 91% success rate.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Week3/prob-06-distributions.html#summary",
    "href": "Week3/prob-06-distributions.html#summary",
    "title": "Probability Distributions",
    "section": "Summary",
    "text": "Summary\n\n\nCore Concepts:\n\nProbability measures uncertainty\nThree approaches:\n\nPersonal belief\nEmpirical frequency\nClassical probability\n\nFundamental rules:\n\nAddition\nMultiplication\nSubtraction\n\n\n\nAdvanced Topics:\n\nConditional probability\nIndependence\nBayes’ rule\nProbability distributions\n\nApplications:\n\nMedical screening\nData analysis\nDecision making\nStatistical inference\n\n\n\n\nThese concepts form the foundation for statistical inference, which we will explore in later chapters. Having read this chapter, you should be able to:\n\nDescribe the sample space for a selected random experiment\nCompute relative frequency and empirical probability\nCompute probabilities of single events, complementary events, and unions/intersections\nDescribe the law of large numbers\nUnderstand conditional probability and independence\nUse Bayes’ theorem",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Week3/prob-07-sampling-slides.html",
    "href": "Week3/prob-07-sampling-slides.html",
    "title": "Part 2: Statistical Sampling",
    "section": "",
    "text": "Why Study Sampling?\nMaking inferences about populations from samples",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Part 2: Statistical Sampling</span>"
    ]
  },
  {
    "objectID": "Week3/prob-07-sampling-slides.html#why-study-sampling",
    "href": "Week3/prob-07-sampling-slides.html#why-study-sampling",
    "title": "Part 2: Statistical Sampling",
    "section": "",
    "text": "The Power of Sampling:\nNate Silver’s 2012 Election Prediction:\n\nCorrectly predicted all 50 states\nUsed only 21,000 people\nTo predict 125 million votes\nCombined data from 21 polls\n\n\nKey Insights:\n\nSmall samples can be powerful\nProper methodology is crucial\nCombining data improves accuracy\nStatistical rigor matters\n\n\n\n\nOne of the foundational ideas in statistics is that we can make inferences about an entire population based on a relatively small sample of individuals from that population.\nAnyone living in the United States will be familiar with the concept of sampling from the political polls that have become a central part of our electoral process. In some cases, these polls can be incredibly accurate at predicting the outcomes of elections. The best known example comes from the 2008 and 2012 US Presidential elections, when the pollster Nate Silver correctly predicted electoral outcomes for 49/50 states in 2008 and for all 50 states in 2012.\nSilver did this by combining data from 21 different polls, which vary in the degree to which they tend to lean towards either the Republican or Democratic side. Each of these polls included data from about 1000 likely voters – meaning that Silver was able to almost perfectly predict the pattern of votes of more than 125 million voters using data from only about 21,000 people, along with other knowledge.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Part 2: Statistical Sampling</span>"
    ]
  },
  {
    "objectID": "Week3/prob-07-sampling-slides.html#sampling-fundamentals",
    "href": "Week3/prob-07-sampling-slides.html#sampling-fundamentals",
    "title": "Part 2: Statistical Sampling",
    "section": "Sampling Fundamentals",
    "text": "Sampling Fundamentals\n\n\n\n\nPopulation vs Sample:\n\nPopulation: Entire group of interest\nSample: Subset used for measurement\nGoal: Infer population parameters from sample statistics\n\nRepresentative Sampling:\n\nEqual chance of selection\nAvoid systematic bias\nRandom selection crucial\n\n\n\n\n\n\nTypes of Sampling:\n\nWith replacement: Items can be selected multiple times\nWithout replacement: Items selected only once\nChoice affects probability calculations\n\nKey Terms:\n\nParameter: Population value (usually unknown)\nStatistic: Sample value (our estimate)\nSampling Error: Difference between statistic and parameter\n\n\n\n\n\n\nOur goal in sampling is to determine the value of a statistic for an entire population of interest, using just a small subset of the population. We do this primarily to save time and effort – why go to the trouble of measuring every individual in the population when just a small sample is sufficient to accurately estimate the statistic of interest?\nIn the election example, the population is all registered voters in the region being polled, and the sample is the set of 1000 individuals selected by the polling organization. The way in which we select the sample is critical to ensuring that the sample is representative of the entire population, which is a main goal of statistical sampling.\nIt’s important to also distinguish between two different ways of sampling: with replacement versus without replacement. In sampling with replacement, after a member of the population has been sampled, they are put back into the pool so that they can potentially be sampled again. In sampling without replacement, once a member has been sampled they are not eligible to be sampled again.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Part 2: Statistical Sampling</span>"
    ]
  },
  {
    "objectID": "Week3/prob-07-sampling-slides.html#sampling-error-distribution",
    "href": "Week3/prob-07-sampling-slides.html#sampling-error-distribution",
    "title": "Part 2: Statistical Sampling",
    "section": "Sampling Error & Distribution",
    "text": "Sampling Error & Distribution\n\nConcept\nWhat is Sampling Error?\n\nDifference between sample and population\nVaries across samples\nAffects measurement quality\nCan be quantified",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Part 2: Statistical Sampling</span>"
    ]
  },
  {
    "objectID": "Week3/prob-07-sampling-slides.html#sampling-error-distribution-1",
    "href": "Week3/prob-07-sampling-slides.html#sampling-error-distribution-1",
    "title": "Part 2: Statistical Sampling",
    "section": "Sampling Error & Distribution",
    "text": "Sampling Error & Distribution\n\nConcept\n\n\n\n# Take 5 samples of 50 adults each\nset.seed(123)\nsamples &lt;- map_df(\n  1:5,\n  ~{\n    NHANES_adult |&gt;\n      sample_n(50) |&gt;\n      summarise(\n        mean_height = mean(Height),\n        sd_height = sd(Height)\n      )\n  }\n)\nsamples\n\n# A tibble: 5 × 2\n  mean_height sd_height\n        &lt;dbl&gt;     &lt;dbl&gt;\n1        169.     11.6 \n2        167.      9.13\n3        169.     11.2 \n4        166.      9.62\n5        169.     11.0 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegardless of how representative our sample is, it’s likely that the statistic that we compute from the sample is going to differ at least slightly from the population parameter. We refer to this as sampling error. If we take multiple samples, the value of our statistical estimate will also vary from sample to sample; we refer to this distribution of our statistic across samples as the sampling distribution.\nSampling error is directly related to the quality of our measurement of the population. Clearly we want the estimates obtained from our sample to be as close as possible to the true value of the population parameter. However, even if our statistic is unbiased (that is, we expect it to have the same value as the population parameter), the value for any particular estimate will differ from the population value, and those differences will be greater when the sampling error is greater.\nThe visualization shows how sample means distribute around the true population mean (red line) when we take many samples.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Part 2: Statistical Sampling</span>"
    ]
  },
  {
    "objectID": "Week3/prob-07-sampling-slides.html#standard-error-of-the-mean",
    "href": "Week3/prob-07-sampling-slides.html#standard-error-of-the-mean",
    "title": "Part 2: Statistical Sampling",
    "section": "Standard Error of the Mean",
    "text": "Standard Error of the Mean\n\n\nDefinition:\n\\(SEM = \\frac{\\hat{\\sigma}}{\\sqrt{n}}\\)\nWhere:\n\n\\(\\hat{\\sigma}\\) is estimated standard deviation\n\\(n\\) is sample size\n\nKey Properties:\n\nMeasures sampling distribution variability\nDecreases with larger samples\nIncreases with population variability\n\n\nExample with NHANES:\n\n\nCode\n# Population SEM\npop_sd &lt;- sd(NHANES_adult$Height)\nn &lt;- 50\nsem_theoretical &lt;- pop_sd / sqrt(n)\n\n# Observed SEM from samples\nsem_observed &lt;- sd(samples_large$mean_height)\n\ncat(\"Theoretical SEM:\", round(sem_theoretical, 2), \"\\n\")\n\n\nTheoretical SEM: 1.44 \n\n\nCode\ncat(\"Observed SEM:\", round(sem_observed, 2))\n\n\nObserved SEM: 1.42\n\n\n\n\n\nLater in the course it will become essential to be able to characterize how variable our samples are, in order to make inferences about the sample statistics. For the mean, we do this using a quantity called the standard error of the mean (SEM), which one can think of as the standard deviation of the sampling distribution of the mean.\nThe formula for the standard error of the mean implies that the quality of our measurement involves two quantities: the population variability, and the size of our sample. Because the sample size is the denominator in the formula for SEM, a larger sample size will yield a smaller SEM when holding the population variability constant.\nWe have no control over the population variability, but we do have control over the sample size. Thus, if we wish to improve our sample statistics (by reducing their sampling variability) then we should use larger samples. However, the formula also tells us something very fundamental about statistical sampling – namely, that the utility of larger samples diminishes with the square root of the sample size.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Part 2: Statistical Sampling</span>"
    ]
  },
  {
    "objectID": "Week3/prob-07-sampling-slides.html#sample-size-effects",
    "href": "Week3/prob-07-sampling-slides.html#sample-size-effects",
    "title": "Part 2: Statistical Sampling",
    "section": "Sample Size Effects",
    "text": "Sample Size Effects\n\nTheoryVisualizationCode\n\n\nImpact of Sample Size:\n\nLarger n → Smaller SEM\nRelationship is not linear\nDiminishing returns\nSquare root relationship\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Compare SEM for different sample sizes\nn1 &lt;- 50\nn2 &lt;- 200 # 4 times larger\n\nsem1 &lt;- pop_sd / sqrt(n1)\nsem2 &lt;- pop_sd / sqrt(n2)\n\n# Improvement factor\nimprovement &lt;- sem1 / sem2\ncat(\"Improvement factor:\", round(improvement, 2))\n\n\n\n\n\n\nThe relationship between sample size and standard error is not linear. Doubling the sample size will not double the quality of the statistics; rather, it will improve it by a factor of √2. This has important implications for study design and resource allocation.\nThe visualization shows how the standard error decreases as sample size increases, but with diminishing returns. This means that after a certain point, increasing sample size may not be worth the additional cost and effort.\nThis relationship is fundamental to statistical power, which we will discuss in later sections. Understanding this relationship helps researchers make informed decisions about sample size requirements for their studies.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Part 2: Statistical Sampling</span>"
    ]
  },
  {
    "objectID": "Week3/prob-07-sampling-slides.html#the-central-limit-theorem",
    "href": "Week3/prob-07-sampling-slides.html#the-central-limit-theorem",
    "title": "Part 2: Statistical Sampling",
    "section": "The Central Limit Theorem",
    "text": "The Central Limit Theorem\nKey Points:\n\nAs sample size increases:\n\nSampling distribution becomes normal\nRegardless of population distribution\nMean approaches population mean\nVariance decreases\n\nImplications:\n\nEnables statistical inference\nJustifies normal approximation\nExplains real-world patterns\n\n\nThe Central Limit Theorem tells us that as sample sizes get larger, the sampling distribution of the mean will become normally distributed, even if the data within each sample are not normally distributed. This is a powerful result that allows us to make inferences about population parameters based on sample statistics.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Part 2: Statistical Sampling</span>"
    ]
  },
  {
    "objectID": "Week3/prob-07-sampling-slides.html#the-central-limit-theorem-1",
    "href": "Week3/prob-07-sampling-slides.html#the-central-limit-theorem-1",
    "title": "Part 2: Statistical Sampling",
    "section": "The Central Limit Theorem",
    "text": "The Central Limit Theorem\n\nNormal Distribution:\n\n\n\nBell-shaped curve\nDefined by mean (\\(\\mu\\)) and SD (\\(\\sigma\\))\nSymmetric around mean\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Central Limit Theorem tells us that as sample sizes get larger, the sampling distribution of the mean will become normally distributed, even if the data within each sample are not normally distributed.\nThe normal distribution is described in terms of two parameters: the mean (which you can think of as the location of the peak), and the standard deviation (which specifies the width of the distribution). The bell-like shape of the distribution never changes, only its location and width.\nThe normal distribution is commonly observed in data collected in the real world – and the central limit theorem gives us some insight into why that occurs. For example, the height of any adult depends on a complex mixture of their genetics and experience; even if those individual contributions may not be normally distributed, when we combine them the result is a normal distribution.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Part 2: Statistical Sampling</span>"
    ]
  },
  {
    "objectID": "Week3/prob-07-sampling-slides.html#clt-in-action-nhanes-example",
    "href": "Week3/prob-07-sampling-slides.html#clt-in-action-nhanes-example",
    "title": "Part 2: Statistical Sampling",
    "section": "CLT in Action: NHANES Example",
    "text": "CLT in Action: NHANES Example\n\nOriginal DistributionCode ExampleKey Insights\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Compare skewness\nlibrary(moments)\noriginal_skew &lt;- skewness(NHANES_clean$AlcoholYear)\nsampling_skew &lt;- skewness(samples_alc$mean_alcohol)\n\ncat(\"Original Distribution Skewness:\", round(original_skew, 2), \"\\n\")\ncat(\"Sampling Distribution Skewness:\", round(sampling_skew, 2))\n\n\n\n\n\nOriginal data is highly skewed\nSampling distribution is nearly normal\nCLT works even with:\n\nNon-normal data\nSkewed distributions\nDiscrete values\n\nSample size of 50 is sufficient\n\n\n\n\n\nLet’s work with the variable AlcoholYear from the NHANES dataset, which is highly skewed. This distribution is, for lack of a better word, funky – and definitely not normally distributed.\nNow let’s look at the sampling distribution of the mean for this variable. Despite the clear non-normality of the original data, the sampling distribution is remarkably close to the normal.\nThe Central Limit Theorem is important for statistics because it allows us to safely assume that the sampling distribution of the mean will be normal in most cases. This means that we can take advantage of statistical techniques that assume a normal distribution.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Part 2: Statistical Sampling</span>"
    ]
  },
  {
    "objectID": "Week3/prob-07-sampling-slides.html#summary",
    "href": "Week3/prob-07-sampling-slides.html#summary",
    "title": "Part 2: Statistical Sampling",
    "section": "Summary",
    "text": "Summary\n\n\n\n\nSampling Fundamentals:\n\nPopulation vs Sample\nRepresentative sampling\nWith/without replacement\nSampling error\n\nStandard Error:\n\nMeasures sampling variability\nDecreases with √n\nGuides sample size decisions\nQuantifies precision\n\n\n\n\n\n\nCentral Limit Theorem:\n\nSampling distribution normality\nIndependent of original distribution\nEnables statistical inference\nFoundation for hypothesis testing\n\nApplications:\n\nPolitical polling\nClinical trials\nQuality control\nResearch design\n\n\n\n\n\n\nIn this lecture, we covered: - The fundamentals of statistical sampling and why it works - How to characterize sampling error and the sampling distribution - The standard error of the mean and its relationship with sample size - The Central Limit Theorem and its importance in statistical inference - Real-world applications and examples using the NHANES dataset",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Part 2: Statistical Sampling</span>"
    ]
  }
]