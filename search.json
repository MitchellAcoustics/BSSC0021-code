[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BSSC0021 Professional Practice 4: Business Statistics and Data Analytics",
    "section": "",
    "text": "Welcome",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>BSSC0021 Professional Practice 4: Business Statistics and Data Analytics</span>"
    ]
  },
  {
    "objectID": "Week3/index-slides.html",
    "href": "Week3/index-slides.html",
    "title": "Week 3",
    "section": "",
    "text": "This week’s lecture:\nYou can view the full slides here:\nor continue on to the next pages for lecture notes.",
    "crumbs": [
      "Week 3"
    ]
  },
  {
    "objectID": "Week3/index-slides.html#this-weeks-lecture",
    "href": "Week3/index-slides.html#this-weeks-lecture",
    "title": "Week 3",
    "section": "",
    "text": "Slides\n\n Download PDF Slides",
    "crumbs": [
      "Week 3"
    ]
  },
  {
    "objectID": "Week3/index-slides.html#suggested-readings",
    "href": "Week3/index-slides.html#suggested-readings",
    "title": "Week 3",
    "section": "Suggested Readings",
    "text": "Suggested Readings\nThis week’s lectures are adapted from Chapter 6 and Chapter 7 of Statistical Thinking by Russell Poldrack.\nAdditional resources for further exploration:\n\nProbability Lecture Notes from the Stat20 course at Berkeley.\nA Student’s Guide to Bayesian Statistics by Ben Lambert\nThe Drunkard’s Walk: How Randomness Rules Our Lives by Leonard Mlodinow\nTen Great Ideas about Chance by Persi Diaconis and Brian Skyrms\n\n\nThese books provide excellent additional resources for understanding probability theory and its applications.",
    "crumbs": [
      "Week 3"
    ]
  },
  {
    "objectID": "Week3/prob-01-intro.html",
    "href": "Week3/prob-01-intro.html",
    "title": "Part 1: Introduction to Probability",
    "section": "",
    "text": "What is Probability Theory?",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Part 1: Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "Week3/prob-01-intro.html#what-is-probability-theory",
    "href": "Week3/prob-01-intro.html#what-is-probability-theory",
    "title": "Part 1: Introduction to Probability",
    "section": "",
    "text": "Branch of mathematics dealing with chance and uncertainty\nFoundation for statistics\nProvides tools to describe uncertain events\nHistorical origins in games of chance\nDeep questions about meaning and interpretation\n\n\nProbability theory is the branch of mathematics that deals with chance and uncertainty. It forms an important part of the foundation for statistics, because it provides us with the mathematical tools to describe uncertain events.\nThe study of probability arose in part due to interest in understanding games of chance, like cards or dice. These games provide useful examples of many statistical concepts, because when we repeat these games the likelihood of different outcomes remains (mostly) the same. However, there are deep questions about the meaning of probability that we will not address here.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Part 1: Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "Week3/prob-01-intro.html#experiment-sample-space-events",
    "href": "Week3/prob-01-intro.html#experiment-sample-space-events",
    "title": "Part 1: Introduction to Probability",
    "section": "Experiment, Sample Space, Events",
    "text": "Experiment, Sample Space, Events\n\n\nAn experiment is any activity that produces or observes an outcome. Examples are flipping a coin, rolling a 6-sided die, or trying a new route to work to see if it’s faster than the old route.\n\nCoin flip: {heads, tails}\nDie roll: {1,2,3,4,5,6}\nTravel time: (0,∞)\n\nThe sample space is the set of possible outcomes for an experiment. We represent these by listing them within a set of squiggly brackets.\nAn event is a subset of the sample space. In principle it could be one or more of possible outcomes in the sample space, but here we will focus primarily on elementary events which consist of exactly one possible outcome.\n\nSubset of sample space\nCan be elementary or compound\nExample: rolling a 4\n\n\n\n\nTo formalize probability theory, we first need to define a few terms:\n\nAn experiment is any activity that produces or observes an outcome. Examples are flipping a coin, rolling a 6-sided die, or trying a new route to work to see if it’s faster than the old route.\nThe sample space is the set of possible outcomes for an experiment. We represent these by listing them within a set of squiggly brackets.\nAn event is a subset of the sample space. In principle it could be one or more of possible outcomes in the sample space, but here we will focus primarily on elementary events which consist of exactly one possible outcome.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Part 1: Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "Week3/prob-01-intro.html#kolmogorovs-axioms",
    "href": "Week3/prob-01-intro.html#kolmogorovs-axioms",
    "title": "Part 1: Introduction to Probability",
    "section": "Kolmogorov’s Axioms",
    "text": "Kolmogorov’s Axioms\nFor events \\({E_1, E_2, ... , E_N}\\) and random variable \\(X\\):\n\n\n\nNon-negativity:\n\\(P(X=E_i) \\ge 0\\)\nNormalization:\n\\(\\sum_{i=1}^N{P(X=E_i)} = 1\\)\nBoundedness:\n\\(P(X=E_i)\\le 1\\)\n\n\n\nImplications:\n\nAll probabilities are between 0 and 1\nTotal probability must sum to 1\nIndividual probabilities ≤ 1\n\n\n\n\n\nThese are the features that a value has to have if it is going to be a probability, which were first defined by the Russian mathematician Andrei Kolmogorov.\nThe summation is interpreted as saying “Take all of the N elementary events, which we have labeled from 1 to N, and add up their probabilities. These must sum to one.”\nThe third point is implied by the previous points; since they must sum to one, and they can’t be negative, then any particular probability cannot exceed one.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Part 1: Introduction to Probability</span>"
    ]
  },
  {
    "objectID": "Week3/prob-02-rules.html",
    "href": "Week3/prob-02-rules.html",
    "title": "Probability Rules and Classical Probability",
    "section": "",
    "text": "Basic Rules",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability Rules and Classical Probability</span>"
    ]
  },
  {
    "objectID": "Week3/prob-02-rules.html#basic-rules",
    "href": "Week3/prob-02-rules.html#basic-rules",
    "title": "Probability Rules and Classical Probability",
    "section": "",
    "text": "Rule of Subtraction:\n\\(P(\\neg A) = 1 - P(A)\\)\nExample: P(not rolling a 1) = \\(1 - \\frac{1}{6} = \\frac{5}{6}\\)\nIntersection Rule (independent events):\n\\(P(A \\cap B) = P(A) * P(B)\\)\nExample: P(six on both rolls) = \\(\\frac{1}{6} * \\frac{1}{6} = \\frac{1}{36}\\)\nAddition Rule:\n\\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)\n\n\nTo understand de Méré’s error, we need to introduce some of the rules of probability theory:\n\nThe rule of subtraction says that the probability of some event A not happening is one minus the probability of the event happening\nFor independent events, we compute the probability of both occurring by multiplying their individual probabilities\nThe addition rule tells us that to obtain the probability of either of two events occurring, we add together the individual probabilities, but then subtract the likelihood of both occurring together",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability Rules and Classical Probability</span>"
    ]
  },
  {
    "objectID": "Week3/prob-02-rules.html#classical-probability",
    "href": "Week3/prob-02-rules.html#classical-probability",
    "title": "Probability Rules and Classical Probability",
    "section": "Classical Probability",
    "text": "Classical Probability\n\n\nKey Principles:\n\nEqual likelihood assumption\nBased on counting outcomes\nNo experiments needed\nCommon in games of chance\n\nBasic Formula:\n\\(P(outcome_i) = \\frac{1}{\\text{number of possible outcomes}}\\)\n\nExamples:\n\nFair coin: P(heads) = 1/2\nFair die: P(6) = 1/6\nTwo dice: P(double-six) = 1/36\n\n\n\n\nClassical probability arose from the study of games of chance such as dice and cards. In this approach, we compute the probability directly based on our knowledge of the situation.\nWe start with the assumption that all of the elementary events in the sample space are equally likely; that is, when you roll a die, each of the possible outcomes ({1,2,3,4,5,6}) is equally likely to occur.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability Rules and Classical Probability</span>"
    ]
  },
  {
    "objectID": "Week3/prob-02-rules.html#de-mérés-problem",
    "href": "Week3/prob-02-rules.html#de-mérés-problem",
    "title": "Probability Rules and Classical Probability",
    "section": "de Méré’s Problem",
    "text": "de Méré’s Problem\n\n\nFrench gambler Chevalier de Méré played two games:\n\nBet on ≥1 six in 4 die rolls\nBet on ≥1 double-six in 24 rolls of two dice\n\nHe thought both had probability \\(\\frac{2}{3}\\) but…\n\nWon money on first bet\nLost money on second bet\n\n\nHis reasoning:\nFor first bet:\n\\(4 * \\frac{1}{6} = \\frac{2}{3}\\)\nFor second bet:\n\\(24 * \\frac{1}{36} = \\frac{2}{3}\\)\n\n\n\nA famous example arose from a problem encountered by a French gambler who went by the name of Chevalier de Méré. de Méré played two different dice games: In the first he bet on the chance of at least one six on four rolls of a six-sided die, while in the second he bet on the chance of at least one double-six on 24 rolls of two dice. He expected to win money on both of these gambles, but he found that while on average he won money on the first gamble, he actually lost money on average when he played the second gamble many times.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability Rules and Classical Probability</span>"
    ]
  },
  {
    "objectID": "Week3/prob-02-rules.html#visualizing-multiple-events",
    "href": "Week3/prob-02-rules.html#visualizing-multiple-events",
    "title": "Probability Rules and Classical Probability",
    "section": "Visualizing Multiple Events",
    "text": "Visualizing Multiple Events\n\n\nMatrix of Outcomes:\n\n\n\n\n\n\n\n\n\n\nKey Points:\n\nRed cells: six on either throw\nTotal red cells: 11\nExplains \\(\\frac{11}{36}\\) probability\nShows de Méré’s error\n\n\n\n\nThis matrix represents all possible combinations of results across two throws, and highlights the cells that involve a six on either the first or second throw. If you count up the cells in red you will see that there are 11 such cells. This shows why the addition rule gives a different answer from de Méré’s; if we were to simply add together the probabilities for the two throws as he did, then we would count (6,6) towards both, when it should really only be counted once.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability Rules and Classical Probability</span>"
    ]
  },
  {
    "objectID": "Week3/prob-02-rules.html#pascals-solution",
    "href": "Week3/prob-02-rules.html#pascals-solution",
    "title": "Probability Rules and Classical Probability",
    "section": "Pascal’s Solution",
    "text": "Pascal’s Solution\n\n\nFirst bet:\n\\(P(\\text{no sixes}) = \\bigg(\\frac{5}{6}\\bigg)^4=0.482\\)\n\\(P(\\text{≥1 six}) = 1 - 0.482 = 0.517\\)\n\nSecond bet:\n\\(P(\\text{no double six}) = \\bigg(\\frac{35}{36}\\bigg)^{24}=0.509\\)\n\\(P(\\text{≥1 double six}) = 1 - 0.509 = 0.491\\)\n\nKey Insights:\n\nEasier to compute complement\nFirst bet: P &gt; 0.5\nSecond bet: P &lt; 0.5\nExplains gambling results\n\n\n\n\nBlaise Pascal used the rules of probability to solve de Méré’s problem. First, he realized that computing the probability of at least one event out of a combination was tricky, whereas computing the probability that something does not occur across several events is relatively easy – it’s just the product of the probabilities of the individual events.\nThe first bet has probability &gt; 0.5, explaining why de Méré made money on this bet on average. The second bet has probability &lt; 0.5, explaining why de Méré lost money on average on this bet.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability Rules and Classical Probability</span>"
    ]
  },
  {
    "objectID": "Week3/prob-03-empirical.html",
    "href": "Week3/prob-03-empirical.html",
    "title": "Determining Probabilities",
    "section": "",
    "text": "Three Approaches",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Determining Probabilities</span>"
    ]
  },
  {
    "objectID": "Week3/prob-03-empirical.html#three-approaches",
    "href": "Week3/prob-03-empirical.html#three-approaches",
    "title": "Determining Probabilities",
    "section": "",
    "text": "Personal Belief\n\nSubjective assessment\nBased on knowledge/experience\nLimited scientific validity\nOften only available approach\n\nEmpirical Frequency\n\nBased on repeated experiments\nLaw of large numbers\nReal-world data collection\n\n\n\n\nClassical Probability\n\nBased on equally likely outcomes\nMathematical approach\nCommon in games of chance\nNo experiments needed\n\n\n\n\n\nNow that we know what a probability is, how do we actually figure out what the probability is for any particular event? There are three main approaches, each with their own strengths and limitations.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Determining Probabilities</span>"
    ]
  },
  {
    "objectID": "Week3/prob-03-empirical.html#personal-belief",
    "href": "Week3/prob-03-empirical.html#personal-belief",
    "title": "Determining Probabilities",
    "section": "Personal Belief",
    "text": "Personal Belief\n\n\nExample Question:\nWhat was the probability that Bernie Sanders would have won the 2016 presidential election if he had been the democratic nominee?\nKey Points:\n\nCan’t run this experiment\nPeople can still estimate based on knowledge\nNot scientifically satisfying\nOften the only available approach\n\n\nOther Examples:\n\nWeather forecasts\nSports predictions\nEconomic forecasts\nPersonal decisions\n\n\n\n\nLet’s say that I asked you what the probability was that Bernie Sanders would have won the 2016 presidential election if he had been the democratic nominee instead of Hilary Clinton? We can’t actually do the experiment to find the outcome. However, most people with knowledge of American politics would be willing to at least offer a guess at the probability of this event. In many cases personal knowledge and/or opinion is the only guide we have determining the probability of an event, but this is not very scientifically satisfying.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Determining Probabilities</span>"
    ]
  },
  {
    "objectID": "Week3/prob-03-empirical.html#empirical-frequency",
    "href": "Week3/prob-03-empirical.html#empirical-frequency",
    "title": "Determining Probabilities",
    "section": "Empirical Frequency",
    "text": "Empirical Frequency\n\n\nSan Francisco Rain Example:\n\nTotal days in 2017: 365\nRainy days: 73\nP(rain in SF) = 73/365 = 0.2\n\nKey Steps:\n\nDefine experiment clearly\nCount occurrences\nDivide by total trials\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnother way to determine the probability of an event is to do the experiment many times and count how often each event happens. From the relative frequency of the different outcomes, we can compute the probability of each outcome. For example, let’s say that we are interested in knowing the probability of rain in San Francisco. We first have to define the experiment — let’s say that we will look at the National Weather Service data for each day in 2017 and determine whether there was any rain at the downtown San Francisco weather station. According to these data, in 2017 there were 73 rainy days. To compute the probability of rain in San Francisco, we simply divide the number of rainy days by the number of days counted (365), giving P(rain in SF in 2017) = 0.2.\nThe graph shows how the empirical probability of rain converges to 0.2 as we accumulate more days of data throughout the year.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Determining Probabilities</span>"
    ]
  },
  {
    "objectID": "Week3/prob-03-empirical.html#law-of-large-numbers",
    "href": "Week3/prob-03-empirical.html#law-of-large-numbers",
    "title": "Determining Probabilities",
    "section": "Law of Large Numbers",
    "text": "Law of Large Numbers\n\n\nCoin Flip Example:\n\nTrue probability of heads = 0.5\nSmall samples vary widely\nMore flips = better estimate\nConverges to true probability\n“Law of small numbers” fallacy\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe graph shows how early results from coin flips can be highly variable and unrepresentative of the true value. Even though we know a fair coin has a probability of 0.5 for heads, small samples can give very different results. This demonstrates how small samples can give misleading results.\nThis was referred to as the “law of small numbers” by psychologists Danny Kahneman and Amos Tversky, who showed that people (even trained researchers) often behave as if the law of large numbers applies even to small samples, leading them to draw strong conclusions from insufficient data.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Determining Probabilities</span>"
    ]
  },
  {
    "objectID": "Week3/prob-03-empirical.html#real-world-example-alabama-election",
    "href": "Week3/prob-03-empirical.html#real-world-example-alabama-election",
    "title": "Determining Probabilities",
    "section": "Real-World Example: Alabama Election",
    "text": "Real-World Example: Alabama Election\n\n\n2017 Senate Race:\n\nRoy Moore vs Doug Jones\nEarly results volatile\nFinal outcome different\nSmall sample warning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA real-world example of this was seen in the 2017 special election for the US Senate in Alabama. Early in the evening the vote counts were especially volatile, swinging from a large initial lead for Jones to a long period where Moore had the lead, until finally Jones took the lead to win the race.\nThis demonstrates how small samples can give misleading results. Unfortunately, many people forget this and overinterpret results from small samples.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Determining Probabilities</span>"
    ]
  },
  {
    "objectID": "Week3/prob-04-conditional.html",
    "href": "Week3/prob-04-conditional.html",
    "title": "Conditional Probability and Independence",
    "section": "",
    "text": "What is Conditional Probability?",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conditional Probability and Independence</span>"
    ]
  },
  {
    "objectID": "Week3/prob-04-conditional.html#what-is-conditional-probability",
    "href": "Week3/prob-04-conditional.html#what-is-conditional-probability",
    "title": "Conditional Probability and Independence",
    "section": "",
    "text": "Definition:\n\nProbability of A given B occurred\nWritten as \\(P(A|B)\\)\nUpdates probability based on new information\n\nFormula:\n\\(P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo far we have limited ourselves to simple probabilities - that is, the probability of a single event or combination of events. However, we often wish to determine the probability of some event given that some other event has occurred, which are known as conditional probabilities.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conditional Probability and Independence</span>"
    ]
  },
  {
    "objectID": "Week3/prob-04-conditional.html#nhanes-example-physical-activity",
    "href": "Week3/prob-04-conditional.html#nhanes-example-physical-activity",
    "title": "Conditional Probability and Independence",
    "section": "NHANES Example: Physical Activity",
    "text": "NHANES Example: Physical Activity\n\n\nQuestion:\nWhat is P(diabetes|inactive)?\n\n\n\n\n\ntotal\ninactive\ndiabetes\ndiabetes_given_inactive\n\n\n\n\n5443\n0.454\n0.101\n0.141\n\n\n\n\n\n\nJoint Probabilities:\n\n\n\nJoint probabilities\n\n\nDiabetes\nPhysActive\nn\nprob\n\n\n\n\nNo\nNo\n2123\n0.3900423\n\n\nNo\nYes\n2770\n0.5089105\n\n\nYes\nNo\n349\n0.0641191\n\n\nYes\nYes\n201\n0.0369282\n\n\n\n\n\n\n\n\nWe can compute conditional probabilities directly from data. Let’s say that we are interested in the following question: What is the probability that someone has diabetes, given that they are not physically active? The NHANES dataset includes two variables that address the two parts of this question: Diabetes and PhysActive.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conditional Probability and Independence</span>"
    ]
  },
  {
    "objectID": "Week3/prob-04-conditional.html#independence",
    "href": "Week3/prob-04-conditional.html#independence",
    "title": "Conditional Probability and Independence",
    "section": "Independence",
    "text": "Independence\n\n\nStatistical Independence:\n\\(P(A|B) = P(A)\\)\nKey Points:\n\nB tells us nothing about A\nDifferent from everyday usage\nMust check with data\n\n\nExample: Jefferson State\n\nP(Jeffersonian) = 0.014\nP(Californian) = 0.986\nNot independent!\nMutually exclusive\n\n\n\n\nThe term “independent” has a very specific meaning in statistics, which is somewhat different from the common usage of the term. Statistical independence between two variables means that knowing the value of one variable doesn’t tell us anything about the value of the other.\nFor example, there is currently a move by a small group of California citizens to declare a new independent state called Jefferson. The new states might be politically independent, but they would not be statistically independent, because if we know that a person is Jeffersonian, then we can be sure they are not Californian!",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conditional Probability and Independence</span>"
    ]
  },
  {
    "objectID": "Week3/prob-04-conditional.html#mental-health-and-physical-activity",
    "href": "Week3/prob-04-conditional.html#mental-health-and-physical-activity",
    "title": "Conditional Probability and Independence",
    "section": "Mental Health and Physical Activity",
    "text": "Mental Health and Physical Activity\n\n\nQuestion: Are physical and mental health independent?\nVariables:\n\nPhysActive: physically active?\nDaysMentHlthBad: bad mental health days\nThreshold: &gt;7 days = bad mental health\n\n\n\n\n\n\n\nPhysActive\nBad Mental Health\nGood Mental Health\nTotal\n\n\n\n\nNo\n629\n2510\n3139\n\n\nYes\n471\n3095\n3566\n\n\nTotal\n1100\n5605\n6705\n\n\n\n\n\n\n\n\nLet’s look at another example, using the NHANES data: Are physical health and mental health independent of one another? To determine whether mental health and physical activity are independent, we would compare the simple probability of bad mental health to the conditional probability of bad mental health given that one is physically active.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conditional Probability and Independence</span>"
    ]
  },
  {
    "objectID": "Week3/prob-05-bayes.html",
    "href": "Week3/prob-05-bayes.html",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "",
    "text": "The Basic Formula",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayes' Rule and Learning from Data</span>"
    ]
  },
  {
    "objectID": "Week3/prob-05-bayes.html#the-basic-formula",
    "href": "Week3/prob-05-bayes.html#the-basic-formula",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "",
    "text": "When we know \\(P(A|B)\\) but want \\(P(B|A)\\):\n\\(P(B|A) = \\frac{P(A|B)*P(B)}{P(A)}\\)\nAlternative Form:\n\\(P(B|A) = \\frac{P(A|B)*P(B)}{P(A|B)*P(B) + P(A|\\neg B)*P(\\neg B)}\\)\n\n\nComponents:\n\nPrior: \\(P(B)\\)\nLikelihood: \\(P(A|B)\\)\nMarginal likelihood: \\(P(A)\\)\nPosterior: \\(P(B|A)\\)\n\n\n\n\n\nIn many cases, we know P(A|B) but we really want to know P(B|A). This commonly occurs in medical screening, where we know P(positive test result| disease) but what we want to know is P(disease|positive test result).\nIf we have only two outcomes, we can express Bayes’ rule in a somewhat clearer way, using the sum rule to redefine P(A).",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayes' Rule and Learning from Data</span>"
    ]
  },
  {
    "objectID": "Week3/prob-05-bayes.html#putting-bayes-into-practice",
    "href": "Week3/prob-05-bayes.html#putting-bayes-into-practice",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "Putting Bayes into Practice",
    "text": "Putting Bayes into Practice\n\nConstruction company drug testing\nA major construction company conducts mandatory random drug and alcohol screening using rapid saliva tests. Consider the following scenario:\n\n\nIn the UK construction industry during 2023, the prevalence of substance use affecting workplace safety was estimated at 2.5% of the workforce\nThe rapid saliva test used has a sensitivity (true positive rate) of 85% when conducted according to protocol\nThe specificity (true negative rate) of these tests is 99.2%\n\n\n\nLet’s consider a specific example. Suppose that a worker is selected for a random drug screening. The test result is positive. What is the probability that this worker is actually positive for substances?",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayes' Rule and Learning from Data</span>"
    ]
  },
  {
    "objectID": "Week3/prob-05-bayes.html#putting-bayes-into-practice-1",
    "href": "Week3/prob-05-bayes.html#putting-bayes-into-practice-1",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "Putting Bayes into Practice",
    "text": "Putting Bayes into Practice\n\nConstruction company drug testing\nLet’s consider a specific example. Suppose that a worker is selected for a random drug screening. The test result is positive. What is the probability that this worker is actually positive for substances?\n\nContext: The company’s current policy is immediate suspension without pay following a positive test result, pending a more accurate laboratory confirmation test that takes 48 hours.\n\nMandatory screening\nRapid saliva test\nSafety-critical roles\nImmediate consequences",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayes' Rule and Learning from Data</span>"
    ]
  },
  {
    "objectID": "Week3/prob-05-bayes.html#putting-bayes-into-practice-2",
    "href": "Week3/prob-05-bayes.html#putting-bayes-into-practice-2",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "Putting Bayes into Practice",
    "text": "Putting Bayes into Practice\n\nConstruction company drug testing\nConstruction Site Testing:\n\nSensitivity: P(positive|substance) = 0.85\nSpecificity: P(negative|no substance) = 0.992\nBase rate: P(substance) = 0.025\n\n\nKey Values:\n\nP(S) = 0.025 (prevalence)\nP(P|S) = 0.85 (sensitivity)\nP(P|not S) = 0.008 (1 - specificity)\n\n\n\nA major construction company conducts mandatory random drug and alcohol screening using rapid saliva tests. In the UK construction industry during 2023, the prevalence of substance use affecting workplace safety was estimated at 2.5% of the workforce. The rapid saliva test used has a sensitivity of 85% when conducted according to protocol, and a specificity of 99.2%.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayes' Rule and Learning from Data</span>"
    ]
  },
  {
    "objectID": "Week3/prob-05-bayes.html#lets-work-through-it",
    "href": "Week3/prob-05-bayes.html#lets-work-through-it",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "Let’s Work Through It",
    "text": "Let’s Work Through It\nUsing Bayes’ Theorem, calculate the probability that this worker is actually positive for substances given their positive test result.\n\nP(S) = 0.025 (prevalence)\nP(P|S) = 0.85 (sensitivity)\nP(P|not S) = 0.008 (1 - specificity)\n\n\nA construction worker is randomly selected for testing at the start of their shift. Their saliva test comes back positive. Using Bayes’ Theorem, calculate the probability that this worker is actually positive for substances given their positive test result.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayes' Rule and Learning from Data</span>"
    ]
  },
  {
    "objectID": "Week3/prob-05-bayes.html#solution",
    "href": "Week3/prob-05-bayes.html#solution",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "Solution",
    "text": "Solution\n\n\nCalculate P(substance|positive):\n\n\\[\\begin{align*}\nP(P) &= P(P|S) \\times P(S) + P(P|not S) \\times P(not S) \\\\\n&= (0.85 \\times 0.025) + (0.008 \\times 0.975) \\\\\n&= 0.02125 + 0.0078 \\\\\n&= 0.02905\n\\end{align*}\\]\n\n\n\\[\\begin{align*}\nP(S|P) &= \\frac{P(P|S) \\times P(S)}{P(P)} \\\\\n&= \\frac{0.85 \\times 0.025}{0.02905} \\\\\n&= 0.7314974 \\text{ or } 73.1\\%\n\\end{align*}\\]\n\n\n\nInterpretation:\n\n~73.1% chance true positive\n~26.9% chance false positive\nMuch higher than 2.5% base rate\nStill significant uncertainty\n\n\n\n\n\nUsing Bayes’ Theorem, we find that given a positive test result, there is a 73.1% probability that the worker actually has substances present. This is much higher than the base rate of 2.5%, but still leaves significant uncertainty with a 26.9% false positive rate.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayes' Rule and Learning from Data</span>"
    ]
  },
  {
    "objectID": "Week3/prob-05-bayes.html#discussion-the-real-world-implications",
    "href": "Week3/prob-05-bayes.html#discussion-the-real-world-implications",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "Discussion: The Real-world Implications",
    "text": "Discussion: The Real-world Implications\nThe company’s current policy is immediate suspension without pay following a positive test result.\nWhat do these results mean for this business policy? Is it fair to immediately suspend workers without pay for a positive test result?\n\nThe company’s current policy is immediate suspension without pay following a positive test result, pending a more accurate laboratory confirmation test that takes 48 hours.\nGiven that approximately 26.9% of positive test results may be false positives, an immediate suspension without pay could unfairly penalize innocent workers; however, the high stakes of construction safety and the 73.1% probability of a true positive suggest that temporary removal from safety-critical roles is prudent while awaiting confirmation.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayes' Rule and Learning from Data</span>"
    ]
  },
  {
    "objectID": "Week3/prob-05-bayes.html#learning-from-data",
    "href": "Week3/prob-05-bayes.html#learning-from-data",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "Learning from Data",
    "text": "Learning from Data\n\n\nBayes’ Rule as Learning:\n\\(P(B|A) = \\frac{P(A|B)}{P(A)}*P(B)\\)\nComponents:\n\nPrior belief: \\(P(B)\\)\nEvidence strength: \\(\\frac{P(A|B)}{P(A)}\\)\nUpdated belief: \\(P(B|A)\\)\n\n\nKey Insights:\n\nUpdates prior knowledge\nEvidence can strengthen/weaken\nSystematic way to learn\nCombines knowledge & data\n\n\n\n\nAnother way to think of Bayes’ rule is as a way to update our beliefs on the basis of data. The different parts of Bayes’ rule have specific names, that relate to their role in using Bayes’ rule to update our beliefs.\nThe part on the left tells us how much more or less likely the data A are given B, relative to the overall likelihood of the data, while the part on the right side tells us how likely we thought B was before we knew anything about the data.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayes' Rule and Learning from Data</span>"
    ]
  },
  {
    "objectID": "Week3/prob-05-bayes.html#odds-and-odds-ratios",
    "href": "Week3/prob-05-bayes.html#odds-and-odds-ratios",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "Odds and Odds Ratios",
    "text": "Odds and Odds Ratios\n\n\nConverting to Odds:\n\\(\\text{odds of A} = \\frac{P(A)}{P(\\neg A)}\\)\nExample:\nDrug test odds:\n\nPrior: \\(\\frac{0.025}{0.975} = 0.026\\)\nPosterior: \\(\\frac{0.7314974}{0.2685026} = 2.724\\)\n\n\nOdds Ratio:\n\\(\\frac{\\text{posterior odds}}{\\text{prior odds}} = \\frac{2.724}{0.026} = 106.25\\)\nInterpretation:\n\nOdds increased 105×\nMuch stronger evidence\nShows test’s power\nDespite false positives\n\n\n\n\nWe can convert probabilities into odds which express the relative likelihood of something happening or not. An odds ratio is an example of what we will later call an effect size, which is a way of quantifying how relatively large any particular statistical effect is.\nFirst, remember the rule for computing a conditional probability. We can rearrange this to get the formula to compute the joint probability using the conditional. Using this we can compute the inverse probability.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Bayes' Rule and Learning from Data</span>"
    ]
  },
  {
    "objectID": "Week3/prob-06-distributions.html",
    "href": "Week3/prob-06-distributions.html",
    "title": "Probability Distributions",
    "section": "",
    "text": "What is a Probability Distribution?",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Week3/prob-06-distributions.html#what-is-a-probability-distribution",
    "href": "Week3/prob-06-distributions.html#what-is-a-probability-distribution",
    "title": "Probability Distributions",
    "section": "",
    "text": "Definition:\n\nDescribes all possible outcomes\nAssigns probability to each\nDifferent types for different data\nMathematical formulation\n\nExamples:\n\nBinomial (success/failure)\nNormal (continuous)\nPoisson (counts)\n\n\n\n\nCode\n# Create example distributions\nx &lt;- seq(-4, 4, length.out = 100)\nnormal_df &lt;- data.frame(\n  x = x,\n  y = dnorm(x),\n  type = \"Normal\"\n)\n\nx &lt;- 0:10\npoisson_df &lt;- data.frame(\n  x = x,\n  y = dpois(x, lambda = 3),\n  type = \"Poisson\"\n)\n\ncolors &lt;- c(\n  \"Normal\" = \"blue\",\n  \"Poisson\" = \"red\"\n)\n\n# Plot distributions\nggplot() +\n  geom_line(data = normal_df, aes(x = x, y = y, color = \"Normal\"), size = 1) +\n  geom_point(\n    data = poisson_df,\n    aes(x = x, y = y, color = \"Poisson\"),\n    size = 1.5\n  ) +\n  labs(\n    title = \"Example Distributions\",\n    x = \"Value\",\n    y = \"Probability\",\n    color = \"Legend\"\n  ) +\n  theme(legend.position = \"top\") +\n  scale_color_manual(values = colors) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\nA probability distribution describes the probability of all of the possible outcomes in an experiment. Throughout this section we will encounter a number of these probability distributions, each of which is appropriate to describe different types of data.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Week3/prob-06-distributions.html#the-binomial-distribution",
    "href": "Week3/prob-06-distributions.html#the-binomial-distribution",
    "title": "Probability Distributions",
    "section": "The Binomial Distribution",
    "text": "The Binomial Distribution\n\n\n\nProperties:\n\nIndependent trials\nTwo outcomes\nFixed probability\nOrder doesn’t matter\n\nFormula:\n\\(P(k; n,p) = \\binom{n}{k} p^k(1-p)^{n-k}\\)\nWhere:\n\nk = successes\nn = trials\np = probability per trial\n\nBinomial Coefficient:\n\\(\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\\)\n\n\n\nCode\n# Create binomial distribution plot\nx &lt;- 0:10\nn &lt;- 10\np &lt;- 0.5\nbinom_df &lt;- data.frame(\n  x = x,\n  y = dbinom(x, size = n, prob = p)\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe binomial distribution provides a way to compute the probability of some number of successes out of a number of trials on which there is either success or failure and nothing in between (known as “Bernoulli trials”), given some known probability of success on each trial.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Week3/prob-06-distributions.html#example-steph-currys-free-throws",
    "href": "Week3/prob-06-distributions.html#example-steph-currys-free-throws",
    "title": "Probability Distributions",
    "section": "Example: Steph Curry’s Free Throws",
    "text": "Example: Steph Curry’s Free Throws\n\n\nScenario:\n\nSteph Curry hits 91% of his free throws\nIn a game in Jan, 2018, he hit only 2 out of 4 free throws\nIt seems pretty unlikely that he would hit only 50% of his free throws in a game, but exactly how unlikely is it?\n\n\nCalculation:\n\\(P(2;4,0.91) = \\binom{4}{2} 0.91^2(1-0.91)^{2}\\)\n\\(= 6 * 0.8281 * 0.0081\\)\n\\(= 0.040\\)\n\n\n\nInterpretation:\n\nVery unlikely (4%)\nYet it happened\nRare events do occur\nDon’t overinterpret\n\n\n\n\n\nOn Jan 20 2018, the basketball player Steph Curry hit only 2 out of 4 free throws in a game against the Houston Rockets. We know that Curry’s overall probability of hitting free throws across the entire season was 0.91, so it seems pretty unlikely that he would hit only 50% of his free throws in a game, but exactly how unlikely is it?",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Week3/prob-06-distributions.html#cumulative-distributions",
    "href": "Week3/prob-06-distributions.html#cumulative-distributions",
    "title": "Probability Distributions",
    "section": "Cumulative Distributions",
    "text": "Cumulative Distributions\nOften we want to know not just how likely a specific value is, but how likely it is to find a value that is as extreme or more than a particular value?\nDefinition:\n\nProbability of value ≤ x\nAccumulates probabilities\nOften more useful\nImportant for testing\n\nExample:\n\\(P(k\\le2)= P(k=2) + P(k=1) + P(k=0)\\)\n\nOften we want to know not just how likely a specific value is, but how likely it is to find a value that is as extreme or more than a particular value?",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Week3/prob-06-distributions.html#cumulative-distributions-1",
    "href": "Week3/prob-06-distributions.html#cumulative-distributions-1",
    "title": "Probability Distributions",
    "section": "Cumulative Distributions",
    "text": "Cumulative Distributions\n\n\n\n\nCode\n# curry_df &lt;- tibble(\n#   numSuccesses = seq(0, 4)\n# ) %&gt;%\n#   mutate(\n#     Probability = dbinom(numSuccesses, size = 4, prob = 0.91),\n#     CumulativeProbability = pbinom(numSuccesses, size = 4, prob = 0.91)\n#   )\n# Create data for Curry's free throw distributions\nn_throws &lt;- 4\ncurry_prob &lt;- 0.91\nx &lt;- 0:n_throws\n\ncurry_dist_df &lt;- data.frame(\n  x = x,\n  Simple = dbinom(x, size = n_throws, prob = curry_prob),\n  Cumulative = pbinom(x, size = n_throws, prob = curry_prob)\n)\n\nkable(\n  curry_dist_df,\n  caption = \"Simple and cumulative probability distributions\",\n  digits = 3\n)\n\n\n\nSimple and cumulative probability distributions\n\n\nx\nSimple\nCumulative\n\n\n\n\n0\n0.000\n0.000\n\n\n1\n0.003\n0.003\n\n\n2\n0.040\n0.043\n\n\n3\n0.271\n0.314\n\n\n4\n0.686\n1.000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe binomial distribution is a discrete probability distribution that describes the number of successes in a sequence of independent experiments, each of which has a constant probability of success. In this example, we are looking at the probability of Steph Curry making a certain number of free throws out of 4 attempts, given that his overall success rate is 91%.\nThis visualization shows both the probability of making exactly k free throws (blue bars) and the probability of making k or fewer free throws (red line) for Curry’s specific scenario of 4 attempts with a 91% success rate.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Week3/prob-06-distributions.html#summary",
    "href": "Week3/prob-06-distributions.html#summary",
    "title": "Probability Distributions",
    "section": "Summary",
    "text": "Summary\n\n\nCore Concepts:\n\nProbability measures uncertainty\nThree approaches:\n\nPersonal belief\nEmpirical frequency\nClassical probability\n\nFundamental rules:\n\nAddition\nMultiplication\nSubtraction\n\n\n\nAdvanced Topics:\n\nConditional probability\nIndependence\nBayes’ rule\nProbability distributions\n\nApplications:\n\nMedical screening\nData analysis\nDecision making\nStatistical inference\n\n\n\n\nThese concepts form the foundation for statistical inference, which we will explore in later chapters. Having read this chapter, you should be able to:\n\nDescribe the sample space for a selected random experiment\nCompute relative frequency and empirical probability\nCompute probabilities of single events, complementary events, and unions/intersections\nDescribe the law of large numbers\nUnderstand conditional probability and independence\nUse Bayes’ theorem",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Probability Distributions</span>"
    ]
  },
  {
    "objectID": "Week3/prob-07-sampling-slides.html",
    "href": "Week3/prob-07-sampling-slides.html",
    "title": "Part 2: Statistical Sampling",
    "section": "",
    "text": "Why Study Sampling?\nMaking inferences about populations from samples",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Part 2: Statistical Sampling</span>"
    ]
  },
  {
    "objectID": "Week3/prob-07-sampling-slides.html#why-study-sampling",
    "href": "Week3/prob-07-sampling-slides.html#why-study-sampling",
    "title": "Part 2: Statistical Sampling",
    "section": "",
    "text": "The Power of Sampling:\nNate Silver’s 2012 Election Prediction:\n\nCorrectly predicted all 50 states\nUsed only 21,000 people\nTo predict 125 million votes\nCombined data from 21 polls\n\n\nKey Insights:\n\nSmall samples can be powerful\nProper methodology is crucial\nCombining data improves accuracy\nStatistical rigor matters\n\n\n\n\nOne of the foundational ideas in statistics is that we can make inferences about an entire population based on a relatively small sample of individuals from that population.\nAnyone living in the United States will be familiar with the concept of sampling from the political polls that have become a central part of our electoral process. In some cases, these polls can be incredibly accurate at predicting the outcomes of elections. The best known example comes from the 2008 and 2012 US Presidential elections, when the pollster Nate Silver correctly predicted electoral outcomes for 49/50 states in 2008 and for all 50 states in 2012.\nSilver did this by combining data from 21 different polls, which vary in the degree to which they tend to lean towards either the Republican or Democratic side. Each of these polls included data from about 1000 likely voters – meaning that Silver was able to almost perfectly predict the pattern of votes of more than 125 million voters using data from only about 21,000 people, along with other knowledge.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Part 2: Statistical Sampling</span>"
    ]
  },
  {
    "objectID": "Week3/prob-07-sampling-slides.html#sampling-fundamentals",
    "href": "Week3/prob-07-sampling-slides.html#sampling-fundamentals",
    "title": "Part 2: Statistical Sampling",
    "section": "Sampling Fundamentals",
    "text": "Sampling Fundamentals\n\n\n\n\nPopulation vs Sample:\n\nPopulation: Entire group of interest\nSample: Subset used for measurement\nGoal: Infer population parameters from sample statistics\n\nRepresentative Sampling:\n\nEqual chance of selection\nAvoid systematic bias\nRandom selection crucial\n\n\n\n\n\n\nTypes of Sampling:\n\nWith replacement: Items can be selected multiple times\nWithout replacement: Items selected only once\nChoice affects probability calculations\n\nKey Terms:\n\nParameter: Population value (usually unknown)\nStatistic: Sample value (our estimate)\nSampling Error: Difference between statistic and parameter\n\n\n\n\n\n\nOur goal in sampling is to determine the value of a statistic for an entire population of interest, using just a small subset of the population. We do this primarily to save time and effort – why go to the trouble of measuring every individual in the population when just a small sample is sufficient to accurately estimate the statistic of interest?\nIn the election example, the population is all registered voters in the region being polled, and the sample is the set of 1000 individuals selected by the polling organization. The way in which we select the sample is critical to ensuring that the sample is representative of the entire population, which is a main goal of statistical sampling.\nIt’s important to also distinguish between two different ways of sampling: with replacement versus without replacement. In sampling with replacement, after a member of the population has been sampled, they are put back into the pool so that they can potentially be sampled again. In sampling without replacement, once a member has been sampled they are not eligible to be sampled again.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Part 2: Statistical Sampling</span>"
    ]
  },
  {
    "objectID": "Week3/prob-07-sampling-slides.html#sampling-error-distribution",
    "href": "Week3/prob-07-sampling-slides.html#sampling-error-distribution",
    "title": "Part 2: Statistical Sampling",
    "section": "Sampling Error & Distribution",
    "text": "Sampling Error & Distribution\n\nConcept\nWhat is Sampling Error?\n\nDifference between sample and population\nVaries across samples\nAffects measurement quality\nCan be quantified",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Part 2: Statistical Sampling</span>"
    ]
  },
  {
    "objectID": "Week3/prob-07-sampling-slides.html#sampling-error-distribution-1",
    "href": "Week3/prob-07-sampling-slides.html#sampling-error-distribution-1",
    "title": "Part 2: Statistical Sampling",
    "section": "Sampling Error & Distribution",
    "text": "Sampling Error & Distribution\n\nConcept\n\n\n\n# Take 5 samples of 50 adults each\nset.seed(123)\nsamples &lt;- map_df(\n  1:5,\n  ~{\n    NHANES_adult |&gt;\n      sample_n(50) |&gt;\n      summarise(\n        mean_height = mean(Height),\n        sd_height = sd(Height),\n      )\n  }\n)\nsamples\n\n# A tibble: 5 × 2\n  mean_height sd_height\n        &lt;dbl&gt;     &lt;dbl&gt;\n1        169.     11.6 \n2        167.      9.13\n3        169.     11.2 \n4        166.      9.62\n5        169.     11.0 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegardless of how representative our sample is, it’s likely that the statistic that we compute from the sample is going to differ at least slightly from the population parameter. We refer to this as sampling error. If we take multiple samples, the value of our statistical estimate will also vary from sample to sample; we refer to this distribution of our statistic across samples as the sampling distribution.\nSampling error is directly related to the quality of our measurement of the population. Clearly we want the estimates obtained from our sample to be as close as possible to the true value of the population parameter. However, even if our statistic is unbiased (that is, we expect it to have the same value as the population parameter), the value for any particular estimate will differ from the population value, and those differences will be greater when the sampling error is greater.\nThe visualization shows how sample means distribute around the true population mean (red line) when we take many samples.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Part 2: Statistical Sampling</span>"
    ]
  },
  {
    "objectID": "Week3/prob-07-sampling-slides.html#standard-error-of-the-mean",
    "href": "Week3/prob-07-sampling-slides.html#standard-error-of-the-mean",
    "title": "Part 2: Statistical Sampling",
    "section": "Standard Error of the Mean",
    "text": "Standard Error of the Mean\n\n\nDefinition:\n\\(SEM = \\frac{\\hat{\\sigma}}{\\sqrt{n}}\\)\nWhere:\n\n\\(\\hat{\\sigma}\\) is estimated standard deviation\n\\(n\\) is sample size\n\nKey Properties:\n\nMeasures sampling distribution variability\nDecreases with larger samples\nIncreases with population variability\n\n\nExample with NHANES:\n\n\nCode\n# Population SEM\npop_sd &lt;- sd(NHANES_adult$Height)\nn &lt;- 50\nsem_theoretical &lt;- pop_sd / sqrt(n)\n\n# Observed SEM from samples\nsem_observed &lt;- sd(samples_large$mean_height)\n\ncat(\"Theoretical SEM:\", round(sem_theoretical, 2), \"\\n\")\n\n\nTheoretical SEM: 1.44 \n\n\nCode\ncat(\"Observed SEM:\", round(sem_observed, 2))\n\n\nObserved SEM: 1.42\n\n\n\n\n\nLater in the course it will become essential to be able to characterize how variable our samples are, in order to make inferences about the sample statistics. For the mean, we do this using a quantity called the standard error of the mean (SEM), which one can think of as the standard deviation of the sampling distribution of the mean.\nThe formula for the standard error of the mean implies that the quality of our measurement involves two quantities: the population variability, and the size of our sample. Because the sample size is the denominator in the formula for SEM, a larger sample size will yield a smaller SEM when holding the population variability constant.\nWe have no control over the population variability, but we do have control over the sample size. Thus, if we wish to improve our sample statistics (by reducing their sampling variability) then we should use larger samples. However, the formula also tells us something very fundamental about statistical sampling – namely, that the utility of larger samples diminishes with the square root of the sample size.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Part 2: Statistical Sampling</span>"
    ]
  },
  {
    "objectID": "Week3/prob-07-sampling-slides.html#sample-size-effects",
    "href": "Week3/prob-07-sampling-slides.html#sample-size-effects",
    "title": "Part 2: Statistical Sampling",
    "section": "Sample Size Effects",
    "text": "Sample Size Effects\n\nTheoryVisualizationCode\n\n\nImpact of Sample Size:\n\nLarger n → Smaller SEM\nRelationship is not linear\nDiminishing returns\nSquare root relationship\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Compare SEM for different sample sizes\nn1 &lt;- 50\nn2 &lt;- 200 # 4 times larger\n\nsem1 &lt;- pop_sd / sqrt(n1)\nsem2 &lt;- pop_sd / sqrt(n2)\n\n# Improvement factor\nimprovement &lt;- sem1 / sem2\ncat(\"Improvement factor:\", round(improvement, 2))\n\n\n\n\n\n\nThe relationship between sample size and standard error is not linear. Doubling the sample size will not double the quality of the statistics; rather, it will improve it by a factor of √2. This has important implications for study design and resource allocation.\nThe visualization shows how the standard error decreases as sample size increases, but with diminishing returns. This means that after a certain point, increasing sample size may not be worth the additional cost and effort.\nThis relationship is fundamental to statistical power, which we will discuss in later sections. Understanding this relationship helps researchers make informed decisions about sample size requirements for their studies.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Part 2: Statistical Sampling</span>"
    ]
  },
  {
    "objectID": "Week3/prob-07-sampling-slides.html#the-central-limit-theorem",
    "href": "Week3/prob-07-sampling-slides.html#the-central-limit-theorem",
    "title": "Part 2: Statistical Sampling",
    "section": "The Central Limit Theorem",
    "text": "The Central Limit Theorem\nKey Points:\n\nAs sample size increases:\n\nSampling distribution becomes normal\nRegardless of population distribution\nMean approaches population mean\nVariance decreases\n\nImplications:\n\nEnables statistical inference\nJustifies normal approximation\nExplains real-world patterns\n\n\nThe Central Limit Theorem tells us that as sample sizes get larger, the sampling distribution of the mean will become normally distributed, even if the data within each sample are not normally distributed. This is a powerful result that allows us to make inferences about population parameters based on sample statistics.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Part 2: Statistical Sampling</span>"
    ]
  },
  {
    "objectID": "Week3/prob-07-sampling-slides.html#the-central-limit-theorem-1",
    "href": "Week3/prob-07-sampling-slides.html#the-central-limit-theorem-1",
    "title": "Part 2: Statistical Sampling",
    "section": "The Central Limit Theorem",
    "text": "The Central Limit Theorem\n\nNormal Distribution:\n\n\n\nBell-shaped curve\nDefined by mean (\\(\\mu\\)) and SD (\\(\\sigma\\))\nSymmetric around mean\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Central Limit Theorem tells us that as sample sizes get larger, the sampling distribution of the mean will become normally distributed, even if the data within each sample are not normally distributed.\nThe normal distribution is described in terms of two parameters: the mean (which you can think of as the location of the peak), and the standard deviation (which specifies the width of the distribution). The bell-like shape of the distribution never changes, only its location and width.\nThe normal distribution is commonly observed in data collected in the real world – and the central limit theorem gives us some insight into why that occurs. For example, the height of any adult depends on a complex mixture of their genetics and experience; even if those individual contributions may not be normally distributed, when we combine them the result is a normal distribution.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Part 2: Statistical Sampling</span>"
    ]
  },
  {
    "objectID": "Week3/prob-07-sampling-slides.html#clt-in-action-nhanes-example",
    "href": "Week3/prob-07-sampling-slides.html#clt-in-action-nhanes-example",
    "title": "Part 2: Statistical Sampling",
    "section": "CLT in Action: NHANES Example",
    "text": "CLT in Action: NHANES Example\n\nOriginal DistributionCode ExampleKey Insights\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Compare skewness\nlibrary(moments)\noriginal_skew &lt;- skewness(NHANES_clean$AlcoholYear)\nsampling_skew &lt;- skewness(samples_alc$mean_alcohol)\n\ncat(\"Original Distribution Skewness:\", round(original_skew, 2), \"\\n\")\ncat(\"Sampling Distribution Skewness:\", round(sampling_skew, 2))\n\n\n\n\n\nOriginal data is highly skewed\nSampling distribution is nearly normal\nCLT works even with:\n\nNon-normal data\nSkewed distributions\nDiscrete values\n\nSample size of 50 is sufficient\n\n\n\n\n\nLet’s work with the variable AlcoholYear from the NHANES dataset, which is highly skewed. This distribution is, for lack of a better word, funky – and definitely not normally distributed.\nNow let’s look at the sampling distribution of the mean for this variable. Despite the clear non-normality of the original data, the sampling distribution is remarkably close to the normal.\nThe Central Limit Theorem is important for statistics because it allows us to safely assume that the sampling distribution of the mean will be normal in most cases. This means that we can take advantage of statistical techniques that assume a normal distribution.",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Part 2: Statistical Sampling</span>"
    ]
  },
  {
    "objectID": "Week3/prob-07-sampling-slides.html#summary",
    "href": "Week3/prob-07-sampling-slides.html#summary",
    "title": "Part 2: Statistical Sampling",
    "section": "Summary",
    "text": "Summary\n\n\n\n\nSampling Fundamentals:\n\nPopulation vs Sample\nRepresentative sampling\nWith/without replacement\nSampling error\n\nStandard Error:\n\nMeasures sampling variability\nDecreases with √n\nGuides sample size decisions\nQuantifies precision\n\n\n\n\n\n\nCentral Limit Theorem:\n\nSampling distribution normality\nIndependent of original distribution\nEnables statistical inference\nFoundation for hypothesis testing\n\nApplications:\n\nPolitical polling\nClinical trials\nQuality control\nResearch design\n\n\n\n\n\n\nIn this lecture, we covered: - The fundamentals of statistical sampling and why it works - How to characterize sampling error and the sampling distribution - The standard error of the mean and its relationship with sample size - The Central Limit Theorem and its importance in statistical inference - Real-world applications and examples using the NHANES dataset",
    "crumbs": [
      "Week 3",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Part 2: Statistical Sampling</span>"
    ]
  },
  {
    "objectID": "Week4/index-slides.html",
    "href": "Week4/index-slides.html",
    "title": "Week 4",
    "section": "",
    "text": "This week’s lecture:\nYou can view the full slides here:\nor continue on to the next pages for lecture notes.",
    "crumbs": [
      "Week 4"
    ]
  },
  {
    "objectID": "Week4/index-slides.html#this-weeks-lecture",
    "href": "Week4/index-slides.html#this-weeks-lecture",
    "title": "Week 4",
    "section": "",
    "text": "Slides\n\n Download PDF Slides",
    "crumbs": [
      "Week 4"
    ]
  },
  {
    "objectID": "Week4/index-slides.html#overview",
    "href": "Week4/index-slides.html#overview",
    "title": "Week 4",
    "section": "Overview",
    "text": "Overview\n\nThree major goals of statistics:\nDescribe\nDecide\nPredict",
    "crumbs": [
      "Week 4"
    ]
  },
  {
    "objectID": "Week4/index-slides.html#focus-statistical-decision-making",
    "href": "Week4/index-slides.html#focus-statistical-decision-making",
    "title": "Week 4",
    "section": "Focus: Statistical Decision Making",
    "text": "Focus: Statistical Decision Making\n\nUsing statistics to make decisions about hypotheses\nSpecifically: Null Hypothesis Statistical Testing (NHST)\nEssential for understanding research results\nImportant to understand both uses and limitations",
    "crumbs": [
      "Week 4"
    ]
  },
  {
    "objectID": "Week4/index-slides.html#what-well-cover",
    "href": "Week4/index-slides.html#what-well-cover",
    "title": "Week 4",
    "section": "What We’ll Cover",
    "text": "What We’ll Cover\n\nIntroduction to NHST\nSteps in hypothesis testing\nTest statistics and distributions\nP-values and their interpretation\nStatistical significance\n\n\nThis chapter introduces the concepts behind using statistics to make decisions – specifically, decisions about whether particular hypotheses are supported by data.\nKey points to emphasize: - NHST is widely used but has limitations - Understanding both uses and criticisms is essential - Focus on practical application and interpretation",
    "crumbs": [
      "Week 4"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html",
    "href": "Week4/inference-slides.html",
    "title": "Using Statistics to make decisions",
    "section": "",
    "text": "Learning Objectives\nToday, we’ll discuss the use of statistics to make decisions – in particular, decisions about whether a particular hypothesis is supported by the data. There are three major goals of statistics:\nWe’ll cover:",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#learning-objectives",
    "href": "Week4/inference-slides.html#learning-objectives",
    "title": "Using Statistics to make decisions",
    "section": "",
    "text": "Understand point estimation\nApply and interpret the Central Limit Theorem\nConstruct and interpret confidence intervals for means\nUnderstand the behaviour of confidence intervals\nCarry out hypothesis tests for means (t-test)\nUnderstand the probabilities of error in hypypothesis tests\n\n\n\n\n\n\nIf you want to figure out the distribution of the change people carry in their pockets, and your sample is large enough, you will find that the distribution follows certain patterns.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#statistical-inference-1",
    "href": "Week4/inference-slides.html#statistical-inference-1",
    "title": "Using Statistics to make decisions",
    "section": "Statistical Inference",
    "text": "Statistical Inference\nThe goal of statistical inference is to generalise - to make statements about a population based on a sample.\nStatistical inference uses what we know about probability to make our best “guesses” from samples about what we don’t know about the population.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#statistical-inference-2",
    "href": "Week4/inference-slides.html#statistical-inference-2",
    "title": "Using Statistics to make decisions",
    "section": "Statistical Inference",
    "text": "Statistical Inference\n\nMain forms of statistical inference\n\n\nPoint estimation\n\nUsing sample data to calculate a single statistic as an estimate of an unknown population parameter\nExample: What is the average height of undergraduates at this university? What is the average construction cost of an office building in London? What was it in 2019?\n\nConfidence intervals\n\nAn interval built around a point estimate for an unknown population parameter.\n\nHypothesis testing\n\nA decision making procedure for determining whether sample evidence supports a hypothesis.\n\n\n\n\nThese three examples make up the main forms of statistical inference. However, there are many other forms of statistical inference, such as regression analysis - e.g. How much does building energy use change as occupancy increases?",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#point-estimation",
    "href": "Week4/inference-slides.html#point-estimation",
    "title": "Using Statistics to make decisions",
    "section": "Point Estimation",
    "text": "Point Estimation\n\nSuppose you were trying to determine the mean rent of a two-bedroom apartment in your town. You might look in the classified section of the newspaper, write down several rents listed, and average them together. You would have obtained a point estimate of the true mean. If you are trying to determine the percentage of times you make a basket when shooting a basketball, you might count the number of shots you make and divide that by the number of shots you attempted. In this case, you would have obtained a point estimate for the true proportion.\n\nThe most natural way to estimate features of the population (parameters) is to use the corresponding summary statistic calculated from the sample. Some common point estimates and their corresponding parameters are found in the following table:\n\n\nParameters and Point Estimates\n\n\n\n\n\n\n\nParameter\nMeasure\nStatistic\n\n\n\n\n\\(\\mu\\)\nMean of a single population\n\\(\\bar{x}\\)\n\n\n\\(p\\)\nProportion of a single population\n\\(\\hat{p}\\)\n\n\n\\(\\mu_D\\)\nMean difference of two dependent populations\n\\(\\bar{x}_D\\)\n\n\n\\(\\mu_1 - \\mu_2\\)\nDifference in means of two independent populations\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n\n\n\\(p_1 - p_2\\)\nDifference in proportions of two population\n\\(\\hat{p}_1 - \\hat{p}_2\\)\n\n\n\\(\\sigma^2\\)\nVariance of a single population\n\\(S^2\\)\n\n\n\\(\\sigma\\)\nStandard deviation of a single population\n\\(S\\)",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#point-estimation-1",
    "href": "Week4/inference-slides.html#point-estimation-1",
    "title": "Using Statistics to make decisions",
    "section": "Point Estimation",
    "text": "Point Estimation\n\n\n\n\nParameters and Point Estimates\n\n\nParameter\nStatistic\n\n\n\n\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\n\\(p\\)\n\\(\\hat{p}\\)\n\n\n\\(\\mu_1 - \\mu_2\\)\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n\n\n\\(p_1 - p_2\\)\n\\(\\hat{p}_1 - \\hat{p}_2\\)\n\n\n\\(\\sigma^2\\)\n\\(S^2\\)\n\n\n\\(\\sigma\\)\n\\(S\\)\n\n\n\n\n\nSuppose the mean weight of a sample of 60 adults is 173.3 lbs; this sample mean is a point estimate of the population mean weight, \\(\\mu\\).\nRemember: this is one of many samples that we could have taken from the population.\nIf a different random sample of 60 individuals were taken from the same population, the new sample mean would likely be different as a result of sampling variability. While estimates generally vary from one sample to another, the population mean is a fixed value.\n\n\n\nSuppose a poll suggested the US President’s approval rating is 45%. We would consider 45% to be a point estimate of the approval rating we might see if we collected responses from the entire population. This entire-population response proportion is generally referred to as the parameter of interest. When the parameter is a proportion, it is often denoted by p, and we often refer to the sample proportion as \\(\\hat{p}\\) (pronounced “p-hat”). Unless we collect responses from every individual in the population, p remains unknown, and we use $ as our estimate of p.\nHow would one estimate the difference in average weight between men and women? Suppose a sample of men yields a mean of 185.1 lbs and a sample of women men yields a mean of 162.3 lbs. What is a good point estimate for the difference in these two population means? We will expand on this in following chapters.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#unbiased-estimation",
    "href": "Week4/inference-slides.html#unbiased-estimation",
    "title": "Using Statistics to make decisions",
    "section": "Unbiased Estimation",
    "text": "Unbiased Estimation\n\nSampling variability\nWe have established that different samples yield different statistics due to sampling variability. These statistics have their own distributions, called sampling distributions, that reflect this as a random variable. The sampling distribution of a sample statistic is the distribution of the point estimates based on samples of a fixed size, n, from a certain population. It is useful to think of a particular point estimate as being drawn from a sampling distribution.\nRecall the sample mean weight calculated from a previous sample of 173.3 lbs. Suppose another random sample of 60 participants might produce a different value of x, such as 169.5 lbs. Repeated random sampling could result in additional different values, perhaps 172.1 lbs, 168.5 lbs, and so on. Each sample mean can be thought of as a single observation from a random variable X. The distribution of X is called the sampling distribution of the sample mean, and has its own mean and standard deviation like the random variables discussed previously. We will simulate the concept of a sampling distribution using technology to repeatedly sample, calculate statistics, and graph them. However, the actual sampling distribution would only be attainable if we could theoretically take an infinite amount of samples.\nEach of the point estimates in the table above have their own unique sampling distributions which we will look at in the future\n\n\nWhat makes a statistical estimate of this parameter of interest a “Good” one? It must be both accurate and precise.\nAlthough variability in samples is present, there remains a fixed value for any population parameter.\n\nAccording to the law of large numbers, probabilities converge to what we expect over time.\nPoint estimates follow this rule, becoming more accurate with increasing sample size.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#example-dataset---nhanes",
    "href": "Week4/inference-slides.html#example-dataset---nhanes",
    "title": "Using Statistics to make decisions",
    "section": "Example Dataset - NHANES",
    "text": "Example Dataset - NHANES\nNational Health and Nutrition Examination Survey (NHANES) from the US Centers for Disease Control (CDC)\n\n\nCode\nNHANES_adult |&gt;\n    select(c(\"SurveyYr\", \"Gender\", \"Age\", \"Race1\", \"Education\", \"Weight\", \"Height\", \"Pulse\", \"Diabetes\")) |&gt;\n    gt_preview() |&gt;\n    tab_header(title = \"NHANES Dataset\")\n\n\n\n\n\n\n\n\nNHANES Dataset\n\n\n\nSurveyYr\nGender\nAge\nRace1\nEducation\nWeight\nHeight\nPulse\nDiabetes\n\n\n\n\n1\n2009_10\nmale\n34\nWhite\nHigh School\n87.4\n164.7\n70\nNo\n\n\n2\n2009_10\nfemale\n49\nWhite\nSome College\n86.7\n168.4\n86\nNo\n\n\n3\n2009_10\nfemale\n45\nWhite\nCollege Grad\n75.7\n166.7\n62\nNo\n\n\n4\n2009_10\nmale\n66\nWhite\nSome College\n68.0\n169.5\n60\nNo\n\n\n5\n2009_10\nmale\n58\nWhite\nCollege Grad\n78.4\n181.9\n62\nNo\n\n\n6..4785\n\n\n\n\n\n\n\n\n\n\n\n4786\n2011_12\nmale\n60\nWhite\nCollege Grad\n78.4\n168.8\n76\nYes",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#unbiased-estimation-1",
    "href": "Week4/inference-slides.html#unbiased-estimation-1",
    "title": "Using Statistics to make decisions",
    "section": "Unbiased Estimation",
    "text": "Unbiased Estimation\n\n\n\n\n\n\n\n\n\n\nThe accuracy of an estimate refers to how well it estimates the actual value of that parameter. Mathematically, this is true when that the expected value your statistic is equal to the value of that parameter. This can be visualized as the center of the sampling distribution appearing to be situated at the value of that parameter.\nAccording to the law of large numbers, probabilities converge to what we expect over time. Point estimates follow this rule, becoming more accurate with increasing sample size. The figure above shows the sample mean weight calculated for random samples drawn, where sample size increases by 1 for each draw until sample size equals 500. The maroon dashed horizontal line is drawn at the average weight of all adults 169.7 lbs, which represents the population mean weight according to the CDC.\nThe figure above shows the sample mean weight calculated for random samples drawn, where sample size increases by 1 for each draw until sample size equals 500. The maroon dashed horizontal line is drawn at the average weight of all adults 169.7 lbs, which represents the population mean weight according to the CDC.\nNote how a sample size around 50 may produce a sample mean that is as much as 10 lbs higher or lower than the population mean. As sample size increases, the fluctuations around the population mean decrease; in other words, as sample size increases, the sample mean becomes less variable and provides a more reliable estimate of the population mean.\nIn addition to accuracy, a precise estimate is also more useful. This means when repeatedly sampling, the values of the statistics seem pretty close together. The precision of an estimate can be visualized as the spread of the sampling distribution, usually quantified by the standard deviation. The phrase “the standard deviation of a sampling distribution” is often shortened to the standard error. A smaller standard error means a more precise estimate and is also effected by sample size.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#central-limit-theorem",
    "href": "Week4/inference-slides.html#central-limit-theorem",
    "title": "Using Statistics to make decisions",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nThe central limit theorem (CLT) is one of the most powerful and useful ideas in all of statistics. There are two alternative forms of the theorem, and both alternatives are concerned with drawing finite samples size n from a population with a known mean, \\(\\mu\\), and a known standard deviation, \\(\\sigma\\). The first alternative says that if we collect samples of size n with a “large enough n,” then the resulting distribution can be approximated by the normal distribution.\nApplying the law of large numbers here, we could say that if you take larger and larger samples from a population, then the mean {x} of the sample tends to get closer and closer to \\(\\mu\\). From the central limit theorem, we know that as n gets larger and larger, the sample means follow a normal distribution. The larger n gets, the smaller the standard deviation gets. (Remember that the standard deviation for {x} is .) This means that the sample mean {x} must be close to the population mean \\(\\mu\\). We can say that \\(\\mu\\) is the value that the sample means approach as n gets larger. The central limit theorem illustrates the law of large numbers.\nThe size of the sample, n, that is required in order to be “large enough” depends on the original population from which the samples are drawn (the sample size should be at least 30 or the data should come from a normal distribution). If the original population is far from normal, then more observations are needed for the sample means or sums to be normal. Sampling is done with replacement.\n\nThe CLT means says that if you keep drawing larger and larger samples and calculating their means, the sample means form their own normal distribution (the sampling distribution).\nThe sampling distribution of the mean is generated by repeated sampling from the same population and recording the sample mean per sample. This forms a distribution of different means, and this distribution has its own mean and variance.\nThe normal distribution has the same mean as the original distribution and a variance that equals the original variance divided by the sample size.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#drawing-samples-of-peoples-weight-from-the-nhanes-dataset.",
    "href": "Week4/inference-slides.html#drawing-samples-of-peoples-weight-from-the-nhanes-dataset.",
    "title": "Using Statistics to make decisions",
    "section": "Drawing samples of people’s weight from the NHANES dataset.",
    "text": "Drawing samples of people’s weight from the NHANES dataset.\n\nWe have established that different samples yield different statistics due to sampling variability . These statistics have their own distributions, called sampling distributions, that reflect this as a random variable. The sampling distribution of a sample statistic is the distribution of the point estimates based on samples of a fixed size, n, from a certain population. It is useful to think of a particular point estimate as being drawn from a sampling distribution.\nRecall the sample mean weight calculated from a previous sample of 173.3 lbs. Suppose another random sample of 60 participants might produce a different value of x, such as 169.5 lbs. Repeated random sampling could result in additional different values, perhaps 172.1 lbs, 168.5 lbs, and so on. Each sample mean can be thought of as a single observation from a random variable X. The distribution of X is called the sampling distribution of the sample mean, and has its own mean and standard deviation like the random variables discussed previously. We will simulate the concept of a sampling distribution using technology to repeatedly sample, calculate statistics, and graph them. However, the actual sampling distribution would only be attainable if we could theoretically take an infinite amount of samples.\nEach of the point estimates in the table above have their own unique sampling distributions which we will look at in the future\n\n\n\n\n\n\n\nWe are drawing a random sample of people from the dataset and calculating the mean weight for that sample. Sample size is the number of data points we pull. We then repeat this 5000 times (n_samples) to build up the sampling distribution.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#standard-error",
    "href": "Week4/inference-slides.html#standard-error",
    "title": "Using Statistics to make decisions",
    "section": "Standard Error",
    "text": "Standard Error\n\nA sampling distribution is what we get by simulating multiple samples (of sample size \\(n\\)) from a population.\nRecall: The Standard Error is the standard deviation of the sampling distribution.\n\\[\nSEM = \\sigma_{\\bar{x}\\ (means)}\n\\]",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#standard-error-1",
    "href": "Week4/inference-slides.html#standard-error-1",
    "title": "Using Statistics to make decisions",
    "section": "Standard Error",
    "text": "Standard Error\n\nA sampling distribution is a probability distribution of a statistic at a given sample size.\nRecall: The Standard Error is the standard deviation of the sampling distribution. This is also equal to the standard deviation \\(\\sigma\\) of the population divided by the square root of the sample size.\n\\[\nSEM = \\sigma_{\\bar{x}\\ (means)} = \\frac{\\sigma}{\\sqrt{n}} \\approx \\frac{\\sigma_{x}}{\\sqrt{n}} \\left[ i.e. \\frac{\\text{Est. Std Dev of the sample}}{\\sqrt{\\text{Sample size}}} \\right]\n\\]\n\nIn other words:\n\nIf you draw random samples of size \\(n\\), the distribution of the random variable \\(\\bar{X}\\), which consists of sample means, is called the sampling distribution of the sample mean. The sampling distribution of the mean approaches a normal distribution as \\(n\\), the sample size, increases.\n\n\n\nIn the SEM formula, remember the sampling distribution is the distribution of multiple means - not the distribution of our sample.\nQuote from https://pressbooks.lib.vt.edu/introstatistics/chapter/the-central-limit-theorem-for-sample-means-averages/",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#standard-error-2",
    "href": "Week4/inference-slides.html#standard-error-2",
    "title": "Using Statistics to make decisions",
    "section": "Standard Error",
    "text": "Standard Error\n\nKey Takeaways\n\nA sampling distribution is what we get by simulating multiple samples from a population.\nThe Standard Error is the standard deviation \\(\\sigma_{\\bar{x}}\\) of the sampling distribution.\nThe SE decreases as the sample size \\(n\\) increases.\nBecause of this relationship - we can estimate the SE from a single sample \\(\\frac{\\sigma_x}{\\sqrt{n}}\\)\n\n\n\\[\nSEM = \\sigma_{\\bar{x}\\ (means)} = \\frac{\\sigma}{\\sqrt{n}} \\approx \\frac{\\sigma_{x}}{\\sqrt{n}} \\left[ i.e. \\frac{\\text{Est. Std Dev of the sample}}{\\sqrt{\\text{Sample size}}} \\right]\n\\]",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#exercise-dataset",
    "href": "Week4/inference-slides.html#exercise-dataset",
    "title": "Using Statistics to make decisions",
    "section": "Exercise Dataset",
    "text": "Exercise Dataset\n\nData OverviewData Preview\n\n\nThe “AI-Powered Job Market Insights” dataset provides a snapshot of the modern job market, particularly focusing on the role of artificial intelligence (AI) and automation across various industries.\nThis dataset includes 500 unique job listings, each characterized by different factors like industry, company size, AI adoption level, automation risk, required skills, and job growth projections.\n\n\n\n\nCode\nai_jobs |&gt;\n  gt_preview() |&gt;\n  tab_header(title = \"AI-Powered Job Market Insights\") |&gt;\n  tab_source_note(source_note = \"Source: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\")\n\n\n\n\n\n\n\n\nAI-Powered Job Market Insights\n\n\n\njob_title\nindustry\ncompany_size\nlocation\nai_adoption_level\nautomation_risk\nrequired_skills\nsalary_usd\nremote_friendly\njob_growth_projection\n\n\n\n\n1\nCybersecurity Analyst\nEntertainment\nSmall\nDubai\nMedium\nHigh\nUX/UI Design\n111392.17\nYes\nGrowth\n\n\n2\nMarketing Specialist\nTechnology\nLarge\nSingapore\nMedium\nHigh\nMarketing\n73792.56\nNo\nDecline\n\n\n3\nAI Researcher\nTechnology\nLarge\nSingapore\nMedium\nMedium\nUX/UI Design\n137170.26\nYes\nGrowth\n\n\n4\nSales Manager\nRetail\nSmall\nBerlin\nLow\nMedium\nProject Management\n83027.95\nNo\nGrowth\n\n\n5\nCybersecurity Analyst\nEntertainment\nSmall\nTokyo\nLow\nLow\nJavaScript\n87752.92\nYes\nDecline\n\n\n6..499\n\n\n\n\n\n\n\n\n\n\n\n\n500\nHR Manager\nEntertainment\nMedium\nBerlin\nMedium\nHigh\nProject Management\n53764.38\nYes\nDecline\n\n\n\nSource: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\n\n\n\n\n\n\n\n\n\n\n\n\nTo simplify our later code, I have created a separate table which is already filtered for the groups we will be looking at:\n\n\nCode\nai_jobs_risk &lt;- ai_jobs |&gt;\n  filter(automation_risk %in% c(\"Low\", \"High\"))\n\nai_jobs_high &lt;- ai_jobs_risk |&gt;\n  filter(automation_risk == \"High\")\n\nai_jobs_low &lt;- ai_jobs_risk |&gt;\n  filter(automation_risk == \"Low\")\n\n\nDataset Features:\nSource: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\n\nJob_Title:\n\nDescription: The title of the job role.\nType: Categorical\nExample Values: “Data Scientist”, “Software Engineer”, “HR Manager”\n\nIndustry:\n\nDescription: The industry in which the job is located.\nType: Categorical\nExample Values: “Technology”, “Healthcare”, “Finance”\n\nCompany_Size:\n\nDescription: The size of the company offering the job.\nType: Ordinal\nCategories: “Small”, “Medium”, “Large”\n\nLocation:\n\nDescription: The geographic location of the job.\nType: Categorical\nExample Values: “New York”, “San Francisco”, “London”\n\nAI_Adoption_Level:\n\nDescription: The extent to which the company has adopted AI in its operations.\nType: Ordinal\nCategories: “Low”, “Medium”, “High”\n\nAutomation_Risk:\n\nDescription: The estimated risk that the job could be automated within the next 10 years.\nType: Ordinal\nCategories: “Low”, “Medium”, “High”\n\nRequired_Skills:\n\nDescription: The key skills required for the job role.\nType: Categorical\nExample Values: “Python”, “Data Analysis”, “Project Management”\n\nSalary_USD:\n\nDescription: The annual salary offered for the job in USD.\nType: Numerical\nValue Range: $30,000 - $200,000\n\nRemote_Friendly:\n\nDescription: Indicates whether the job can be performed remotely.\nType: Categorical\nCategories: “Yes”, “No”\n\nJob_Growth_Projection:\n\nDescription: The projected growth or decline of the job role over the next five years.\nType: Categorical\nCategories: “Decline”, “Stable”, “Growth”",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#exercise---mystery-bags",
    "href": "Week4/inference-slides.html#exercise---mystery-bags",
    "title": "Using Statistics to make decisions",
    "section": "Exercise - Mystery bags",
    "text": "Exercise - Mystery bags\nTo begin, split into three groups. Decide on the following roles, one per person:\n\nSampler - draws samples from the population\nRecording - records the draws in R\n\nYour group has been presented with a population model (bag containing salaries). There may be multiple populations represented in the various bags around the room.\nPlease DO NOT look in the bag OR empty the contents of the bag.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#exercise---mystery-bags-nonincremental",
    "href": "Week4/inference-slides.html#exercise---mystery-bags-nonincremental",
    "title": "Using Statistics to make decisions",
    "section": "Exercise - Mystery bags {nonincremental}",
    "text": "Exercise - Mystery bags {nonincremental}\n\nThe task\n\nThe Sampler draws 10 slips from the bag - this is your sample of size 5.\nThe Recorder records the values on the slips into the sampling-exercise.R file.\n\nCreate a new list for each sample (sample_1 &lt;- c(1, 2, 3, 2, 2))\n\nCalculate the mean for the new sample and add it to the sample_means table.\nRun the code to plot the histogram of your samples.\nReturn the slips to the bag.\nEnsure the data is well mixed between samples.\nRepeat",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#statistical-inference-4",
    "href": "Week4/inference-slides.html#statistical-inference-4",
    "title": "Using Statistics to make decisions",
    "section": "Statistical Inference",
    "text": "Statistical Inference\nUsing a sample to generalize (or infer) about the population.\n\nWe know how to make a point estimate of a population - what else do we need in order to make a decision?\nHow confident are we that our estimate can generalize to the rest of the population?\nWe need to determine the uncertainty in our estimate\n\n\nWe use inferential statistics to make generalizations about an unknown population. The simplest way of doing this is to use the sample data help us to make a point estimate of a population parameter. We realize that due to sampling variability the point estimate is most likely not the exact value of the population parameter, but should be close to it. After calculating point estimates, we can build off of them to construct interval estimates, called confidence intervals.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#confidence-intervals",
    "href": "Week4/inference-slides.html#confidence-intervals",
    "title": "Using Statistics to make decisions",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nA confidence interval is another type of estimate, but instead of being just one number, it is an interval of numbers\n\nProvides a range a range of reasonable values where we expect the true population parameter to fall.\nPoint estimate (statistic) has some variability and uncertainty since we estimate it based on a sample.\nWe want to quantify and communicate this uncertainty.\n\n\nThere is no guarantee that a given confidence interval does capture the parameter, but there is a predictable probability of success. It is important to keep in mind that the confidence interval itself is a random variable, while the population parameter is fixed.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#business-example",
    "href": "Week4/inference-slides.html#business-example",
    "title": "Using Statistics to make decisions",
    "section": "Business Example",
    "text": "Business Example\n\nAverage streams per month\nYou work in the marketing department of a music company. You want to know the mean number of songs a consumer streams per month.\n\nYou conduct a survey of 100 customers and calculate the sample mean (\\(\\bar{x}\\)) and use it as the point estimate for the population mean (\\(\\mu\\))\nSuppose we know that the standard deviation \\(\\sigma = 100\\).\nFollowing the Central Limit Theorem, the Standard Error is:\n\n\n\\[\n\\frac{\\sigma}{\\sqrt{n}} = \\frac{100}{\\sqrt{100}} = 10\n\\]\n\n\nIf you worked in the marketing department of an entertainment company, you might be interested in the mean number of songs a consumer downloads a month from iTunes. If so, you could conduct a survey and calculate the sample mean, \\(\\bar{x}\\). You would use \\(\\bar{x}\\) to estimate the population mean. The sample mean, \\(\\bar{x}\\), is the point estimate for the population mean, μ.\nSuppose, for the iTunes example, we do not know the population mean μ, but we do know that the population standard deviation is \\(\\sigma = 100\\) and our sample size is 100. Then, by the central limit theorem, the standard deviation for the sample mean is \\(\\frac{\\sigma }{\\sqrt{n}}=\\frac{100}{\\sqrt{100}}=10\\).",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#what-is-the-probability-of-sampling-a-certain-mean-value",
    "href": "Week4/inference-slides.html#what-is-the-probability-of-sampling-a-certain-mean-value",
    "title": "Using Statistics to make decisions",
    "section": "What is the probability of sampling a certain mean value?",
    "text": "What is the probability of sampling a certain mean value?\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Empirical Rule says that in approximately 95% of the samples, the sample mean, \\(\\bar{x}\\), will be within two standard deviations of the population mean \\(\\mu\\) .\nFor our example, two standard deviations is \\((2)(10) = 20\\). The sample mean \\(\\bar{x}\\) is likely to be within 20 units of \\(\\mu\\).\nBecause \\(\\bar{x}\\) is within 20 units of \\(\\mu\\), which is unknown, then \\(\\mu\\) is likely to be within 20 units of \\(\\bar{x}\\) in 95% of the samples.\n\n\n\n\nBecause \\(\\bar{x}\\) is within 0.2 units of μ, which is unknown, then μ is likely to be within 0.2 units of \\(\\bar{x}\\) in 95% of the samples. The population mean μ is contained in an interval whose lower number is calculated by taking the sample mean and subtracting two standard deviations (2)(0.1) and whose upper number is calculated by taking the sample mean and adding two standard deviations. In other words, μ is between \\(\\bar{x}\\text{ }-\\text{ 0}\\text{.2}\\) and \\(\\bar{x}\\text{ }+\\text{ 0}\\text{.2}\\) in 95% of all the samples.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#calculate-the-confidence-interval",
    "href": "Week4/inference-slides.html#calculate-the-confidence-interval",
    "title": "Using Statistics to make decisions",
    "section": "Calculate the Confidence Interval",
    "text": "Calculate the Confidence Interval\nWe want to calculate the range of values which the true mean is likely to fall within 95% of the time, given our sample.\n\nFor the streaming example, suppose that a sample produced a sample mean \\(\\bar{x} = 200\\). Then the unknown population mean \\(\\mu\\) is between \\(\\bar{x}-20=200-20=180\\) and \\(\\bar{x}+20=200+20=220\\) songs per month.\n\n\nWe can say that we are about 95% confident that the unknown population mean number of songs streamed per month is between 180 and 220. The approximate 95% confidence interval is (1.8, 2.2).\n\n\nConfidence Interval: (Point Estimate \\(\\pm\\) Margin of error) = \\(200 \\pm 20 \\text{ songs}\\)",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#calculate-the-confidence-interval-1",
    "href": "Week4/inference-slides.html#calculate-the-confidence-interval-1",
    "title": "Using Statistics to make decisions",
    "section": "Calculate the Confidence Interval",
    "text": "Calculate the Confidence Interval\n\nBased on our sample, we can say two things. Either:\n\nThe interval (180, 220) contains the true mean \\(\\mu\\), or…\nOur sample prodcued an \\(\\bar{x}\\) that is not within 20 units of the true mean \\(\\mu\\). This would only happen for 5% of the samples.\n\n\n\nWe can say that we are about 95% confident that the unknown population mean number of songs downloaded from iTunes per month is between 1.8 and 2.2. The approximate 95% confidence interval is (1.8, 2.2). This approximate 95% confidence interval implies two possibilities. Either the interval (1.8, 2.2) contains the true mean μ or our sample produced an that is not within 0.2 units of the true mean μ. The second possibility happens for only 5% of all the samples (95–100%).\nRemember that a confidence intervals are created for an unknown population parameter. Confidence intervals for most parameters have the form: (Point Estimate ± Margin of Error) = (Point Estimate – Margin of Error, Point Estimate + Margin of Error) The margin of error (MoE) depends on the confidence level or percentage of confidence and the standard error of the mean. When you read newspapers and journals, some reports will use the phrase “margin of error.” Other reports will not use that phrase, but include a confidence interval as the point estimate plus or minus the margin of error. These are two ways of expressing the same concept. A confidence interval for a population mean with a known standard deviation is based on the fact that the sample means follow an approximately normal distribution. Suppose that our sample has a mean of and we have constructed the 90% confidence interval (5, 15) where MoE = 5.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#communicating-confidence-intervals",
    "href": "Week4/inference-slides.html#communicating-confidence-intervals",
    "title": "Using Statistics to make decisions",
    "section": "Communicating Confidence Intervals",
    "text": "Communicating Confidence Intervals\n\nThe interpretation should clearly state the confidence level (CL), explain what population parameter is being estimated (here the population mean), and state the confidence interval (both endpoints).\n“We can be _______ % confident that the interval we created, _______ to ________ captures the true population mean (include the context of the problem and appropriate units).”\nWe state the range within which our evidence indicates contains the population mean.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#case-study",
    "href": "Week4/inference-slides.html#case-study",
    "title": "Using Statistics to make decisions",
    "section": "Case Study",
    "text": "Case Study\n\nData OverviewData Preview\n\n\nThe “AI-Powered Job Market Insights” dataset provides a snapshot of the modern job market, particularly focusing on the role of artificial intelligence (AI) and automation across various industries.\nThis dataset includes 500 unique job listings, each characterized by different factors like industry, company size, AI adoption level, automation risk, required skills, and job growth projections.\n\n\n\n\nCode\nai_jobs |&gt;\n  gt_preview() |&gt;\n  tab_header(title = \"AI-Powered Job Market Insights\") |&gt;\n  tab_source_note(source_note = \"Source: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\")\n\n\n\n\n\n\n\n\nAI-Powered Job Market Insights\n\n\n\njob_title\nindustry\ncompany_size\nlocation\nai_adoption_level\nautomation_risk\nrequired_skills\nsalary_usd\nremote_friendly\njob_growth_projection\n\n\n\n\n1\nCybersecurity Analyst\nEntertainment\nSmall\nDubai\nMedium\nHigh\nUX/UI Design\n111392.17\nYes\nGrowth\n\n\n2\nMarketing Specialist\nTechnology\nLarge\nSingapore\nMedium\nHigh\nMarketing\n73792.56\nNo\nDecline\n\n\n3\nAI Researcher\nTechnology\nLarge\nSingapore\nMedium\nMedium\nUX/UI Design\n137170.26\nYes\nGrowth\n\n\n4\nSales Manager\nRetail\nSmall\nBerlin\nLow\nMedium\nProject Management\n83027.95\nNo\nGrowth\n\n\n5\nCybersecurity Analyst\nEntertainment\nSmall\nTokyo\nLow\nLow\nJavaScript\n87752.92\nYes\nDecline\n\n\n6..499\n\n\n\n\n\n\n\n\n\n\n\n\n500\nHR Manager\nEntertainment\nMedium\nBerlin\nMedium\nHigh\nProject Management\n53764.38\nYes\nDecline\n\n\n\nSource: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\n\n\n\n\n\n\n\n\n\n\n\n\nTo simplify our later code, I have created a separate table which is already filtered for the groups we will be looking at:\n\n\nCode\nai_jobs_risk &lt;- ai_jobs |&gt;\n  filter(automation_risk %in% c(\"Low\", \"High\"))\n\nai_jobs_high &lt;- ai_jobs_risk |&gt;\n  filter(automation_risk == \"High\")\n\nai_jobs_low &lt;- ai_jobs_risk |&gt;\n  filter(automation_risk == \"Low\")\n\n\nDataset Features:\nSource: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\n\nJob_Title:\n\nDescription: The title of the job role.\nType: Categorical\nExample Values: “Data Scientist”, “Software Engineer”, “HR Manager”\n\nIndustry:\n\nDescription: The industry in which the job is located.\nType: Categorical\nExample Values: “Technology”, “Healthcare”, “Finance”\n\nCompany_Size:\n\nDescription: The size of the company offering the job.\nType: Ordinal\nCategories: “Small”, “Medium”, “Large”\n\nLocation:\n\nDescription: The geographic location of the job.\nType: Categorical\nExample Values: “New York”, “San Francisco”, “London”\n\nAI_Adoption_Level:\n\nDescription: The extent to which the company has adopted AI in its operations.\nType: Ordinal\nCategories: “Low”, “Medium”, “High”\n\nAutomation_Risk:\n\nDescription: The estimated risk that the job could be automated within the next 10 years.\nType: Ordinal\nCategories: “Low”, “Medium”, “High”\n\nRequired_Skills:\n\nDescription: The key skills required for the job role.\nType: Categorical\nExample Values: “Python”, “Data Analysis”, “Project Management”\n\nSalary_USD:\n\nDescription: The annual salary offered for the job in USD.\nType: Numerical\nValue Range: $30,000 - $200,000\n\nRemote_Friendly:\n\nDescription: Indicates whether the job can be performed remotely.\nType: Categorical\nCategories: “Yes”, “No”\n\nJob_Growth_Projection:\n\nDescription: The projected growth or decline of the job role over the next five years.\nType: Categorical\nCategories: “Decline”, “Stable”, “Growth”",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#motivation",
    "href": "Week4/inference-slides.html#motivation",
    "title": "Using Statistics to make decisions",
    "section": "Motivation",
    "text": "Motivation\nYou have been tasked with examining the impact of AI skills and company AI adoption on the job market across the world. Think about what we might want to know about this sector.\nWhat might some be some interesting questions we could ask based on this data?\nHow would we answer them? How would we know whether our answer is generalizable?\n\nWe want to generalize the results of our analysis from the data we have to the situation we care about.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#the-logic-of-testing-hypotheses",
    "href": "Week4/inference-slides.html#the-logic-of-testing-hypotheses",
    "title": "Using Statistics to make decisions",
    "section": "The Logic of Testing Hypotheses",
    "text": "The Logic of Testing Hypotheses\nA hypothesis is a statement about a population, or general pattern.\nTesting a hypothesis amounts to gathering information (sampling) from a dataset and, based on that information, deciding whether that hypothesis is false or true in the population.\nTwo decisions are possible:\n\nRejecting the hypothesis (if there is enough evidence against it)\nNot rejecting it (if there is not enough evidence against it)\n\nRejecting a hypothesis is a more conclusive decision than not rejecting it.\n\nOne such focused approach uses a statistic (e.g. a difference in two means) computed from our data to see whether its true value is equal to something we assume (e.g. the difference is zero). This is called hypothesis testing: using results in the data to see if we have enough evidence to tell whether a hypothesis (the two means are equal) is wrong (they are not equal) or whether we don’t have enough evidence (they may be equal.\nTesting a hypothesis is a way of making an inference, with a focus on a specific statement. As with any kind of inference, we have to assess external validity too: the extent to which the population, or general pattern, represented by our data is the same as the population, or general pattern, we are truly interested in.\n\nGiven a population (i.e. a distribution) with a parameter of interest (which could be the mean, variance, correlation, etc.), we would like to decide between to complementary statements concerning the parameter.\nThese statements are called statistical hypotheses.\nThe choice or decision between these hypotheses is to be based on a sample taken from the population of interest.\nThe goal is to be able to choose which hypothesis is true in reality based on the sample data.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#steps-of-analysis-hypothesis-testing",
    "href": "Week4/inference-slides.html#steps-of-analysis-hypothesis-testing",
    "title": "Using Statistics to make decisions",
    "section": "Steps of Analysis & Hypothesis Testing",
    "text": "Steps of Analysis & Hypothesis Testing\n\n\n\n1. Formulate research question\n\n\n2. Specify hypotheses\n\nWhat statistic is appropriate to answer our question?\nNull and Alternative Hypotheses\n\n\n\n3. Collect relevant data\n\nWhat information is needed to answer our question?\nWhat is our population? How do we sample from the population in a statistically valid way?\n\n\n\n\n4. Compute test statistic\n\nFit appropriate model\nCalculate test statistic\nAccount for variability\n\n\n\n5. Determine probability under the null hypothesis\n\n\n6. Assess significance and meaningfulness",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#formulate-research-question-1",
    "href": "Week4/inference-slides.html#formulate-research-question-1",
    "title": "Using Statistics to make decisions",
    "section": "1. Formulate research question",
    "text": "1. Formulate research question\nThis is where all analysis or research starts - what is it you want to know, and what will you do with that information?\nFor our AI Jobs dataset some research questions we might ask are:\n\nWhich jobs are the highest paid?\nWhich industries have seen the most AI adoption?\nAre lower paid jobs at more risk of being automated?",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#specify-hypotheses-1",
    "href": "Week4/inference-slides.html#specify-hypotheses-1",
    "title": "Using Statistics to make decisions",
    "section": "2. Specify hypotheses",
    "text": "2. Specify hypotheses\nNow we translate our general research questions into specific and testable hypotheses.\nThe actual test begins by considering two hypotheses. They are called the null hypothesis and the alternative hypothesis. These hypotheses contain opposing viewpoints.\n\nThe null hypothesis (\\(H_0\\)) : It is often a statement of the accepted historical value or norm. This is your starting point that you must assume from the beginning in order to show an effect exists.\nThe alternative hypothesis (\\(H_a\\)) : It is a claim about the population that is contradictory to \\(H_0\\) and what we conclude when we reject \\(H_0\\)",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#specify-hypotheses-2",
    "href": "Week4/inference-slides.html#specify-hypotheses-2",
    "title": "Using Statistics to make decisions",
    "section": "2. Specify Hypotheses",
    "text": "2. Specify Hypotheses\nAfter you have determined which hypothesis the sample supports, you make a decision.\nThere are two options for a decision:\n\n“reject \\(H_0\\)” if the sample information favors the alternative hypothesis, or\n“do not reject \\(H_0\\)” or “decline to reject \\(H_0\\)” if the sample information is insufficient to reject the null hypothesis.\n\n\nMathematical symbols used in H0 and Ha:\nFigure 6.12: Null and Alternative Hypotheses\n\n\n\n\n\n\n\n\\(H_0\\)\n\\(H_a\\)\n\n\n\n\nequal (=)\nnot equal (\\(\\neq\\)) or greater than (\\(&gt;\\)) or less than (\\(&lt;\\))\n\n\ngreater than or equal to (\\(\\geq\\))\nless than (\\(&lt;\\))\n\n\nless than or equal to (\\(\\leq\\))\nmore than (\\(&gt;\\))",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#specify-hypotheses-3",
    "href": "Week4/inference-slides.html#specify-hypotheses-3",
    "title": "Using Statistics to make decisions",
    "section": "2. Specify Hypotheses",
    "text": "2. Specify Hypotheses\n\nDecide on statistic of interest\nDecide what statistic is appropriate to answer our question. What do we need to calculate from our sample?\n\nDifference in mean(salary_usd) for each level of automation_risk\nSince there are three automation risk levels (Low, Medium, High), the comparison we could make are:\n\n\\(\\text{mean}_{\\text{high risk}} - \\text{mean}_{\\text{medium risk}}\\)\n\\(\\text{mean}_{\\text{high}} - \\text{mean}_{\\text{low}}\\)\n\\(\\text{mean}_{\\text{medium}} - \\text{mean}_{\\text{low}}\\)",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#specify-hypotheses-4",
    "href": "Week4/inference-slides.html#specify-hypotheses-4",
    "title": "Using Statistics to make decisions",
    "section": "2. Specify Hypotheses",
    "text": "2. Specify Hypotheses\n\nDecide on statistic of interest\nFor now, let’s focus on just (2). Therefore:\n\\(s = \\overline{salary}_{\\text{high risk}} - \\overline{salary}_{\\text{low risk}}\\)\n\\(s\\) is our statistic of interest. We are interested in the true value of \\(s\\) in the population (\\(s_{true}\\)).\nWhat we can actually calculate is \\(\\hat{s}\\) the value of \\(s\\) in our sample.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#specify-hypotheses-5",
    "href": "Week4/inference-slides.html#specify-hypotheses-5",
    "title": "Using Statistics to make decisions",
    "section": "2. Specify Hypotheses",
    "text": "2. Specify Hypotheses\n\nExpress the hypotheses mathematically\nOur hypothesis, plainly stated, is:\nThere is a relationship between job salary and the likelihood of automation. Expressed another way, the average salary of high risk jobs is different from that of low risk jobs.\nWhat are the Null Hypothesis \\(H_0\\) and Alternative Hypothesis \\(H_A\\) ?",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#specify-hypotheses-6",
    "href": "Week4/inference-slides.html#specify-hypotheses-6",
    "title": "Using Statistics to make decisions",
    "section": "2. Specify Hypotheses",
    "text": "2. Specify Hypotheses\n\nExpress the hypotheses mathematically\n\n\nNull Hypothesis \\(H_0\\)\nThere is no difference in the average salary between those jobs at high risk of automation, and those at low risk: \\[\nH_0: s_{true} = 0\n\\]\n\n\n\n\nAlternative Hypothesis \\(H_a\\)\nThe difference in the average salary of high risk jobs and low risk jobs is not zero: \\[\nH_A: s_{true} \\neq 0\n\\]\n\n\n\nThe null says that the true value of the statistic is zero; the alternative says that it’s not zero. Together, these cover all logical possibilities for the true value of the statistic.\nIt may seem odd to have \\(H_0: s_{true} = 0\\) when, presumably, we analyze the data because we suspect that the true value of \\(s\\) is not zero. This seemingly twisted logic comes from the fact that testing a hypothesis amounts to seeing if there is enough evidence in our data to reject the null. It is sometimes said that the null is protected: it should not be too easy to reject it otherwise the conclusions of hypothesis testing would not be strong.\nAs we introduce the concept of hypothesis testing, it is helpful to relate its logic to the logic of a criminal court procedure. At court the task is to decide whether an accused person is guilty or innocent of a certain crime. In most modern societies the starting point is the assumption of innocence: the accused person should be judged guilty only if there is enough evidence against their innocence. This is so even though the accused person was brought before court presumably because there was a suspicion of their guilt. To translate this procedure to the language of hypothesis testing, H0 is that the person is innocent, and HA is that the person is guilty.\nMedical tests are another instructive example. When testing whether a person has a certain medical condition, the null is that the person does not have the condition (healthy), and the alternative is that they have it (sick). The testing procedure amounts to gathering information to see if there is evidence to decide that the person has the condition.\nThe case when we test if HA: strue ≠ 0 is called a two-sided alternative as it allows for strue to be either greater than zero or less than zero. For instance, we focus on the difference in online and offline prices, with H0 being the equality. In such a case we are not really interested if the difference is positive or not, or whether it is negative or not.\nThe other case is working with a one-sided alternative, when we are indeed interested if a statistic is positive. The null and the alternative should be set up so that the hypothesis we are truly interested in is in the alternative set. So when we want to know if strue is positive, we want to put strue &gt; 0 in the alternative thus, making the null strue ≤ 0:\n\\[\nH_0: s_{true} \\leq 0\n\\]\n\\[\nH_A: s_{true} &gt; 0\n\\]",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#using-the-sample-to-test-the-null-hypothesis",
    "href": "Week4/inference-slides.html#using-the-sample-to-test-the-null-hypothesis",
    "title": "Using Statistics to make decisions",
    "section": "3. Using the Sample to Test the Null Hypothesis",
    "text": "3. Using the Sample to Test the Null Hypothesis\nOnce you have defined your hypotheses the next step in the process, is to collect sample data.\nIn this case, we already have the data in ai_jobs. Before moving on to actually testing the hypothesis, let’s take the naive approach - just calculate \\(\\hat{s}\\) the difference between the two groups.\n\n\nCode\nmean_high &lt;- mean(ai_jobs_high$salary_usd)\nmean_low &lt;- mean(ai_jobs_low$salary_usd)\ns_hat &lt;- mean_high - mean_low\n\n\n\\[\n\\hat{s} = \\overline{\\text{salary}}_\\text{high risk} - \\overline{\\text{salary}}_{low risk} = \\$80 k - \\$100k = \\textbf{-20.2k}\n\\]",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#compute-the-test-statistic",
    "href": "Week4/inference-slides.html#compute-the-test-statistic",
    "title": "Using Statistics to make decisions",
    "section": "4. Compute the test statistic",
    "text": "4. Compute the test statistic\nThere are several statistical tests used in Hypothesis Testing. Which one you use depends on what type of hypothesis you are testing and what kind of data you have.\nFor this example, where we are testing the difference in means of a numerical variable (salary_usd) across different groups (automation_risk), we use a test called the t-test.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#compute-the-test-statistic-1",
    "href": "Week4/inference-slides.html#compute-the-test-statistic-1",
    "title": "Using Statistics to make decisions",
    "section": "4. Compute the test statistic",
    "text": "4. Compute the test statistic\n\nFollowing the logic of hypothesis testing, we start from the assumption that the null (\\(H_0\\)) is true and thus \\(s_{true} = 0\\).\nWe look at the evidence to see if we want to reject this null or maintain our assumption that it’s true.\nThe evidence we look for is how far the estimated value \\(\\hat{s}\\) is from zero.\nWe reject \\(H_0\\) if the distance is large (i.e. \\(\\hat{s}\\) is sufficiently greater or lesser than 0)\n\n\nHow far is far enough?\nThe test statistic is the measure of how far the estimated value \\(\\hat{s}\\) is from what its true value would be if \\(H_0\\) is true.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#compute-the-test-statistic-2",
    "href": "Week4/inference-slides.html#compute-the-test-statistic-2",
    "title": "Using Statistics to make decisions",
    "section": "4. Compute the test statistic",
    "text": "4. Compute the test statistic\nt-statistic:\n\\[\nt = \\frac{\\hat{s}}{\\mathrm{SE}(\\hat{s})} = \\frac{\\bar{x}_A - \\bar{x}_B}{\\mathrm{SE}(\\bar{x}_A - \\bar{x}_B)}\n\\]\n\nThe t-test is a procedure to decide whether we can reject the null \\(H_0\\).\nThe magnitude of the t-statistic \\(t\\) measures the distance of \\(\\hat{s}\\) from what \\(s_{true}\\) would be if the null were true.\n\nThe unit of distance is the standard error.\n\nThe t-statistic transforms the original statistic of interest into a standardized version\n\nFor example: if \\(t = 1\\) (or -1), it means \\(\\hat{s}\\) is exactly one standard error away from zero.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#compute-the-test-statistic-3",
    "href": "Week4/inference-slides.html#compute-the-test-statistic-3",
    "title": "Using Statistics to make decisions",
    "section": "4. Compute the test statistic",
    "text": "4. Compute the test statistic\nR makes it very easy to apply a test such as the t-test. For most statistical tests, there exists a simple function to compute it.\nFor the t-test, we use the t.test() function:\n\n\nCode\nt_res &lt;- t.test(ai_jobs_high$salary_usd, ai_jobs_low$salary_usd)\n\n\n\\(t = -7.34\\)",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#making-a-decision",
    "href": "Week4/inference-slides.html#making-a-decision",
    "title": "Using Statistics to make decisions",
    "section": "Making a Decision",
    "text": "Making a Decision\n\nThe following step is making a decision: either rejecting the null or not rejecting it.\nIn hypothesis testing, this decision is based on a clear rule specified in advance.\nWe specify in advance to avoid bias - before looking at the data, we state what it would take to reject the null hypothesis. We follow what the data says, whatever result that may be.\n\n\nA clear rule also makes the decision transparent, which helps avoid biases in the decision. Unfortunately, we humans are often tempted to use evidence to support our pre-existing views or prejudices. If, for example, we think that jobs which are more highly valued by companies (i.e. have a higher salary) we may pay more attention to the evidence that supports that belief than to the evidence against it.\nIn partiular, we may be tempted to say that the estimated \\(\\hat{s}\\) difference is large enough to reject the null, because we believe that the null isn’t true. Clear decision rules are designed to minimize the room for such temptations.\n\n\nOnce you have your test statistic there are two methods to use it to make your decision:\n\nCritical value method\nP-Value method – This is the preferred method we mostly will focus on.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#critical-values",
    "href": "Week4/inference-slides.html#critical-values",
    "title": "Using Statistics to make decisions",
    "section": "Critical Values",
    "text": "Critical Values\nWe use a critical value to tell us whether the test statistic is large enough - is it far enough away from zero to reject the null?\nTo define the critical value, we need to decide how conservative we want to be with the evidence.\nThe larger we set the critical value, the harder it is to reject the null hypothesis.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#critical-values-1",
    "href": "Week4/inference-slides.html#critical-values-1",
    "title": "Using Statistics to make decisions",
    "section": "Critical Values",
    "text": "Critical Values\n\nThe test sampling distribution\nAs with the sampling distribution for means we looked at earlier, our test-statistic \\(t\\) also has a sampling distribution. If we were to sample many times and calculate \\(t\\) for each sample, we would again get a distribution with a specific shape and parameters.\n\n\n\nThe sampling distribution of the test statistic when the null is true.\n\n\nRecall: Approximately 95% of values fall within two standard deviations of the distribution.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#critical-values-2",
    "href": "Week4/inference-slides.html#critical-values-2",
    "title": "Using Statistics to make decisions",
    "section": "Critical Values",
    "text": "Critical Values\n\nPicking a critical value\n\n\n\n\n\nThe sampling distribution of the test statistic when the null is true.\n\n\n\nRecall: Approximately 95% of values fall within two standard deviations of the distribution.\nSince 95% of values fall within 2 SD, if we want to reject the null hypothesis with 95% confidence, then we say that our test statistic must fall outside of 2 SD.\nIn other words, since the units of \\(\\hat{s}\\) are standard deviations: \\(\\hat{s} \\geq \\pm 2\\)",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#critical-values-3",
    "href": "Week4/inference-slides.html#critical-values-3",
    "title": "Using Statistics to make decisions",
    "section": "Critical Values",
    "text": "Critical Values\nA critical value of 2 is standard. However, it is ultimately just a convention. We could choose to set other critical values that correspond to different probabilities. There is not anything inherently special about setting our threshold at 95% vs 90%.\nIf we make the critical value \\(\\pm 1.6\\), the chance of a false positive is 10%.\n\n\n\n\n\n\nDifferent fields have different standards for evidence - for instance, a critical value of 5 (99.994%) is standard in particle physics (referred to as \\(5\\sigma\\)).",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#interpret-our-results",
    "href": "Week4/inference-slides.html#interpret-our-results",
    "title": "Using Statistics to make decisions",
    "section": "Interpret our results",
    "text": "Interpret our results\nSince our test statistic \\(t = -7.5 &lt; -2\\), at a confidence level of 95%, we would have sufficient evidence to reject \\(H_0\\)\nTherefore, we would say:\n\nThe average salary of jobs at high risk of automation is not the same as the average salary of jobs at low risk.\nWe have evidence that higher salary jobs are at less risk of automation than low salary jobs.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#interpret-our-results-1",
    "href": "Week4/inference-slides.html#interpret-our-results-1",
    "title": "Using Statistics to make decisions",
    "section": "Interpret our results",
    "text": "Interpret our results\n\nImportant!\nThis does not inherently mean we accept the alternative hypothesis. We are narrowing the realm of possible answers, but very rarely (perhaps never) are we able to statistically prove a single explanation in one go.\nWe have increased our reasons to believe our hypothesis, but several other possibilities exist.\nScience is then the process of continually investigating our hypothesis and pitting it against new null hypotheses and rejecting them as well.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#p-value-method",
    "href": "Week4/inference-slides.html#p-value-method",
    "title": "Using Statistics to make decisions",
    "section": "P-Value Method",
    "text": "P-Value Method\n\nHopefully, the critical value is fairly intuitive to you now. However, it is not the typical way that statistical results are presented.\nInstead, you will typically see something called a p-value.\n\n\np-value: The probability than an event will occur, assuming the null hypothesis is true.\nThe p-value essentially flips the critical value statement:\n\nInstead of saying a test statistic value &gt; 2 falls outside the 95% bound, we calculate where out test statistic falls in the distribution\n\n\nThe p-value is the probability that the test statistic will be as large, or larger, than we calculate from the data, if the null hypothesis is true. i.e. \\(P(data | H_0)\\).",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#p-value",
    "href": "Week4/inference-slides.html#p-value",
    "title": "Using Statistics to make decisions",
    "section": "P-value",
    "text": "P-value\n\\(p = P(|t| &gt; \\text{critical value})\\)\nBecause the p-value tells us the smallest level of significance at which we can reject the null hypothesis, it summarizes all the information we need to make the decision.\nThis is why the p-value is used - rather than needing to set a critical value and calculate the test statistic, we can instead use just the p-value.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#interpreting-the-p-value",
    "href": "Week4/inference-slides.html#interpreting-the-p-value",
    "title": "Using Statistics to make decisions",
    "section": "Interpreting the P-value",
    "text": "Interpreting the P-value\n\nLike with the critical value, we should set our desired significance level before carrying out the analysis.\nWe then compare our calculated p-value with the significance level. If it is less, we reject the null hypothesis.\nThe significance level (\\(\\alpha\\)) is the probability that a true null hypothesis will be rejected.\nA typical significance level is \\(\\alpha &lt; 0.05\\), which corresponds with a critical value of 2, or a probability of 5%.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#interpreting-the-p-value-1",
    "href": "Week4/inference-slides.html#interpreting-the-p-value-1",
    "title": "Using Statistics to make decisions",
    "section": "Interpreting the P-value",
    "text": "Interpreting the P-value\n\nR outputT distribution\n\n\nAgain, R will provide us with the p-value. Let’s now look at the full output from our t.test():\n\n\nCode\nt.test(ai_jobs_high$salary_usd, ai_jobs_low$salary_usd)\n\n\n\n    Welch Two Sample t-test\n\ndata:  ai_jobs_high$salary_usd and ai_jobs_low$salary_usd\nt = -7.3358, df = 302.91, p-value = 2.031e-12\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -25596.33 -14768.53\nsample estimates:\nmean of x mean of y \n 79966.86 100149.29 \n\n\n\nIf our p-value is smaller than our pre-set significance level (\\(\\alpha\\)), we reject the null hypothesis and can say the result is “statistically significant” at \\(p &lt; 0.05\\).\n\n\n\n\n\nCode\nxpos &lt;- seq(-5, 5, by = 0.01)\n\ndegree &lt;- 280\nypos &lt;- dt(xpos, df = degree)\n\nggplot() +\n  xlim(-8, 8) +\n  geom_function(aes(colour = \"t, df=280\"), fun = dt, args = list(df = 280), linewidth = 1.5) +\n  geom_vline(xintercept = t_res$statistic, color = \"black\", linewidth = 1.5) +\n  xlab(\"Test statistic under the null hypothesis P(data | H0)\") +\n  ylab(\"Frequency\") +\n  ggtitle(\"Student's T distribution\")",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#decision-and-conclusion",
    "href": "Week4/inference-slides.html#decision-and-conclusion",
    "title": "Using Statistics to make decisions",
    "section": "Decision and conclusion",
    "text": "Decision and conclusion\nThe preset \\(\\alpha\\) is the probability of a False Positive error (called a Type I error) - rejecting the null hypothesis when the null hypothesis is true.\nBack to our two possible decisions:\n\nIf \\(\\alpha &gt; \\text{p-value}\\), reject \\(H_0\\).\n\nThe results of the sample are statistically significant.\nWe can say there is sufficient evidence to conclude that \\(H_0\\) is an incorrect believe and that the alternative hypothesis, \\(H_A\\) may be correct.\n\nIf \\(\\alpha &lt; \\text{p-value}\\), fail to reject \\(H_0\\).\n\nThe results of the sample are not significant. There is not sufficient evidence to conclude that the alternative hypothesis \\(H_A\\) may be correct.\n\n\n\nNOTE: When you “do not reject \\(H_0\\)”, it does not mean that you should believe that \\(H_0\\) is true. It simply means that the sample data have failed to provide sufficient evidence to cast serious doubt about the truthfulness of \\(H_0\\).",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#closing---what-does-a-statistically-significant-result-mean",
    "href": "Week4/inference-slides.html#closing---what-does-a-statistically-significant-result-mean",
    "title": "Using Statistics to make decisions",
    "section": "Closing - What does a statistically significant result mean?",
    "text": "Closing - What does a statistically significant result mean?\n\nDoes it mean our result is meaningful or practically important?\n\nEffect sizeSample size\n\n\nNo. There is an essential distinction between statistical significance and practical significance.\nLet’s say we performed an experiment to examine the effect of a particular diet on body weight, which gives a statistically significant effect at \\(p &lt; 0.05\\). This doesn’t tell us how much weight was lost, which we refer to as the effect size.\nWould the loss of 20 grams (i.e. the weight of a few potato chips) be practically significant, even if it were statistically significant?\nWhether a result is practically significant depends on the effect size and the context of the research question. It’s up to the researcher to know whether it is meaningful.\n\n\nAs with the standard error (and a direct result of it), the p-value depends on the sample size. A very large sample size will give a statistically significant result in many cases, even with a very small effect size.\n\n\n\nThe proportion of significant results for a very small change (~20g which is about 0.001 standard deviations) as a function of sample size",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#further-reading",
    "href": "Week4/inference-slides.html#further-reading",
    "title": "Using Statistics to make decisions",
    "section": "Further Reading",
    "text": "Further Reading\nThere are several more topics to understand about p-values which we cannot cover today: - One-sided vs Two-sided t-test. - Are we testing “there is no difference” or are we testing “\\(mean_A &gt; mean_B\\)? - Type I and Type II Errors - False positive vs False negative - Multiple Comparisons - What happens to \\(P(data|H_0\\)) when we run multiple tests on the same data? - How do we control the error rate across our entire family of tests?\n\nThere are also numerous modern critiques of p-values and how they are used and interpreted.\nSee Statistical Thinking Chapter 9 for a detailed discussion.",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  },
  {
    "objectID": "Week4/inference-slides.html#further-reading-1",
    "href": "Week4/inference-slides.html#further-reading-1",
    "title": "Using Statistics to make decisions",
    "section": "Further Reading",
    "text": "Further Reading\n\nPoldrack, Statistical Thinking, Chapter 9\nSignificant Statistics from Virginia Tech: https://pressbooks.lib.vt.edu/introstatistics/chapter/null-and-alternative-hypotheses/\nBekes & Kezdi, Data Analysis for Business, Economics, and Policy, Chapter 6",
    "crumbs": [
      "Week 4",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Using Statistics to make decisions</span>"
    ]
  }
]