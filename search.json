[
  {
    "objectID": "slides.html",
    "href": "slides.html",
    "title": "Lecture Slides",
    "section": "",
    "text": "Lecture Slides\n\n\n\n\n\n\n\n\nData Visualization\n\n\nThe Grammar of Graphics with ggplot2\n\n\n\n\n\n\n\n\n\n\n\nProbability, Sampling, and Experiments\n\n\n\n\n\n\n\n\n\n\n\nStatistical Inference and Hypothesis Testing\n\n\n\n\n\n\n\n\n\n\n\nWeek 5\n\n\nApplying a critical eye to statistical reporting\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes",
    "section": "",
    "text": "Navigate the notes using the menu in the left sidebar or the search in the upper right corner of the page.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Salaries in the AI Job Market\n\n\n\n\n\n\n\n\n\n\n\nAndrew Mitchell\n\n\n\n\n\n\n\n\n\n\n\n\nBayes’ Rule and Learning from Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommunicating Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConditional Probability and Independence\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Visualization\n\n\nThe Grammar of Graphics with ggplot2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDetermining Probabilities\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFormative Assessment: Deceptive Visualization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHypothesis Testing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMisleading Statistics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart 1: Introduction to Probability\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart 2: Statistical Sampling\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProbability Distributions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProbability Rules and Classical Probability\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSampling Distribution of the Mean\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSampling Exercise Review\n\n\n\n\n\n\n\n\n\n\n\nAndrew Mitchell\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Inference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Inference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Inference and Hypothesis Testing\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Data Analysis Workflow\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 3\n\n\nProbability, Sampling, and Experiments\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with Quarto\n\n\n\n\n\n\n\n\n\n\n\nAndrew Mitchell\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes"
    ]
  },
  {
    "objectID": "WeekNew/notes.html",
    "href": "WeekNew/notes.html",
    "title": "This week’s topic",
    "section": "",
    "text": "Slides\n\n Download PDF Slides"
  },
  {
    "objectID": "WeekNew/notes.html#this-weeks-lecture",
    "href": "WeekNew/notes.html#this-weeks-lecture",
    "title": "This week’s topic",
    "section": "",
    "text": "Slides\n\n Download PDF Slides"
  },
  {
    "objectID": "WeekNew/2-content.html",
    "href": "WeekNew/2-content.html",
    "title": "Section 2",
    "section": "",
    "text": "Section 2"
  },
  {
    "objectID": "Week5/sampling-exercise-walkthrough.html",
    "href": "Week5/sampling-exercise-walkthrough.html",
    "title": "Working with Quarto",
    "section": "",
    "text": "Now that we’ve seen how .R files help us save and organize our code, let’s explore how Quarto can make our analysis even better. When we write code in an .R file, we often find ourselves adding comments to explain what each part does. These comments help us remember our thinking and help others understand our code. But what if we could write proper explanations, include our code, and show the results all in one place?\nThis is exactly what Quarto lets us do. Think of it as a document where we can write explanations in normal text, include our R code in special sections called “chunks”, and show the output of that code (like tables and plots) right where we need it. This makes it much easier to explain our analysis to others - or even to ourselves when we come back to it later!\nYou can download the .R file, data file, and source .qmd file for this page here:\n Download Resources \n\nLet’s take the sampling exercise we did in class and see how we can make it clearer using Quarto. We’ll take the same code from our .R file but now we can properly explain what each part does.\nFirst, just like in our .R file, we need to load our tidyverse package. In Quarto, we put R code in special sections marked with three backticks and {r}. These are called “code chunks”:\n\nlibrary(tidyverse)\n\nNow we can input our sample data. Remember these are the measurements we took in class:\n\nsample_1 &lt;- c(83.2, 82.6, 82.6, 82.6, 93.2, 94, 94, 48.5, 33.6, 33.6)\nsample_2 &lt;- c(48.8, 86.5, 67.5, 84.5, 97.6, 92, 60.7, 108, 84.3, 58.5)\nsample_3 &lt;- c(128, 53.7, 70.9, 75.2, 84.9, 91.2, 70.2, 82, 88.8, 82)\n\nI’ve just shown three samples here to keep things clear, but you can add all of your samples just like we did in the .R file. If we ask R to output one of our samples, it will print out within the document:\n\nsample_2\n\n [1]  48.8  86.5  67.5  84.5  97.6  92.0  60.7 108.0  84.3  58.5\n\n\nIf you are viewing the .qmd document itself (i.e. not the rendered version) try running these code blocks by pressing the ‘run’ button (looks like a triangular play button).\nOne nice thing about Quarto is that we can explain our thinking right alongside our code. For instance, we can explain that each sample contains 10 measurements, and we’re storing them in variables named sample_1, sample_2, etc.\nNow let’s calculate the mean for each sample:\n\nmean_1 &lt;- mean(sample_1)\nmean_2 &lt;- mean(sample_2)\nmean_3 &lt;- mean(sample_3)\n\nWe can even show the results right in our text! For example, the mean of our first sample is 72.79 (this is printed by just putting {r} mean_1 inline) This is much nicer than having to print values separately - we can discuss the numbers right where they make sense in our explanation.\nNext, let’s create our table of means just like we did in the .R file:\n\nsample_means &lt;- tibble(mean_values = c(mean_1, mean_2, mean_3))\n\nsample_means\n\n# A tibble: 3 × 1\n  mean_values\n        &lt;dbl&gt;\n1        72.8\n2        78.8\n3        82.7\n\n\nSee how Quarto automatically displays the table? In our .R file, we had to specifically print the table by typing sample_means on a line by itself. Here, Quarto shows us the output of our code automatically.\nFinally, let’s create a histogram of our sample means:\n\nggplot(sample_means, mapping = aes(x = mean_values)) +\n  geom_histogram() +\n  theme_minimal() +\n  labs(\n    title = \"Distribution of Sample Means\",\n    x = \"Mean Value\",\n    y = \"Count\"\n  )\n\n\n\n\n\n\n\nThe plot appears right after the code that created it. This makes it easy to discuss what we see in the plot right afterwards.\n\nYou might be wondering why we’d use Quarto instead of just writing comments in our .R file. Here are a few reasons:\n\nEverything is in one place - your explanations, code, and the results all flow together naturally.\nYou can write proper explanations using text formatting - headings, bullet points, even links to other resources.\nThe output (like tables and plots) appears right where you need it, making it easier to refer to specific results.\nWhen you share your analysis with others, they can see both your code AND your thinking.\n\nWorking in Quarto is very similar to working with .R files, but with some powerful extra features. Let’s look at how to work with a Quarto document:\n\nWhen you’re writing a Quarto document, you can:\n\nClick the “Run” button (play icon) at the top of any code chunk to run just that chunk\nUse the same keyboard shortcuts we learned for .R files\nSee the output right below each chunk as you go\n\nOne of the great things about Quarto is that you can create different types of documents from the same source. When you click “Render” at the top of the document, Quarto will create a final version that combines your code, text, and results.\nBy default, Quarto creates an HTML document that you can open in any web browser. This is great for sharing your analysis because anyone can view it, even if they don’t have R installed. The preview will appear right inside RStudio, making it easy to see how your final document will look.\nYou can also render to other formats like PDF or Word documents. This is really useful when you need to share your analysis in different ways - maybe your professor wants a PDF for an assignment, or your colleague prefers to read documents in Word.\nTo preview your document as you work:\n\nClick the “Render” button (or press Cmd/Ctrl + Shift + K)\nRStudio will show you a preview of your document\nThe preview updates automatically when you render again\n\nThis makes it easy to build your document step by step and see exactly how it will look to others.\n\n\nHint: Just like with .R files, build your analysis step by step. Run each chunk as you write it to make sure it works before moving on. This makes it much easier to find and fix any problems!\nThis workflow lets you develop your analysis gradually, explaining each step as you go. It’s like having a lab notebook where you can record both what you did and why you did it.\nIn the next section, we’ll use these Quarto features to do a more complete analysis of some the AI Jobs data! We’ll look at salary data from the AI job market and see how we can use sampling to understand salary variations across different job roles. This will show you how the sampling concepts we’ve learned can help us answer interesting questions about real data.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Sampling Exercise & Quarto",
      "Working with Quarto"
    ]
  },
  {
    "objectID": "Week5/sampling-exercise-walkthrough.html#converting-our-sampling-exercise",
    "href": "Week5/sampling-exercise-walkthrough.html#converting-our-sampling-exercise",
    "title": "Working with Quarto",
    "section": "",
    "text": "Let’s take the sampling exercise we did in class and see how we can make it clearer using Quarto. We’ll take the same code from our .R file but now we can properly explain what each part does.\nFirst, just like in our .R file, we need to load our tidyverse package. In Quarto, we put R code in special sections marked with three backticks and {r}. These are called “code chunks”:\n\nlibrary(tidyverse)\n\nNow we can input our sample data. Remember these are the measurements we took in class:\n\nsample_1 &lt;- c(83.2, 82.6, 82.6, 82.6, 93.2, 94, 94, 48.5, 33.6, 33.6)\nsample_2 &lt;- c(48.8, 86.5, 67.5, 84.5, 97.6, 92, 60.7, 108, 84.3, 58.5)\nsample_3 &lt;- c(128, 53.7, 70.9, 75.2, 84.9, 91.2, 70.2, 82, 88.8, 82)\n\nI’ve just shown three samples here to keep things clear, but you can add all of your samples just like we did in the .R file. If we ask R to output one of our samples, it will print out within the document:\n\nsample_2\n\n [1]  48.8  86.5  67.5  84.5  97.6  92.0  60.7 108.0  84.3  58.5\n\n\nIf you are viewing the .qmd document itself (i.e. not the rendered version) try running these code blocks by pressing the ‘run’ button (looks like a triangular play button).\nOne nice thing about Quarto is that we can explain our thinking right alongside our code. For instance, we can explain that each sample contains 10 measurements, and we’re storing them in variables named sample_1, sample_2, etc.\nNow let’s calculate the mean for each sample:\n\nmean_1 &lt;- mean(sample_1)\nmean_2 &lt;- mean(sample_2)\nmean_3 &lt;- mean(sample_3)\n\nWe can even show the results right in our text! For example, the mean of our first sample is 72.79 (this is printed by just putting {r} mean_1 inline) This is much nicer than having to print values separately - we can discuss the numbers right where they make sense in our explanation.\nNext, let’s create our table of means just like we did in the .R file:\n\nsample_means &lt;- tibble(mean_values = c(mean_1, mean_2, mean_3))\n\nsample_means\n\n# A tibble: 3 × 1\n  mean_values\n        &lt;dbl&gt;\n1        72.8\n2        78.8\n3        82.7\n\n\nSee how Quarto automatically displays the table? In our .R file, we had to specifically print the table by typing sample_means on a line by itself. Here, Quarto shows us the output of our code automatically.\nFinally, let’s create a histogram of our sample means:\n\nggplot(sample_means, mapping = aes(x = mean_values)) +\n  geom_histogram() +\n  theme_minimal() +\n  labs(\n    title = \"Distribution of Sample Means\",\n    x = \"Mean Value\",\n    y = \"Count\"\n  )\n\n\n\n\n\n\n\nThe plot appears right after the code that created it. This makes it easy to discuss what we see in the plot right afterwards.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Sampling Exercise & Quarto",
      "Working with Quarto"
    ]
  },
  {
    "objectID": "Week5/sampling-exercise-walkthrough.html#why-quarto-makes-life-easier",
    "href": "Week5/sampling-exercise-walkthrough.html#why-quarto-makes-life-easier",
    "title": "Working with Quarto",
    "section": "",
    "text": "You might be wondering why we’d use Quarto instead of just writing comments in our .R file. Here are a few reasons:\n\nEverything is in one place - your explanations, code, and the results all flow together naturally.\nYou can write proper explanations using text formatting - headings, bullet points, even links to other resources.\nThe output (like tables and plots) appears right where you need it, making it easier to refer to specific results.\nWhen you share your analysis with others, they can see both your code AND your thinking.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Sampling Exercise & Quarto",
      "Working with Quarto"
    ]
  },
  {
    "objectID": "Week5/sampling-exercise-walkthrough.html#workflow-in-quarto",
    "href": "Week5/sampling-exercise-walkthrough.html#workflow-in-quarto",
    "title": "Working with Quarto",
    "section": "",
    "text": "Working in Quarto is very similar to working with .R files, but with some powerful extra features. Let’s look at how to work with a Quarto document:\n\nWhen you’re writing a Quarto document, you can:\n\nClick the “Run” button (play icon) at the top of any code chunk to run just that chunk\nUse the same keyboard shortcuts we learned for .R files\nSee the output right below each chunk as you go\n\nOne of the great things about Quarto is that you can create different types of documents from the same source. When you click “Render” at the top of the document, Quarto will create a final version that combines your code, text, and results.\nBy default, Quarto creates an HTML document that you can open in any web browser. This is great for sharing your analysis because anyone can view it, even if they don’t have R installed. The preview will appear right inside RStudio, making it easy to see how your final document will look.\nYou can also render to other formats like PDF or Word documents. This is really useful when you need to share your analysis in different ways - maybe your professor wants a PDF for an assignment, or your colleague prefers to read documents in Word.\nTo preview your document as you work:\n\nClick the “Render” button (or press Cmd/Ctrl + Shift + K)\nRStudio will show you a preview of your document\nThe preview updates automatically when you render again\n\nThis makes it easy to build your document step by step and see exactly how it will look to others.\n\n\nHint: Just like with .R files, build your analysis step by step. Run each chunk as you write it to make sure it works before moving on. This makes it much easier to find and fix any problems!\nThis workflow lets you develop your analysis gradually, explaining each step as you go. It’s like having a lab notebook where you can record both what you did and why you did it.\nIn the next section, we’ll use these Quarto features to do a more complete analysis of some the AI Jobs data! We’ll look at salary data from the AI job market and see how we can use sampling to understand salary variations across different job roles. This will show you how the sampling concepts we’ve learned can help us answer interesting questions about real data.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Sampling Exercise & Quarto",
      "Working with Quarto"
    ]
  },
  {
    "objectID": "Week5/sampling-exercise-extended.html",
    "href": "Week5/sampling-exercise-extended.html",
    "title": "Analyzing Salaries in the AI Job Market",
    "section": "",
    "text": "In our previous exercises, we practiced taking samples and calculating means using some simple measurements. Now let’s apply these sampling concepts to the actual dataset and automate the sampling, rather than manually inputting each sample. We’ll look at salary data from the AI job market and use sampling to understand the variation in salaries across different job roles.\nThe previous pages were about introducing you to how Quarto works - here we’ll show what an actual analysis document might look like.\nYou can look at the actual .qmd file that generated this page here: sampling-exercise-extended.qmd\n\nLoad packages:\n\nlibrary(tidyverse)\n\n\nNow let’s load our AI jobs dataset. This dataset contains information about various jobs in the AI industry, including salaries, job titles, and other interesting information:\n\nai_jobs &lt;- read_csv(\"data/ai_jobs.csv\")\n\n# Let's take a look at what job titles we have\nai_jobs |&gt;\n  count(job_title) |&gt;\n  arrange(desc(n))\n\n# A tibble: 10 × 2\n   job_title                 n\n   &lt;chr&gt;                 &lt;int&gt;\n 1 Data Scientist           62\n 2 HR Manager               57\n 3 Cybersecurity Analyst    55\n 4 UX Designer              54\n 5 AI Researcher            51\n 6 Sales Manager            49\n 7 Marketing Specialist     48\n 8 Operations Manager       44\n 9 Software Engineer        41\n10 Product Manager          39\n\n\nLooking at the output above, we can see we have several different job titles in our dataset. For this analysis, let’s focus on comparing salaries between different levels of automation risk. Our goal is to see whether there is a relationship between salary levels and the risk of a job being automated.\n\n\nHint: When working with real data, it’s good practice to look at your data first before diving into analysis. This helps you understand what you’re working with and spot any potential issues.\n\nInstead of manually recording samples like we did in class, we can use R to take random samples from our dataset. Let’s take 1000 samples of size 50 from the low and high automation risk groups.\n\nWhen looking at this code, try to break it down into its component parts. We are using a for loop, which allows us to repeat a process multiple times. Start by understanding what is happening within the for loop - what are we doing each time?\n\nWe’re using the pipe operate to filter the dataset, then sample it, calculate the mean of the sample, and output the result into a list.\n\nBy repeating this in a for loop, we repeat this process many times, each time adding the new calculated sample mean on to the end of the list.\nNext, understand how we tell the for loop how many times to perform the sampling.\n\nWe define an n_samples variable at the beginning. Then, when starting up the for loop, we tell it to loop from 1 to n_samples: for(1:n_samples) {}.\n\n\n\nsample_size &lt;- 50\nn_samples &lt;- 1000\n\n# Set up an empty list to add to\nhigh_sample_means &lt;- c()\n\nfor (i in 1:n_samples) {\n  # Sample the High automation risk observations\n  mean &lt;- ai_jobs |&gt;\n    filter(automation_risk == \"High\") |&gt;\n    sample_n(sample_size) |&gt;\n    summarise(mean(salary_usd)) |&gt;\n    pull()\n\n  # Add to the sample_means list\n  high_sample_means &lt;- append(high_sample_means, mean)\n}\n\n# Repeat for the Low risk group\nlow_sample_means &lt;- c()\nfor (i in 1:n_samples) {\n  # Sample the High automation risk observations\n  mean &lt;- ai_jobs |&gt;\n    filter(automation_risk == \"Low\") |&gt;\n    sample_n(sample_size) |&gt;\n    summarise(mean(salary_usd)) |&gt;\n    pull()\n\n  # Add to the sample_means list\n  low_sample_means &lt;- append(low_sample_means, mean)\n}\n\nNow we have our sample means! Let’s combine these into a single table:\n\nmeans_table &lt;- tibble(\n  \"High\" = high_sample_means,\n  \"Low\" = low_sample_means,\n)\nmeans_table\n\n# A tibble: 1,000 × 2\n     High     Low\n    &lt;dbl&gt;   &lt;dbl&gt;\n 1 80325.  97347.\n 2 84461. 100727.\n 3 79401. 100062.\n 4 84083.  97947.\n 5 82547. 102260.\n 6 89679.  99635.\n 7 82624. 100586.\n 8 79814.  98631.\n 9 86217.  99474.\n10 81497.  97962.\n# ℹ 990 more rows\n\n\nAnd reshape it into a tidy long table format. This just means we’re stacking the two columns from the table above into one column with a label for which automation risk the sample is from:\n\nmeans_table &lt;- means_table |&gt;\n  gather(key = \"automation_risk\", value = \"sample_mean\")\n\nmeans_table\n\n# A tibble: 2,000 × 2\n   automation_risk sample_mean\n   &lt;chr&gt;                 &lt;dbl&gt;\n 1 High                 80325.\n 2 High                 84461.\n 3 High                 79401.\n 4 High                 84083.\n 5 High                 82547.\n 6 High                 89679.\n 7 High                 82624.\n 8 High                 79814.\n 9 High                 86217.\n10 High                 81497.\n# ℹ 1,990 more rows\n\n\n\nNow we can create a visualization to compare the distribution of sample means between these two job titles.\n\n# Create a plot comparing the distributions\nggplot(means_table, aes(x = sample_mean, fill = automation_risk)) +\n  geom_histogram(alpha = 0.5) +\n  theme_minimal() +\n  labs(\n    title = \"Distribution of Sample Means by Automation Risk\",\n    x = \"Mean Salary (USD)\",\n    y = \"Count\",\n    fill = \"Automation Risk\"\n  )\n\n\n\n\n\n\n\n\nThis visualization shows us how the sample means are distributed for each job title. The overlapping histograms make it easy to compare the two distributions. Another way we could visualise this is with side by side points with error bars:\n\n# Create a plot comparing the distributions\n\n# We need to start by calculating the mean and\n# standard deviation of the sampling distributions\n# to pass to the errorbar layer:\nsummary_stats &lt;- means_table |&gt;\n  group_by(automation_risk) |&gt;\n  summarise(\n    mean = mean(sample_mean),\n    sd = sd(sample_mean)\n  ) |&gt;\n  ggplot(aes(y = mean, x = automation_risk, color = automation_risk)) +\n  geom_point() +\n  geom_errorbar(aes(ymin = mean - (2 * sd), ymax = mean + (2 * sd))) +\n  theme_minimal() +\n  labs(\n    title = \"Distribution of Sample Means by Automation Risk\",\n    x = \"Automation Risk\",\n  ) +\n  scale_y_continuous(name = \"Mean Salary (USD)\", labels = scales::comma)\n\nThis plot shows us quite clearly that the sampling distributions are quite different between the different automation risk groups.\nThink back to our Hypothesis Testing using the t-test. The goal there was to tell whether the difference between the means in the two groups was significantly different. How we define statistically significant is based on the estimated sampling distribution. We estimated this based on just a single sample. Here, we’re directly looking at the the actually sampling distributions (for sample size = 25). Another way to interpret the t-test statistical significance is to look at whether these sampling distributions are far enough apart and have a low enough variance that their standard error bars don’t overlap.\nIn the boxplot, we can see that the external lines of the boxplots (representing 2 standard error) don’t overlap with each other. This would indicate that, with at least 95% confidence (remember, 2 SE encompasses 95% of the distribution), the mean of a given sample of 50 salaries from the High risk group would not overlap with the mean from the Low risk group.\nLet’s translate this into a t-test:\n\n# Start by drawing a new sample of 50 from each group\nhigh_sample &lt;- ai_jobs |&gt;\n  filter(automation_risk == \"High\") |&gt;\n  sample_n(sample_size) |&gt;\n  pull(salary_usd)\n\nlow_sample &lt;- ai_jobs |&gt;\n  filter(automation_risk == \"Low\") |&gt;\n  sample_n(sample_size) |&gt;\n  pull(salary_usd)\n\n# Run the t-test\nt.test(high_sample, low_sample)\n\n\n    Welch Two Sample t-test\n\ndata:  high_sample and low_sample\nt = -2.7435, df = 88.723, p-value = 0.007356\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -24138.721  -3859.707\nsample estimates:\nmean of x mean of y \n 82377.14  96376.36 \n\n\nYes! Our t-test results match up to what our plots showed! This demonstrates how the Hypothesis Testing framework allows us to estimate patterns in the sampling distribution even from a single sample.\n\nThis exercise shows how sampling helps us understand patterns in real-world data. Instead of looking at every single salary in our dataset, we can take samples and use their means to get a good idea of typical salaries in different groups.\nSome key points to notice:\n\nWe used the same basic sampling concepts as in our class exercise, used the power of R to take many simulated samples.\nWe can easily compare different groups (job titles) by taking samples from each group.\nVisualizing our sample means helps us see patterns that might not be obvious just looking at numbers.\nWe looked at the relationship between simulating the sampling distribution to estimating the properties of the sampling distribution to apply in hypothesis testing.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Sampling Exercise & Quarto",
      "Analyzing Salaries in the AI Job Market"
    ]
  },
  {
    "objectID": "Week5/sampling-exercise-extended.html#setting-up-our-analysis",
    "href": "Week5/sampling-exercise-extended.html#setting-up-our-analysis",
    "title": "Analyzing Salaries in the AI Job Market",
    "section": "",
    "text": "Load packages:\n\nlibrary(tidyverse)\n\n\nNow let’s load our AI jobs dataset. This dataset contains information about various jobs in the AI industry, including salaries, job titles, and other interesting information:\n\nai_jobs &lt;- read_csv(\"data/ai_jobs.csv\")\n\n# Let's take a look at what job titles we have\nai_jobs |&gt;\n  count(job_title) |&gt;\n  arrange(desc(n))\n\n# A tibble: 10 × 2\n   job_title                 n\n   &lt;chr&gt;                 &lt;int&gt;\n 1 Data Scientist           62\n 2 HR Manager               57\n 3 Cybersecurity Analyst    55\n 4 UX Designer              54\n 5 AI Researcher            51\n 6 Sales Manager            49\n 7 Marketing Specialist     48\n 8 Operations Manager       44\n 9 Software Engineer        41\n10 Product Manager          39\n\n\nLooking at the output above, we can see we have several different job titles in our dataset. For this analysis, let’s focus on comparing salaries between different levels of automation risk. Our goal is to see whether there is a relationship between salary levels and the risk of a job being automated.\n\n\nHint: When working with real data, it’s good practice to look at your data first before diving into analysis. This helps you understand what you’re working with and spot any potential issues.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Sampling Exercise & Quarto",
      "Analyzing Salaries in the AI Job Market"
    ]
  },
  {
    "objectID": "Week5/sampling-exercise-extended.html#taking-samples-from-our-population",
    "href": "Week5/sampling-exercise-extended.html#taking-samples-from-our-population",
    "title": "Analyzing Salaries in the AI Job Market",
    "section": "",
    "text": "Instead of manually recording samples like we did in class, we can use R to take random samples from our dataset. Let’s take 1000 samples of size 50 from the low and high automation risk groups.\n\nWhen looking at this code, try to break it down into its component parts. We are using a for loop, which allows us to repeat a process multiple times. Start by understanding what is happening within the for loop - what are we doing each time?\n\nWe’re using the pipe operate to filter the dataset, then sample it, calculate the mean of the sample, and output the result into a list.\n\nBy repeating this in a for loop, we repeat this process many times, each time adding the new calculated sample mean on to the end of the list.\nNext, understand how we tell the for loop how many times to perform the sampling.\n\nWe define an n_samples variable at the beginning. Then, when starting up the for loop, we tell it to loop from 1 to n_samples: for(1:n_samples) {}.\n\n\n\nsample_size &lt;- 50\nn_samples &lt;- 1000\n\n# Set up an empty list to add to\nhigh_sample_means &lt;- c()\n\nfor (i in 1:n_samples) {\n  # Sample the High automation risk observations\n  mean &lt;- ai_jobs |&gt;\n    filter(automation_risk == \"High\") |&gt;\n    sample_n(sample_size) |&gt;\n    summarise(mean(salary_usd)) |&gt;\n    pull()\n\n  # Add to the sample_means list\n  high_sample_means &lt;- append(high_sample_means, mean)\n}\n\n# Repeat for the Low risk group\nlow_sample_means &lt;- c()\nfor (i in 1:n_samples) {\n  # Sample the High automation risk observations\n  mean &lt;- ai_jobs |&gt;\n    filter(automation_risk == \"Low\") |&gt;\n    sample_n(sample_size) |&gt;\n    summarise(mean(salary_usd)) |&gt;\n    pull()\n\n  # Add to the sample_means list\n  low_sample_means &lt;- append(low_sample_means, mean)\n}\n\nNow we have our sample means! Let’s combine these into a single table:\n\nmeans_table &lt;- tibble(\n  \"High\" = high_sample_means,\n  \"Low\" = low_sample_means,\n)\nmeans_table\n\n# A tibble: 1,000 × 2\n     High     Low\n    &lt;dbl&gt;   &lt;dbl&gt;\n 1 80325.  97347.\n 2 84461. 100727.\n 3 79401. 100062.\n 4 84083.  97947.\n 5 82547. 102260.\n 6 89679.  99635.\n 7 82624. 100586.\n 8 79814.  98631.\n 9 86217.  99474.\n10 81497.  97962.\n# ℹ 990 more rows\n\n\nAnd reshape it into a tidy long table format. This just means we’re stacking the two columns from the table above into one column with a label for which automation risk the sample is from:\n\nmeans_table &lt;- means_table |&gt;\n  gather(key = \"automation_risk\", value = \"sample_mean\")\n\nmeans_table\n\n# A tibble: 2,000 × 2\n   automation_risk sample_mean\n   &lt;chr&gt;                 &lt;dbl&gt;\n 1 High                 80325.\n 2 High                 84461.\n 3 High                 79401.\n 4 High                 84083.\n 5 High                 82547.\n 6 High                 89679.\n 7 High                 82624.\n 8 High                 79814.\n 9 High                 86217.\n10 High                 81497.\n# ℹ 1,990 more rows",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Sampling Exercise & Quarto",
      "Analyzing Salaries in the AI Job Market"
    ]
  },
  {
    "objectID": "Week5/sampling-exercise-extended.html#visualizing-our-sample-means",
    "href": "Week5/sampling-exercise-extended.html#visualizing-our-sample-means",
    "title": "Analyzing Salaries in the AI Job Market",
    "section": "",
    "text": "Now we can create a visualization to compare the distribution of sample means between these two job titles.\n\n# Create a plot comparing the distributions\nggplot(means_table, aes(x = sample_mean, fill = automation_risk)) +\n  geom_histogram(alpha = 0.5) +\n  theme_minimal() +\n  labs(\n    title = \"Distribution of Sample Means by Automation Risk\",\n    x = \"Mean Salary (USD)\",\n    y = \"Count\",\n    fill = \"Automation Risk\"\n  )\n\n\n\n\n\n\n\n\nThis visualization shows us how the sample means are distributed for each job title. The overlapping histograms make it easy to compare the two distributions. Another way we could visualise this is with side by side points with error bars:\n\n# Create a plot comparing the distributions\n\n# We need to start by calculating the mean and\n# standard deviation of the sampling distributions\n# to pass to the errorbar layer:\nsummary_stats &lt;- means_table |&gt;\n  group_by(automation_risk) |&gt;\n  summarise(\n    mean = mean(sample_mean),\n    sd = sd(sample_mean)\n  ) |&gt;\n  ggplot(aes(y = mean, x = automation_risk, color = automation_risk)) +\n  geom_point() +\n  geom_errorbar(aes(ymin = mean - (2 * sd), ymax = mean + (2 * sd))) +\n  theme_minimal() +\n  labs(\n    title = \"Distribution of Sample Means by Automation Risk\",\n    x = \"Automation Risk\",\n  ) +\n  scale_y_continuous(name = \"Mean Salary (USD)\", labels = scales::comma)\n\nThis plot shows us quite clearly that the sampling distributions are quite different between the different automation risk groups.\nThink back to our Hypothesis Testing using the t-test. The goal there was to tell whether the difference between the means in the two groups was significantly different. How we define statistically significant is based on the estimated sampling distribution. We estimated this based on just a single sample. Here, we’re directly looking at the the actually sampling distributions (for sample size = 25). Another way to interpret the t-test statistical significance is to look at whether these sampling distributions are far enough apart and have a low enough variance that their standard error bars don’t overlap.\nIn the boxplot, we can see that the external lines of the boxplots (representing 2 standard error) don’t overlap with each other. This would indicate that, with at least 95% confidence (remember, 2 SE encompasses 95% of the distribution), the mean of a given sample of 50 salaries from the High risk group would not overlap with the mean from the Low risk group.\nLet’s translate this into a t-test:\n\n# Start by drawing a new sample of 50 from each group\nhigh_sample &lt;- ai_jobs |&gt;\n  filter(automation_risk == \"High\") |&gt;\n  sample_n(sample_size) |&gt;\n  pull(salary_usd)\n\nlow_sample &lt;- ai_jobs |&gt;\n  filter(automation_risk == \"Low\") |&gt;\n  sample_n(sample_size) |&gt;\n  pull(salary_usd)\n\n# Run the t-test\nt.test(high_sample, low_sample)\n\n\n    Welch Two Sample t-test\n\ndata:  high_sample and low_sample\nt = -2.7435, df = 88.723, p-value = 0.007356\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -24138.721  -3859.707\nsample estimates:\nmean of x mean of y \n 82377.14  96376.36 \n\n\nYes! Our t-test results match up to what our plots showed! This demonstrates how the Hypothesis Testing framework allows us to estimate patterns in the sampling distribution even from a single sample.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Sampling Exercise & Quarto",
      "Analyzing Salaries in the AI Job Market"
    ]
  },
  {
    "objectID": "Week5/sampling-exercise-extended.html#what-have-we-learned",
    "href": "Week5/sampling-exercise-extended.html#what-have-we-learned",
    "title": "Analyzing Salaries in the AI Job Market",
    "section": "",
    "text": "This exercise shows how sampling helps us understand patterns in real-world data. Instead of looking at every single salary in our dataset, we can take samples and use their means to get a good idea of typical salaries in different groups.\nSome key points to notice:\n\nWe used the same basic sampling concepts as in our class exercise, used the power of R to take many simulated samples.\nWe can easily compare different groups (job titles) by taking samples from each group.\nVisualizing our sample means helps us see patterns that might not be obvious just looking at numbers.\nWe looked at the relationship between simulating the sampling distribution to estimating the properties of the sampling distribution to apply in hypothesis testing.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Sampling Exercise & Quarto",
      "Analyzing Salaries in the AI Job Market"
    ]
  },
  {
    "objectID": "Week5/lecture.html#misleading-statistics-1",
    "href": "Week5/lecture.html#misleading-statistics-1",
    "title": "Week 5",
    "section": "Misleading Statistics",
    "text": "Misleading Statistics\nWe’ll use a case study of data presented by climate change sceptics to illustrate how even real data and “mathematically correct” statistical analysis can be used to mislead.\nSome types of misleading statistics:\n\ncherry-picking data\novergeneralization\nfaulty causality\nbiased sampling\nmisleading graphs\nreporting non-statistically-significant results as significant\nreporting statistically-significant but not practically-significant results as meaningful\n\nSee Calling Bullshit - The Art of of Skepticism in a Data-Driven World (Bergstrom and West 2021) for more examples."
  },
  {
    "objectID": "Week5/lecture.html#hiatus-in-global-warming",
    "href": "Week5/lecture.html#hiatus-in-global-warming",
    "title": "Week 5",
    "section": "“Hiatus in Global Warming”",
    "text": "“Hiatus in Global Warming”\nIn the 2010’s a claim began to circulate that data showed that “global mean surface temperature \\(T_S\\) has not risen since 1998, and may have fallen since late 2001” (Monckton 2008).\nAlong with some controversies about the source of climate data, this became known as ‘ClimateGate’. Similar claims arose again in 2022, showing an apparent pause in climate change from 2015-2022.\nThese claims were based on data - they presented analyses and visualisations of global temperature data which in fact did appear to show a pause or a decrease in global temperature, apparently disproving anthroprogenic climate change.\nLet’s take a look back at this data with a critical eye and see whether we find them convincing."
  },
  {
    "objectID": "Week5/lecture.html#climate-change-measurements",
    "href": "Week5/lecture.html#climate-change-measurements",
    "title": "Week 5",
    "section": "Climate Change measurements",
    "text": "Climate Change measurements\nWe won’t be getting into the science of climate change here, but it’s good to understand the basic arguments and sources of evidence.\n\n\nTo get a complete picture of Earth’s temperature, scientists combine measurements from the air above land and the ocean surface collected by ships, buoys and sometimes satellites, too.\nThe temperature at each land and ocean station is compared daily to what is ‘normal’ for that location and time, typically the long-term average over a 30-year period. The differences are called an ‘anomalies’ and they help scientists evaluate how temperature is changing over time.\nA ‘positive’ anomaly means the temperature is warmer than the long-term average, a ‘negative’ anomaly means it’s cooler.\n(Pidcock 2015)"
  },
  {
    "objectID": "Week5/lecture.html#climate-change-measurements-1",
    "href": "Week5/lecture.html#climate-change-measurements-1",
    "title": "Week 5",
    "section": "Climate Change measurements",
    "text": "Climate Change measurements\n\nGISS Surface Temperature Analysis. Source: NASA (2025)"
  },
  {
    "objectID": "Week5/lecture.html#the-first-global-warming-pause",
    "href": "Week5/lecture.html#the-first-global-warming-pause",
    "title": "Week 5",
    "section": "The First Global Warming Pause",
    "text": "The First Global Warming Pause\n\nIn Monckton (2008), published in a peer-reviewed journal of The American Physical Society, Christopher Monckton claimed that the mean global temperature data across the four major data sources showed that \\(T_S\\) has not risen between 2001 and 2008.\nHe presented a time series plot which confirms this.\n\n\n\nthe conclusion is that, perhaps, there is no “climate crisis”, and that currently-fashionable efforts by governments to reduce anthropogenic CO2 emissions are pointless, may be ill-conceived, and could even be harmful."
  },
  {
    "objectID": "Week5/lecture.html#the-first-global-warming-pause-1",
    "href": "Week5/lecture.html#the-first-global-warming-pause-1",
    "title": "Week 5",
    "section": "The First Global Warming Pause",
    "text": "The First Global Warming Pause\n\nMean global surface temperature anomalies, 2001-2008. Source: Monckton (2008)"
  },
  {
    "objectID": "Week5/lecture.html#the-new-global-warming-pause",
    "href": "Week5/lecture.html#the-new-global-warming-pause",
    "title": "Week 5",
    "section": "The New Global Warming Pause",
    "text": "The New Global Warming Pause\nNearly a decade later, talk of a pause has re-emerged with claims in the media such as:\n\ncontrary to the dogma which holds that a rise in carbon dioxide inescapably heats up the atmosphere global temperature has embarassingly flatlined for more than seven years even as CO2 levels have risen. (Phillips 2022)\n\nAgain, the claim comes from a blog post written by Christopher Monckton titled The New Pause Lengthens to 7 years 10 months (Monckton 2022). Let’s look in depth at the data used to make this claim."
  },
  {
    "objectID": "Week5/lecture.html#the-new-global-warming-pause-1",
    "href": "Week5/lecture.html#the-new-global-warming-pause-1",
    "title": "Week 5",
    "section": "The New Global Warming Pause",
    "text": "The New Global Warming Pause\n\n\n\n\n“This Pause […] is, as always, not cherry-picked. It is derived from the UAH monthly global mean lower-troposphere temperature anomalies as the period from the earliest month starting with which the least-squares linear-regression trend to the most recent month for which data are available does not exceed zero.” (Monckton 2022)\n\n\n\n\nSo, what is wrong with this presentation? Why might it be misleading?"
  },
  {
    "objectID": "Week5/lecture.html#the-new-global-warming-pause-2",
    "href": "Week5/lecture.html#the-new-global-warming-pause-2",
    "title": "Week 5",
    "section": "The New Global Warming Pause",
    "text": "The New Global Warming Pause\n\n\n\n\n\n\n\nFigure 1: Annual global surface temperature data from ERA5, along with Carbon Brief’s estimate of annual 2022 temperatures based on the first six months of the year and the linear trend over the 2015 to 2022 period. Warming since pre-industrial is calculated using the Berkeley Earth dataset for the period prior to 1979. (Hausfather 2022)"
  },
  {
    "objectID": "Week5/lecture.html#cherrypicking-data",
    "href": "Week5/lecture.html#cherrypicking-data",
    "title": "Week 5",
    "section": "Cherrypicking Data",
    "text": "Cherrypicking Data\nLooking at these eight years in isolation ignores the larger context.\n\n\nA slightly different eight-year period - 2011 to 2018 rather than 2015 to 2022 - would offer the opposite conclusion, namely that global warming had massively accelerated to a rate of 5.6C per century.\n\n\n\nSame as the prior plot, but showing annual global surface temperature data from 2000 and the trend over the 8-year period from 2011 through to 2018. (Hausfather 2022)"
  },
  {
    "objectID": "Week5/lecture.html#cherrypicking-data-1",
    "href": "Week5/lecture.html#cherrypicking-data-1",
    "title": "Week 5",
    "section": "Cherrypicking Data",
    "text": "Cherrypicking Data\nIn reality, both of these are acts of “cherry-picking” - overemphasising short-term variability.\nAlso note that Monckton picks his time periods carefully - the first ‘pause’ is from 2001 to 2008. Next, he shows the data from 2015 to 2022 - so what happened from 2008 to 2015? That is left out."
  },
  {
    "objectID": "Week5/lecture.html#finding-spurious-patterns-within-natural-variance",
    "href": "Week5/lecture.html#finding-spurious-patterns-within-natural-variance",
    "title": "Week 5",
    "section": "Finding spurious patterns within natural variance",
    "text": "Finding spurious patterns within natural variance\nSo the questions we should ask, from a statistics perspective are:\n\n\n\nHow large is the expected variability over any given period?\nDoes the apparent downward trend in the period 2015-2022 fit within this variability, meaning we might just be looking at what is effectively noise?\nOr is the trend large enough to be seen without this random variability?\n\n\n\n\n\n\n\n\nFigure 2: Same as the prior plots, but highlighting the years from 2015 onward compared to the 1979-2022 trend. (Hausfather 2022)"
  },
  {
    "objectID": "Week5/lecture.html#other-types-of-misleading-or-inappropriate-visualisations",
    "href": "Week5/lecture.html#other-types-of-misleading-or-inappropriate-visualisations",
    "title": "Week 5",
    "section": "Other types of misleading or inappropriate visualisations",
    "text": "Other types of misleading or inappropriate visualisations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigures from (Bolton 2023)"
  },
  {
    "objectID": "Week5/lecture.html#concluding-thoughts",
    "href": "Week5/lecture.html#concluding-thoughts",
    "title": "Week 5",
    "section": "Concluding Thoughts",
    "text": "Concluding Thoughts\nWhen presented with a statistical analysis or visualisation, what questions should we ask?\nHow can we make sure we’re thinking about the data critically?"
  },
  {
    "objectID": "Week5/lecture.html#further-reading",
    "href": "Week5/lecture.html#further-reading",
    "title": "Week 5",
    "section": "Further Reading",
    "text": "Further Reading\n(Bolton 2023). How to spot spin and inappropriate use of statistics (Research Briefing No. 4446). UK House of Commons Library.\n(Bolton 2007). Statistical literacy guide: How to read charts (Research Briefing No. SN04445). UK House of Commons Library."
  },
  {
    "objectID": "Week5/lecture.html#references",
    "href": "Week5/lecture.html#references",
    "title": "Week 5",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "Week5/lecture.html#the-process-of-statistical-modeling",
    "href": "Week5/lecture.html#the-process-of-statistical-modeling",
    "title": "Week 5",
    "section": "The process of statistical modeling",
    "text": "The process of statistical modeling\nThere is a set of steps that we generally go through when we want to use our statistical model to test a scientific hypothesis:\n\nSpecify your question of interest\nIdentify or collect the appropriate data\nPrepare the data for analysis\nDetermine the appropriate model\nFit the model to the data\nCriticize the model to make sure it fits properly\nTest hypothesis and quantify effect size\nCommunicate your analysis"
  },
  {
    "objectID": "Week5/lecture.html#data-analysis-workflow",
    "href": "Week5/lecture.html#data-analysis-workflow",
    "title": "Week 5",
    "section": "Data Analysis Workflow",
    "text": "Data Analysis Workflow"
  },
  {
    "objectID": "Week5/lecture.html#import",
    "href": "Week5/lecture.html#import",
    "title": "Week 5",
    "section": "Import",
    "text": "Import\n\n\nThroughout, we have been using the tidyverse library of packages for data analysis.\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.\n\nlibrary(tidyverse)"
  },
  {
    "objectID": "Week5/lecture.html#import-1",
    "href": "Week5/lecture.html#import-1",
    "title": "Week 5",
    "section": "Import",
    "text": "Import\n\nThere are tools for reading data from almost any source:\n\nread_csv(), read_excel(), read_rds(), …\n\nWhen we load a dataset with a tidyverse() function, it will return a tibble\n\n\ndata &lt;- read_csv(\"data/Apple_Emissions/greenhouse_gas_emissions.csv\")"
  },
  {
    "objectID": "Week5/lecture.html#tidy",
    "href": "Week5/lecture.html#tidy",
    "title": "Week 5",
    "section": "Tidy",
    "text": "Tidy\nThe same data can be represented in multiple ways. Here’s the same data organized three different ways:\n\nEach dataset shows the same values of four variables: country, year, population, and number of documented cases of TB (tuberculosis), but each dataset organizes the values in a different way.\n\n\n\n\ntable1\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\ntable3\n\n# A tibble: 6 × 3\n  country      year rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n\n\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583"
  },
  {
    "objectID": "Week5/lecture.html#transform",
    "href": "Week5/lecture.html#transform",
    "title": "Week 5",
    "section": "Transform",
    "text": "Transform\nWe’ve dealt with data transformations quite a bit already. This includes operations like calculating the mean for different groups, or for multiple groups:\n\ndata |&gt;\n  group_by(category) |&gt;\n  summarise(\n    mean_emissions = mean(emissions, na.rm = TRUE),\n  )\n\n# A tibble: 2 × 2\n  category                     mean_emissions\n  &lt;chr&gt;                                 &lt;dbl&gt;\n1 Corporate emissions                  35594.\n2 Product life cycle emissions       5630000"
  },
  {
    "objectID": "Week5/lecture.html#visualize",
    "href": "Week5/lecture.html#visualize",
    "title": "Week 5",
    "section": "Visualize",
    "text": "Visualize\n\ndata |&gt;\n  group_by(category, fiscal_year) |&gt;\n  summarise(emissions = sum(emissions, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = fiscal_year, y = emissions, color = category)) +\n  geom_line()"
  },
  {
    "objectID": "Week5/lecture.html#communicate",
    "href": "Week5/lecture.html#communicate",
    "title": "Week 5",
    "section": "Communicate",
    "text": "Communicate\nThis is where we will dive into using Quarto. Start by downloading the Apple Emissions dataset from Moodle and open RStudio.\nWe’ll go through how to create and write a full analysis in a .qmd file using this dataset.\nRefer to our lecture notes specifically on using Quarto"
  },
  {
    "objectID": "Week5/lecture.html#references-1",
    "href": "Week5/lecture.html#references-1",
    "title": "Week 5",
    "section": "References",
    "text": "References\n\n\n\n\nBergstrom, Carl T., and Jevin D. West. 2021. Calling Bullshit: The Art of Scepticism in a Data-Driven World. First published by Allen Lane. An Allen Lane Book. London, UK USA Canada Ireland Australia: Penguin Books.\n\n\nBolton, Paul. 2007. “Statistical Literacy Guide: How to Read Charts.” Research Briefing SN04445. UK House of Commons Library. https://researchbriefings.files.parliament.uk/documents/SN04445/SN04445.pdf.\n\n\n———. 2023. “How to Spot Spin and Inappropriate Use of Statistics.” Research Briefing 4446. UK House of Commons Library. https://researchbriefings.files.parliament.uk/documents/SN04446/SN04446.pdf.\n\n\nHausfather, Zeke. 2022. “Factcheck: No, Global Warming Has Not ‘Paused’ over the Past Eight Years.” Carbon Brief. July 14, 2022. https://www.carbonbrief.org/factcheck-no-global-warming-has-not-paused-over-the-past-eight-years/.\n\n\nMonckton, Christopher. 2008. “Climate Sensitivity Reconsidered.” Physics and Society 37 (3): 6–19. https://higherlogicdownload.s3.amazonaws.com/APS/a05ec1cf-2e34-4fb3-816e-ea1497930d75/UploadedImages/Newsletter_PDF/july08.pdf.\n\n\n———. 2022. “The New Pause Lengthens to 7 Years 10 Months.” Watts Up With That? July 2, 2022. https://wattsupwiththat.com/2022/07/02/the-new-pause-lengthens-to-7-years-10-months/.\n\n\nNASA. 2025. “GISS Surface Temperature Analysis (V4): Global Maps.” NASA Goddard Institute for Space Studies. 2025. https://data.giss.nasa.gov/gistemp/maps/.\n\n\nPhillips, Melanie. 2022. “Sri Lanka Shows the Danger of Green Dogma.” The Times, July 11, 2022. https://www.thetimes.com/article/sri-lanka-shows-the-danger-of-green-dogma-sf69m752q.\n\n\nPidcock, Roz. 2015. “Explainer: How Do Scientists Measure Global Temperature?” Carbon Brief. January 16, 2015. https://www.carbonbrief.org/explainer-how-do-scientists-measure-global-temperature/.\n\n\nPoldrack, Russell A. 2023. Statistical Thinking. Analyzing Data in an Uncertain World. Princeton: Princeton University Press. https://statsthinking21.github.io/statsthinking21-core-site/.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. Second edition. Beijing ; Sebastopol, CA: O’Reilly."
  },
  {
    "objectID": "Week5/4-assignment.html",
    "href": "Week5/4-assignment.html",
    "title": "Formative Assessment: Deceptive Visualization",
    "section": "",
    "text": "Data visualizations are becoming a key medium for the public to understand news and information. It’s crucial to recognize how the design of a visualization can affect what people understand and remember from the data. In this task, you need to pick a dataset you find interesting and create two static visualizations using the same dataset. The first should be a truthful representation of the data. The second should be a deceptive visualization, designed to deceive the viewer. However, you should avoid clear distortions or leaving out information for this deceptive visualization.\n\n\nYour objective is to create two static (single image) visualizations of a selected dataset. The first visualization should be designed to clearly and sincerely convey insights from the data. In contrast, the second should be crafted to intentionally mislead the viewer, causing them to make incorrect inferences. Additionally, you are required to write a brief explanation, limited to four paragraphs, outlining your design strategy for both visualizations.\nIn this task, an earnest visualization is defined as one that:\n\nIs easily understandable and can be interpreted by the general public.\nUses visual encodings that are suitable and effective for the desired purpose.\nClearly and openly describes any transformations made to the data.\nTransparently communicates the source of the data and any possible biases involved.\n\nConversely, a deceptive visualization typically displays these characteristics:\n\nThe graphical depiction is deliberately unsuitable or deceptive.\nHeadings are crafted to influence the viewer’s understanding in a biased manner.\nThere is intentional manipulation or selective filtering of data or axes to deceive.\nIt’s not transparent about possible bias present in the data.\n\nFor the earnest visualization, your goal is to be as clear and transparent as possible to help viewers answer your intended question. For the deceptive visualization, your goal is to trick the viewer (including the course staff!) into believing that the visualization is legitimate and earnest. It should not be immediately obvious which visualization is trying to be deceptive. Subtle ineffective choices in the design should require close and careful reading to be identified.\nFor the deceptive visualization, misleading strategies are fine but outright lying is not. For example, sketchy, unreliable or untrustworthy input datasets are discouraged, but misleading omission, filtering, or transformation of trustworthy data records is fine. Deliberate lies in the title, axes, labels, or annotations is discouraged, but technically true/relevant but otherwise misleading text in the visualization is fine.\nFor both visualization designs, start by choosing a question you would like to answer. Design your visualization to answer that question either correctly (for the earnest visualization) or incorrectly (for the deceptive visualization). You may choose to address a different question with each visualization. Be sure to document the question as part of the visualization design (e.g., title, subtitle, or caption) and in your assignment write-up.\nYour write-up should contain the following information:\n\nThe specific question each visualization aims to answer.\nA description of your design rationale and important considerations for each visualization.\n\n\n\n\nTo help get you started, this assignment, we’ve provided two possible datasets for you to use, although you’re welcome to select any dataset you prefer. In class we demonstrated the data analysis workflow using a dataset of emissions data from Apple. We also looked at climate change data of the global mean temperature anomaly. Both of these datasets have been provided for you on Moodle and are well suited to use in the assignment. However, you may use any dataset you find interesting, whether it’s one we’ve used in class before, the others listed below, or any dataset you can find.\nYou must use the same dataset for both visualizations, but you may transform the data differently, use additional data variables, or choose to address a different question for each design. These datasets are intentionally chosen to cover politically charged topics for the simple reason that these are typically the types of data where deceptive visualizations may proliferate.\n\n\nYou may use the Apple Emissions dataset we started to look at in class which can be downloaded from Moodle. Find the source here\nA breakdown of Apple’s greenhouse gas emissions from 2015 to 2022 as they aim to reach net zero emissions by 2030. This includes every source of emissions from both their corporate operations and their product life cycle, the carbon footprint of their baseline iPhone in the same period, and normalizing factors like sales, market cap, and employees.\nSome Recommended Analysis\n\nHow much has Apple reduced their emissions from 2015 to 2022?\nHow does this trend compare to their revenue & market cap trend in the same period?\nWhich areas have seen the most improvement? What about the least?\nIs Apple on track to meet their 2030 goal of net zero emissions?\n\n\n\n\n\n\n\nOur World in Data, a non-profit that gathers and analyzes data about global issues, has published data about energy usage for countries (e.g. coal consumption, hydropower consumption, etc.) around the world since 1900. You can download the data here.\n\n\n\nEvery year, the federal government releases large amounts of data on US schools, districts, and colleges. However, this information is scattered across multiple datasets. Urban Institute’s Education Data Explorer tries to fix this issue by putting together data from various sources such as the National Center for Education Statistics’ Common Core of Data (CCD), the Civil Rights Data Collection (CRDC), the US Department of Education’s EDFacts, and IPUMS’ National Historical Geographic Information System (NHGIS) and makes it available as an API. You can download the data by making an API call using the code available on the website or alternatively clicking on the downloads button on the website.\n\n\n\nUNdata brings international statistical databases within easy reach of users through a single-entry point. It is maintained by the Development Data Section of the Development Data and Outreach Branch within the Statistics Division of the Department of Economic and Social Affairs (UN DESA) of the UN Secretariat. You can find the internet usage data here. Feel free to take a look at some of the other datasets made available by UNdata here.\nThis data has the following columns:\n\nRegion/country Code: code representing the country or region.\nRegion or Country Name: Field containing the country name.\nYear: Field containing the year at which the data was collected.\nValue: Field denoting the Percentage of individuals using the internet.\nSource: Field denoting the source of the data.\n\nHere are some other possible sources to consider. You are also free to use data from a source different from those included here. If you have any questions on whether a dataset is appropriate, please ask the course staff ASAP!\n\nCity of San Diego open data\nU.S. Government Open Datasets\nU.S. Census Bureau - Census Datasets\nIPUMS.org - Integrated Census & Survey Data from around the World\nFederal Elections Commission - Campaign Finance & Expenditures\nFederal Aviation Administration - FAA Data & Research\nNOAA Daily Weather - NOAA Daily Global Historical Climatology Network Data\nyelp.com/dataset - Yelp Open Dataset\nfivethirtyeight.com - Data and Code behind the Stories and Interactives\nBuzzfeed News - Open-source data from BuzzFeed’s newsroom",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Formative Assessment: Deceptive Visualization"
    ]
  },
  {
    "objectID": "Week5/4-assignment.html#assignment",
    "href": "Week5/4-assignment.html#assignment",
    "title": "Formative Assessment: Deceptive Visualization",
    "section": "",
    "text": "Your objective is to create two static (single image) visualizations of a selected dataset. The first visualization should be designed to clearly and sincerely convey insights from the data. In contrast, the second should be crafted to intentionally mislead the viewer, causing them to make incorrect inferences. Additionally, you are required to write a brief explanation, limited to four paragraphs, outlining your design strategy for both visualizations.\nIn this task, an earnest visualization is defined as one that:\n\nIs easily understandable and can be interpreted by the general public.\nUses visual encodings that are suitable and effective for the desired purpose.\nClearly and openly describes any transformations made to the data.\nTransparently communicates the source of the data and any possible biases involved.\n\nConversely, a deceptive visualization typically displays these characteristics:\n\nThe graphical depiction is deliberately unsuitable or deceptive.\nHeadings are crafted to influence the viewer’s understanding in a biased manner.\nThere is intentional manipulation or selective filtering of data or axes to deceive.\nIt’s not transparent about possible bias present in the data.\n\nFor the earnest visualization, your goal is to be as clear and transparent as possible to help viewers answer your intended question. For the deceptive visualization, your goal is to trick the viewer (including the course staff!) into believing that the visualization is legitimate and earnest. It should not be immediately obvious which visualization is trying to be deceptive. Subtle ineffective choices in the design should require close and careful reading to be identified.\nFor the deceptive visualization, misleading strategies are fine but outright lying is not. For example, sketchy, unreliable or untrustworthy input datasets are discouraged, but misleading omission, filtering, or transformation of trustworthy data records is fine. Deliberate lies in the title, axes, labels, or annotations is discouraged, but technically true/relevant but otherwise misleading text in the visualization is fine.\nFor both visualization designs, start by choosing a question you would like to answer. Design your visualization to answer that question either correctly (for the earnest visualization) or incorrectly (for the deceptive visualization). You may choose to address a different question with each visualization. Be sure to document the question as part of the visualization design (e.g., title, subtitle, or caption) and in your assignment write-up.\nYour write-up should contain the following information:\n\nThe specific question each visualization aims to answer.\nA description of your design rationale and important considerations for each visualization.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Formative Assessment: Deceptive Visualization"
    ]
  },
  {
    "objectID": "Week5/4-assignment.html#recommended-data-sources",
    "href": "Week5/4-assignment.html#recommended-data-sources",
    "title": "Formative Assessment: Deceptive Visualization",
    "section": "",
    "text": "To help get you started, this assignment, we’ve provided two possible datasets for you to use, although you’re welcome to select any dataset you prefer. In class we demonstrated the data analysis workflow using a dataset of emissions data from Apple. We also looked at climate change data of the global mean temperature anomaly. Both of these datasets have been provided for you on Moodle and are well suited to use in the assignment. However, you may use any dataset you find interesting, whether it’s one we’ve used in class before, the others listed below, or any dataset you can find.\nYou must use the same dataset for both visualizations, but you may transform the data differently, use additional data variables, or choose to address a different question for each design. These datasets are intentionally chosen to cover politically charged topics for the simple reason that these are typically the types of data where deceptive visualizations may proliferate.\n\n\nYou may use the Apple Emissions dataset we started to look at in class which can be downloaded from Moodle. Find the source here\nA breakdown of Apple’s greenhouse gas emissions from 2015 to 2022 as they aim to reach net zero emissions by 2030. This includes every source of emissions from both their corporate operations and their product life cycle, the carbon footprint of their baseline iPhone in the same period, and normalizing factors like sales, market cap, and employees.\nSome Recommended Analysis\n\nHow much has Apple reduced their emissions from 2015 to 2022?\nHow does this trend compare to their revenue & market cap trend in the same period?\nWhich areas have seen the most improvement? What about the least?\nIs Apple on track to meet their 2030 goal of net zero emissions?",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Formative Assessment: Deceptive Visualization"
    ]
  },
  {
    "objectID": "Week5/4-assignment.html#other-data-sources",
    "href": "Week5/4-assignment.html#other-data-sources",
    "title": "Formative Assessment: Deceptive Visualization",
    "section": "",
    "text": "Our World in Data, a non-profit that gathers and analyzes data about global issues, has published data about energy usage for countries (e.g. coal consumption, hydropower consumption, etc.) around the world since 1900. You can download the data here.\n\n\n\nEvery year, the federal government releases large amounts of data on US schools, districts, and colleges. However, this information is scattered across multiple datasets. Urban Institute’s Education Data Explorer tries to fix this issue by putting together data from various sources such as the National Center for Education Statistics’ Common Core of Data (CCD), the Civil Rights Data Collection (CRDC), the US Department of Education’s EDFacts, and IPUMS’ National Historical Geographic Information System (NHGIS) and makes it available as an API. You can download the data by making an API call using the code available on the website or alternatively clicking on the downloads button on the website.\n\n\n\nUNdata brings international statistical databases within easy reach of users through a single-entry point. It is maintained by the Development Data Section of the Development Data and Outreach Branch within the Statistics Division of the Department of Economic and Social Affairs (UN DESA) of the UN Secretariat. You can find the internet usage data here. Feel free to take a look at some of the other datasets made available by UNdata here.\nThis data has the following columns:\n\nRegion/country Code: code representing the country or region.\nRegion or Country Name: Field containing the country name.\nYear: Field containing the year at which the data was collected.\nValue: Field denoting the Percentage of individuals using the internet.\nSource: Field denoting the source of the data.\n\nHere are some other possible sources to consider. You are also free to use data from a source different from those included here. If you have any questions on whether a dataset is appropriate, please ask the course staff ASAP!\n\nCity of San Diego open data\nU.S. Government Open Datasets\nU.S. Census Bureau - Census Datasets\nIPUMS.org - Integrated Census & Survey Data from around the World\nFederal Elections Commission - Campaign Finance & Expenditures\nFederal Aviation Administration - FAA Data & Research\nNOAA Daily Weather - NOAA Daily Global Historical Climatology Network Data\nyelp.com/dataset - Yelp Open Dataset\nfivethirtyeight.com - Data and Code behind the Stories and Interactives\nBuzzfeed News - Open-source data from BuzzFeed’s newsroom",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Formative Assessment: Deceptive Visualization"
    ]
  },
  {
    "objectID": "Week5/1-carbonbrief.html",
    "href": "Week5/1-carbonbrief.html",
    "title": "Misleading Statistics",
    "section": "",
    "text": "Code# setup\nlibrary(tidyverse)\nlibrary(httr)\n\n\nSource: Factcheck: No, global warming has not ‘paused’ over the past eight years, Carbon Brief. [@Hausfather2022Factcheck]\n\nWe’ll use a case study of data presented by climate change sceptics to illustrate how even real data and “mathematically correct” statistical analysis can be used to mislead.\nSome types of misleading statistics:\n\ncherry-picking data\novergeneralization\nfaulty causality\nbiased sampling\nmisleading graphs\nreporting non-statistically-significant results as significant\nreporting statistically-significant but not practically-significant results as meaningful\n\nSee Calling Bullshit - The Art of of Skepticism in a Data-Driven World [@Bergstrom2021Calling] for more examples.\n\nIn the 2010’s a claim began to circulate that data showed that “global mean surface temperature T_S has not risen since 1998, and may have fallen since late 2001” [@Monckton2008Climate].\nAlong with some controversies about the source of climate data, this became known as ‘ClimateGate’. Similar claims arose again in 2022, showing an apparent pause in climate change from 2015-2022.\nThese claims were based on data - they presented analyses and visualisations of global temperature data which in fact did appear to show a pause or a decrease in global temperature, apparently disproving anthroprogenic climate change.\nLet’s take a look back at this data with a critical eye and see whether we find them convincing.\n\nWe won’t be getting into the science of climate change here, but it’s good to understand the basic arguments and sources of evidence.\n\n\nTo get a complete picture of Earth’s temperature, scientists combine measurements from the air above land and the ocean surface collected by ships, buoys and sometimes satellites, too.\nThe temperature at each land and ocean station is compared daily to what is ‘normal’ for that location and time, typically the long-term average over a 30-year period. The differences are called an ‘anomalies’ and they help scientists evaluate how temperature is changing over time.\nA ‘positive’ anomaly means the temperature is warmer than the long-term average, a ‘negative’ anomaly means it’s cooler.\n[@Pidcock2015Explainer]\n\n\n\n\n\nGISS Surface Temperature Analysis. Source: @NASA2025GISS\n\n\n\nIn @Monckton2008Climate, published in a peer-reviewed journal of The American Physical Society, Christopher Monckton claimed that the mean global temperature data across the four major data sources showed that T_S has not risen between 2001 and 2008.\nHe presented a time series plot which confirms this.\n\n\n\nthe conclusion is that, perhaps, there is no “climate crisis”, and that currently-fashionable efforts by governments to reduce anthropogenic CO2 emissions are pointless, may be ill-conceived, and could even be harmful.\n\n\n\n\n\nMean global surface temperature anomalies, 2001-2008. Source: @Monckton2008Climate\n\n\nNearly a decade later, talk of a pause has re-emerged with claims in the media such as:\n\ncontrary to the dogma which holds that a rise in carbon dioxide inescapably heats up the atmosphere global temperature has embarassingly flatlined for more than seven years even as CO2 levels have risen. [@Phillips2022Sri]\n\nAgain, the claim comes from a blog post written by Christopher Monckton titled The New Pause Lengthens to 7 years 10 months [@Monckton2022New]. Let’s look in depth at the data used to make this claim.\n\n\n\n\n“This Pause […] is, as always, not cherry-picked. It is derived from the UAH monthly global mean lower-troposphere temperature anomalies as the period from the earliest month starting with which the least-squares linear-regression trend to the most recent month for which data are available does not exceed zero.” [@Monckton2022New]\n\n\n\nSo, what is wrong with this presentation? Why might it be misleading?\n\n\n\n\n\n\n\n\nFigure 1: Annual global surface temperature data from ERA5, along with Carbon Brief’s estimate of annual 2022 temperatures based on the first six months of the year and the linear trend over the 2015 to 2022 period. Warming since pre-industrial is calculated using the Berkeley Earth dataset for the period prior to 1979. [@Hausfather2022Factcheck]\n\n\n\nLooking at these eight years in isolation ignores the larger context.\n\n\nA slightly different eight-year period - 2011 to 2018 rather than 2015 to 2022 - would offer the opposite conclusion, namely that global warming had massively accelerated to a rate of 5.6C per century.\n\n\n\nSame as the prior plot, but showing annual global surface temperature data from 2000 and the trend over the 8-year period from 2011 through to 2018. [@Hausfather2022Factcheck]\n\n\n\n\nIn reality, both of these are acts of “cherry-picking” - overemphasising short-term variability.\nAlso note that Monckton picks his time periods carefully - the first ‘pause’ is from 2001 to 2008. Next, he shows the data from 2015 to 2022 - so what happened from 2008 to 2015? That is left out.\n\nSo the questions we should ask, from a statistics perspective are:\n\n\n\nHow large is the expected variability over any given period?\nDoes the apparent downward trend in the period 2015-2022 fit within this variability, meaning we might just be looking at what is effectively noise?\nOr is the trend large enough to be seen without this random variability?\n\n\n\n\n\n\n\nFigure 2: Same as the prior plots, but highlighting the years from 2015 onward compared to the 1979-2022 trend. [@Hausfather2022Factcheck]\n\n\n\n\n\n\nThe fluctuations in recent years are well within the range of expected variability, and do not indicate any departure from the long-term warming trend in surface temperatures the world has experienced over the past 50 years.\nThe acceleration started from below the trendline and brought temperatures well above it, while the pause started above the trendline and brought temperatures back down to around what would be expected for 2021 and 2022.\n[@Hausfather2022Factcheck]\n\n\n\n\nZooming out further makes the trend very clear.\nThe ‘pause’ periods fit well within the natural variability. By intentionally focusing in on the periods which decrease effectively by chance over a short period of time, we can make the data appear to show a trend which is not there.\n\n\n\nSame as the prior plots, but including Berkeley Earth data from 1850 through 2021. [@Hausfather2022Factcheck]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigures from [@Bolton2023How]\n\nWhen presented with a statistical analysis or visualisation, what questions should we ask?\nHow can we make sure we’re thinking about the data critically?\n\n[@Bolton2023How]. How to spot spin and inappropriate use of statistics (Research Briefing No. 4446). UK House of Commons Library.\n[@Bolton2007Statistical]. Statistical literacy guide: How to read charts (Research Briefing No. SN04445). UK House of Commons Library.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Misleading Statistics"
    ]
  },
  {
    "objectID": "Week5/1-carbonbrief.html#misleading-statistics-1",
    "href": "Week5/1-carbonbrief.html#misleading-statistics-1",
    "title": "Misleading Statistics",
    "section": "",
    "text": "We’ll use a case study of data presented by climate change sceptics to illustrate how even real data and “mathematically correct” statistical analysis can be used to mislead.\nSome types of misleading statistics:\n\ncherry-picking data\novergeneralization\nfaulty causality\nbiased sampling\nmisleading graphs\nreporting non-statistically-significant results as significant\nreporting statistically-significant but not practically-significant results as meaningful\n\nSee Calling Bullshit - The Art of of Skepticism in a Data-Driven World [@Bergstrom2021Calling] for more examples.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Misleading Statistics"
    ]
  },
  {
    "objectID": "Week5/1-carbonbrief.html#hiatus-in-global-warming",
    "href": "Week5/1-carbonbrief.html#hiatus-in-global-warming",
    "title": "Misleading Statistics",
    "section": "",
    "text": "In the 2010’s a claim began to circulate that data showed that “global mean surface temperature T_S has not risen since 1998, and may have fallen since late 2001” [@Monckton2008Climate].\nAlong with some controversies about the source of climate data, this became known as ‘ClimateGate’. Similar claims arose again in 2022, showing an apparent pause in climate change from 2015-2022.\nThese claims were based on data - they presented analyses and visualisations of global temperature data which in fact did appear to show a pause or a decrease in global temperature, apparently disproving anthroprogenic climate change.\nLet’s take a look back at this data with a critical eye and see whether we find them convincing.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Misleading Statistics"
    ]
  },
  {
    "objectID": "Week5/1-carbonbrief.html#climate-change-measurements",
    "href": "Week5/1-carbonbrief.html#climate-change-measurements",
    "title": "Misleading Statistics",
    "section": "",
    "text": "We won’t be getting into the science of climate change here, but it’s good to understand the basic arguments and sources of evidence.\n\n\nTo get a complete picture of Earth’s temperature, scientists combine measurements from the air above land and the ocean surface collected by ships, buoys and sometimes satellites, too.\nThe temperature at each land and ocean station is compared daily to what is ‘normal’ for that location and time, typically the long-term average over a 30-year period. The differences are called an ‘anomalies’ and they help scientists evaluate how temperature is changing over time.\nA ‘positive’ anomaly means the temperature is warmer than the long-term average, a ‘negative’ anomaly means it’s cooler.\n[@Pidcock2015Explainer]",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Misleading Statistics"
    ]
  },
  {
    "objectID": "Week5/1-carbonbrief.html#climate-change-measurements-1",
    "href": "Week5/1-carbonbrief.html#climate-change-measurements-1",
    "title": "Misleading Statistics",
    "section": "",
    "text": "GISS Surface Temperature Analysis. Source: @NASA2025GISS",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Misleading Statistics"
    ]
  },
  {
    "objectID": "Week5/1-carbonbrief.html#the-first-global-warming-pause",
    "href": "Week5/1-carbonbrief.html#the-first-global-warming-pause",
    "title": "Misleading Statistics",
    "section": "",
    "text": "In @Monckton2008Climate, published in a peer-reviewed journal of The American Physical Society, Christopher Monckton claimed that the mean global temperature data across the four major data sources showed that T_S has not risen between 2001 and 2008.\nHe presented a time series plot which confirms this.\n\n\n\nthe conclusion is that, perhaps, there is no “climate crisis”, and that currently-fashionable efforts by governments to reduce anthropogenic CO2 emissions are pointless, may be ill-conceived, and could even be harmful.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Misleading Statistics"
    ]
  },
  {
    "objectID": "Week5/1-carbonbrief.html#the-first-global-warming-pause-1",
    "href": "Week5/1-carbonbrief.html#the-first-global-warming-pause-1",
    "title": "Misleading Statistics",
    "section": "",
    "text": "Mean global surface temperature anomalies, 2001-2008. Source: @Monckton2008Climate",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Misleading Statistics"
    ]
  },
  {
    "objectID": "Week5/1-carbonbrief.html#the-new-global-warming-pause",
    "href": "Week5/1-carbonbrief.html#the-new-global-warming-pause",
    "title": "Misleading Statistics",
    "section": "",
    "text": "Nearly a decade later, talk of a pause has re-emerged with claims in the media such as:\n\ncontrary to the dogma which holds that a rise in carbon dioxide inescapably heats up the atmosphere global temperature has embarassingly flatlined for more than seven years even as CO2 levels have risen. [@Phillips2022Sri]\n\nAgain, the claim comes from a blog post written by Christopher Monckton titled The New Pause Lengthens to 7 years 10 months [@Monckton2022New]. Let’s look in depth at the data used to make this claim.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Misleading Statistics"
    ]
  },
  {
    "objectID": "Week5/1-carbonbrief.html#the-new-global-warming-pause-1",
    "href": "Week5/1-carbonbrief.html#the-new-global-warming-pause-1",
    "title": "Misleading Statistics",
    "section": "",
    "text": "“This Pause […] is, as always, not cherry-picked. It is derived from the UAH monthly global mean lower-troposphere temperature anomalies as the period from the earliest month starting with which the least-squares linear-regression trend to the most recent month for which data are available does not exceed zero.” [@Monckton2022New]\n\n\n\nSo, what is wrong with this presentation? Why might it be misleading?",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Misleading Statistics"
    ]
  },
  {
    "objectID": "Week5/1-carbonbrief.html#the-new-global-warming-pause-2",
    "href": "Week5/1-carbonbrief.html#the-new-global-warming-pause-2",
    "title": "Misleading Statistics",
    "section": "",
    "text": "Figure 1: Annual global surface temperature data from ERA5, along with Carbon Brief’s estimate of annual 2022 temperatures based on the first six months of the year and the linear trend over the 2015 to 2022 period. Warming since pre-industrial is calculated using the Berkeley Earth dataset for the period prior to 1979. [@Hausfather2022Factcheck]",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Misleading Statistics"
    ]
  },
  {
    "objectID": "Week5/1-carbonbrief.html#cherrypicking-data",
    "href": "Week5/1-carbonbrief.html#cherrypicking-data",
    "title": "Misleading Statistics",
    "section": "",
    "text": "Looking at these eight years in isolation ignores the larger context.\n\n\nA slightly different eight-year period - 2011 to 2018 rather than 2015 to 2022 - would offer the opposite conclusion, namely that global warming had massively accelerated to a rate of 5.6C per century.\n\n\n\nSame as the prior plot, but showing annual global surface temperature data from 2000 and the trend over the 8-year period from 2011 through to 2018. [@Hausfather2022Factcheck]",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Misleading Statistics"
    ]
  },
  {
    "objectID": "Week5/1-carbonbrief.html#cherrypicking-data-1",
    "href": "Week5/1-carbonbrief.html#cherrypicking-data-1",
    "title": "Misleading Statistics",
    "section": "",
    "text": "In reality, both of these are acts of “cherry-picking” - overemphasising short-term variability.\nAlso note that Monckton picks his time periods carefully - the first ‘pause’ is from 2001 to 2008. Next, he shows the data from 2015 to 2022 - so what happened from 2008 to 2015? That is left out.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Misleading Statistics"
    ]
  },
  {
    "objectID": "Week5/1-carbonbrief.html#finding-spurious-patterns-within-natural-variance",
    "href": "Week5/1-carbonbrief.html#finding-spurious-patterns-within-natural-variance",
    "title": "Misleading Statistics",
    "section": "",
    "text": "So the questions we should ask, from a statistics perspective are:\n\n\n\nHow large is the expected variability over any given period?\nDoes the apparent downward trend in the period 2015-2022 fit within this variability, meaning we might just be looking at what is effectively noise?\nOr is the trend large enough to be seen without this random variability?\n\n\n\n\n\n\n\nFigure 2: Same as the prior plots, but highlighting the years from 2015 onward compared to the 1979-2022 trend. [@Hausfather2022Factcheck]\n\n\n\n\n\n\nThe fluctuations in recent years are well within the range of expected variability, and do not indicate any departure from the long-term warming trend in surface temperatures the world has experienced over the past 50 years.\nThe acceleration started from below the trendline and brought temperatures well above it, while the pause started above the trendline and brought temperatures back down to around what would be expected for 2021 and 2022.\n[@Hausfather2022Factcheck]\n\n\n\n\nZooming out further makes the trend very clear.\nThe ‘pause’ periods fit well within the natural variability. By intentionally focusing in on the periods which decrease effectively by chance over a short period of time, we can make the data appear to show a trend which is not there.\n\n\n\nSame as the prior plots, but including Berkeley Earth data from 1850 through 2021. [@Hausfather2022Factcheck]",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Misleading Statistics"
    ]
  },
  {
    "objectID": "Week5/1-carbonbrief.html#other-types-of-misleading-or-inappropriate-visualisations",
    "href": "Week5/1-carbonbrief.html#other-types-of-misleading-or-inappropriate-visualisations",
    "title": "Misleading Statistics",
    "section": "",
    "text": "Figures from [@Bolton2023How]",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Misleading Statistics"
    ]
  },
  {
    "objectID": "Week5/1-carbonbrief.html#concluding-thoughts",
    "href": "Week5/1-carbonbrief.html#concluding-thoughts",
    "title": "Misleading Statistics",
    "section": "",
    "text": "When presented with a statistical analysis or visualisation, what questions should we ask?\nHow can we make sure we’re thinking about the data critically?",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Misleading Statistics"
    ]
  },
  {
    "objectID": "Week5/1-carbonbrief.html#further-reading",
    "href": "Week5/1-carbonbrief.html#further-reading",
    "title": "Misleading Statistics",
    "section": "",
    "text": "[@Bolton2023How]. How to spot spin and inappropriate use of statistics (Research Briefing No. 4446). UK House of Commons Library.\n[@Bolton2007Statistical]. Statistical literacy guide: How to read charts (Research Briefing No. SN04445). UK House of Commons Library.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Misleading Statistics"
    ]
  },
  {
    "objectID": "Week4/lecture.html#overview",
    "href": "Week4/lecture.html#overview",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Overview",
    "text": "Overview\n\nThree major goals of statistics:\nDescribe\nDecide\nPredict"
  },
  {
    "objectID": "Week4/lecture.html#focus-statistical-decision-making",
    "href": "Week4/lecture.html#focus-statistical-decision-making",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Focus: Statistical Decision Making",
    "text": "Focus: Statistical Decision Making\n\nUsing statistics to make decisions about hypotheses\nSpecifically: Null Hypothesis Statistical Testing (NHST)\nEssential for understanding research results\nImportant to understand both uses and limitations"
  },
  {
    "objectID": "Week4/lecture.html#what-well-cover",
    "href": "Week4/lecture.html#what-well-cover",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "What We’ll Cover",
    "text": "What We’ll Cover\n\nIntroduction to NHST\nSteps in hypothesis testing\nTest statistics and distributions\nP-values and their interpretation\nStatistical significance\n\n\nThis chapter introduces the concepts behind using statistics to make decisions – specifically, decisions about whether particular hypotheses are supported by data.\nKey points to emphasize:\n\nNHST is widely used but has limitations\nUnderstanding both uses and criticisms is essential\nFocus on practical application and interpretation"
  },
  {
    "objectID": "Week4/lecture.html#using-statistics-to-make-decisions",
    "href": "Week4/lecture.html#using-statistics-to-make-decisions",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Using Statistics to make decisions",
    "text": "Using Statistics to make decisions\nToday, we’ll discuss the use of statistics to make decisions – in particular, decisions about whether a particular hypothesis is supported by the data. There are three major goals of statistics:\n\nDescribe\nDecide\nPredict\n\nWe’ll cover:\n\nStatistical inference - Using a sample to generalize (or infer) about the population.\nSampling distributions and standard error - What does it actually mean to analyse a sample?\nConfidence interval - How certain are we about our estimate?\nHypothesis Testing - How do we use data to answer a hypothesis and make a decision?"
  },
  {
    "objectID": "Week4/lecture.html#learning-objectives",
    "href": "Week4/lecture.html#learning-objectives",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\n\n\n\nUnderstand point estimation\nApply and interpret the Central Limit Theorem\nConstruct and interpret confidence intervals for means\nUnderstand the behaviour of confidence intervals\nCarry out hypothesis tests for means (t-test)\nUnderstand the probabilities of error in hypypothesis tests\n\n\n\n\n\n\nIf you want to figure out the distribution of the change people carry in their pockets, and your sample is large enough, you will find that the distribution follows certain patterns."
  },
  {
    "objectID": "Week4/lecture.html#statistical-inference-1",
    "href": "Week4/lecture.html#statistical-inference-1",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Statistical Inference",
    "text": "Statistical Inference\nThe goal of statistical inference is to generalise - to make statements about a population based on a sample.\nStatistical inference uses what we know about probability to make our best “guesses” from samples about what we don’t know about the population."
  },
  {
    "objectID": "Week4/lecture.html#statistical-inference-2",
    "href": "Week4/lecture.html#statistical-inference-2",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Statistical Inference",
    "text": "Statistical Inference\nMain forms of statistical inference\n\n\nPoint estimation\n\nUsing sample data to calculate a single statistic as an estimate of an unknown population parameter\nExample: What is the average height of undergraduates at this university? What is the average construction cost of an office building in London? What was it in 2019?\n\nConfidence intervals\n\nAn interval built around a point estimate for an unknown population parameter.\n\nHypothesis testing\n\nA decision making procedure for determining whether sample evidence supports a hypothesis.\n\n\n\n\nThese three examples make up the main forms of statistical inference. However, there are many other forms of statistical inference, such as regression analysis - e.g. How much does building energy use change as occupancy increases?"
  },
  {
    "objectID": "Week4/lecture.html#point-estimation",
    "href": "Week4/lecture.html#point-estimation",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Point Estimation",
    "text": "Point Estimation\n\nSuppose you were trying to determine the mean rent of a two-bedroom apartment in your town. You might look in the classified section of the newspaper, write down several rents listed, and average them together. You would have obtained a point estimate of the true mean. If you are trying to determine the percentage of times you make a basket when shooting a basketball, you might count the number of shots you make and divide that by the number of shots you attempted. In this case, you would have obtained a point estimate for the true proportion.\n\nThe most natural way to estimate features of the population (parameters) is to use the corresponding summary statistic calculated from the sample. Some common point estimates and their corresponding parameters are found in the following table:\n\n\nParameters and Point Estimates\n\n\n\n\n\n\n\nParameter\nMeasure\nStatistic\n\n\n\n\n\\(\\mu\\)\nMean of a single population\n\\(\\bar{x}\\)\n\n\n\\(p\\)\nProportion of a single population\n\\(\\hat{p}\\)\n\n\n\\(\\mu_D\\)\nMean difference of two dependent populations\n\\(\\bar{x}_D\\)\n\n\n\\(\\mu_1 - \\mu_2\\)\nDifference in means of two independent populations\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n\n\n\\(p_1 - p_2\\)\nDifference in proportions of two population\n\\(\\hat{p}_1 - \\hat{p}_2\\)\n\n\n\\(\\sigma^2\\)\nVariance of a single population\n\\(S^2\\)\n\n\n\\(\\sigma\\)\nStandard deviation of a single population\n\\(S\\)"
  },
  {
    "objectID": "Week4/lecture.html#point-estimation-1",
    "href": "Week4/lecture.html#point-estimation-1",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Point Estimation",
    "text": "Point Estimation\n\n\n\n\nParameters and Point Estimates\n\n\nParameter\nStatistic\n\n\n\n\n\\(\\mu\\)\n\\(\\bar{x}\\)\n\n\n\\(p\\)\n\\(\\hat{p}\\)\n\n\n\\(\\mu_1 - \\mu_2\\)\n\\(\\bar{x}_1 - \\bar{x}_2\\)\n\n\n\\(p_1 - p_2\\)\n\\(\\hat{p}_1 - \\hat{p}_2\\)\n\n\n\\(\\sigma^2\\)\n\\(S^2\\)\n\n\n\\(\\sigma\\)\n\\(S\\)\n\n\n\n\n\nSuppose the mean weight of a sample of 60 adults is 173.3 lbs; this sample mean is a point estimate of the population mean weight, \\(\\mu\\).\nRemember: this is one of many samples that we could have taken from the population.\nIf a different random sample of 60 individuals were taken from the same population, the new sample mean would likely be different as a result of sampling variability. While estimates generally vary from one sample to another, the population mean is a fixed value.\n\n\nSuppose a poll suggested the US President’s approval rating is 45%. We would consider 45% to be a point estimate of the approval rating we might see if we collected responses from the entire population. This entire-population response proportion is generally referred to as the parameter of interest. When the parameter is a proportion, it is often denoted by p, and we often refer to the sample proportion as \\(\\hat{p}\\) (pronounced “p-hat”). Unless we collect responses from every individual in the population, p remains unknown, and we use $ as our estimate of p.\nHow would one estimate the difference in average weight between men and women? Suppose a sample of men yields a mean of 185.1 lbs and a sample of women men yields a mean of 162.3 lbs. What is a good point estimate for the difference in these two population means? We will expand on this in following chapters."
  },
  {
    "objectID": "Week4/lecture.html#unbiased-estimation",
    "href": "Week4/lecture.html#unbiased-estimation",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Unbiased Estimation",
    "text": "Unbiased Estimation\n\nSampling variability\nWe have established that different samples yield different statistics due to sampling variability. These statistics have their own distributions, called sampling distributions, that reflect this as a random variable. The sampling distribution of a sample statistic is the distribution of the point estimates based on samples of a fixed size, n, from a certain population. It is useful to think of a particular point estimate as being drawn from a sampling distribution.\nRecall the sample mean weight calculated from a previous sample of 173.3 lbs. Suppose another random sample of 60 participants might produce a different value of x, such as 169.5 lbs. Repeated random sampling could result in additional different values, perhaps 172.1 lbs, 168.5 lbs, and so on. Each sample mean can be thought of as a single observation from a random variable X. The distribution of X is called the sampling distribution of the sample mean, and has its own mean and standard deviation like the random variables discussed previously. We will simulate the concept of a sampling distribution using technology to repeatedly sample, calculate statistics, and graph them. However, the actual sampling distribution would only be attainable if we could theoretically take an infinite amount of samples.\nEach of the point estimates in the table above have their own unique sampling distributions which we will look at in the future\n\n\nWhat makes a statistical estimate of this parameter of interest a “Good” one? It must be both accurate and precise.\nAlthough variability in samples is present, there remains a fixed value for any population parameter.\n\nAccording to the law of large numbers, probabilities converge to what we expect over time.\nPoint estimates follow this rule, becoming more accurate with increasing sample size."
  },
  {
    "objectID": "Week4/lecture.html#example-dataset---nhanes",
    "href": "Week4/lecture.html#example-dataset---nhanes",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Example Dataset - NHANES",
    "text": "Example Dataset - NHANES\nNational Health and Nutrition Examination Survey (NHANES) from the US Centers for Disease Control (CDC)\n\n\n\n\n\n\n\n\nNHANES Dataset\n\n\n\nSurveyYr\nGender\nAge\nRace1\nEducation\nWeight\nHeight\nPulse\nDiabetes\n\n\n\n\n1\n2009_10\nmale\n34\nWhite\nHigh School\n87.4\n164.7\n70\nNo\n\n\n2\n2009_10\nfemale\n49\nWhite\nSome College\n86.7\n168.4\n86\nNo\n\n\n3\n2009_10\nfemale\n45\nWhite\nCollege Grad\n75.7\n166.7\n62\nNo\n\n\n4\n2009_10\nmale\n66\nWhite\nSome College\n68.0\n169.5\n60\nNo\n\n\n5\n2009_10\nmale\n58\nWhite\nCollege Grad\n78.4\n181.9\n62\nNo\n\n\n6..4785\n\n\n\n\n\n\n\n\n\n\n\n4786\n2011_12\nmale\n60\nWhite\nCollege Grad\n78.4\n168.8\n76\nYes"
  },
  {
    "objectID": "Week4/lecture.html#unbiased-estimation-1",
    "href": "Week4/lecture.html#unbiased-estimation-1",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Unbiased Estimation",
    "text": "Unbiased Estimation\n\n\nThe accuracy of an estimate refers to how well it estimates the actual value of that parameter. Mathematically, this is true when that the expected value your statistic is equal to the value of that parameter. This can be visualized as the center of the sampling distribution appearing to be situated at the value of that parameter.\nAccording to the law of large numbers, probabilities converge to what we expect over time. Point estimates follow this rule, becoming more accurate with increasing sample size. The figure above shows the sample mean weight calculated for random samples drawn, where sample size increases by 1 for each draw until sample size equals 500. The maroon dashed horizontal line is drawn at the average weight of all adults 169.7 lbs, which represents the population mean weight according to the CDC.\nThe figure above shows the sample mean weight calculated for random samples drawn, where sample size increases by 1 for each draw until sample size equals 500. The maroon dashed horizontal line is drawn at the average weight of all adults 169.7 lbs, which represents the population mean weight according to the CDC.\nNote how a sample size around 50 may produce a sample mean that is as much as 10 lbs higher or lower than the population mean. As sample size increases, the fluctuations around the population mean decrease; in other words, as sample size increases, the sample mean becomes less variable and provides a more reliable estimate of the population mean.\nIn addition to accuracy, a precise estimate is also more useful. This means when repeatedly sampling, the values of the statistics seem pretty close together. The precision of an estimate can be visualized as the spread of the sampling distribution, usually quantified by the standard deviation. The phrase “the standard deviation of a sampling distribution” is often shortened to the standard error. A smaller standard error means a more precise estimate and is also effected by sample size."
  },
  {
    "objectID": "Week4/lecture.html#central-limit-theorem",
    "href": "Week4/lecture.html#central-limit-theorem",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\n\nThe central limit theorem (CLT) is one of the most powerful and useful ideas in all of statistics. There are two alternative forms of the theorem, and both alternatives are concerned with drawing finite samples size n from a population with a known mean, \\(\\mu\\), and a known standard deviation, \\(\\sigma\\). The first alternative says that if we collect samples of size n with a “large enough n,” then the resulting distribution can be approximated by the normal distribution.\nApplying the law of large numbers here, we could say that if you take larger and larger samples from a population, then the mean {x} of the sample tends to get closer and closer to \\(\\mu\\). From the central limit theorem, we know that as n gets larger and larger, the sample means follow a normal distribution. The larger n gets, the smaller the standard deviation gets. (Remember that the standard deviation for {x} is .) This means that the sample mean {x} must be close to the population mean \\(\\mu\\). We can say that \\(\\mu\\) is the value that the sample means approach as n gets larger. The central limit theorem illustrates the law of large numbers.\nThe size of the sample, n, that is required in order to be “large enough” depends on the original population from which the samples are drawn (the sample size should be at least 30 or the data should come from a normal distribution). If the original population is far from normal, then more observations are needed for the sample means or sums to be normal. Sampling is done with replacement.\n\nThe CLT means says that if you keep drawing larger and larger samples and calculating their means, the sample means form their own normal distribution (the sampling distribution).\nThe sampling distribution of the mean is generated by repeated sampling from the same population and recording the sample mean per sample. This forms a distribution of different means, and this distribution has its own mean and variance.\nThe normal distribution has the same mean as the original distribution and a variance that equals the original variance divided by the sample size."
  },
  {
    "objectID": "Week4/lecture.html#drawing-samples-of-peoples-weight-from-the-nhanes-dataset.",
    "href": "Week4/lecture.html#drawing-samples-of-peoples-weight-from-the-nhanes-dataset.",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Drawing samples of people’s weight from the NHANES dataset.",
    "text": "Drawing samples of people’s weight from the NHANES dataset.\n\nWe have established that different samples yield different statistics due to sampling variability . These statistics have their own distributions, called sampling distributions, that reflect this as a random variable. The sampling distribution of a sample statistic is the distribution of the point estimates based on samples of a fixed size, n, from a certain population. It is useful to think of a particular point estimate as being drawn from a sampling distribution.\nRecall the sample mean weight calculated from a previous sample of 173.3 lbs. Suppose another random sample of 60 participants might produce a different value of x, such as 169.5 lbs. Repeated random sampling could result in additional different values, perhaps 172.1 lbs, 168.5 lbs, and so on. Each sample mean can be thought of as a single observation from a random variable X. The distribution of X is called the sampling distribution of the sample mean, and has its own mean and standard deviation like the random variables discussed previously. We will simulate the concept of a sampling distribution using technology to repeatedly sample, calculate statistics, and graph them. However, the actual sampling distribution would only be attainable if we could theoretically take an infinite amount of samples.\nEach of the point estimates in the table above have their own unique sampling distributions which we will look at in the future\n\n\n\nWe are drawing a random sample of people from the dataset and calculating the mean weight for that sample. Sample size is the number of data points we pull. We then repeat this 5000 times (n_samples) to build up the sampling distribution."
  },
  {
    "objectID": "Week4/lecture.html#standard-error",
    "href": "Week4/lecture.html#standard-error",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Standard Error",
    "text": "Standard Error\nA sampling distribution is what we get by simulating multiple samples (of sample size \\(n\\)) from a population.\nRecall: The Standard Error is the standard deviation of the sampling distribution.\n\\[\nSEM = \\sigma_{\\bar{x}\\ (means)}\n\\]"
  },
  {
    "objectID": "Week4/lecture.html#standard-error-1",
    "href": "Week4/lecture.html#standard-error-1",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Standard Error",
    "text": "Standard Error\nA sampling distribution is a probability distribution of a statistic at a given sample size.\nRecall: The Standard Error is the standard deviation of the sampling distribution. This is also equal to the standard deviation \\(\\sigma\\) of the population divided by the square root of the sample size.\n\\[\nSEM = \\sigma_{\\bar{x}\\ (means)} = \\frac{\\sigma}{\\sqrt{n}} \\approx \\frac{\\sigma_{x}}{\\sqrt{n}} \\left[ i.e. \\frac{\\text{Est. Std Dev of the sample}}{\\sqrt{\\text{Sample size}}} \\right]\n\\]\n\nIn other words:\n\nIf you draw random samples of size \\(n\\), the distribution of the random variable \\(\\bar{X}\\), which consists of sample means, is called the sampling distribution of the sample mean. The sampling distribution of the mean approaches a normal distribution as \\(n\\), the sample size, increases.\n\n\n\nIn the SEM formula, remember the sampling distribution is the distribution of multiple means - not the distribution of our sample.\nQuote from https://pressbooks.lib.vt.edu/introstatistics/chapter/the-central-limit-theorem-for-sample-means-averages/"
  },
  {
    "objectID": "Week4/lecture.html#standard-error-2",
    "href": "Week4/lecture.html#standard-error-2",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Standard Error",
    "text": "Standard Error\nKey Takeaways\n\nA sampling distribution is what we get by simulating multiple samples from a population.\nThe Standard Error is the standard deviation \\(\\sigma_{\\bar{x}}\\) of the sampling distribution.\nThe SE decreases as the sample size \\(n\\) increases.\nBecause of this relationship - we can estimate the SE from a single sample \\(\\frac{\\sigma_x}{\\sqrt{n}}\\)\n\n\n\\[\nSEM = \\sigma_{\\bar{x}\\ (means)} = \\frac{\\sigma}{\\sqrt{n}} \\approx \\frac{\\sigma_{x}}{\\sqrt{n}} \\left[ i.e. \\frac{\\text{Est. Std Dev of the sample}}{\\sqrt{\\text{Sample size}}} \\right]\n\\]"
  },
  {
    "objectID": "Week4/lecture.html#exercise-dataset",
    "href": "Week4/lecture.html#exercise-dataset",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Exercise Dataset",
    "text": "Exercise Dataset\n\nData OverviewData Preview\n\n\nThe “AI-Powered Job Market Insights” dataset provides a snapshot of the modern job market, particularly focusing on the role of artificial intelligence (AI) and automation across various industries.\nThis dataset includes 500 unique job listings, each characterized by different factors like industry, company size, AI adoption level, automation risk, required skills, and job growth projections.\n\n\n\n\n\n\n\n\n\n\nAI-Powered Job Market Insights\n\n\n\njob_title\nindustry\ncompany_size\nlocation\nai_adoption_level\nautomation_risk\nrequired_skills\nsalary_usd\nremote_friendly\njob_growth_projection\n\n\n\n\n1\nCybersecurity Analyst\nEntertainment\nSmall\nDubai\nMedium\nHigh\nUX/UI Design\n111392.17\nYes\nGrowth\n\n\n2\nMarketing Specialist\nTechnology\nLarge\nSingapore\nMedium\nHigh\nMarketing\n73792.56\nNo\nDecline\n\n\n3\nAI Researcher\nTechnology\nLarge\nSingapore\nMedium\nLow\nUX/UI Design\n137170.26\nYes\nGrowth\n\n\n4\nSales Manager\nRetail\nSmall\nBerlin\nLow\nMedium\nProject Management\n83027.95\nNo\nGrowth\n\n\n5\nCybersecurity Analyst\nEntertainment\nSmall\nTokyo\nLow\nLow\nJavaScript\n87752.92\nYes\nDecline\n\n\n6..499\n\n\n\n\n\n\n\n\n\n\n\n\n500\nHR Manager\nEntertainment\nMedium\nBerlin\nMedium\nHigh\nProject Management\n53764.38\nYes\nDecline\n\n\n\nSource: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\n\n\n\n\n\n\n\n\n\n\n\n\nTo simplify our later code, I have created a separate table which is already filtered for the groups we will be looking at:\n\nai_jobs_risk &lt;- ai_jobs |&gt;\n  filter(automation_risk %in% c(\"Low\", \"High\"))\n\nai_jobs_high &lt;- ai_jobs_risk |&gt;\n  filter(automation_risk == \"High\")\n\nai_jobs_low &lt;- ai_jobs_risk |&gt;\n  filter(automation_risk == \"Low\")\n\nDataset Features:\nSource: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\n\nJob_Title:\n\nDescription: The title of the job role.\nType: Categorical\nExample Values: “Data Scientist”, “Software Engineer”, “HR Manager”\n\nIndustry:\n\nDescription: The industry in which the job is located.\nType: Categorical\nExample Values: “Technology”, “Healthcare”, “Finance”\n\nCompany_Size:\n\nDescription: The size of the company offering the job.\nType: Ordinal\nCategories: “Small”, “Medium”, “Large”\n\nLocation:\n\nDescription: The geographic location of the job.\nType: Categorical\nExample Values: “New York”, “San Francisco”, “London”\n\nAI_Adoption_Level:\n\nDescription: The extent to which the company has adopted AI in its operations.\nType: Ordinal\nCategories: “Low”, “Medium”, “High”\n\nAutomation_Risk:\n\nDescription: The estimated risk that the job could be automated within the next 10 years.\nType: Ordinal\nCategories: “Low”, “Medium”, “High”\n\nRequired_Skills:\n\nDescription: The key skills required for the job role.\nType: Categorical\nExample Values: “Python”, “Data Analysis”, “Project Management”\n\nSalary_USD:\n\nDescription: The annual salary offered for the job in USD.\nType: Numerical\nValue Range: $30,000 - $200,000\n\nRemote_Friendly:\n\nDescription: Indicates whether the job can be performed remotely.\nType: Categorical\nCategories: “Yes”, “No”\n\nJob_Growth_Projection:\n\nDescription: The projected growth or decline of the job role over the next five years.\nType: Categorical\nCategories: “Decline”, “Stable”, “Growth”"
  },
  {
    "objectID": "Week4/lecture.html#exercise---mystery-bags",
    "href": "Week4/lecture.html#exercise---mystery-bags",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Exercise - Mystery bags",
    "text": "Exercise - Mystery bags\nTo begin, split into three groups. Decide on the following roles, one per person:\n\nSampler - draws samples from the population\nRecording - records the draws in R\n\nYour group has been presented with a population model (bag containing salaries). There may be multiple populations represented in the various bags around the room.\nPlease DO NOT look in the bag OR empty the contents of the bag."
  },
  {
    "objectID": "Week4/lecture.html#exercise---mystery-bags-nonincremental",
    "href": "Week4/lecture.html#exercise---mystery-bags-nonincremental",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Exercise - Mystery bags {nonincremental}",
    "text": "Exercise - Mystery bags {nonincremental}\nThe task\n\nThe Sampler draws 10 slips from the bag - this is your sample of size 5.\nThe Recorder records the values on the slips into the sampling-exercise.R file.\n\nCreate a new list for each sample (sample_1 &lt;- c(1, 2, 3, 2, 2))\n\nCalculate the mean for the new sample and add it to the sample_means table.\nRun the code to plot the histogram of your samples.\nReturn the slips to the bag.\nEnsure the data is well mixed between samples.\nRepeat"
  },
  {
    "objectID": "Week4/lecture.html#statistical-inference-4",
    "href": "Week4/lecture.html#statistical-inference-4",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Statistical Inference",
    "text": "Statistical Inference\nUsing a sample to generalize (or infer) about the population.\n\nWe know how to make a point estimate of a population - what else do we need in order to make a decision?\nHow confident are we that our estimate can generalize to the rest of the population?\nWe need to determine the uncertainty in our estimate\n\n\nWe use inferential statistics to make generalizations about an unknown population. The simplest way of doing this is to use the sample data help us to make a point estimate of a population parameter. We realize that due to sampling variability the point estimate is most likely not the exact value of the population parameter, but should be close to it. After calculating point estimates, we can build off of them to construct interval estimates, called confidence intervals."
  },
  {
    "objectID": "Week4/lecture.html#confidence-intervals",
    "href": "Week4/lecture.html#confidence-intervals",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nA confidence interval is another type of estimate, but instead of being just one number, it is an interval of numbers\n\nProvides a range a range of reasonable values where we expect the true population parameter to fall.\nPoint estimate (statistic) has some variability and uncertainty since we estimate it based on a sample.\nWe want to quantify and communicate this uncertainty.\n\n\nThere is no guarantee that a given confidence interval does capture the parameter, but there is a predictable probability of success. It is important to keep in mind that the confidence interval itself is a random variable, while the population parameter is fixed."
  },
  {
    "objectID": "Week4/lecture.html#business-example",
    "href": "Week4/lecture.html#business-example",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Business Example",
    "text": "Business Example\nAverage streams per month\nYou work in the marketing department of a music company. You want to know the mean number of songs a consumer streams per month.\n\nYou conduct a survey of 100 customers and calculate the sample mean (\\(\\bar{x}\\)) and use it as the point estimate for the population mean (\\(\\mu\\))\nSuppose we know that the standard deviation \\(\\sigma = 100\\).\nFollowing the Central Limit Theorem, the Standard Error is:\n\n\n\\[\n\\frac{\\sigma}{\\sqrt{n}} = \\frac{100}{\\sqrt{100}} = 10\n\\]\n\n\nIf you worked in the marketing department of an entertainment company, you might be interested in the mean number of songs a consumer downloads a month from iTunes. If so, you could conduct a survey and calculate the sample mean, \\(\\bar{x}\\). You would use \\(\\bar{x}\\) to estimate the population mean. The sample mean, \\(\\bar{x}\\), is the point estimate for the population mean, μ.\nSuppose, for the iTunes example, we do not know the population mean μ, but we do know that the population standard deviation is \\(\\sigma = 100\\) and our sample size is 100. Then, by the central limit theorem, the standard deviation for the sample mean is \\(\\frac{\\sigma }{\\sqrt{n}}=\\frac{100}{\\sqrt{100}}=10\\)."
  },
  {
    "objectID": "Week4/lecture.html#what-is-the-probability-of-sampling-a-certain-mean-value",
    "href": "Week4/lecture.html#what-is-the-probability-of-sampling-a-certain-mean-value",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "What is the probability of sampling a certain mean value?",
    "text": "What is the probability of sampling a certain mean value?\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Empirical Rule says that in approximately 95% of the samples, the sample mean, \\(\\bar{x}\\), will be within two standard deviations of the population mean \\(\\mu\\) .\nFor our example, two standard deviations is \\((2)(10) = 20\\). The sample mean \\(\\bar{x}\\) is likely to be within 20 units of \\(\\mu\\).\nBecause \\(\\bar{x}\\) is within 20 units of \\(\\mu\\), which is unknown, then \\(\\mu\\) is likely to be within 20 units of \\(\\bar{x}\\) in 95% of the samples.\n\n\n\nBecause \\(\\bar{x}\\) is within 0.2 units of μ, which is unknown, then μ is likely to be within 0.2 units of \\(\\bar{x}\\) in 95% of the samples. The population mean μ is contained in an interval whose lower number is calculated by taking the sample mean and subtracting two standard deviations (2)(0.1) and whose upper number is calculated by taking the sample mean and adding two standard deviations. In other words, μ is between \\(\\bar{x}\\text{ }-\\text{ 0}\\text{.2}\\) and \\(\\bar{x}\\text{ }+\\text{ 0}\\text{.2}\\) in 95% of all the samples."
  },
  {
    "objectID": "Week4/lecture.html#calculate-the-confidence-interval",
    "href": "Week4/lecture.html#calculate-the-confidence-interval",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Calculate the Confidence Interval",
    "text": "Calculate the Confidence Interval\nWe want to calculate the range of values which the true mean is likely to fall within 95% of the time, given our sample.\n\nFor the streaming example, suppose that a sample produced a sample mean \\(\\bar{x} = 200\\). Then the unknown population mean \\(\\mu\\) is between \\(\\bar{x}-20=200-20=180\\) and \\(\\bar{x}+20=200+20=220\\) songs per month.\n\n\nWe can say that we are about 95% confident that the unknown population mean number of songs streamed per month is between 180 and 220. The approximate 95% confidence interval is (1.8, 2.2).\n\n\nConfidence Interval: (Point Estimate \\(\\pm\\) Margin of error) = \\(200 \\pm 20 \\text{ songs}\\)"
  },
  {
    "objectID": "Week4/lecture.html#calculate-the-confidence-interval-1",
    "href": "Week4/lecture.html#calculate-the-confidence-interval-1",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Calculate the Confidence Interval",
    "text": "Calculate the Confidence Interval\n\nBased on our sample, we can say two things. Either:\n\nThe interval (180, 220) contains the true mean \\(\\mu\\), or…\nOur sample prodcued an \\(\\bar{x}\\) that is not within 20 units of the true mean \\(\\mu\\). This would only happen for 5% of the samples.\n\n\n\nWe can say that we are about 95% confident that the unknown population mean number of songs downloaded from iTunes per month is between 1.8 and 2.2. The approximate 95% confidence interval is (1.8, 2.2). This approximate 95% confidence interval implies two possibilities. Either the interval (1.8, 2.2) contains the true mean μ or our sample produced an that is not within 0.2 units of the true mean μ. The second possibility happens for only 5% of all the samples (95–100%).\nRemember that a confidence intervals are created for an unknown population parameter. Confidence intervals for most parameters have the form: (Point Estimate ± Margin of Error) = (Point Estimate – Margin of Error, Point Estimate + Margin of Error) The margin of error (MoE) depends on the confidence level or percentage of confidence and the standard error of the mean. When you read newspapers and journals, some reports will use the phrase “margin of error.” Other reports will not use that phrase, but include a confidence interval as the point estimate plus or minus the margin of error. These are two ways of expressing the same concept. A confidence interval for a population mean with a known standard deviation is based on the fact that the sample means follow an approximately normal distribution. Suppose that our sample has a mean of and we have constructed the 90% confidence interval (5, 15) where MoE = 5."
  },
  {
    "objectID": "Week4/lecture.html#communicating-confidence-intervals",
    "href": "Week4/lecture.html#communicating-confidence-intervals",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Communicating Confidence Intervals",
    "text": "Communicating Confidence Intervals\n\nThe interpretation should clearly state the confidence level (CL), explain what population parameter is being estimated (here the population mean), and state the confidence interval (both endpoints).\n“We can be _______ % confident that the interval we created, _______ to ________ captures the true population mean (include the context of the problem and appropriate units).”\nWe state the range within which our evidence indicates contains the population mean."
  },
  {
    "objectID": "Week4/lecture.html#case-study",
    "href": "Week4/lecture.html#case-study",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Case Study",
    "text": "Case Study\n\nData OverviewData Preview\n\n\nThe “AI-Powered Job Market Insights” dataset provides a snapshot of the modern job market, particularly focusing on the role of artificial intelligence (AI) and automation across various industries.\nThis dataset includes 500 unique job listings, each characterized by different factors like industry, company size, AI adoption level, automation risk, required skills, and job growth projections.\n\n\n\n\n\n\n\n\n\n\nAI-Powered Job Market Insights\n\n\n\njob_title\nindustry\ncompany_size\nlocation\nai_adoption_level\nautomation_risk\nrequired_skills\nsalary_usd\nremote_friendly\njob_growth_projection\n\n\n\n\n1\nCybersecurity Analyst\nEntertainment\nSmall\nDubai\nMedium\nHigh\nUX/UI Design\n111392.17\nYes\nGrowth\n\n\n2\nMarketing Specialist\nTechnology\nLarge\nSingapore\nMedium\nHigh\nMarketing\n73792.56\nNo\nDecline\n\n\n3\nAI Researcher\nTechnology\nLarge\nSingapore\nMedium\nLow\nUX/UI Design\n137170.26\nYes\nGrowth\n\n\n4\nSales Manager\nRetail\nSmall\nBerlin\nLow\nMedium\nProject Management\n83027.95\nNo\nGrowth\n\n\n5\nCybersecurity Analyst\nEntertainment\nSmall\nTokyo\nLow\nLow\nJavaScript\n87752.92\nYes\nDecline\n\n\n6..499\n\n\n\n\n\n\n\n\n\n\n\n\n500\nHR Manager\nEntertainment\nMedium\nBerlin\nMedium\nHigh\nProject Management\n53764.38\nYes\nDecline\n\n\n\nSource: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\n\n\n\n\n\n\n\n\n\n\n\n\nTo simplify our later code, I have created a separate table which is already filtered for the groups we will be looking at:\n\nai_jobs_risk &lt;- ai_jobs |&gt;\n  filter(automation_risk %in% c(\"Low\", \"High\"))\n\nai_jobs_high &lt;- ai_jobs_risk |&gt;\n  filter(automation_risk == \"High\")\n\nai_jobs_low &lt;- ai_jobs_risk |&gt;\n  filter(automation_risk == \"Low\")\n\nDataset Features:\nSource: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\n\nJob_Title:\n\nDescription: The title of the job role.\nType: Categorical\nExample Values: “Data Scientist”, “Software Engineer”, “HR Manager”\n\nIndustry:\n\nDescription: The industry in which the job is located.\nType: Categorical\nExample Values: “Technology”, “Healthcare”, “Finance”\n\nCompany_Size:\n\nDescription: The size of the company offering the job.\nType: Ordinal\nCategories: “Small”, “Medium”, “Large”\n\nLocation:\n\nDescription: The geographic location of the job.\nType: Categorical\nExample Values: “New York”, “San Francisco”, “London”\n\nAI_Adoption_Level:\n\nDescription: The extent to which the company has adopted AI in its operations.\nType: Ordinal\nCategories: “Low”, “Medium”, “High”\n\nAutomation_Risk:\n\nDescription: The estimated risk that the job could be automated within the next 10 years.\nType: Ordinal\nCategories: “Low”, “Medium”, “High”\n\nRequired_Skills:\n\nDescription: The key skills required for the job role.\nType: Categorical\nExample Values: “Python”, “Data Analysis”, “Project Management”\n\nSalary_USD:\n\nDescription: The annual salary offered for the job in USD.\nType: Numerical\nValue Range: $30,000 - $200,000\n\nRemote_Friendly:\n\nDescription: Indicates whether the job can be performed remotely.\nType: Categorical\nCategories: “Yes”, “No”\n\nJob_Growth_Projection:\n\nDescription: The projected growth or decline of the job role over the next five years.\nType: Categorical\nCategories: “Decline”, “Stable”, “Growth”"
  },
  {
    "objectID": "Week4/lecture.html#motivation",
    "href": "Week4/lecture.html#motivation",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Motivation",
    "text": "Motivation\nYou have been tasked with examining the impact of AI skills and company AI adoption on the job market across the world. Think about what we might want to know about this sector.\nWhat might some be some interesting questions we could ask based on this data?\nHow would we answer them? How would we know whether our answer is generalizable?\n\nWe want to generalize the results of our analysis from the data we have to the situation we care about."
  },
  {
    "objectID": "Week4/lecture.html#the-logic-of-testing-hypotheses",
    "href": "Week4/lecture.html#the-logic-of-testing-hypotheses",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "The Logic of Testing Hypotheses",
    "text": "The Logic of Testing Hypotheses\nA hypothesis is a statement about a population, or general pattern.\nTesting a hypothesis amounts to gathering information (sampling) from a dataset and, based on that information, deciding whether that hypothesis is false or true in the population.\nTwo decisions are possible:\n\nRejecting the hypothesis (if there is enough evidence against it)\nNot rejecting it (if there is not enough evidence against it)\n\nRejecting a hypothesis is a more conclusive decision than not rejecting it.\n\nOne such focused approach uses a statistic (e.g. a difference in two means) computed from our data to see whether its true value is equal to something we assume (e.g. the difference is zero). This is called hypothesis testing: using results in the data to see if we have enough evidence to tell whether a hypothesis (the two means are equal) is wrong (they are not equal) or whether we don’t have enough evidence (they may be equal.\nTesting a hypothesis is a way of making an inference, with a focus on a specific statement. As with any kind of inference, we have to assess external validity too: the extent to which the population, or general pattern, represented by our data is the same as the population, or general pattern, we are truly interested in.\n\nGiven a population (i.e. a distribution) with a parameter of interest (which could be the mean, variance, correlation, etc.), we would like to decide between to complementary statements concerning the parameter.\nThese statements are called statistical hypotheses.\nThe choice or decision between these hypotheses is to be based on a sample taken from the population of interest.\nThe goal is to be able to choose which hypothesis is true in reality based on the sample data."
  },
  {
    "objectID": "Week4/lecture.html#steps-of-analysis-hypothesis-testing",
    "href": "Week4/lecture.html#steps-of-analysis-hypothesis-testing",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Steps of Analysis & Hypothesis Testing",
    "text": "Steps of Analysis & Hypothesis Testing\n\n\n1. Formulate research question\n2. Specify hypotheses\n\nWhat statistic is appropriate to answer our question?\nNull and Alternative Hypotheses\n\n3. Collect relevant data\n\nWhat information is needed to answer our question?\nWhat is our population? How do we sample from the population in a statistically valid way?\n\n\n4. Compute test statistic\n\nFit appropriate model\nCalculate test statistic\nAccount for variability\n\n5. Determine probability under the null hypothesis\n6. Assess significance and meaningfulness"
  },
  {
    "objectID": "Week4/lecture.html#formulate-research-question-1",
    "href": "Week4/lecture.html#formulate-research-question-1",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "1. Formulate research question",
    "text": "1. Formulate research question\nThis is where all analysis or research starts - what is it you want to know, and what will you do with that information?\nFor our AI Jobs dataset some research questions we might ask are:\n\nWhich jobs are the highest paid?\nWhich industries have seen the most AI adoption?\nAre lower paid jobs at more risk of being automated?"
  },
  {
    "objectID": "Week4/lecture.html#specify-hypotheses-1",
    "href": "Week4/lecture.html#specify-hypotheses-1",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "2. Specify hypotheses",
    "text": "2. Specify hypotheses\nNow we translate our general research questions into specific and testable hypotheses.\nThe actual test begins by considering two hypotheses. They are called the null hypothesis and the alternative hypothesis. These hypotheses contain opposing viewpoints.\n\nThe null hypothesis (\\(H_0\\)) : It is often a statement of the accepted historical value or norm. This is your starting point that you must assume from the beginning in order to show an effect exists.\nThe alternative hypothesis (\\(H_a\\)) : It is a claim about the population that is contradictory to \\(H_0\\) and what we conclude when we reject \\(H_0\\)"
  },
  {
    "objectID": "Week4/lecture.html#specify-hypotheses-2",
    "href": "Week4/lecture.html#specify-hypotheses-2",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "2. Specify Hypotheses",
    "text": "2. Specify Hypotheses\nAfter you have determined which hypothesis the sample supports, you make a decision.\nThere are two options for a decision:\n\n“reject \\(H_0\\)” if the sample information favors the alternative hypothesis, or\n“do not reject \\(H_0\\)” or “decline to reject \\(H_0\\)” if the sample information is insufficient to reject the null hypothesis.\n\n\nMathematical symbols used in H0 and Ha:\nFigure 6.12: Null and Alternative Hypotheses\n\n\n\n\n\n\n\n\\(H_0\\)\n\\(H_a\\)\n\n\n\n\nequal (=)\nnot equal (\\(\\neq\\)) or greater than (\\(&gt;\\)) or less than (\\(&lt;\\))\n\n\ngreater than or equal to (\\(\\geq\\))\nless than (\\(&lt;\\))\n\n\nless than or equal to (\\(\\leq\\))\nmore than (\\(&gt;\\))"
  },
  {
    "objectID": "Week4/lecture.html#specify-hypotheses-3",
    "href": "Week4/lecture.html#specify-hypotheses-3",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "2. Specify Hypotheses",
    "text": "2. Specify Hypotheses\nDecide on statistic of interest\nDecide what statistic is appropriate to answer our question. What do we need to calculate from our sample?\n\nDifference in mean(salary_usd) for each level of automation_risk\nSince there are three automation risk levels (Low, Medium, High), the comparison we could make are:\n\n\\(\\text{mean}_{\\text{high risk}} - \\text{mean}_{\\text{medium risk}}\\)\n\\(\\text{mean}_{\\text{high}} - \\text{mean}_{\\text{low}}\\)\n\\(\\text{mean}_{\\text{medium}} - \\text{mean}_{\\text{low}}\\)"
  },
  {
    "objectID": "Week4/lecture.html#specify-hypotheses-4",
    "href": "Week4/lecture.html#specify-hypotheses-4",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "2. Specify Hypotheses",
    "text": "2. Specify Hypotheses\nDecide on statistic of interest\nFor now, let’s focus on just (2). Therefore:\n\\(s = \\overline{salary}_{\\text{high risk}} - \\overline{salary}_{\\text{low risk}}\\)\n\\(s\\) is our statistic of interest. We are interested in the true value of \\(s\\) in the population (\\(s_{true}\\)).\nWhat we can actually calculate is \\(\\hat{s}\\) the value of \\(s\\) in our sample."
  },
  {
    "objectID": "Week4/lecture.html#specify-hypotheses-5",
    "href": "Week4/lecture.html#specify-hypotheses-5",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "2. Specify Hypotheses",
    "text": "2. Specify Hypotheses\nExpress the hypotheses mathematically\nOur hypothesis, plainly stated, is:\nThere is a relationship between job salary and the likelihood of automation. Expressed another way, the average salary of high risk jobs is different from that of low risk jobs.\nWhat are the Null Hypothesis \\(H_0\\) and Alternative Hypothesis \\(H_A\\) ?"
  },
  {
    "objectID": "Week4/lecture.html#specify-hypotheses-6",
    "href": "Week4/lecture.html#specify-hypotheses-6",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "2. Specify Hypotheses",
    "text": "2. Specify Hypotheses\nExpress the hypotheses mathematically\n\nNull Hypothesis \\(H_0\\)\nThere is no difference in the average salary between those jobs at high risk of automation, and those at low risk: \\[\nH_0: s_{true} = 0\n\\]\n\n\nAlternative Hypothesis \\(H_a\\)\nThe difference in the average salary of high risk jobs and low risk jobs is not zero: \\[\nH_A: s_{true} \\neq 0\n\\]\n\n\nThe null says that the true value of the statistic is zero; the alternative says that it’s not zero. Together, these cover all logical possibilities for the true value of the statistic.\nIt may seem odd to have \\(H_0: s_{true} = 0\\) when, presumably, we analyze the data because we suspect that the true value of \\(s\\) is not zero. This seemingly twisted logic comes from the fact that testing a hypothesis amounts to seeing if there is enough evidence in our data to reject the null. It is sometimes said that the null is protected: it should not be too easy to reject it otherwise the conclusions of hypothesis testing would not be strong.\nAs we introduce the concept of hypothesis testing, it is helpful to relate its logic to the logic of a criminal court procedure. At court the task is to decide whether an accused person is guilty or innocent of a certain crime. In most modern societies the starting point is the assumption of innocence: the accused person should be judged guilty only if there is enough evidence against their innocence. This is so even though the accused person was brought before court presumably because there was a suspicion of their guilt. To translate this procedure to the language of hypothesis testing, H0 is that the person is innocent, and HA is that the person is guilty.\nMedical tests are another instructive example. When testing whether a person has a certain medical condition, the null is that the person does not have the condition (healthy), and the alternative is that they have it (sick). The testing procedure amounts to gathering information to see if there is evidence to decide that the person has the condition.\nThe case when we test if HA: strue ≠ 0 is called a two-sided alternative as it allows for strue to be either greater than zero or less than zero. For instance, we focus on the difference in online and offline prices, with H0 being the equality. In such a case we are not really interested if the difference is positive or not, or whether it is negative or not.\nThe other case is working with a one-sided alternative, when we are indeed interested if a statistic is positive. The null and the alternative should be set up so that the hypothesis we are truly interested in is in the alternative set. So when we want to know if strue is positive, we want to put strue &gt; 0 in the alternative thus, making the null strue ≤ 0:\n\\[\nH_0: s_{true} \\leq 0\n\\]\n\\[\nH_A: s_{true} &gt; 0\n\\]"
  },
  {
    "objectID": "Week4/lecture.html#using-the-sample-to-test-the-null-hypothesis",
    "href": "Week4/lecture.html#using-the-sample-to-test-the-null-hypothesis",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "3. Using the Sample to Test the Null Hypothesis",
    "text": "3. Using the Sample to Test the Null Hypothesis\nOnce you have defined your hypotheses the next step in the process, is to collect sample data.\nIn this case, we already have the data in ai_jobs. Before moving on to actually testing the hypothesis, let’s take the naive approach - just calculate \\(\\hat{s}\\) the difference between the two groups.\n\nmean_high &lt;- mean(ai_jobs_high$salary_usd)\nmean_low &lt;- mean(ai_jobs_low$salary_usd)\ns_hat &lt;- mean_high - mean_low\n\n\\[\n\\hat{s} = \\overline{\\text{salary}}_\\text{high risk} - \\overline{\\text{salary}}_{low risk} = \\$81.7 k - \\$99.7k = \\textbf{-18.1k}\n\\]"
  },
  {
    "objectID": "Week4/lecture.html#compute-the-test-statistic",
    "href": "Week4/lecture.html#compute-the-test-statistic",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "4. Compute the test statistic",
    "text": "4. Compute the test statistic\nThere are several statistical tests used in Hypothesis Testing. Which one you use depends on what type of hypothesis you are testing and what kind of data you have.\nFor this example, where we are testing the difference in means of a numerical variable (salary_usd) across different groups (automation_risk), we use a test called the t-test."
  },
  {
    "objectID": "Week4/lecture.html#compute-the-test-statistic-1",
    "href": "Week4/lecture.html#compute-the-test-statistic-1",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "4. Compute the test statistic",
    "text": "4. Compute the test statistic\n\nFollowing the logic of hypothesis testing, we start from the assumption that the null (\\(H_0\\)) is true and thus \\(s_{true} = 0\\).\nWe look at the evidence to see if we want to reject this null or maintain our assumption that it’s true.\nThe evidence we look for is how far the estimated value \\(\\hat{s}\\) is from zero.\nWe reject \\(H_0\\) if the distance is large (i.e. \\(\\hat{s}\\) is sufficiently greater or lesser than 0)\n\n\nHow far is far enough?\nThe test statistic is the measure of how far the estimated value \\(\\hat{s}\\) is from what its true value would be if \\(H_0\\) is true."
  },
  {
    "objectID": "Week4/lecture.html#compute-the-test-statistic-2",
    "href": "Week4/lecture.html#compute-the-test-statistic-2",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "4. Compute the test statistic",
    "text": "4. Compute the test statistic\nt-statistic:\n\\[\nt = \\frac{\\hat{s}}{\\mathrm{SE}(\\hat{s})} = \\frac{\\bar{x}_A - \\bar{x}_B}{\\mathrm{SE}(\\bar{x}_A - \\bar{x}_B)}\n\\]\n\nThe t-test is a procedure to decide whether we can reject the null \\(H_0\\).\nThe magnitude of the t-statistic \\(t\\) measures the distance of \\(\\hat{s}\\) from what \\(s_{true}\\) would be if the null were true.\n\nThe unit of distance is the standard error.\n\nThe t-statistic transforms the original statistic of interest into a standardized version\n\nFor example: if \\(t = 1\\) (or -1), it means \\(\\hat{s}\\) is exactly one standard error away from zero."
  },
  {
    "objectID": "Week4/lecture.html#compute-the-test-statistic-3",
    "href": "Week4/lecture.html#compute-the-test-statistic-3",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "4. Compute the test statistic",
    "text": "4. Compute the test statistic\nR makes it very easy to apply a test such as the t-test. For most statistical tests, there exists a simple function to compute it.\nFor the t-test, we use the t.test() function:\n\nt_res &lt;- t.test(ai_jobs_high$salary_usd, ai_jobs_low$salary_usd)\n\n\\(t = -6.54\\)"
  },
  {
    "objectID": "Week4/lecture.html#making-a-decision",
    "href": "Week4/lecture.html#making-a-decision",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Making a Decision",
    "text": "Making a Decision\n\nThe following step is making a decision: either rejecting the null or not rejecting it.\nIn hypothesis testing, this decision is based on a clear rule specified in advance.\nWe specify in advance to avoid bias - before looking at the data, we state what it would take to reject the null hypothesis. We follow what the data says, whatever result that may be.\n\n\nA clear rule also makes the decision transparent, which helps avoid biases in the decision. Unfortunately, we humans are often tempted to use evidence to support our pre-existing views or prejudices. If, for example, we think that jobs which are more highly valued by companies (i.e. have a higher salary) we may pay more attention to the evidence that supports that belief than to the evidence against it.\nIn partiular, we may be tempted to say that the estimated \\(\\hat{s}\\) difference is large enough to reject the null, because we believe that the null isn’t true. Clear decision rules are designed to minimize the room for such temptations.\n\n\nOnce you have your test statistic there are two methods to use it to make your decision:\n\nCritical value method\nP-Value method – This is the preferred method we mostly will focus on."
  },
  {
    "objectID": "Week4/lecture.html#critical-values",
    "href": "Week4/lecture.html#critical-values",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Critical Values",
    "text": "Critical Values\nWe use a critical value to tell us whether the test statistic is large enough - is it far enough away from zero to reject the null?\nTo define the critical value, we need to decide how conservative we want to be with the evidence.\nThe larger we set the critical value, the harder it is to reject the null hypothesis."
  },
  {
    "objectID": "Week4/lecture.html#critical-values-1",
    "href": "Week4/lecture.html#critical-values-1",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Critical Values",
    "text": "Critical Values\nThe test sampling distribution\nAs with the sampling distribution for means we looked at earlier, our test-statistic \\(t\\) also has a sampling distribution. If we were to sample many times and calculate \\(t\\) for each sample, we would again get a distribution with a specific shape and parameters.\n\nThe sampling distribution of the test statistic when the null is true.Recall: Approximately 95% of values fall within two standard deviations of the distribution."
  },
  {
    "objectID": "Week4/lecture.html#critical-values-2",
    "href": "Week4/lecture.html#critical-values-2",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Critical Values",
    "text": "Critical Values\nPicking a critical value\n\n\n\n\n\nThe sampling distribution of the test statistic when the null is true.\n\n\n\nRecall: Approximately 95% of values fall within two standard deviations of the distribution.\nSince 95% of values fall within 2 SD, if we want to reject the null hypothesis with 95% confidence, then we say that our test statistic must fall outside of 2 SD.\nIn other words, since the units of \\(\\hat{s}\\) are standard deviations: \\(\\hat{s} \\geq \\pm 2\\)"
  },
  {
    "objectID": "Week4/lecture.html#critical-values-3",
    "href": "Week4/lecture.html#critical-values-3",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Critical Values",
    "text": "Critical Values\nA critical value of 2 is standard. However, it is ultimately just a convention. We could choose to set other critical values that correspond to different probabilities. There is not anything inherently special about setting our threshold at 95% vs 90%.\nIf we make the critical value \\(\\pm 1.6\\), the chance of a false positive is 10%.\n\n\nDifferent fields have different standards for evidence - for instance, a critical value of 5 (99.994%) is standard in particle physics (referred to as \\(5\\sigma\\))."
  },
  {
    "objectID": "Week4/lecture.html#interpret-our-results",
    "href": "Week4/lecture.html#interpret-our-results",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Interpret our results",
    "text": "Interpret our results\nSince our test statistic \\(t = -7.5 &lt; -2\\), at a confidence level of 95%, we would have sufficient evidence to reject \\(H_0\\)\nTherefore, we would say:\n\nThe average salary of jobs at high risk of automation is not the same as the average salary of jobs at low risk.\nWe have evidence that higher salary jobs are at less risk of automation than low salary jobs."
  },
  {
    "objectID": "Week4/lecture.html#interpret-our-results-1",
    "href": "Week4/lecture.html#interpret-our-results-1",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Interpret our results",
    "text": "Interpret our results\nImportant!\nThis does not inherently mean we accept the alternative hypothesis. We are narrowing the realm of possible answers, but very rarely (perhaps never) are we able to statistically prove a single explanation in one go.\nWe have increased our reasons to believe our hypothesis, but several other possibilities exist.\nScience is then the process of continually investigating our hypothesis and pitting it against new null hypotheses and rejecting them as well."
  },
  {
    "objectID": "Week4/lecture.html#p-value-method",
    "href": "Week4/lecture.html#p-value-method",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "P-Value Method",
    "text": "P-Value Method\n\nHopefully, the critical value is fairly intuitive to you now. However, it is not the typical way that statistical results are presented.\nInstead, you will typically see something called a p-value.\n\n\np-value: The probability than an event will occur, assuming the null hypothesis is true.\nThe p-value essentially flips the critical value statement:\n\nInstead of saying a test statistic value &gt; 2 falls outside the 95% bound, we calculate where out test statistic falls in the distribution\n\n\nThe p-value is the probability that the test statistic will be as large, or larger, than we calculate from the data, if the null hypothesis is true. i.e. \\(P(data | H_0)\\)."
  },
  {
    "objectID": "Week4/lecture.html#p-value",
    "href": "Week4/lecture.html#p-value",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "P-value",
    "text": "P-value\n\\(p = P(|t| &gt; \\text{critical value})\\)\nBecause the p-value tells us the smallest level of significance at which we can reject the null hypothesis, it summarizes all the information we need to make the decision.\nThis is why the p-value is used - rather than needing to set a critical value and calculate the test statistic, we can instead use just the p-value."
  },
  {
    "objectID": "Week4/lecture.html#interpreting-the-p-value",
    "href": "Week4/lecture.html#interpreting-the-p-value",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Interpreting the P-value",
    "text": "Interpreting the P-value\n\nLike with the critical value, we should set our desired significance level before carrying out the analysis.\nWe then compare our calculated p-value with the significance level. If it is less, we reject the null hypothesis.\nThe significance level (\\(\\alpha\\)) is the probability that a true null hypothesis will be rejected.\nA typical significance level is \\(\\alpha &lt; 0.05\\), which corresponds with a critical value of 2, or a probability of 5%."
  },
  {
    "objectID": "Week4/lecture.html#interpreting-the-p-value-1",
    "href": "Week4/lecture.html#interpreting-the-p-value-1",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Interpreting the P-value",
    "text": "Interpreting the P-value\n\nR outputT distribution\n\n\nAgain, R will provide us with the p-value. Let’s now look at the full output from our t.test():\n\nt.test(ai_jobs_high$salary_usd, ai_jobs_low$salary_usd)\n\n\n    Welch Two Sample t-test\n\ndata:  ai_jobs_high$salary_usd and ai_jobs_low$salary_usd\nt = -6.5366, df = 288.07, p-value = 2.854e-10\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -23513.51 -12630.27\nsample estimates:\nmean of x mean of y \n 81673.57  99745.46 \n\n\n\nIf our p-value is smaller than our pre-set significance level (\\(\\alpha\\)), we reject the null hypothesis and can say the result is “statistically significant” at \\(p &lt; 0.05\\)."
  },
  {
    "objectID": "Week4/lecture.html#decision-and-conclusion",
    "href": "Week4/lecture.html#decision-and-conclusion",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Decision and conclusion",
    "text": "Decision and conclusion\nThe preset \\(\\alpha\\) is the probability of a False Positive error (called a Type I error) - rejecting the null hypothesis when the null hypothesis is true.\nBack to our two possible decisions:\n\nIf \\(\\alpha &gt; \\text{p-value}\\), reject \\(H_0\\).\n\nThe results of the sample are statistically significant.\nWe can say there is sufficient evidence to conclude that \\(H_0\\) is an incorrect believe and that the alternative hypothesis, \\(H_A\\) may be correct.\n\nIf \\(\\alpha &lt; \\text{p-value}\\), fail to reject \\(H_0\\).\n\nThe results of the sample are not significant. There is not sufficient evidence to conclude that the alternative hypothesis \\(H_A\\) may be correct.\n\n\n\nNOTE: When you “do not reject \\(H_0\\)”, it does not mean that you should believe that \\(H_0\\) is true. It simply means that the sample data have failed to provide sufficient evidence to cast serious doubt about the truthfulness of \\(H_0\\)."
  },
  {
    "objectID": "Week4/lecture.html#closing---what-does-a-statistically-significant-result-mean",
    "href": "Week4/lecture.html#closing---what-does-a-statistically-significant-result-mean",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Closing - What does a statistically significant result mean?",
    "text": "Closing - What does a statistically significant result mean?\nDoes it mean our result is meaningful or practically important?\n\nEffect sizeSample size\n\n\nNo. There is an essential distinction between statistical significance and practical significance.\nLet’s say we performed an experiment to examine the effect of a particular diet on body weight, which gives a statistically significant effect at \\(p &lt; 0.05\\). This doesn’t tell us how much weight was lost, which we refer to as the effect size.\nWould the loss of 20 grams (i.e. the weight of a few potato chips) be practically significant, even if it were statistically significant?\nWhether a result is practically significant depends on the effect size and the context of the research question. It’s up to the researcher to know whether it is meaningful.\n\n\nAs with the standard error (and a direct result of it), the p-value depends on the sample size. A very large sample size will give a statistically significant result in many cases, even with a very small effect size.\n\n\n\nThe proportion of significant results for a very small change (~20g which is about 0.001 standard deviations) as a function of sample size"
  },
  {
    "objectID": "Week4/lecture.html#further-reading",
    "href": "Week4/lecture.html#further-reading",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Further Reading",
    "text": "Further Reading\nThere are several more topics to understand about p-values which we cannot cover today: - One-sided vs Two-sided t-test. - Are we testing “there is no difference” or are we testing “\\(mean_A &gt; mean_B\\)? - Type I and Type II Errors - False positive vs False negative - Multiple Comparisons - What happens to \\(P(data|H_0\\)) when we run multiple tests on the same data? - How do we control the error rate across our entire family of tests?\n\nThere are also numerous modern critiques of p-values and how they are used and interpreted.\nSee Statistical Thinking Chapter 9 for a detailed discussion."
  },
  {
    "objectID": "Week4/lecture.html#further-reading-1",
    "href": "Week4/lecture.html#further-reading-1",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "Further Reading",
    "text": "Further Reading\n\nPoldrack, Statistical Thinking, Chapter 9\nSignificant Statistics from Virginia Tech: https://pressbooks.lib.vt.edu/introstatistics/chapter/null-and-alternative-hypotheses/\nBekes & Kezdi, Data Analysis for Business, Economics, and Policy, Chapter 6"
  },
  {
    "objectID": "Week4/05-hyptest.html",
    "href": "Week4/05-hyptest.html",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Data Overview\nData Preview\n\n\n\nThe “AI-Powered Job Market Insights” dataset provides a snapshot of the modern job market, particularly focusing on the role of artificial intelligence (AI) and automation across various industries.\nThis dataset includes 500 unique job listings, each characterized by different factors like industry, company size, AI adoption level, automation risk, required skills, and job growth projections.\n\n\n\nCodeai_jobs |&gt;\n  gt_preview() |&gt;\n  tab_header(title = \"AI-Powered Job Market Insights\") |&gt;\n  tab_source_note(source_note = \"Source: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\")\n\n\n\n\n\n\nAI-Powered Job Market Insights\n\n\n\njob_title\nindustry\ncompany_size\nlocation\nai_adoption_level\nautomation_risk\nrequired_skills\nsalary_usd\nremote_friendly\njob_growth_projection\n\n\n\n\n1\nCybersecurity Analyst\nEntertainment\nSmall\nDubai\nMedium\nHigh\nUX/UI Design\n111392.17\nYes\nGrowth\n\n\n2\nMarketing Specialist\nTechnology\nLarge\nSingapore\nMedium\nHigh\nMarketing\n73792.56\nNo\nDecline\n\n\n3\nAI Researcher\nTechnology\nLarge\nSingapore\nMedium\nLow\nUX/UI Design\n137170.26\nYes\nGrowth\n\n\n4\nSales Manager\nRetail\nSmall\nBerlin\nLow\nMedium\nProject Management\n83027.95\nNo\nGrowth\n\n\n5\nCybersecurity Analyst\nEntertainment\nSmall\nTokyo\nLow\nLow\nJavaScript\n87752.92\nYes\nDecline\n\n\n6..499\n\n\n\n\n\n\n\n\n\n\n\n\n500\nHR Manager\nEntertainment\nMedium\nBerlin\nMedium\nHigh\nProject Management\n53764.38\nYes\nDecline\n\n\n\nSource: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\n\n\n\n\n\n\n\n\n\nTo simplify our later code, I have created a separate table which is already filtered for the groups we will be looking at:\n\nCodeai_jobs_risk &lt;- ai_jobs |&gt;\n  filter(automation_risk %in% c(\"Low\", \"High\"))\n\nai_jobs_high &lt;- ai_jobs_risk |&gt;\n  filter(automation_risk == \"High\")\n\nai_jobs_low &lt;- ai_jobs_risk |&gt;\n  filter(automation_risk == \"Low\")\n\n\nDataset Features:\nSource: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\n\n\nJob_Title:\n\n\nDescription: The title of the job role.\n\nType: Categorical\n\nExample Values: “Data Scientist”, “Software Engineer”, “HR Manager”\n\n\n\nIndustry:\n\n\nDescription: The industry in which the job is located.\n\nType: Categorical\n\nExample Values: “Technology”, “Healthcare”, “Finance”\n\n\n\nCompany_Size:\n\n\nDescription: The size of the company offering the job.\n\nType: Ordinal\n\nCategories: “Small”, “Medium”, “Large”\n\n\n\nLocation:\n\n\nDescription: The geographic location of the job.\n\nType: Categorical\n\nExample Values: “New York”, “San Francisco”, “London”\n\n\n\nAI_Adoption_Level:\n\n\nDescription: The extent to which the company has adopted AI in its operations.\n\nType: Ordinal\n\nCategories: “Low”, “Medium”, “High”\n\n\n\nAutomation_Risk:\n\n\nDescription: The estimated risk that the job could be automated within the next 10 years.\n\nType: Ordinal\n\nCategories: “Low”, “Medium”, “High”\n\n\n\nRequired_Skills:\n\n\nDescription: The key skills required for the job role.\n\nType: Categorical\n\nExample Values: “Python”, “Data Analysis”, “Project Management”\n\n\n\nSalary_USD:\n\n\nDescription: The annual salary offered for the job in USD.\n\nType: Numerical\n\nValue Range: $30,000 - $200,000\n\n\n\nRemote_Friendly:\n\n\nDescription: Indicates whether the job can be performed remotely.\n\nType: Categorical\n\nCategories: “Yes”, “No”\n\n\n\nJob_Growth_Projection:\n\n\nDescription: The projected growth or decline of the job role over the next five years.\n\nType: Categorical\n\nCategories: “Decline”, “Stable”, “Growth”\n\n\n\n\n\nYou have been tasked with examining the impact of AI skills and company AI adoption on the job market across the world. Think about what we might want to know about this sector.\nWhat might some be some interesting questions we could ask based on this data?\nHow would we answer them? How would we know whether our answer is generalizable?\n\nWe want to generalize the results of our analysis from the data we have to the situation we care about.\n\n\nA hypothesis is a statement about a population, or general pattern.\nTesting a hypothesis amounts to gathering information (sampling) from a dataset and, based on that information, deciding whether that hypothesis is false or true in the population.\nTwo decisions are possible:\n\nRejecting the hypothesis (if there is enough evidence against it)\nNot rejecting it (if there is not enough evidence against it)\n\nRejecting a hypothesis is a more conclusive decision than not rejecting it.\n\nOne such focused approach uses a statistic (e.g. a difference in two means) computed from our data to see whether its true value is equal to something we assume (e.g. the difference is zero). This is called hypothesis testing: using results in the data to see if we have enough evidence to tell whether a hypothesis (the two means are equal) is wrong (they are not equal) or whether we don’t have enough evidence (they may be equal.\nTesting a hypothesis is a way of making an inference, with a focus on a specific statement. As with any kind of inference, we have to assess external validity too: the extent to which the population, or general pattern, represented by our data is the same as the population, or general pattern, we are truly interested in.\n\nGiven a population (i.e. a distribution) with a parameter of interest (which could be the mean, variance, correlation, etc.), we would like to decide between to complementary statements concerning the parameter.\nThese statements are called statistical hypotheses.\nThe choice or decision between these hypotheses is to be based on a sample taken from the population of interest.\nThe goal is to be able to choose which hypothesis is true in reality based on the sample data.\n\n\n\n\n\n\n\n\nWhat statistic is appropriate to answer our question?\nNull and Alternative Hypotheses\n\n\nWhat information is needed to answer our question?\nWhat is our population? How do we sample from the population in a statistically valid way?\n\n\n\n\nFit appropriate model\nCalculate test statistic\nAccount for variability\n\n\n\n\n\n\nThis is where all analysis or research starts - what is it you want to know, and what will you do with that information?\nFor our AI Jobs dataset some research questions we might ask are:\n\nWhich jobs are the highest paid?\nWhich industries have seen the most AI adoption?\nAre lower paid jobs at more risk of being automated?\n\nNow we translate our general research questions into specific and testable hypotheses.\nThe actual test begins by considering two hypotheses. They are called the null hypothesis and the alternative hypothesis. These hypotheses contain opposing viewpoints.\n\n\nThe null hypothesis (H_0) : It is often a statement of the accepted historical value or norm. This is your starting point that you must assume from the beginning in order to show an effect exists.\n\nThe alternative hypothesis (H_a) : It is a claim about the population that is contradictory to H_0 and what we conclude when we reject H_0\n\n\nAfter you have determined which hypothesis the sample supports, you make a decision.\nThere are two options for a decision:\n\n“reject H_0” if the sample information favors the alternative hypothesis, or\n“do not reject H_0” or “decline to reject H_0” if the sample information is insufficient to reject the null hypothesis.\n\n\nMathematical symbols used in H0 and Ha:\nFigure 6.12: Null and Alternative Hypotheses\n\n\n\n\n\n\nH_0\nH_a\n\n\n\nequal (=)\nnot equal (\\neq) or greater than (&gt;) or less than (&lt;)\n\n\ngreater than or equal to (\\geq)\nless than (&lt;)\n\n\nless than or equal to (\\leq)\nmore than (&gt;)\n\n\n\n\n\n\nDecide what statistic is appropriate to answer our question. What do we need to calculate from our sample?\n\nDifference in mean(salary_usd) for each level of automation_risk\n\nSince there are three automation risk levels (Low, Medium, High), the comparison we could make are:\n\n\\text{mean}_{\\text{high risk}} - \\text{mean}_{\\text{medium risk}}\n\\text{mean}_{\\text{high}} - \\text{mean}_{\\text{low}}\n\\text{mean}_{\\text{medium}} - \\text{mean}_{\\text{low}}\n\n\n\n\nFor now, let’s focus on just (2). Therefore:\ns = \\overline{salary}_{\\text{high risk}} - \\overline{salary}_{\\text{low risk}}\ns is our statistic of interest. We are interested in the true value of s in the population (s_{true}).\nWhat we can actually calculate is \\hat{s} the value of s in our sample.\n\n\nOur hypothesis, plainly stated, is:\nThere is a relationship between job salary and the likelihood of automation. Expressed another way, the average salary of high risk jobs is different from that of low risk jobs.\nWhat are the Null Hypothesis H_0 and Alternative Hypothesis H_A ?\n\n\n\n\nThere is no difference in the average salary between those jobs at high risk of automation, and those at low risk: \nH_0: s_{true} = 0\n\n\n\n\n\nThe difference in the average salary of high risk jobs and low risk jobs is not zero: \nH_A: s_{true} \\neq 0\n\n\n\n\nThe null says that the true value of the statistic is zero; the alternative says that it’s not zero. Together, these cover all logical possibilities for the true value of the statistic.\nIt may seem odd to have H_0: s_{true} = 0 when, presumably, we analyze the data because we suspect that the true value of s is not zero. This seemingly twisted logic comes from the fact that testing a hypothesis amounts to seeing if there is enough evidence in our data to reject the null. It is sometimes said that the null is protected: it should not be too easy to reject it otherwise the conclusions of hypothesis testing would not be strong.\nAs we introduce the concept of hypothesis testing, it is helpful to relate its logic to the logic of a criminal court procedure. At court the task is to decide whether an accused person is guilty or innocent of a certain crime. In most modern societies the starting point is the assumption of innocence: the accused person should be judged guilty only if there is enough evidence against their innocence. This is so even though the accused person was brought before court presumably because there was a suspicion of their guilt. To translate this procedure to the language of hypothesis testing, H0 is that the person is innocent, and HA is that the person is guilty.\nMedical tests are another instructive example. When testing whether a person has a certain medical condition, the null is that the person does not have the condition (healthy), and the alternative is that they have it (sick). The testing procedure amounts to gathering information to see if there is evidence to decide that the person has the condition.\nThe case when we test if HA: strue ≠ 0 is called a two-sided alternative as it allows for strue to be either greater than zero or less than zero. For instance, we focus on the difference in online and offline prices, with H0 being the equality. In such a case we are not really interested if the difference is positive or not, or whether it is negative or not.\nThe other case is working with a one-sided alternative, when we are indeed interested if a statistic is positive. The null and the alternative should be set up so that the hypothesis we are truly interested in is in the alternative set. So when we want to know if strue is positive, we want to put strue &gt; 0 in the alternative thus, making the null strue ≤ 0:\n\nH_0: s_{true} \\leq 0\n\n\nH_A: s_{true} &gt; 0\n\n\n\nOnce you have defined your hypotheses the next step in the process, is to collect sample data.\nIn this case, we already have the data in ai_jobs. Before moving on to actually testing the hypothesis, let’s take the naive approach - just calculate \\hat{s} the difference between the two groups.\n\nCodemean_high &lt;- mean(ai_jobs_high$salary_usd)\nmean_low &lt;- mean(ai_jobs_low$salary_usd)\ns_hat &lt;- mean_high - mean_low\n\n\n\n\\hat{s} = \\overline{\\text{salary}}_\\text{high risk} - \\overline{\\text{salary}}_{low risk} = \\$81.7 k - \\$99.7k = \\textbf{-18.1k}\n\n\nThere are several statistical tests used in Hypothesis Testing. Which one you use depends on what type of hypothesis you are testing and what kind of data you have.\nFor this example, where we are testing the difference in means of a numerical variable (salary_usd) across different groups (automation_risk), we use a test called the t-test.\n\n\nFollowing the logic of hypothesis testing, we start from the assumption that the null (H_0) is true and thus s_{true} = 0.\nWe look at the evidence to see if we want to reject this null or maintain our assumption that it’s true.\nThe evidence we look for is how far the estimated value \\hat{s} is from zero.\nWe reject H_0 if the distance is large (i.e. \\hat{s} is sufficiently greater or lesser than 0)\n\n\nHow far is far enough?\nThe test statistic is the measure of how far the estimated value \\hat{s} is from what its true value would be if H_0 is true.\n\n\nt-statistic:\n\nt = \\frac{\\hat{s}}{\\mathrm{SE}(\\hat{s})} = \\frac{\\bar{x}_A - \\bar{x}_B}{\\mathrm{SE}(\\bar{x}_A - \\bar{x}_B)}\n\n\nThe t-test is a procedure to decide whether we can reject the null H_0.\nThe magnitude of the t-statistic t measures the distance of \\hat{s} from what s_{true} would be if the null were true.\n\nThe unit of distance is the standard error.\n\n\nThe t-statistic transforms the original statistic of interest into a standardized version\n\nFor example: if t = 1 (or -1), it means \\hat{s} is exactly one standard error away from zero.\n\n\n\nR makes it very easy to apply a test such as the t-test. For most statistical tests, there exists a simple function to compute it.\nFor the t-test, we use the t.test() function:\n\nCodet_res &lt;- t.test(ai_jobs_high$salary_usd, ai_jobs_low$salary_usd)\n\n\nt = -6.54\n\n\nThe following step is making a decision: either rejecting the null or not rejecting it.\nIn hypothesis testing, this decision is based on a clear rule specified in advance.\nWe specify in advance to avoid bias - before looking at the data, we state what it would take to reject the null hypothesis. We follow what the data says, whatever result that may be.\n\n\nA clear rule also makes the decision transparent, which helps avoid biases in the decision. Unfortunately, we humans are often tempted to use evidence to support our pre-existing views or prejudices. If, for example, we think that jobs which are more highly valued by companies (i.e. have a higher salary) we may pay more attention to the evidence that supports that belief than to the evidence against it.\nIn partiular, we may be tempted to say that the estimated \\hat{s} difference is large enough to reject the null, because we believe that the null isn’t true. Clear decision rules are designed to minimize the room for such temptations.\n\n\nOnce you have your test statistic there are two methods to use it to make your decision:\n\nCritical value method\nP-Value method – This is the preferred method we mostly will focus on.\n\n\n\nWe use a critical value to tell us whether the test statistic is large enough - is it far enough away from zero to reject the null?\nTo define the critical value, we need to decide how conservative we want to be with the evidence.\nThe larger we set the critical value, the harder it is to reject the null hypothesis.\n\n\nAs with the sampling distribution for means we looked at earlier, our test-statistic t also has a sampling distribution. If we were to sample many times and calculate t for each sample, we would again get a distribution with a specific shape and parameters.\n\n\nThe sampling distribution of the test statistic when the null is true.\n\nRecall: Approximately 95% of values fall within two standard deviations of the distribution.\n\n\n\n\n\n\nThe sampling distribution of the test statistic when the null is true.\n\n\nRecall: Approximately 95% of values fall within two standard deviations of the distribution.\nSince 95% of values fall within 2 SD, if we want to reject the null hypothesis with 95% confidence, then we say that our test statistic must fall outside of 2 SD.\nIn other words, since the units of \\hat{s} are standard deviations: \\hat{s} \\geq \\pm 2\n\n\n\nA critical value of 2 is standard. However, it is ultimately just a convention. We could choose to set other critical values that correspond to different probabilities. There is not anything inherently special about setting our threshold at 95% vs 90%.\nIf we make the critical value \\pm 1.6, the chance of a false positive is 10%.\n\n\n\n\n\nDifferent fields have different standards for evidence - for instance, a critical value of 5 (99.994%) is standard in particle physics (referred to as 5\\sigma).\n\n\nSince our test statistic t = -7.5 &lt; -2, at a confidence level of 95%, we would have sufficient evidence to reject H_0\nTherefore, we would say:\n\nThe average salary of jobs at high risk of automation is not the same as the average salary of jobs at low risk.\nWe have evidence that higher salary jobs are at less risk of automation than low salary jobs.\n\n\nThis does not inherently mean we accept the alternative hypothesis. We are narrowing the realm of possible answers, but very rarely (perhaps never) are we able to statistically prove a single explanation in one go.\nWe have increased our reasons to believe our hypothesis, but several other possibilities exist.\nScience is then the process of continually investigating our hypothesis and pitting it against new null hypotheses and rejecting them as well.\n\n\nHopefully, the critical value is fairly intuitive to you now. However, it is not the typical way that statistical results are presented.\nInstead, you will typically see something called a p-value.\n\n\np-value: The probability than an event will occur, assuming the null hypothesis is true.\nThe p-value essentially flips the critical value statement:\n\nInstead of saying a test statistic value &gt; 2 falls outside the 95% bound, we calculate where out test statistic falls in the distribution\n\n\nThe p-value is the probability that the test statistic will be as large, or larger, than we calculate from the data, if the null hypothesis is true. i.e. P(data | H_0).\n\n\n\np = P(|t| &gt; \\text{critical value})\nBecause the p-value tells us the smallest level of significance at which we can reject the null hypothesis, it summarizes all the information we need to make the decision.\nThis is why the p-value is used - rather than needing to set a critical value and calculate the test statistic, we can instead use just the p-value.\n\n\nLike with the critical value, we should set our desired significance level before carrying out the analysis.\nWe then compare our calculated p-value with the significance level. If it is less, we reject the null hypothesis.\nThe significance level (\\alpha) is the probability that a true null hypothesis will be rejected.\nA typical significance level is \\alpha &lt; 0.05, which corresponds with a critical value of 2, or a probability of 5%.\n\n\n\nR output\nT distribution\n\n\n\nAgain, R will provide us with the p-value. Let’s now look at the full output from our t.test():\n\nCodet.test(ai_jobs_high$salary_usd, ai_jobs_low$salary_usd)\n\n\n    Welch Two Sample t-test\n\ndata:  ai_jobs_high$salary_usd and ai_jobs_low$salary_usd\nt = -6.5366, df = 288.07, p-value = 2.854e-10\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -23513.51 -12630.27\nsample estimates:\nmean of x mean of y \n 81673.57  99745.46 \n\n\n\nIf our p-value is smaller than our pre-set significance level (\\alpha), we reject the null hypothesis and can say the result is “statistically significant” at p &lt; 0.05.\n\n\n\n\nCodexpos &lt;- seq(-5, 5, by = 0.01)\n\ndegree &lt;- 280\nypos &lt;- dt(xpos, df = degree)\n\nggplot() +\n  xlim(-8, 8) +\n  geom_function(aes(colour = \"t, df=280\"), fun = dt, args = list(df = 280), linewidth = 1.5) +\n  geom_vline(xintercept = t_res$statistic, color = \"black\", linewidth = 1.5) +\n  xlab(\"Test statistic under the null hypothesis P(data | H0)\") +\n  ylab(\"Frequency\") +\n  ggtitle(\"Student's T distribution\")\n\n\n\n\n\n\n\n\n\n\n\nThe preset \\alpha is the probability of a False Positive error (called a Type I error) - rejecting the null hypothesis when the null hypothesis is true.\nBack to our two possible decisions:\n\n\nIf \\alpha &gt; \\text{p-value}, reject H_0.\n\nThe results of the sample are statistically significant.\nWe can say there is sufficient evidence to conclude that H_0 is an incorrect believe and that the alternative hypothesis, H_A may be correct.\n\n\n\nIf \\alpha &lt; \\text{p-value}, fail to reject H_0.\n\nThe results of the sample are not significant. There is not sufficient evidence to conclude that the alternative hypothesis H_A may be correct.\n\n\n\n\nNOTE: When you “do not reject H_0”, it does not mean that you should believe that H_0 is true. It simply means that the sample data have failed to provide sufficient evidence to cast serious doubt about the truthfulness of H_0.\n\n\n\n\n\nEffect size\nSample size\n\n\n\nNo. There is an essential distinction between statistical significance and practical significance.\nLet’s say we performed an experiment to examine the effect of a particular diet on body weight, which gives a statistically significant effect at p &lt; 0.05. This doesn’t tell us how much weight was lost, which we refer to as the effect size.\nWould the loss of 20 grams (i.e. the weight of a few potato chips) be practically significant, even if it were statistically significant?\nWhether a result is practically significant depends on the effect size and the context of the research question. It’s up to the researcher to know whether it is meaningful.\n\n\nAs with the standard error (and a direct result of it), the p-value depends on the sample size. A very large sample size will give a statistically significant result in many cases, even with a very small effect size.\n\n\nThe proportion of significant results for a very small change (~20g which is about 0.001 standard deviations) as a function of sample size",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#case-study",
    "href": "Week4/05-hyptest.html#case-study",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Data Overview\nData Preview\n\n\n\nThe “AI-Powered Job Market Insights” dataset provides a snapshot of the modern job market, particularly focusing on the role of artificial intelligence (AI) and automation across various industries.\nThis dataset includes 500 unique job listings, each characterized by different factors like industry, company size, AI adoption level, automation risk, required skills, and job growth projections.\n\n\n\nCodeai_jobs |&gt;\n  gt_preview() |&gt;\n  tab_header(title = \"AI-Powered Job Market Insights\") |&gt;\n  tab_source_note(source_note = \"Source: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\")\n\n\n\n\n\n\nAI-Powered Job Market Insights\n\n\n\njob_title\nindustry\ncompany_size\nlocation\nai_adoption_level\nautomation_risk\nrequired_skills\nsalary_usd\nremote_friendly\njob_growth_projection\n\n\n\n\n1\nCybersecurity Analyst\nEntertainment\nSmall\nDubai\nMedium\nHigh\nUX/UI Design\n111392.17\nYes\nGrowth\n\n\n2\nMarketing Specialist\nTechnology\nLarge\nSingapore\nMedium\nHigh\nMarketing\n73792.56\nNo\nDecline\n\n\n3\nAI Researcher\nTechnology\nLarge\nSingapore\nMedium\nLow\nUX/UI Design\n137170.26\nYes\nGrowth\n\n\n4\nSales Manager\nRetail\nSmall\nBerlin\nLow\nMedium\nProject Management\n83027.95\nNo\nGrowth\n\n\n5\nCybersecurity Analyst\nEntertainment\nSmall\nTokyo\nLow\nLow\nJavaScript\n87752.92\nYes\nDecline\n\n\n6..499\n\n\n\n\n\n\n\n\n\n\n\n\n500\nHR Manager\nEntertainment\nMedium\nBerlin\nMedium\nHigh\nProject Management\n53764.38\nYes\nDecline\n\n\n\nSource: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\n\n\n\n\n\n\n\n\n\nTo simplify our later code, I have created a separate table which is already filtered for the groups we will be looking at:\n\nCodeai_jobs_risk &lt;- ai_jobs |&gt;\n  filter(automation_risk %in% c(\"Low\", \"High\"))\n\nai_jobs_high &lt;- ai_jobs_risk |&gt;\n  filter(automation_risk == \"High\")\n\nai_jobs_low &lt;- ai_jobs_risk |&gt;\n  filter(automation_risk == \"Low\")\n\n\nDataset Features:\nSource: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\n\n\nJob_Title:\n\n\nDescription: The title of the job role.\n\nType: Categorical\n\nExample Values: “Data Scientist”, “Software Engineer”, “HR Manager”\n\n\n\nIndustry:\n\n\nDescription: The industry in which the job is located.\n\nType: Categorical\n\nExample Values: “Technology”, “Healthcare”, “Finance”\n\n\n\nCompany_Size:\n\n\nDescription: The size of the company offering the job.\n\nType: Ordinal\n\nCategories: “Small”, “Medium”, “Large”\n\n\n\nLocation:\n\n\nDescription: The geographic location of the job.\n\nType: Categorical\n\nExample Values: “New York”, “San Francisco”, “London”\n\n\n\nAI_Adoption_Level:\n\n\nDescription: The extent to which the company has adopted AI in its operations.\n\nType: Ordinal\n\nCategories: “Low”, “Medium”, “High”\n\n\n\nAutomation_Risk:\n\n\nDescription: The estimated risk that the job could be automated within the next 10 years.\n\nType: Ordinal\n\nCategories: “Low”, “Medium”, “High”\n\n\n\nRequired_Skills:\n\n\nDescription: The key skills required for the job role.\n\nType: Categorical\n\nExample Values: “Python”, “Data Analysis”, “Project Management”\n\n\n\nSalary_USD:\n\n\nDescription: The annual salary offered for the job in USD.\n\nType: Numerical\n\nValue Range: $30,000 - $200,000\n\n\n\nRemote_Friendly:\n\n\nDescription: Indicates whether the job can be performed remotely.\n\nType: Categorical\n\nCategories: “Yes”, “No”\n\n\n\nJob_Growth_Projection:\n\n\nDescription: The projected growth or decline of the job role over the next five years.\n\nType: Categorical\n\nCategories: “Decline”, “Stable”, “Growth”",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#motivation",
    "href": "Week4/05-hyptest.html#motivation",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "You have been tasked with examining the impact of AI skills and company AI adoption on the job market across the world. Think about what we might want to know about this sector.\nWhat might some be some interesting questions we could ask based on this data?\nHow would we answer them? How would we know whether our answer is generalizable?\n\nWe want to generalize the results of our analysis from the data we have to the situation we care about.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#the-logic-of-testing-hypotheses",
    "href": "Week4/05-hyptest.html#the-logic-of-testing-hypotheses",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "A hypothesis is a statement about a population, or general pattern.\nTesting a hypothesis amounts to gathering information (sampling) from a dataset and, based on that information, deciding whether that hypothesis is false or true in the population.\nTwo decisions are possible:\n\nRejecting the hypothesis (if there is enough evidence against it)\nNot rejecting it (if there is not enough evidence against it)\n\nRejecting a hypothesis is a more conclusive decision than not rejecting it.\n\nOne such focused approach uses a statistic (e.g. a difference in two means) computed from our data to see whether its true value is equal to something we assume (e.g. the difference is zero). This is called hypothesis testing: using results in the data to see if we have enough evidence to tell whether a hypothesis (the two means are equal) is wrong (they are not equal) or whether we don’t have enough evidence (they may be equal.\nTesting a hypothesis is a way of making an inference, with a focus on a specific statement. As with any kind of inference, we have to assess external validity too: the extent to which the population, or general pattern, represented by our data is the same as the population, or general pattern, we are truly interested in.\n\nGiven a population (i.e. a distribution) with a parameter of interest (which could be the mean, variance, correlation, etc.), we would like to decide between to complementary statements concerning the parameter.\nThese statements are called statistical hypotheses.\nThe choice or decision between these hypotheses is to be based on a sample taken from the population of interest.\nThe goal is to be able to choose which hypothesis is true in reality based on the sample data.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#steps-of-analysis-hypothesis-testing",
    "href": "Week4/05-hyptest.html#steps-of-analysis-hypothesis-testing",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "What statistic is appropriate to answer our question?\nNull and Alternative Hypotheses\n\n\nWhat information is needed to answer our question?\nWhat is our population? How do we sample from the population in a statistically valid way?\n\n\n\n\nFit appropriate model\nCalculate test statistic\nAccount for variability",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#formulate-research-question-1",
    "href": "Week4/05-hyptest.html#formulate-research-question-1",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "This is where all analysis or research starts - what is it you want to know, and what will you do with that information?\nFor our AI Jobs dataset some research questions we might ask are:\n\nWhich jobs are the highest paid?\nWhich industries have seen the most AI adoption?\nAre lower paid jobs at more risk of being automated?",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#specify-hypotheses-1",
    "href": "Week4/05-hyptest.html#specify-hypotheses-1",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Now we translate our general research questions into specific and testable hypotheses.\nThe actual test begins by considering two hypotheses. They are called the null hypothesis and the alternative hypothesis. These hypotheses contain opposing viewpoints.\n\n\nThe null hypothesis (H_0) : It is often a statement of the accepted historical value or norm. This is your starting point that you must assume from the beginning in order to show an effect exists.\n\nThe alternative hypothesis (H_a) : It is a claim about the population that is contradictory to H_0 and what we conclude when we reject H_0",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#specify-hypotheses-2",
    "href": "Week4/05-hyptest.html#specify-hypotheses-2",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "After you have determined which hypothesis the sample supports, you make a decision.\nThere are two options for a decision:\n\n“reject H_0” if the sample information favors the alternative hypothesis, or\n“do not reject H_0” or “decline to reject H_0” if the sample information is insufficient to reject the null hypothesis.\n\n\nMathematical symbols used in H0 and Ha:\nFigure 6.12: Null and Alternative Hypotheses\n\n\n\n\n\n\nH_0\nH_a\n\n\n\nequal (=)\nnot equal (\\neq) or greater than (&gt;) or less than (&lt;)\n\n\ngreater than or equal to (\\geq)\nless than (&lt;)\n\n\nless than or equal to (\\leq)\nmore than (&gt;)",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#specify-hypotheses-3",
    "href": "Week4/05-hyptest.html#specify-hypotheses-3",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Decide what statistic is appropriate to answer our question. What do we need to calculate from our sample?\n\nDifference in mean(salary_usd) for each level of automation_risk\n\nSince there are three automation risk levels (Low, Medium, High), the comparison we could make are:\n\n\\text{mean}_{\\text{high risk}} - \\text{mean}_{\\text{medium risk}}\n\\text{mean}_{\\text{high}} - \\text{mean}_{\\text{low}}\n\\text{mean}_{\\text{medium}} - \\text{mean}_{\\text{low}}",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#specify-hypotheses-4",
    "href": "Week4/05-hyptest.html#specify-hypotheses-4",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "For now, let’s focus on just (2). Therefore:\ns = \\overline{salary}_{\\text{high risk}} - \\overline{salary}_{\\text{low risk}}\ns is our statistic of interest. We are interested in the true value of s in the population (s_{true}).\nWhat we can actually calculate is \\hat{s} the value of s in our sample.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#specify-hypotheses-5",
    "href": "Week4/05-hyptest.html#specify-hypotheses-5",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Our hypothesis, plainly stated, is:\nThere is a relationship between job salary and the likelihood of automation. Expressed another way, the average salary of high risk jobs is different from that of low risk jobs.\nWhat are the Null Hypothesis H_0 and Alternative Hypothesis H_A ?",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#specify-hypotheses-6",
    "href": "Week4/05-hyptest.html#specify-hypotheses-6",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "There is no difference in the average salary between those jobs at high risk of automation, and those at low risk: \nH_0: s_{true} = 0\n\n\n\n\n\nThe difference in the average salary of high risk jobs and low risk jobs is not zero: \nH_A: s_{true} \\neq 0\n\n\n\n\nThe null says that the true value of the statistic is zero; the alternative says that it’s not zero. Together, these cover all logical possibilities for the true value of the statistic.\nIt may seem odd to have H_0: s_{true} = 0 when, presumably, we analyze the data because we suspect that the true value of s is not zero. This seemingly twisted logic comes from the fact that testing a hypothesis amounts to seeing if there is enough evidence in our data to reject the null. It is sometimes said that the null is protected: it should not be too easy to reject it otherwise the conclusions of hypothesis testing would not be strong.\nAs we introduce the concept of hypothesis testing, it is helpful to relate its logic to the logic of a criminal court procedure. At court the task is to decide whether an accused person is guilty or innocent of a certain crime. In most modern societies the starting point is the assumption of innocence: the accused person should be judged guilty only if there is enough evidence against their innocence. This is so even though the accused person was brought before court presumably because there was a suspicion of their guilt. To translate this procedure to the language of hypothesis testing, H0 is that the person is innocent, and HA is that the person is guilty.\nMedical tests are another instructive example. When testing whether a person has a certain medical condition, the null is that the person does not have the condition (healthy), and the alternative is that they have it (sick). The testing procedure amounts to gathering information to see if there is evidence to decide that the person has the condition.\nThe case when we test if HA: strue ≠ 0 is called a two-sided alternative as it allows for strue to be either greater than zero or less than zero. For instance, we focus on the difference in online and offline prices, with H0 being the equality. In such a case we are not really interested if the difference is positive or not, or whether it is negative or not.\nThe other case is working with a one-sided alternative, when we are indeed interested if a statistic is positive. The null and the alternative should be set up so that the hypothesis we are truly interested in is in the alternative set. So when we want to know if strue is positive, we want to put strue &gt; 0 in the alternative thus, making the null strue ≤ 0:\n\nH_0: s_{true} \\leq 0\n\n\nH_A: s_{true} &gt; 0",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#using-the-sample-to-test-the-null-hypothesis",
    "href": "Week4/05-hyptest.html#using-the-sample-to-test-the-null-hypothesis",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Once you have defined your hypotheses the next step in the process, is to collect sample data.\nIn this case, we already have the data in ai_jobs. Before moving on to actually testing the hypothesis, let’s take the naive approach - just calculate \\hat{s} the difference between the two groups.\n\nCodemean_high &lt;- mean(ai_jobs_high$salary_usd)\nmean_low &lt;- mean(ai_jobs_low$salary_usd)\ns_hat &lt;- mean_high - mean_low\n\n\n\n\\hat{s} = \\overline{\\text{salary}}_\\text{high risk} - \\overline{\\text{salary}}_{low risk} = \\$81.7 k - \\$99.7k = \\textbf{-18.1k}",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#compute-the-test-statistic",
    "href": "Week4/05-hyptest.html#compute-the-test-statistic",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "There are several statistical tests used in Hypothesis Testing. Which one you use depends on what type of hypothesis you are testing and what kind of data you have.\nFor this example, where we are testing the difference in means of a numerical variable (salary_usd) across different groups (automation_risk), we use a test called the t-test.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#compute-the-test-statistic-1",
    "href": "Week4/05-hyptest.html#compute-the-test-statistic-1",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Following the logic of hypothesis testing, we start from the assumption that the null (H_0) is true and thus s_{true} = 0.\nWe look at the evidence to see if we want to reject this null or maintain our assumption that it’s true.\nThe evidence we look for is how far the estimated value \\hat{s} is from zero.\nWe reject H_0 if the distance is large (i.e. \\hat{s} is sufficiently greater or lesser than 0)\n\n\nHow far is far enough?\nThe test statistic is the measure of how far the estimated value \\hat{s} is from what its true value would be if H_0 is true.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#compute-the-test-statistic-2",
    "href": "Week4/05-hyptest.html#compute-the-test-statistic-2",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "t-statistic:\n\nt = \\frac{\\hat{s}}{\\mathrm{SE}(\\hat{s})} = \\frac{\\bar{x}_A - \\bar{x}_B}{\\mathrm{SE}(\\bar{x}_A - \\bar{x}_B)}\n\n\nThe t-test is a procedure to decide whether we can reject the null H_0.\nThe magnitude of the t-statistic t measures the distance of \\hat{s} from what s_{true} would be if the null were true.\n\nThe unit of distance is the standard error.\n\n\nThe t-statistic transforms the original statistic of interest into a standardized version\n\nFor example: if t = 1 (or -1), it means \\hat{s} is exactly one standard error away from zero.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#compute-the-test-statistic-3",
    "href": "Week4/05-hyptest.html#compute-the-test-statistic-3",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "R makes it very easy to apply a test such as the t-test. For most statistical tests, there exists a simple function to compute it.\nFor the t-test, we use the t.test() function:\n\nCodet_res &lt;- t.test(ai_jobs_high$salary_usd, ai_jobs_low$salary_usd)\n\n\nt = -6.54",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#making-a-decision",
    "href": "Week4/05-hyptest.html#making-a-decision",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "The following step is making a decision: either rejecting the null or not rejecting it.\nIn hypothesis testing, this decision is based on a clear rule specified in advance.\nWe specify in advance to avoid bias - before looking at the data, we state what it would take to reject the null hypothesis. We follow what the data says, whatever result that may be.\n\n\nA clear rule also makes the decision transparent, which helps avoid biases in the decision. Unfortunately, we humans are often tempted to use evidence to support our pre-existing views or prejudices. If, for example, we think that jobs which are more highly valued by companies (i.e. have a higher salary) we may pay more attention to the evidence that supports that belief than to the evidence against it.\nIn partiular, we may be tempted to say that the estimated \\hat{s} difference is large enough to reject the null, because we believe that the null isn’t true. Clear decision rules are designed to minimize the room for such temptations.\n\n\nOnce you have your test statistic there are two methods to use it to make your decision:\n\nCritical value method\nP-Value method – This is the preferred method we mostly will focus on.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#critical-values",
    "href": "Week4/05-hyptest.html#critical-values",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "We use a critical value to tell us whether the test statistic is large enough - is it far enough away from zero to reject the null?\nTo define the critical value, we need to decide how conservative we want to be with the evidence.\nThe larger we set the critical value, the harder it is to reject the null hypothesis.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#critical-values-1",
    "href": "Week4/05-hyptest.html#critical-values-1",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "As with the sampling distribution for means we looked at earlier, our test-statistic t also has a sampling distribution. If we were to sample many times and calculate t for each sample, we would again get a distribution with a specific shape and parameters.\n\n\nThe sampling distribution of the test statistic when the null is true.\n\nRecall: Approximately 95% of values fall within two standard deviations of the distribution.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#critical-values-2",
    "href": "Week4/05-hyptest.html#critical-values-2",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "The sampling distribution of the test statistic when the null is true.\n\n\nRecall: Approximately 95% of values fall within two standard deviations of the distribution.\nSince 95% of values fall within 2 SD, if we want to reject the null hypothesis with 95% confidence, then we say that our test statistic must fall outside of 2 SD.\nIn other words, since the units of \\hat{s} are standard deviations: \\hat{s} \\geq \\pm 2",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#critical-values-3",
    "href": "Week4/05-hyptest.html#critical-values-3",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "A critical value of 2 is standard. However, it is ultimately just a convention. We could choose to set other critical values that correspond to different probabilities. There is not anything inherently special about setting our threshold at 95% vs 90%.\nIf we make the critical value \\pm 1.6, the chance of a false positive is 10%.\n\n\n\n\n\nDifferent fields have different standards for evidence - for instance, a critical value of 5 (99.994%) is standard in particle physics (referred to as 5\\sigma).",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#interpret-our-results",
    "href": "Week4/05-hyptest.html#interpret-our-results",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Since our test statistic t = -7.5 &lt; -2, at a confidence level of 95%, we would have sufficient evidence to reject H_0\nTherefore, we would say:\n\nThe average salary of jobs at high risk of automation is not the same as the average salary of jobs at low risk.\nWe have evidence that higher salary jobs are at less risk of automation than low salary jobs.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#interpret-our-results-1",
    "href": "Week4/05-hyptest.html#interpret-our-results-1",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "This does not inherently mean we accept the alternative hypothesis. We are narrowing the realm of possible answers, but very rarely (perhaps never) are we able to statistically prove a single explanation in one go.\nWe have increased our reasons to believe our hypothesis, but several other possibilities exist.\nScience is then the process of continually investigating our hypothesis and pitting it against new null hypotheses and rejecting them as well.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#p-value-method",
    "href": "Week4/05-hyptest.html#p-value-method",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Hopefully, the critical value is fairly intuitive to you now. However, it is not the typical way that statistical results are presented.\nInstead, you will typically see something called a p-value.\n\n\np-value: The probability than an event will occur, assuming the null hypothesis is true.\nThe p-value essentially flips the critical value statement:\n\nInstead of saying a test statistic value &gt; 2 falls outside the 95% bound, we calculate where out test statistic falls in the distribution\n\n\nThe p-value is the probability that the test statistic will be as large, or larger, than we calculate from the data, if the null hypothesis is true. i.e. P(data | H_0).",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#p-value",
    "href": "Week4/05-hyptest.html#p-value",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "p = P(|t| &gt; \\text{critical value})\nBecause the p-value tells us the smallest level of significance at which we can reject the null hypothesis, it summarizes all the information we need to make the decision.\nThis is why the p-value is used - rather than needing to set a critical value and calculate the test statistic, we can instead use just the p-value.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#interpreting-the-p-value",
    "href": "Week4/05-hyptest.html#interpreting-the-p-value",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Like with the critical value, we should set our desired significance level before carrying out the analysis.\nWe then compare our calculated p-value with the significance level. If it is less, we reject the null hypothesis.\nThe significance level (\\alpha) is the probability that a true null hypothesis will be rejected.\nA typical significance level is \\alpha &lt; 0.05, which corresponds with a critical value of 2, or a probability of 5%.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#interpreting-the-p-value-1",
    "href": "Week4/05-hyptest.html#interpreting-the-p-value-1",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "R output\nT distribution\n\n\n\nAgain, R will provide us with the p-value. Let’s now look at the full output from our t.test():\n\nCodet.test(ai_jobs_high$salary_usd, ai_jobs_low$salary_usd)\n\n\n    Welch Two Sample t-test\n\ndata:  ai_jobs_high$salary_usd and ai_jobs_low$salary_usd\nt = -6.5366, df = 288.07, p-value = 2.854e-10\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -23513.51 -12630.27\nsample estimates:\nmean of x mean of y \n 81673.57  99745.46 \n\n\n\nIf our p-value is smaller than our pre-set significance level (\\alpha), we reject the null hypothesis and can say the result is “statistically significant” at p &lt; 0.05.\n\n\n\n\nCodexpos &lt;- seq(-5, 5, by = 0.01)\n\ndegree &lt;- 280\nypos &lt;- dt(xpos, df = degree)\n\nggplot() +\n  xlim(-8, 8) +\n  geom_function(aes(colour = \"t, df=280\"), fun = dt, args = list(df = 280), linewidth = 1.5) +\n  geom_vline(xintercept = t_res$statistic, color = \"black\", linewidth = 1.5) +\n  xlab(\"Test statistic under the null hypothesis P(data | H0)\") +\n  ylab(\"Frequency\") +\n  ggtitle(\"Student's T distribution\")",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#decision-and-conclusion",
    "href": "Week4/05-hyptest.html#decision-and-conclusion",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "The preset \\alpha is the probability of a False Positive error (called a Type I error) - rejecting the null hypothesis when the null hypothesis is true.\nBack to our two possible decisions:\n\n\nIf \\alpha &gt; \\text{p-value}, reject H_0.\n\nThe results of the sample are statistically significant.\nWe can say there is sufficient evidence to conclude that H_0 is an incorrect believe and that the alternative hypothesis, H_A may be correct.\n\n\n\nIf \\alpha &lt; \\text{p-value}, fail to reject H_0.\n\nThe results of the sample are not significant. There is not sufficient evidence to conclude that the alternative hypothesis H_A may be correct.\n\n\n\n\nNOTE: When you “do not reject H_0”, it does not mean that you should believe that H_0 is true. It simply means that the sample data have failed to provide sufficient evidence to cast serious doubt about the truthfulness of H_0.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/05-hyptest.html#closing---what-does-a-statistically-significant-result-mean",
    "href": "Week4/05-hyptest.html#closing---what-does-a-statistically-significant-result-mean",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Effect size\nSample size\n\n\n\nNo. There is an essential distinction between statistical significance and practical significance.\nLet’s say we performed an experiment to examine the effect of a particular diet on body weight, which gives a statistically significant effect at p &lt; 0.05. This doesn’t tell us how much weight was lost, which we refer to as the effect size.\nWould the loss of 20 grams (i.e. the weight of a few potato chips) be practically significant, even if it were statistically significant?\nWhether a result is practically significant depends on the effect size and the context of the research question. It’s up to the researcher to know whether it is meaningful.\n\n\nAs with the standard error (and a direct result of it), the p-value depends on the sample size. A very large sample size will give a statistically significant result in many cases, even with a very small effect size.\n\n\nThe proportion of significant results for a very small change (~20g which is about 0.001 standard deviations) as a function of sample size",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/03-exercise.html",
    "href": "Week4/03-exercise.html",
    "title": "Exercise",
    "section": "",
    "text": "Data Overview\nData Preview\n\n\n\nThe “AI-Powered Job Market Insights” dataset provides a snapshot of the modern job market, particularly focusing on the role of artificial intelligence (AI) and automation across various industries.\nThis dataset includes 500 unique job listings, each characterized by different factors like industry, company size, AI adoption level, automation risk, required skills, and job growth projections.\n\n\n\nCodeai_jobs |&gt;\n  gt_preview() |&gt;\n  tab_header(title = \"AI-Powered Job Market Insights\") |&gt;\n  tab_source_note(source_note = \"Source: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\")\n\n\n\n\n\n\nAI-Powered Job Market Insights\n\n\n\njob_title\nindustry\ncompany_size\nlocation\nai_adoption_level\nautomation_risk\nrequired_skills\nsalary_usd\nremote_friendly\njob_growth_projection\n\n\n\n\n1\nCybersecurity Analyst\nEntertainment\nSmall\nDubai\nMedium\nHigh\nUX/UI Design\n111392.17\nYes\nGrowth\n\n\n2\nMarketing Specialist\nTechnology\nLarge\nSingapore\nMedium\nHigh\nMarketing\n73792.56\nNo\nDecline\n\n\n3\nAI Researcher\nTechnology\nLarge\nSingapore\nMedium\nLow\nUX/UI Design\n137170.26\nYes\nGrowth\n\n\n4\nSales Manager\nRetail\nSmall\nBerlin\nLow\nMedium\nProject Management\n83027.95\nNo\nGrowth\n\n\n5\nCybersecurity Analyst\nEntertainment\nSmall\nTokyo\nLow\nLow\nJavaScript\n87752.92\nYes\nDecline\n\n\n6..499\n\n\n\n\n\n\n\n\n\n\n\n\n500\nHR Manager\nEntertainment\nMedium\nBerlin\nMedium\nHigh\nProject Management\n53764.38\nYes\nDecline\n\n\n\nSource: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\n\n\n\n\n\n\n\n\n\nTo simplify our later code, I have created a separate table which is already filtered for the groups we will be looking at:\n\nCodeai_jobs_risk &lt;- ai_jobs |&gt;\n  filter(automation_risk %in% c(\"Low\", \"High\"))\n\nai_jobs_high &lt;- ai_jobs_risk |&gt;\n  filter(automation_risk == \"High\")\n\nai_jobs_low &lt;- ai_jobs_risk |&gt;\n  filter(automation_risk == \"Low\")\n\n\nDataset Features:\nSource: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\n\n\nJob_Title:\n\n\nDescription: The title of the job role.\n\nType: Categorical\n\nExample Values: “Data Scientist”, “Software Engineer”, “HR Manager”\n\n\n\nIndustry:\n\n\nDescription: The industry in which the job is located.\n\nType: Categorical\n\nExample Values: “Technology”, “Healthcare”, “Finance”\n\n\n\nCompany_Size:\n\n\nDescription: The size of the company offering the job.\n\nType: Ordinal\n\nCategories: “Small”, “Medium”, “Large”\n\n\n\nLocation:\n\n\nDescription: The geographic location of the job.\n\nType: Categorical\n\nExample Values: “New York”, “San Francisco”, “London”\n\n\n\nAI_Adoption_Level:\n\n\nDescription: The extent to which the company has adopted AI in its operations.\n\nType: Ordinal\n\nCategories: “Low”, “Medium”, “High”\n\n\n\nAutomation_Risk:\n\n\nDescription: The estimated risk that the job could be automated within the next 10 years.\n\nType: Ordinal\n\nCategories: “Low”, “Medium”, “High”\n\n\n\nRequired_Skills:\n\n\nDescription: The key skills required for the job role.\n\nType: Categorical\n\nExample Values: “Python”, “Data Analysis”, “Project Management”\n\n\n\nSalary_USD:\n\n\nDescription: The annual salary offered for the job in USD.\n\nType: Numerical\n\nValue Range: $30,000 - $200,000\n\n\n\nRemote_Friendly:\n\n\nDescription: Indicates whether the job can be performed remotely.\n\nType: Categorical\n\nCategories: “Yes”, “No”\n\n\n\nJob_Growth_Projection:\n\n\nDescription: The projected growth or decline of the job role over the next five years.\n\nType: Categorical\n\nCategories: “Decline”, “Stable”, “Growth”\n\n\n\n\n\nTo begin, split into three groups. Decide on the following roles, one per person:\n\nSampler - draws samples from the population\nRecording - records the draws in R\n\nYour group has been presented with a population model (bag containing salaries). There may be multiple populations represented in the various bags around the room.\nPlease DO NOT look in the bag OR empty the contents of the bag.\n\n\n\nThe Sampler draws 10 slips from the bag - this is your sample of size 5.\nThe Recorder records the values on the slips into the sampling-exercise.R file.\n\nCreate a new list for each sample (sample_1 &lt;- c(1, 2, 3, 2, 2))\n\n\nCalculate the mean for the new sample and add it to the sample_means table.\nRun the code to plot the histogram of your samples.\nReturn the slips to the bag.\nEnsure the data is well mixed between samples.\nRepeat"
  },
  {
    "objectID": "Week4/03-exercise.html#exercise-dataset",
    "href": "Week4/03-exercise.html#exercise-dataset",
    "title": "Exercise",
    "section": "",
    "text": "Data Overview\nData Preview\n\n\n\nThe “AI-Powered Job Market Insights” dataset provides a snapshot of the modern job market, particularly focusing on the role of artificial intelligence (AI) and automation across various industries.\nThis dataset includes 500 unique job listings, each characterized by different factors like industry, company size, AI adoption level, automation risk, required skills, and job growth projections.\n\n\n\nCodeai_jobs |&gt;\n  gt_preview() |&gt;\n  tab_header(title = \"AI-Powered Job Market Insights\") |&gt;\n  tab_source_note(source_note = \"Source: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\")\n\n\n\n\n\n\nAI-Powered Job Market Insights\n\n\n\njob_title\nindustry\ncompany_size\nlocation\nai_adoption_level\nautomation_risk\nrequired_skills\nsalary_usd\nremote_friendly\njob_growth_projection\n\n\n\n\n1\nCybersecurity Analyst\nEntertainment\nSmall\nDubai\nMedium\nHigh\nUX/UI Design\n111392.17\nYes\nGrowth\n\n\n2\nMarketing Specialist\nTechnology\nLarge\nSingapore\nMedium\nHigh\nMarketing\n73792.56\nNo\nDecline\n\n\n3\nAI Researcher\nTechnology\nLarge\nSingapore\nMedium\nLow\nUX/UI Design\n137170.26\nYes\nGrowth\n\n\n4\nSales Manager\nRetail\nSmall\nBerlin\nLow\nMedium\nProject Management\n83027.95\nNo\nGrowth\n\n\n5\nCybersecurity Analyst\nEntertainment\nSmall\nTokyo\nLow\nLow\nJavaScript\n87752.92\nYes\nDecline\n\n\n6..499\n\n\n\n\n\n\n\n\n\n\n\n\n500\nHR Manager\nEntertainment\nMedium\nBerlin\nMedium\nHigh\nProject Management\n53764.38\nYes\nDecline\n\n\n\nSource: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\n\n\n\n\n\n\n\n\n\nTo simplify our later code, I have created a separate table which is already filtered for the groups we will be looking at:\n\nCodeai_jobs_risk &lt;- ai_jobs |&gt;\n  filter(automation_risk %in% c(\"Low\", \"High\"))\n\nai_jobs_high &lt;- ai_jobs_risk |&gt;\n  filter(automation_risk == \"High\")\n\nai_jobs_low &lt;- ai_jobs_risk |&gt;\n  filter(automation_risk == \"Low\")\n\n\nDataset Features:\nSource: Kaggle https://www.kaggle.com/datasets/uom190346a/ai-powered-job-market-insights\n\n\nJob_Title:\n\n\nDescription: The title of the job role.\n\nType: Categorical\n\nExample Values: “Data Scientist”, “Software Engineer”, “HR Manager”\n\n\n\nIndustry:\n\n\nDescription: The industry in which the job is located.\n\nType: Categorical\n\nExample Values: “Technology”, “Healthcare”, “Finance”\n\n\n\nCompany_Size:\n\n\nDescription: The size of the company offering the job.\n\nType: Ordinal\n\nCategories: “Small”, “Medium”, “Large”\n\n\n\nLocation:\n\n\nDescription: The geographic location of the job.\n\nType: Categorical\n\nExample Values: “New York”, “San Francisco”, “London”\n\n\n\nAI_Adoption_Level:\n\n\nDescription: The extent to which the company has adopted AI in its operations.\n\nType: Ordinal\n\nCategories: “Low”, “Medium”, “High”\n\n\n\nAutomation_Risk:\n\n\nDescription: The estimated risk that the job could be automated within the next 10 years.\n\nType: Ordinal\n\nCategories: “Low”, “Medium”, “High”\n\n\n\nRequired_Skills:\n\n\nDescription: The key skills required for the job role.\n\nType: Categorical\n\nExample Values: “Python”, “Data Analysis”, “Project Management”\n\n\n\nSalary_USD:\n\n\nDescription: The annual salary offered for the job in USD.\n\nType: Numerical\n\nValue Range: $30,000 - $200,000\n\n\n\nRemote_Friendly:\n\n\nDescription: Indicates whether the job can be performed remotely.\n\nType: Categorical\n\nCategories: “Yes”, “No”\n\n\n\nJob_Growth_Projection:\n\n\nDescription: The projected growth or decline of the job role over the next five years.\n\nType: Categorical\n\nCategories: “Decline”, “Stable”, “Growth”"
  },
  {
    "objectID": "Week4/03-exercise.html#exercise---mystery-bags",
    "href": "Week4/03-exercise.html#exercise---mystery-bags",
    "title": "Exercise",
    "section": "",
    "text": "To begin, split into three groups. Decide on the following roles, one per person:\n\nSampler - draws samples from the population\nRecording - records the draws in R\n\nYour group has been presented with a population model (bag containing salaries). There may be multiple populations represented in the various bags around the room.\nPlease DO NOT look in the bag OR empty the contents of the bag."
  },
  {
    "objectID": "Week4/03-exercise.html#exercise---mystery-bags-nonincremental",
    "href": "Week4/03-exercise.html#exercise---mystery-bags-nonincremental",
    "title": "Exercise",
    "section": "",
    "text": "The Sampler draws 10 slips from the bag - this is your sample of size 5.\nThe Recorder records the values on the slips into the sampling-exercise.R file.\n\nCreate a new list for each sample (sample_1 &lt;- c(1, 2, 3, 2, 2))\n\n\nCalculate the mean for the new sample and add it to the sample_means table.\nRun the code to plot the histogram of your samples.\nReturn the slips to the bag.\nEnsure the data is well mixed between samples.\nRepeat"
  },
  {
    "objectID": "Week4/01-sampling.html",
    "href": "Week4/01-sampling.html",
    "title": "Statistical Inference",
    "section": "",
    "text": "Adapted from:\n\n\nSignificant Statistics, Chapter 6 - Foundations of Inference. John Morgan Russell (2020).\n\nStatistical Thinking, Chapter 9 - Hypothesis Testing. Russell A. Poldrack (2019).\n\n\n\nIt is often necessary to “guess”, infer, or generalize about the outcome of an event in order to make a decision. Politicians study polls to guess their likelihood of winning an election. Teachers choose a particular course of study based on what they think students can comprehend. Doctors choose the treatments needed for various diseases based on their assessment of likely results. You may have visited a casino where people play games chosen because of the belief that the likelihood of winning is good. You may have chosen your course of study based on the probable availability of jobs.\n\n\nThe goal of statistical inference is to generalise - to make statements about a population based on a sample.\nStatistical inference uses what we know about probability to make our best “guesses” from samples about what we don’t know about the population.\n\n\n\n\nPoint estimation\n\nUsing sample data to calculate a single statistic as an estimate of an unknown population parameter\nExample: What is the average height of undergraduates at this university? What is the average construction cost of an office building in London? What was it in 2019?\n\n\nConfidence intervals\n\nAn interval built around a point estimate for an unknown population parameter.\n\n\nHypothesis testing\n\nA decision making procedure for determining whether sample evidence supports a hypothesis.\n\n\n\n\n\nThese three examples make up the main forms of statistical inference. However, there are many other forms of statistical inference, such as regression analysis - e.g. How much does building energy use change as occupancy increases?\n\n\n\nSuppose you were trying to determine the mean rent of a two-bedroom apartment in your town. You might look in the classified section of the newspaper, write down several rents listed, and average them together. You would have obtained a point estimate of the true mean. If you are trying to determine the percentage of times you make a basket when shooting a basketball, you might count the number of shots you make and divide that by the number of shots you attempted. In this case, you would have obtained a point estimate for the true proportion.\n\nThe most natural way to estimate features of the population (parameters) is to use the corresponding summary statistic calculated from the sample. Some common point estimates and their corresponding parameters are found in the following table:\n\n\nParameters and Point Estimates\n\n\n\n\n\n\nParameter\nMeasure\nStatistic\n\n\n\n\\mu\nMean of a single population\n\\bar{x}\n\n\np\nProportion of a single population\n\\hat{p}\n\n\n\\mu_D\nMean difference of two dependent populations\n\\bar{x}_D\n\n\n\\mu_1 - \\mu_2\nDifference in means of two independent populations\n\\bar{x}_1 - \\bar{x}_2\n\n\np_1 - p_2\nDifference in proportions of two population\n\\hat{p}_1 - \\hat{p}_2\n\n\n\\sigma^2\nVariance of a single population\nS^2\n\n\n\\sigma\nStandard deviation of a single population\nS\n\n\n\n\n\n\n\n\n\nParameters and Point Estimates\n\nParameter\nStatistic\n\n\n\n\\mu\n\\bar{x}\n\n\np\n\\hat{p}\n\n\n\\mu_1 - \\mu_2\n\\bar{x}_1 - \\bar{x}_2\n\n\np_1 - p_2\n\\hat{p}_1 - \\hat{p}_2\n\n\n\\sigma^2\nS^2\n\n\n\\sigma\nS\n\n\n\n\n\nSuppose the mean weight of a sample of 60 adults is 173.3 lbs; this sample mean is a point estimate of the population mean weight, \\mu.\nRemember: this is one of many samples that we could have taken from the population.\nIf a different random sample of 60 individuals were taken from the same population, the new sample mean would likely be different as a result of sampling variability. While estimates generally vary from one sample to another, the population mean is a fixed value.\n\n\n\nSuppose a poll suggested the US President’s approval rating is 45%. We would consider 45% to be a point estimate of the approval rating we might see if we collected responses from the entire population. This entire-population response proportion is generally referred to as the parameter of interest. When the parameter is a proportion, it is often denoted by p, and we often refer to the sample proportion as \\hat{p} (pronounced “p-hat”). Unless we collect responses from every individual in the population, p remains unknown, and we use $ as our estimate of p.\nHow would one estimate the difference in average weight between men and women? Suppose a sample of men yields a mean of 185.1 lbs and a sample of women men yields a mean of 162.3 lbs. What is a good point estimate for the difference in these two population means? We will expand on this in following chapters.\n\n\n\nSampling variability\nWe have established that different samples yield different statistics due to sampling variability. These statistics have their own distributions, called sampling distributions, that reflect this as a random variable. The sampling distribution of a sample statistic is the distribution of the point estimates based on samples of a fixed size, n, from a certain population. It is useful to think of a particular point estimate as being drawn from a sampling distribution.\nRecall the sample mean weight calculated from a previous sample of 173.3 lbs. Suppose another random sample of 60 participants might produce a different value of x, such as 169.5 lbs. Repeated random sampling could result in additional different values, perhaps 172.1 lbs, 168.5 lbs, and so on. Each sample mean can be thought of as a single observation from a random variable X. The distribution of X is called the sampling distribution of the sample mean, and has its own mean and standard deviation like the random variables discussed previously. We will simulate the concept of a sampling distribution using technology to repeatedly sample, calculate statistics, and graph them. However, the actual sampling distribution would only be attainable if we could theoretically take an infinite amount of samples.\nEach of the point estimates in the table above have their own unique sampling distributions which we will look at in the future\n\n\nWhat makes a statistical estimate of this parameter of interest a “Good” one? It must be both accurate and precise.\nAlthough variability in samples is present, there remains a fixed value for any population parameter.\n\nAccording to the law of large numbers, probabilities converge to what we expect over time.\nPoint estimates follow this rule, becoming more accurate with increasing sample size.\n\nNational Health and Nutrition Examination Survey (NHANES) from the US Centers for Disease Control (CDC)\n\nCodeNHANES_adult |&gt;\n  select(c(\"SurveyYr\", \"Gender\", \"Age\", \"Race1\", \"Education\", \"Weight\", \"Height\", \"Pulse\", \"Diabetes\")) |&gt;\n  gt_preview() |&gt;\n  tab_header(title = \"NHANES Dataset\")\n\n\n\n\n\n\nNHANES Dataset\n\n\n\nSurveyYr\nGender\nAge\nRace1\nEducation\nWeight\nHeight\nPulse\nDiabetes\n\n\n\n\n1\n2009_10\nmale\n34\nWhite\nHigh School\n87.4\n164.7\n70\nNo\n\n\n2\n2009_10\nfemale\n49\nWhite\nSome College\n86.7\n168.4\n86\nNo\n\n\n3\n2009_10\nfemale\n45\nWhite\nCollege Grad\n75.7\n166.7\n62\nNo\n\n\n4\n2009_10\nmale\n66\nWhite\nSome College\n68.0\n169.5\n60\nNo\n\n\n5\n2009_10\nmale\n58\nWhite\nCollege Grad\n78.4\n181.9\n62\nNo\n\n\n6..4785\n\n\n\n\n\n\n\n\n\n\n\n4786\n2011_12\nmale\n60\nWhite\nCollege Grad\n78.4\n168.8\n76\nYes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe accuracy of an estimate refers to how well it estimates the actual value of that parameter. Mathematically, this is true when that the expected value your statistic is equal to the value of that parameter. This can be visualized as the center of the sampling distribution appearing to be situated at the value of that parameter.\nAccording to the law of large numbers, probabilities converge to what we expect over time. Point estimates follow this rule, becoming more accurate with increasing sample size. The figure above shows the sample mean weight calculated for random samples drawn, where sample size increases by 1 for each draw until sample size equals 500. The maroon dashed horizontal line is drawn at the average weight of all adults 169.7 lbs, which represents the population mean weight according to the CDC.\nThe figure above shows the sample mean weight calculated for random samples drawn, where sample size increases by 1 for each draw until sample size equals 500. The maroon dashed horizontal line is drawn at the average weight of all adults 169.7 lbs, which represents the population mean weight according to the CDC.\nNote how a sample size around 50 may produce a sample mean that is as much as 10 lbs higher or lower than the population mean. As sample size increases, the fluctuations around the population mean decrease; in other words, as sample size increases, the sample mean becomes less variable and provides a more reliable estimate of the population mean.\nIn addition to accuracy, a precise estimate is also more useful. This means when repeatedly sampling, the values of the statistics seem pretty close together. The precision of an estimate can be visualized as the spread of the sampling distribution, usually quantified by the standard deviation. The phrase “the standard deviation of a sampling distribution” is often shortened to the standard error. A smaller standard error means a more precise estimate and is also effected by sample size.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Week4/01-sampling.html#statistical-inference-1",
    "href": "Week4/01-sampling.html#statistical-inference-1",
    "title": "Statistical Inference",
    "section": "",
    "text": "The goal of statistical inference is to generalise - to make statements about a population based on a sample.\nStatistical inference uses what we know about probability to make our best “guesses” from samples about what we don’t know about the population.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Week4/01-sampling.html#statistical-inference-2",
    "href": "Week4/01-sampling.html#statistical-inference-2",
    "title": "Statistical Inference",
    "section": "",
    "text": "Point estimation\n\nUsing sample data to calculate a single statistic as an estimate of an unknown population parameter\nExample: What is the average height of undergraduates at this university? What is the average construction cost of an office building in London? What was it in 2019?\n\n\nConfidence intervals\n\nAn interval built around a point estimate for an unknown population parameter.\n\n\nHypothesis testing\n\nA decision making procedure for determining whether sample evidence supports a hypothesis.\n\n\n\n\n\nThese three examples make up the main forms of statistical inference. However, there are many other forms of statistical inference, such as regression analysis - e.g. How much does building energy use change as occupancy increases?",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Week4/01-sampling.html#point-estimation",
    "href": "Week4/01-sampling.html#point-estimation",
    "title": "Statistical Inference",
    "section": "",
    "text": "Suppose you were trying to determine the mean rent of a two-bedroom apartment in your town. You might look in the classified section of the newspaper, write down several rents listed, and average them together. You would have obtained a point estimate of the true mean. If you are trying to determine the percentage of times you make a basket when shooting a basketball, you might count the number of shots you make and divide that by the number of shots you attempted. In this case, you would have obtained a point estimate for the true proportion.\n\nThe most natural way to estimate features of the population (parameters) is to use the corresponding summary statistic calculated from the sample. Some common point estimates and their corresponding parameters are found in the following table:\n\n\nParameters and Point Estimates\n\n\n\n\n\n\nParameter\nMeasure\nStatistic\n\n\n\n\\mu\nMean of a single population\n\\bar{x}\n\n\np\nProportion of a single population\n\\hat{p}\n\n\n\\mu_D\nMean difference of two dependent populations\n\\bar{x}_D\n\n\n\\mu_1 - \\mu_2\nDifference in means of two independent populations\n\\bar{x}_1 - \\bar{x}_2\n\n\np_1 - p_2\nDifference in proportions of two population\n\\hat{p}_1 - \\hat{p}_2\n\n\n\\sigma^2\nVariance of a single population\nS^2\n\n\n\\sigma\nStandard deviation of a single population\nS",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Week4/01-sampling.html#point-estimation-1",
    "href": "Week4/01-sampling.html#point-estimation-1",
    "title": "Statistical Inference",
    "section": "",
    "text": "Parameters and Point Estimates\n\nParameter\nStatistic\n\n\n\n\\mu\n\\bar{x}\n\n\np\n\\hat{p}\n\n\n\\mu_1 - \\mu_2\n\\bar{x}_1 - \\bar{x}_2\n\n\np_1 - p_2\n\\hat{p}_1 - \\hat{p}_2\n\n\n\\sigma^2\nS^2\n\n\n\\sigma\nS\n\n\n\n\n\nSuppose the mean weight of a sample of 60 adults is 173.3 lbs; this sample mean is a point estimate of the population mean weight, \\mu.\nRemember: this is one of many samples that we could have taken from the population.\nIf a different random sample of 60 individuals were taken from the same population, the new sample mean would likely be different as a result of sampling variability. While estimates generally vary from one sample to another, the population mean is a fixed value.\n\n\n\nSuppose a poll suggested the US President’s approval rating is 45%. We would consider 45% to be a point estimate of the approval rating we might see if we collected responses from the entire population. This entire-population response proportion is generally referred to as the parameter of interest. When the parameter is a proportion, it is often denoted by p, and we often refer to the sample proportion as \\hat{p} (pronounced “p-hat”). Unless we collect responses from every individual in the population, p remains unknown, and we use $ as our estimate of p.\nHow would one estimate the difference in average weight between men and women? Suppose a sample of men yields a mean of 185.1 lbs and a sample of women men yields a mean of 162.3 lbs. What is a good point estimate for the difference in these two population means? We will expand on this in following chapters.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Week4/01-sampling.html#unbiased-estimation",
    "href": "Week4/01-sampling.html#unbiased-estimation",
    "title": "Statistical Inference",
    "section": "",
    "text": "Sampling variability\nWe have established that different samples yield different statistics due to sampling variability. These statistics have their own distributions, called sampling distributions, that reflect this as a random variable. The sampling distribution of a sample statistic is the distribution of the point estimates based on samples of a fixed size, n, from a certain population. It is useful to think of a particular point estimate as being drawn from a sampling distribution.\nRecall the sample mean weight calculated from a previous sample of 173.3 lbs. Suppose another random sample of 60 participants might produce a different value of x, such as 169.5 lbs. Repeated random sampling could result in additional different values, perhaps 172.1 lbs, 168.5 lbs, and so on. Each sample mean can be thought of as a single observation from a random variable X. The distribution of X is called the sampling distribution of the sample mean, and has its own mean and standard deviation like the random variables discussed previously. We will simulate the concept of a sampling distribution using technology to repeatedly sample, calculate statistics, and graph them. However, the actual sampling distribution would only be attainable if we could theoretically take an infinite amount of samples.\nEach of the point estimates in the table above have their own unique sampling distributions which we will look at in the future\n\n\nWhat makes a statistical estimate of this parameter of interest a “Good” one? It must be both accurate and precise.\nAlthough variability in samples is present, there remains a fixed value for any population parameter.\n\nAccording to the law of large numbers, probabilities converge to what we expect over time.\nPoint estimates follow this rule, becoming more accurate with increasing sample size.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Week4/01-sampling.html#example-dataset---nhanes",
    "href": "Week4/01-sampling.html#example-dataset---nhanes",
    "title": "Statistical Inference",
    "section": "",
    "text": "National Health and Nutrition Examination Survey (NHANES) from the US Centers for Disease Control (CDC)\n\nCodeNHANES_adult |&gt;\n  select(c(\"SurveyYr\", \"Gender\", \"Age\", \"Race1\", \"Education\", \"Weight\", \"Height\", \"Pulse\", \"Diabetes\")) |&gt;\n  gt_preview() |&gt;\n  tab_header(title = \"NHANES Dataset\")\n\n\n\n\n\n\nNHANES Dataset\n\n\n\nSurveyYr\nGender\nAge\nRace1\nEducation\nWeight\nHeight\nPulse\nDiabetes\n\n\n\n\n1\n2009_10\nmale\n34\nWhite\nHigh School\n87.4\n164.7\n70\nNo\n\n\n2\n2009_10\nfemale\n49\nWhite\nSome College\n86.7\n168.4\n86\nNo\n\n\n3\n2009_10\nfemale\n45\nWhite\nCollege Grad\n75.7\n166.7\n62\nNo\n\n\n4\n2009_10\nmale\n66\nWhite\nSome College\n68.0\n169.5\n60\nNo\n\n\n5\n2009_10\nmale\n58\nWhite\nCollege Grad\n78.4\n181.9\n62\nNo\n\n\n6..4785\n\n\n\n\n\n\n\n\n\n\n\n4786\n2011_12\nmale\n60\nWhite\nCollege Grad\n78.4\n168.8\n76\nYes",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Week4/01-sampling.html#unbiased-estimation-1",
    "href": "Week4/01-sampling.html#unbiased-estimation-1",
    "title": "Statistical Inference",
    "section": "",
    "text": "The accuracy of an estimate refers to how well it estimates the actual value of that parameter. Mathematically, this is true when that the expected value your statistic is equal to the value of that parameter. This can be visualized as the center of the sampling distribution appearing to be situated at the value of that parameter.\nAccording to the law of large numbers, probabilities converge to what we expect over time. Point estimates follow this rule, becoming more accurate with increasing sample size. The figure above shows the sample mean weight calculated for random samples drawn, where sample size increases by 1 for each draw until sample size equals 500. The maroon dashed horizontal line is drawn at the average weight of all adults 169.7 lbs, which represents the population mean weight according to the CDC.\nThe figure above shows the sample mean weight calculated for random samples drawn, where sample size increases by 1 for each draw until sample size equals 500. The maroon dashed horizontal line is drawn at the average weight of all adults 169.7 lbs, which represents the population mean weight according to the CDC.\nNote how a sample size around 50 may produce a sample mean that is as much as 10 lbs higher or lower than the population mean. As sample size increases, the fluctuations around the population mean decrease; in other words, as sample size increases, the sample mean becomes less variable and provides a more reliable estimate of the population mean.\nIn addition to accuracy, a precise estimate is also more useful. This means when repeatedly sampling, the values of the statistics seem pretty close together. The precision of an estimate can be visualized as the spread of the sampling distribution, usually quantified by the standard deviation. The phrase “the standard deviation of a sampling distribution” is often shortened to the standard error. A smaller standard error means a more precise estimate and is also effected by sample size.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Week3/lecture.html#what-is-probability-theory",
    "href": "Week3/lecture.html#what-is-probability-theory",
    "title": "Probability, Sampling, and Experiments",
    "section": "What is Probability Theory?",
    "text": "What is Probability Theory?\n\nBranch of mathematics dealing with chance and uncertainty\nFoundation for statistics\nProvides tools to describe uncertain events\nHistorical origins in games of chance\nDeep questions about meaning and interpretation\n\n\nProbability theory is the branch of mathematics that deals with chance and uncertainty. It forms an important part of the foundation for statistics, because it provides us with the mathematical tools to describe uncertain events.\nThe study of probability arose in part due to interest in understanding games of chance, like cards or dice. These games provide useful examples of many statistical concepts, because when we repeat these games the likelihood of different outcomes remains (mostly) the same. However, there are deep questions about the meaning of probability that we will not address here."
  },
  {
    "objectID": "Week3/lecture.html#experiment-sample-space-events",
    "href": "Week3/lecture.html#experiment-sample-space-events",
    "title": "Probability, Sampling, and Experiments",
    "section": "Experiment, Sample Space, Events",
    "text": "Experiment, Sample Space, Events\n\n\nAn experiment is any activity that produces or observes an outcome. Examples are flipping a coin, rolling a 6-sided die, or trying a new route to work to see if it’s faster than the old route.\n\nCoin flip: {heads, tails}\nDie roll: {1,2,3,4,5,6}\nTravel time: (0,∞)\n\nThe sample space is the set of possible outcomes for an experiment. We represent these by listing them within a set of squiggly brackets.\nAn event is a subset of the sample space. In principle it could be one or more of possible outcomes in the sample space, but here we will focus primarily on elementary events which consist of exactly one possible outcome.\n\nSubset of sample space\nCan be elementary or compound\nExample: rolling a 4\n\n\n\n\nTo formalize probability theory, we first need to define a few terms:\n\nAn experiment is any activity that produces or observes an outcome. Examples are flipping a coin, rolling a 6-sided die, or trying a new route to work to see if it’s faster than the old route.\nThe sample space is the set of possible outcomes for an experiment. We represent these by listing them within a set of squiggly brackets.\nAn event is a subset of the sample space. In principle it could be one or more of possible outcomes in the sample space, but here we will focus primarily on elementary events which consist of exactly one possible outcome."
  },
  {
    "objectID": "Week3/lecture.html#kolmogorovs-axioms",
    "href": "Week3/lecture.html#kolmogorovs-axioms",
    "title": "Probability, Sampling, and Experiments",
    "section": "Kolmogorov’s Axioms",
    "text": "Kolmogorov’s Axioms\nFor events \\({E_1, E_2, ... , E_N}\\) and random variable \\(X\\):\n\n\n\nNon-negativity:\n\\(P(X=E_i) \\ge 0\\)\nNormalization:\n\\(\\sum_{i=1}^N{P(X=E_i)} = 1\\)\nBoundedness:\n\\(P(X=E_i)\\le 1\\)\n\n\n\nImplications:\n\nAll probabilities are between 0 and 1\nTotal probability must sum to 1\nIndividual probabilities ≤ 1\n\n\n\n\nThese are the features that a value has to have if it is going to be a probability, which were first defined by the Russian mathematician Andrei Kolmogorov.\nThe summation is interpreted as saying “Take all of the N elementary events, which we have labeled from 1 to N, and add up their probabilities. These must sum to one.”\nThe third point is implied by the previous points; since they must sum to one, and they can’t be negative, then any particular probability cannot exceed one."
  },
  {
    "objectID": "Week3/lecture.html#basic-rules",
    "href": "Week3/lecture.html#basic-rules",
    "title": "Probability, Sampling, and Experiments",
    "section": "Basic Rules",
    "text": "Basic Rules\n\nRule of Subtraction:\n\\(P(\\neg A) = 1 - P(A)\\)\nExample: P(not rolling a 1) = \\(1 - \\frac{1}{6} = \\frac{5}{6}\\)\nIntersection Rule (independent events):\n\\(P(A \\cap B) = P(A) * P(B)\\)\nExample: P(six on both rolls) = \\(\\frac{1}{6} * \\frac{1}{6} = \\frac{1}{36}\\)\nAddition Rule:\n\\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)\n\n\nTo understand de Méré’s error, we need to introduce some of the rules of probability theory:\n\nThe rule of subtraction says that the probability of some event A not happening is one minus the probability of the event happening\nFor independent events, we compute the probability of both occurring by multiplying their individual probabilities\nThe addition rule tells us that to obtain the probability of either of two events occurring, we add together the individual probabilities, but then subtract the likelihood of both occurring together"
  },
  {
    "objectID": "Week3/lecture.html#classical-probability",
    "href": "Week3/lecture.html#classical-probability",
    "title": "Probability, Sampling, and Experiments",
    "section": "Classical Probability",
    "text": "Classical Probability\n\n\nKey Principles:\n\nEqual likelihood assumption\nBased on counting outcomes\nNo experiments needed\nCommon in games of chance\n\nBasic Formula:\n\\(P(outcome_i) = \\frac{1}{\\text{number of possible outcomes}}\\)\n\nExamples:\n\nFair coin: P(heads) = 1/2\nFair die: P(6) = 1/6\nTwo dice: P(double-six) = 1/36\n\n\n\nClassical probability arose from the study of games of chance such as dice and cards. In this approach, we compute the probability directly based on our knowledge of the situation.\nWe start with the assumption that all of the elementary events in the sample space are equally likely; that is, when you roll a die, each of the possible outcomes ({1,2,3,4,5,6}) is equally likely to occur."
  },
  {
    "objectID": "Week3/lecture.html#de-mérés-problem",
    "href": "Week3/lecture.html#de-mérés-problem",
    "title": "Probability, Sampling, and Experiments",
    "section": "de Méré’s Problem",
    "text": "de Méré’s Problem\n\n\nFrench gambler Chevalier de Méré played two games:\n\nBet on ≥1 six in 4 die rolls\nBet on ≥1 double-six in 24 rolls of two dice\n\nHe thought both had probability \\(\\frac{2}{3}\\) but…\n\nWon money on first bet\nLost money on second bet\n\n\nHis reasoning:\nFor first bet:\n\\(4 * \\frac{1}{6} = \\frac{2}{3}\\)\nFor second bet:\n\\(24 * \\frac{1}{36} = \\frac{2}{3}\\)\n\n\nA famous example arose from a problem encountered by a French gambler who went by the name of Chevalier de Méré. de Méré played two different dice games: In the first he bet on the chance of at least one six on four rolls of a six-sided die, while in the second he bet on the chance of at least one double-six on 24 rolls of two dice. He expected to win money on both of these gambles, but he found that while on average he won money on the first gamble, he actually lost money on average when he played the second gamble many times."
  },
  {
    "objectID": "Week3/lecture.html#visualizing-multiple-events",
    "href": "Week3/lecture.html#visualizing-multiple-events",
    "title": "Probability, Sampling, and Experiments",
    "section": "Visualizing Multiple Events",
    "text": "Visualizing Multiple Events\n\n\nMatrix of Outcomes:\n\n\n\n\n\n\n\n\n\n\nKey Points:\n\nRed cells: six on either throw\nTotal red cells: 11\nExplains \\(\\frac{11}{36}\\) probability\nShows de Méré’s error\n\n\n\nThis matrix represents all possible combinations of results across two throws, and highlights the cells that involve a six on either the first or second throw. If you count up the cells in red you will see that there are 11 such cells. This shows why the addition rule gives a different answer from de Méré’s; if we were to simply add together the probabilities for the two throws as he did, then we would count (6,6) towards both, when it should really only be counted once."
  },
  {
    "objectID": "Week3/lecture.html#pascals-solution",
    "href": "Week3/lecture.html#pascals-solution",
    "title": "Probability, Sampling, and Experiments",
    "section": "Pascal’s Solution",
    "text": "Pascal’s Solution\n\n\nFirst bet:\n\\(P(\\text{no sixes}) = \\bigg(\\frac{5}{6}\\bigg)^4=0.482\\)\n\\(P(\\text{≥1 six}) = 1 - 0.482 = 0.517\\)\n\nSecond bet:\n\\(P(\\text{no double six}) = \\bigg(\\frac{35}{36}\\bigg)^{24}=0.509\\)\n\\(P(\\text{≥1 double six}) = 1 - 0.509 = 0.491\\)\n\nKey Insights:\n\nEasier to compute complement\nFirst bet: P &gt; 0.5\nSecond bet: P &lt; 0.5\nExplains gambling results\n\n\n\nBlaise Pascal used the rules of probability to solve de Méré’s problem. First, he realized that computing the probability of at least one event out of a combination was tricky, whereas computing the probability that something does not occur across several events is relatively easy – it’s just the product of the probabilities of the individual events.\nThe first bet has probability &gt; 0.5, explaining why de Méré made money on this bet on average. The second bet has probability &lt; 0.5, explaining why de Méré lost money on average on this bet."
  },
  {
    "objectID": "Week3/lecture.html#three-approaches",
    "href": "Week3/lecture.html#three-approaches",
    "title": "Probability, Sampling, and Experiments",
    "section": "Three Approaches",
    "text": "Three Approaches\n\n\n\nPersonal Belief\n\nSubjective assessment\nBased on knowledge/experience\nLimited scientific validity\nOften only available approach\n\nEmpirical Frequency\n\nBased on repeated experiments\nLaw of large numbers\nReal-world data collection\n\n\n\n\nClassical Probability\n\nBased on equally likely outcomes\nMathematical approach\nCommon in games of chance\nNo experiments needed\n\n\n\n\nNow that we know what a probability is, how do we actually figure out what the probability is for any particular event? There are three main approaches, each with their own strengths and limitations."
  },
  {
    "objectID": "Week3/lecture.html#personal-belief",
    "href": "Week3/lecture.html#personal-belief",
    "title": "Probability, Sampling, and Experiments",
    "section": "Personal Belief",
    "text": "Personal Belief\n\n\nExample Question:\nWhat was the probability that Bernie Sanders would have won the 2016 presidential election if he had been the democratic nominee?\nKey Points:\n\nCan’t run this experiment\nPeople can still estimate based on knowledge\nNot scientifically satisfying\nOften the only available approach\n\n\nOther Examples:\n\nWeather forecasts\nSports predictions\nEconomic forecasts\nPersonal decisions\n\n\n\nLet’s say that I asked you what the probability was that Bernie Sanders would have won the 2016 presidential election if he had been the democratic nominee instead of Hilary Clinton? We can’t actually do the experiment to find the outcome. However, most people with knowledge of American politics would be willing to at least offer a guess at the probability of this event. In many cases personal knowledge and/or opinion is the only guide we have determining the probability of an event, but this is not very scientifically satisfying."
  },
  {
    "objectID": "Week3/lecture.html#empirical-frequency",
    "href": "Week3/lecture.html#empirical-frequency",
    "title": "Probability, Sampling, and Experiments",
    "section": "Empirical Frequency",
    "text": "Empirical Frequency\n\n\nSan Francisco Rain Example:\n\nTotal days in 2017: 365\nRainy days: 73\nP(rain in SF) = 73/365 = 0.2\n\nKey Steps:\n\nDefine experiment clearly\nCount occurrences\nDivide by total trials\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnother way to determine the probability of an event is to do the experiment many times and count how often each event happens. From the relative frequency of the different outcomes, we can compute the probability of each outcome. For example, let’s say that we are interested in knowing the probability of rain in San Francisco. We first have to define the experiment — let’s say that we will look at the National Weather Service data for each day in 2017 and determine whether there was any rain at the downtown San Francisco weather station. According to these data, in 2017 there were 73 rainy days. To compute the probability of rain in San Francisco, we simply divide the number of rainy days by the number of days counted (365), giving P(rain in SF in 2017) = 0.2.\nThe graph shows how the empirical probability of rain converges to 0.2 as we accumulate more days of data throughout the year."
  },
  {
    "objectID": "Week3/lecture.html#law-of-large-numbers",
    "href": "Week3/lecture.html#law-of-large-numbers",
    "title": "Probability, Sampling, and Experiments",
    "section": "Law of Large Numbers",
    "text": "Law of Large Numbers\n\n\nCoin Flip Example:\n\nTrue probability of heads = 0.5\nSmall samples vary widely\nMore flips = better estimate\nConverges to true probability\n“Law of small numbers” fallacy\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe graph shows how early results from coin flips can be highly variable and unrepresentative of the true value. Even though we know a fair coin has a probability of 0.5 for heads, small samples can give very different results. This demonstrates how small samples can give misleading results.\nThis was referred to as the “law of small numbers” by psychologists Danny Kahneman and Amos Tversky, who showed that people (even trained researchers) often behave as if the law of large numbers applies even to small samples, leading them to draw strong conclusions from insufficient data."
  },
  {
    "objectID": "Week3/lecture.html#real-world-example-alabama-election",
    "href": "Week3/lecture.html#real-world-example-alabama-election",
    "title": "Probability, Sampling, and Experiments",
    "section": "Real-World Example: Alabama Election",
    "text": "Real-World Example: Alabama Election\n\n\n2017 Senate Race:\n\nRoy Moore vs Doug Jones\nEarly results volatile\nFinal outcome different\nSmall sample warning\n\n\n\n\n\n\n\n\n\n\n\n\n\nA real-world example of this was seen in the 2017 special election for the US Senate in Alabama. Early in the evening the vote counts were especially volatile, swinging from a large initial lead for Jones to a long period where Moore had the lead, until finally Jones took the lead to win the race.\nThis demonstrates how small samples can give misleading results. Unfortunately, many people forget this and overinterpret results from small samples."
  },
  {
    "objectID": "Week3/lecture.html#what-is-conditional-probability",
    "href": "Week3/lecture.html#what-is-conditional-probability",
    "title": "Probability, Sampling, and Experiments",
    "section": "What is Conditional Probability?",
    "text": "What is Conditional Probability?\n\n\nDefinition:\n\nProbability of A given B occurred\nWritten as \\(P(A|B)\\)\nUpdates probability based on new information\n\nFormula:\n\\(P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\)\n\n\n\n\n\n\n\n\n\n\n\n\nSo far we have limited ourselves to simple probabilities - that is, the probability of a single event or combination of events. However, we often wish to determine the probability of some event given that some other event has occurred, which are known as conditional probabilities."
  },
  {
    "objectID": "Week3/lecture.html#nhanes-example-physical-activity",
    "href": "Week3/lecture.html#nhanes-example-physical-activity",
    "title": "Probability, Sampling, and Experiments",
    "section": "NHANES Example: Physical Activity",
    "text": "NHANES Example: Physical Activity\n\n\nQuestion:\nWhat is P(diabetes|inactive)?\n\n\n\n\n\ntotal\ninactive\ndiabetes\ndiabetes_given_inactive\n\n\n\n\n5443\n0.454\n0.101\n0.141\n\n\n\n\n\n\nJoint Probabilities:\n\n\n\nJoint probabilities\n\n\nDiabetes\nPhysActive\nn\nprob\n\n\n\n\nNo\nNo\n2123\n0.3900423\n\n\nNo\nYes\n2770\n0.5089105\n\n\nYes\nNo\n349\n0.0641191\n\n\nYes\nYes\n201\n0.0369282\n\n\n\n\n\n\n\nWe can compute conditional probabilities directly from data. Let’s say that we are interested in the following question: What is the probability that someone has diabetes, given that they are not physically active? The NHANES dataset includes two variables that address the two parts of this question: Diabetes and PhysActive."
  },
  {
    "objectID": "Week3/lecture.html#independence",
    "href": "Week3/lecture.html#independence",
    "title": "Probability, Sampling, and Experiments",
    "section": "Independence",
    "text": "Independence\n\n\nStatistical Independence:\n\\(P(A|B) = P(A)\\)\nKey Points:\n\nB tells us nothing about A\nDifferent from everyday usage\nMust check with data\n\n\nExample: Jefferson State\n\nP(Jeffersonian) = 0.014\nP(Californian) = 0.986\nNot independent!\nMutually exclusive\n\n\n\nThe term “independent” has a very specific meaning in statistics, which is somewhat different from the common usage of the term. Statistical independence between two variables means that knowing the value of one variable doesn’t tell us anything about the value of the other.\nFor example, there is currently a move by a small group of California citizens to declare a new independent state called Jefferson. The new states might be politically independent, but they would not be statistically independent, because if we know that a person is Jeffersonian, then we can be sure they are not Californian!"
  },
  {
    "objectID": "Week3/lecture.html#mental-health-and-physical-activity",
    "href": "Week3/lecture.html#mental-health-and-physical-activity",
    "title": "Probability, Sampling, and Experiments",
    "section": "Mental Health and Physical Activity",
    "text": "Mental Health and Physical Activity\n\n\nQuestion: Are physical and mental health independent?\nVariables:\n\nPhysActive: physically active?\nDaysMentHlthBad: bad mental health days\nThreshold: &gt;7 days = bad mental health\n\n\n\n\n\n\n\nPhysActive\nBad Mental Health\nGood Mental Health\nTotal\n\n\n\n\nNo\n629\n2510\n3139\n\n\nYes\n471\n3095\n3566\n\n\nTotal\n1100\n5605\n6705\n\n\n\n\n\n\n\nLet’s look at another example, using the NHANES data: Are physical health and mental health independent of one another? To determine whether mental health and physical activity are independent, we would compare the simple probability of bad mental health to the conditional probability of bad mental health given that one is physically active."
  },
  {
    "objectID": "Week3/lecture.html#the-basic-formula",
    "href": "Week3/lecture.html#the-basic-formula",
    "title": "Probability, Sampling, and Experiments",
    "section": "The Basic Formula",
    "text": "The Basic Formula\n\n\nWhen we know \\(P(A|B)\\) but want \\(P(B|A)\\):\n\\(P(B|A) = \\frac{P(A|B)*P(B)}{P(A)}\\)\nAlternative Form:\n\\(P(B|A) = \\frac{P(A|B)*P(B)}{P(A|B)*P(B) + P(A|\\neg B)*P(\\neg B)}\\)\n\n\nComponents:\n\nPrior: \\(P(B)\\)\nLikelihood: \\(P(A|B)\\)\nMarginal likelihood: \\(P(A)\\)\nPosterior: \\(P(B|A)\\)\n\n\n\n\nIn many cases, we know P(A|B) but we really want to know P(B|A). This commonly occurs in medical screening, where we know P(positive test result| disease) but what we want to know is P(disease|positive test result).\nIf we have only two outcomes, we can express Bayes’ rule in a somewhat clearer way, using the sum rule to redefine P(A)."
  },
  {
    "objectID": "Week3/lecture.html#putting-bayes-into-practice",
    "href": "Week3/lecture.html#putting-bayes-into-practice",
    "title": "Probability, Sampling, and Experiments",
    "section": "Putting Bayes into Practice",
    "text": "Putting Bayes into Practice\nConstruction company drug testing\nA major construction company conducts mandatory random drug and alcohol screening using rapid saliva tests. Consider the following scenario:\n\n\nIn the UK construction industry during 2023, the prevalence of substance use affecting workplace safety was estimated at 2.5% of the workforce\nThe rapid saliva test used has a sensitivity (true positive rate) of 85% when conducted according to protocol\nThe specificity (true negative rate) of these tests is 99.2%\n\n\n\nLet’s consider a specific example. Suppose that a worker is selected for a random drug screening. The test result is positive. What is the probability that this worker is actually positive for substances?"
  },
  {
    "objectID": "Week3/lecture.html#putting-bayes-into-practice-1",
    "href": "Week3/lecture.html#putting-bayes-into-practice-1",
    "title": "Probability, Sampling, and Experiments",
    "section": "Putting Bayes into Practice",
    "text": "Putting Bayes into Practice\nConstruction company drug testing\nLet’s consider a specific example. Suppose that a worker is selected for a random drug screening. The test result is positive. What is the probability that this worker is actually positive for substances?\n\nContext: The company’s current policy is immediate suspension without pay following a positive test result, pending a more accurate laboratory confirmation test that takes 48 hours.\n\nMandatory screening\nRapid saliva test\nSafety-critical roles\nImmediate consequences"
  },
  {
    "objectID": "Week3/lecture.html#putting-bayes-into-practice-2",
    "href": "Week3/lecture.html#putting-bayes-into-practice-2",
    "title": "Probability, Sampling, and Experiments",
    "section": "Putting Bayes into Practice",
    "text": "Putting Bayes into Practice\nConstruction company drug testing\nConstruction Site Testing:\n\nSensitivity: P(positive|substance) = 0.85\nSpecificity: P(negative|no substance) = 0.992\nBase rate: P(substance) = 0.025\n\n\nKey Values:\n\nP(S) = 0.025 (prevalence)\nP(P|S) = 0.85 (sensitivity)\nP(P|not S) = 0.008 (1 - specificity)\n\n\n\nA major construction company conducts mandatory random drug and alcohol screening using rapid saliva tests. In the UK construction industry during 2023, the prevalence of substance use affecting workplace safety was estimated at 2.5% of the workforce. The rapid saliva test used has a sensitivity of 85% when conducted according to protocol, and a specificity of 99.2%."
  },
  {
    "objectID": "Week3/lecture.html#lets-work-through-it",
    "href": "Week3/lecture.html#lets-work-through-it",
    "title": "Probability, Sampling, and Experiments",
    "section": "Let’s Work Through It",
    "text": "Let’s Work Through It\nUsing Bayes’ Theorem, calculate the probability that this worker is actually positive for substances given their positive test result.\n\nP(S) = 0.025 (prevalence)\nP(P|S) = 0.85 (sensitivity)\nP(P|not S) = 0.008 (1 - specificity)\n\n\nA construction worker is randomly selected for testing at the start of their shift. Their saliva test comes back positive. Using Bayes’ Theorem, calculate the probability that this worker is actually positive for substances given their positive test result."
  },
  {
    "objectID": "Week3/lecture.html#solution",
    "href": "Week3/lecture.html#solution",
    "title": "Probability, Sampling, and Experiments",
    "section": "Solution",
    "text": "Solution\n\n\nCalculate P(substance|positive):\n\n\\[\\begin{align*}\nP(P) &= P(P|S) \\times P(S) + P(P|not S) \\times P(not S) \\\\\n&= (0.85 \\times 0.025) + (0.008 \\times 0.975) \\\\\n&= 0.02125 + 0.0078 \\\\\n&= 0.02905\n\\end{align*}\\]\n\n\n\\[\\begin{align*}\nP(S|P) &= \\frac{P(P|S) \\times P(S)}{P(P)} \\\\\n&= \\frac{0.85 \\times 0.025}{0.02905} \\\\\n&= 0.7314974 \\text{ or } 73.1\\%\n\\end{align*}\\]\n\n\n\nInterpretation:\n\n~73.1% chance true positive\n~26.9% chance false positive\nMuch higher than 2.5% base rate\nStill significant uncertainty\n\n\n\n\nUsing Bayes’ Theorem, we find that given a positive test result, there is a 73.1% probability that the worker actually has substances present. This is much higher than the base rate of 2.5%, but still leaves significant uncertainty with a 26.9% false positive rate."
  },
  {
    "objectID": "Week3/lecture.html#discussion-the-real-world-implications",
    "href": "Week3/lecture.html#discussion-the-real-world-implications",
    "title": "Probability, Sampling, and Experiments",
    "section": "Discussion: The Real-world Implications",
    "text": "Discussion: The Real-world Implications\nThe company’s current policy is immediate suspension without pay following a positive test result.\nWhat do these results mean for this business policy? Is it fair to immediately suspend workers without pay for a positive test result?\n\nThe company’s current policy is immediate suspension without pay following a positive test result, pending a more accurate laboratory confirmation test that takes 48 hours.\nGiven that approximately 26.9% of positive test results may be false positives, an immediate suspension without pay could unfairly penalize innocent workers; however, the high stakes of construction safety and the 73.1% probability of a true positive suggest that temporary removal from safety-critical roles is prudent while awaiting confirmation."
  },
  {
    "objectID": "Week3/lecture.html#learning-from-data",
    "href": "Week3/lecture.html#learning-from-data",
    "title": "Probability, Sampling, and Experiments",
    "section": "Learning from Data",
    "text": "Learning from Data\n\n\nBayes’ Rule as Learning:\n\\(P(B|A) = \\frac{P(A|B)}{P(A)}*P(B)\\)\nComponents:\n\nPrior belief: \\(P(B)\\)\nEvidence strength: \\(\\frac{P(A|B)}{P(A)}\\)\nUpdated belief: \\(P(B|A)\\)\n\n\nKey Insights:\n\nUpdates prior knowledge\nEvidence can strengthen/weaken\nSystematic way to learn\nCombines knowledge & data\n\n\n\nAnother way to think of Bayes’ rule is as a way to update our beliefs on the basis of data. The different parts of Bayes’ rule have specific names, that relate to their role in using Bayes’ rule to update our beliefs.\nThe part on the left tells us how much more or less likely the data A are given B, relative to the overall likelihood of the data, while the part on the right side tells us how likely we thought B was before we knew anything about the data."
  },
  {
    "objectID": "Week3/lecture.html#odds-and-odds-ratios",
    "href": "Week3/lecture.html#odds-and-odds-ratios",
    "title": "Probability, Sampling, and Experiments",
    "section": "Odds and Odds Ratios",
    "text": "Odds and Odds Ratios\n\n\nConverting to Odds:\n\\(\\text{odds of A} = \\frac{P(A)}{P(\\neg A)}\\)\nExample:\nDrug test odds:\n\nPrior: \\(\\frac{0.025}{0.975} = 0.026\\)\nPosterior: \\(\\frac{0.7314974}{0.2685026} = 2.724\\)\n\n\nOdds Ratio:\n\\(\\frac{\\text{posterior odds}}{\\text{prior odds}} = \\frac{2.724}{0.026} = 106.25\\)\nInterpretation:\n\nOdds increased 105×\nMuch stronger evidence\nShows test’s power\nDespite false positives\n\n\n\nWe can convert probabilities into odds which express the relative likelihood of something happening or not. An odds ratio is an example of what we will later call an effect size, which is a way of quantifying how relatively large any particular statistical effect is.\nFirst, remember the rule for computing a conditional probability. We can rearrange this to get the formula to compute the joint probability using the conditional. Using this we can compute the inverse probability."
  },
  {
    "objectID": "Week3/lecture.html#what-is-a-probability-distribution",
    "href": "Week3/lecture.html#what-is-a-probability-distribution",
    "title": "Probability, Sampling, and Experiments",
    "section": "What is a Probability Distribution?",
    "text": "What is a Probability Distribution?\n\n\nDefinition:\n\nDescribes all possible outcomes\nAssigns probability to each\nDifferent types for different data\nMathematical formulation\n\nExamples:\n\nBinomial (success/failure)\nNormal (continuous)\nPoisson (counts)\n\n\n\n\nCode\n# Create example distributions\nx &lt;- seq(-4, 4, length.out = 100)\nnormal_df &lt;- data.frame(\n  x = x,\n  y = dnorm(x),\n  type = \"Normal\"\n)\n\nx &lt;- 0:10\npoisson_df &lt;- data.frame(\n  x = x,\n  y = dpois(x, lambda = 3),\n  type = \"Poisson\"\n)\n\ncolors &lt;- c(\n  \"Normal\" = \"blue\",\n  \"Poisson\" = \"red\"\n)\n\n# Plot distributions\nggplot() +\n  geom_line(data = normal_df, aes(x = x, y = y, color = \"Normal\"), size = 1) +\n  geom_point(\n    data = poisson_df,\n    aes(x = x, y = y, color = \"Poisson\"),\n    size = 1.5\n  ) +\n  labs(\n    title = \"Example Distributions\",\n    x = \"Value\",\n    y = \"Probability\",\n    color = \"Legend\"\n  ) +\n  theme(legend.position = \"top\") +\n  scale_color_manual(values = colors) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nA probability distribution describes the probability of all of the possible outcomes in an experiment. Throughout this section we will encounter a number of these probability distributions, each of which is appropriate to describe different types of data."
  },
  {
    "objectID": "Week3/lecture.html#the-binomial-distribution",
    "href": "Week3/lecture.html#the-binomial-distribution",
    "title": "Probability, Sampling, and Experiments",
    "section": "The Binomial Distribution",
    "text": "The Binomial Distribution\n\n\n\nProperties:\n\nIndependent trials\nTwo outcomes\nFixed probability\nOrder doesn’t matter\n\nFormula:\n\\(P(k; n,p) = \\binom{n}{k} p^k(1-p)^{n-k}\\)\nWhere:\n\nk = successes\nn = trials\np = probability per trial\n\nBinomial Coefficient:\n\\(\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\\)\n\n\n\nCode\n# Create binomial distribution plot\nx &lt;- 0:10\nn &lt;- 10\np &lt;- 0.5\nbinom_df &lt;- data.frame(\n  x = x,\n  y = dbinom(x, size = n, prob = p)\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe binomial distribution provides a way to compute the probability of some number of successes out of a number of trials on which there is either success or failure and nothing in between (known as “Bernoulli trials”), given some known probability of success on each trial."
  },
  {
    "objectID": "Week3/lecture.html#example-steph-currys-free-throws",
    "href": "Week3/lecture.html#example-steph-currys-free-throws",
    "title": "Probability, Sampling, and Experiments",
    "section": "Example: Steph Curry’s Free Throws",
    "text": "Example: Steph Curry’s Free Throws\n\n\nScenario:\n\nSteph Curry hits 91% of his free throws\nIn a game in Jan, 2018, he hit only 2 out of 4 free throws\nIt seems pretty unlikely that he would hit only 50% of his free throws in a game, but exactly how unlikely is it?\n\n\nCalculation:\n\\(P(2;4,0.91) = \\binom{4}{2} 0.91^2(1-0.91)^{2}\\)\n\\(= 6 * 0.8281 * 0.0081\\)\n\\(= 0.040\\)\n\n\n\nInterpretation:\n\nVery unlikely (4%)\nYet it happened\nRare events do occur\nDon’t overinterpret\n\n\n\n\nOn Jan 20 2018, the basketball player Steph Curry hit only 2 out of 4 free throws in a game against the Houston Rockets. We know that Curry’s overall probability of hitting free throws across the entire season was 0.91, so it seems pretty unlikely that he would hit only 50% of his free throws in a game, but exactly how unlikely is it?"
  },
  {
    "objectID": "Week3/lecture.html#cumulative-distributions",
    "href": "Week3/lecture.html#cumulative-distributions",
    "title": "Probability, Sampling, and Experiments",
    "section": "Cumulative Distributions",
    "text": "Cumulative Distributions\nOften we want to know not just how likely a specific value is, but how likely it is to find a value that is as extreme or more than a particular value?\nDefinition:\n\nProbability of value ≤ x\nAccumulates probabilities\nOften more useful\nImportant for testing\n\nExample:\n\\(P(k\\le2)= P(k=2) + P(k=1) + P(k=0)\\)\n\nOften we want to know not just how likely a specific value is, but how likely it is to find a value that is as extreme or more than a particular value?"
  },
  {
    "objectID": "Week3/lecture.html#cumulative-distributions-1",
    "href": "Week3/lecture.html#cumulative-distributions-1",
    "title": "Probability, Sampling, and Experiments",
    "section": "Cumulative Distributions",
    "text": "Cumulative Distributions\n\n\n\n\nCode\n# curry_df &lt;- tibble(\n#   numSuccesses = seq(0, 4)\n# ) %&gt;%\n#   mutate(\n#     Probability = dbinom(numSuccesses, size = 4, prob = 0.91),\n#     CumulativeProbability = pbinom(numSuccesses, size = 4, prob = 0.91)\n#   )\n# Create data for Curry's free throw distributions\nn_throws &lt;- 4\ncurry_prob &lt;- 0.91\nx &lt;- 0:n_throws\n\ncurry_dist_df &lt;- data.frame(\n  x = x,\n  Simple = dbinom(x, size = n_throws, prob = curry_prob),\n  Cumulative = pbinom(x, size = n_throws, prob = curry_prob)\n)\n\nkable(\n  curry_dist_df,\n  caption = \"Simple and cumulative probability distributions\",\n  digits = 3\n)\n\n\n\nSimple and cumulative probability distributions\n\n\nx\nSimple\nCumulative\n\n\n\n\n0\n0.000\n0.000\n\n\n1\n0.003\n0.003\n\n\n2\n0.040\n0.043\n\n\n3\n0.271\n0.314\n\n\n4\n0.686\n1.000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe binomial distribution is a discrete probability distribution that describes the number of successes in a sequence of independent experiments, each of which has a constant probability of success. In this example, we are looking at the probability of Steph Curry making a certain number of free throws out of 4 attempts, given that his overall success rate is 91%.\nThis visualization shows both the probability of making exactly k free throws (blue bars) and the probability of making k or fewer free throws (red line) for Curry’s specific scenario of 4 attempts with a 91% success rate."
  },
  {
    "objectID": "Week3/lecture.html#summary",
    "href": "Week3/lecture.html#summary",
    "title": "Probability, Sampling, and Experiments",
    "section": "Summary",
    "text": "Summary\n\n\nCore Concepts:\n\nProbability measures uncertainty\nThree approaches:\n\nPersonal belief\nEmpirical frequency\nClassical probability\n\nFundamental rules:\n\nAddition\nMultiplication\nSubtraction\n\n\n\nAdvanced Topics:\n\nConditional probability\nIndependence\nBayes’ rule\nProbability distributions\n\nApplications:\n\nMedical screening\nData analysis\nDecision making\nStatistical inference\n\n\n\nThese concepts form the foundation for statistical inference, which we will explore in later chapters. Having read this chapter, you should be able to:\n\nDescribe the sample space for a selected random experiment\nCompute relative frequency and empirical probability\nCompute probabilities of single events, complementary events, and unions/intersections\nDescribe the law of large numbers\nUnderstand conditional probability and independence\nUse Bayes’ theorem"
  },
  {
    "objectID": "Week3/lecture.html#why-study-sampling",
    "href": "Week3/lecture.html#why-study-sampling",
    "title": "Probability, Sampling, and Experiments",
    "section": "Why Study Sampling?",
    "text": "Why Study Sampling?\n\n\nThe Power of Sampling:\nNate Silver’s 2012 Election Prediction:\n\nCorrectly predicted all 50 states\nUsed only 21,000 people\nTo predict 125 million votes\nCombined data from 21 polls\n\n\nKey Insights:\n\nSmall samples can be powerful\nProper methodology is crucial\nCombining data improves accuracy\nStatistical rigor matters\n\n\n\nOne of the foundational ideas in statistics is that we can make inferences about an entire population based on a relatively small sample of individuals from that population.\nAnyone living in the United States will be familiar with the concept of sampling from the political polls that have become a central part of our electoral process. In some cases, these polls can be incredibly accurate at predicting the outcomes of elections. The best known example comes from the 2008 and 2012 US Presidential elections, when the pollster Nate Silver correctly predicted electoral outcomes for 49/50 states in 2008 and for all 50 states in 2012.\nSilver did this by combining data from 21 different polls, which vary in the degree to which they tend to lean towards either the Republican or Democratic side. Each of these polls included data from about 1000 likely voters – meaning that Silver was able to almost perfectly predict the pattern of votes of more than 125 million voters using data from only about 21,000 people, along with other knowledge."
  },
  {
    "objectID": "Week3/lecture.html#sampling-fundamentals",
    "href": "Week3/lecture.html#sampling-fundamentals",
    "title": "Probability, Sampling, and Experiments",
    "section": "Sampling Fundamentals",
    "text": "Sampling Fundamentals\n\n\n\n\nPopulation vs Sample:\n\nPopulation: Entire group of interest\nSample: Subset used for measurement\nGoal: Infer population parameters from sample statistics\n\nRepresentative Sampling:\n\nEqual chance of selection\nAvoid systematic bias\nRandom selection crucial\n\n\n\n\n\n\nTypes of Sampling:\n\nWith replacement: Items can be selected multiple times\nWithout replacement: Items selected only once\nChoice affects probability calculations\n\nKey Terms:\n\nParameter: Population value (usually unknown)\nStatistic: Sample value (our estimate)\nSampling Error: Difference between statistic and parameter\n\n\n\n\n\nOur goal in sampling is to determine the value of a statistic for an entire population of interest, using just a small subset of the population. We do this primarily to save time and effort – why go to the trouble of measuring every individual in the population when just a small sample is sufficient to accurately estimate the statistic of interest?\nIn the election example, the population is all registered voters in the region being polled, and the sample is the set of 1000 individuals selected by the polling organization. The way in which we select the sample is critical to ensuring that the sample is representative of the entire population, which is a main goal of statistical sampling.\nIt’s important to also distinguish between two different ways of sampling: with replacement versus without replacement. In sampling with replacement, after a member of the population has been sampled, they are put back into the pool so that they can potentially be sampled again. In sampling without replacement, once a member has been sampled they are not eligible to be sampled again."
  },
  {
    "objectID": "Week3/lecture.html#sampling-error-distribution",
    "href": "Week3/lecture.html#sampling-error-distribution",
    "title": "Probability, Sampling, and Experiments",
    "section": "Sampling Error & Distribution",
    "text": "Sampling Error & Distribution\nConcept\nWhat is Sampling Error?\n\nDifference between sample and population\nVaries across samples\nAffects measurement quality\nCan be quantified"
  },
  {
    "objectID": "Week3/lecture.html#sampling-error-distribution-1",
    "href": "Week3/lecture.html#sampling-error-distribution-1",
    "title": "Probability, Sampling, and Experiments",
    "section": "Sampling Error & Distribution",
    "text": "Sampling Error & Distribution\nConcept\n\n\n\n# Take 5 samples of 50 adults each\nset.seed(123)\nsamples &lt;- map_df(\n  1:5,\n  ~{\n    NHANES_adult |&gt;\n      sample_n(50) |&gt;\n      summarise(\n        mean_height = mean(Height),\n        sd_height = sd(Height),\n      )\n  }\n)\nsamples\n\n# A tibble: 5 × 2\n  mean_height sd_height\n        &lt;dbl&gt;     &lt;dbl&gt;\n1        169.     11.6 \n2        167.      9.13\n3        169.     11.2 \n4        166.      9.62\n5        169.     11.0 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegardless of how representative our sample is, it’s likely that the statistic that we compute from the sample is going to differ at least slightly from the population parameter. We refer to this as sampling error. If we take multiple samples, the value of our statistical estimate will also vary from sample to sample; we refer to this distribution of our statistic across samples as the sampling distribution.\nSampling error is directly related to the quality of our measurement of the population. Clearly we want the estimates obtained from our sample to be as close as possible to the true value of the population parameter. However, even if our statistic is unbiased (that is, we expect it to have the same value as the population parameter), the value for any particular estimate will differ from the population value, and those differences will be greater when the sampling error is greater.\nThe visualization shows how sample means distribute around the true population mean (red line) when we take many samples."
  },
  {
    "objectID": "Week3/lecture.html#standard-error-of-the-mean",
    "href": "Week3/lecture.html#standard-error-of-the-mean",
    "title": "Probability, Sampling, and Experiments",
    "section": "Standard Error of the Mean",
    "text": "Standard Error of the Mean\n\n\nDefinition:\n\\(SEM = \\frac{\\hat{\\sigma}}{\\sqrt{n}}\\)\nWhere:\n\n\\(\\hat{\\sigma}\\) is estimated standard deviation\n\\(n\\) is sample size\n\nKey Properties:\n\nMeasures sampling distribution variability\nDecreases with larger samples\nIncreases with population variability\n\n\nExample with NHANES:\n\n# Population SEM\npop_sd &lt;- sd(NHANES_adult$Height)\nn &lt;- 50\nsem_theoretical &lt;- pop_sd / sqrt(n)\n\n# Observed SEM from samples\nsem_observed &lt;- sd(samples_large$mean_height)\n\ncat(\"Theoretical SEM:\", round(sem_theoretical, 2), \"\\n\")\n\nTheoretical SEM: 1.44 \n\ncat(\"Observed SEM:\", round(sem_observed, 2))\n\nObserved SEM: 1.42\n\n\n\n\nLater in the course it will become essential to be able to characterize how variable our samples are, in order to make inferences about the sample statistics. For the mean, we do this using a quantity called the standard error of the mean (SEM), which one can think of as the standard deviation of the sampling distribution of the mean.\nThe formula for the standard error of the mean implies that the quality of our measurement involves two quantities: the population variability, and the size of our sample. Because the sample size is the denominator in the formula for SEM, a larger sample size will yield a smaller SEM when holding the population variability constant.\nWe have no control over the population variability, but we do have control over the sample size. Thus, if we wish to improve our sample statistics (by reducing their sampling variability) then we should use larger samples. However, the formula also tells us something very fundamental about statistical sampling – namely, that the utility of larger samples diminishes with the square root of the sample size."
  },
  {
    "objectID": "Week3/lecture.html#sample-size-effects",
    "href": "Week3/lecture.html#sample-size-effects",
    "title": "Probability, Sampling, and Experiments",
    "section": "Sample Size Effects",
    "text": "Sample Size Effects\n\nTheoryVisualizationCode\n\n\nImpact of Sample Size:\n\nLarger n → Smaller SEM\nRelationship is not linear\nDiminishing returns\nSquare root relationship\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Compare SEM for different sample sizes\nn1 &lt;- 50\nn2 &lt;- 200 # 4 times larger\n\nsem1 &lt;- pop_sd / sqrt(n1)\nsem2 &lt;- pop_sd / sqrt(n2)\n\n# Improvement factor\nimprovement &lt;- sem1 / sem2\ncat(\"Improvement factor:\", round(improvement, 2))\n\n\n\n\n\nThe relationship between sample size and standard error is not linear. Doubling the sample size will not double the quality of the statistics; rather, it will improve it by a factor of √2. This has important implications for study design and resource allocation.\nThe visualization shows how the standard error decreases as sample size increases, but with diminishing returns. This means that after a certain point, increasing sample size may not be worth the additional cost and effort.\nThis relationship is fundamental to statistical power, which we will discuss in later sections. Understanding this relationship helps researchers make informed decisions about sample size requirements for their studies."
  },
  {
    "objectID": "Week3/lecture.html#the-central-limit-theorem",
    "href": "Week3/lecture.html#the-central-limit-theorem",
    "title": "Probability, Sampling, and Experiments",
    "section": "The Central Limit Theorem",
    "text": "The Central Limit Theorem\nKey Points:\n\nAs sample size increases:\n\nSampling distribution becomes normal\nRegardless of population distribution\nMean approaches population mean\nVariance decreases\n\nImplications:\n\nEnables statistical inference\nJustifies normal approximation\nExplains real-world patterns\n\n\nThe Central Limit Theorem tells us that as sample sizes get larger, the sampling distribution of the mean will become normally distributed, even if the data within each sample are not normally distributed. This is a powerful result that allows us to make inferences about population parameters based on sample statistics."
  },
  {
    "objectID": "Week3/lecture.html#the-central-limit-theorem-1",
    "href": "Week3/lecture.html#the-central-limit-theorem-1",
    "title": "Probability, Sampling, and Experiments",
    "section": "The Central Limit Theorem",
    "text": "The Central Limit Theorem\nNormal Distribution:\n\n\n\nBell-shaped curve\nDefined by mean (\\(\\mu\\)) and SD (\\(\\sigma\\))\nSymmetric around mean\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Central Limit Theorem tells us that as sample sizes get larger, the sampling distribution of the mean will become normally distributed, even if the data within each sample are not normally distributed.\nThe normal distribution is described in terms of two parameters: the mean (which you can think of as the location of the peak), and the standard deviation (which specifies the width of the distribution). The bell-like shape of the distribution never changes, only its location and width.\nThe normal distribution is commonly observed in data collected in the real world – and the central limit theorem gives us some insight into why that occurs. For example, the height of any adult depends on a complex mixture of their genetics and experience; even if those individual contributions may not be normally distributed, when we combine them the result is a normal distribution."
  },
  {
    "objectID": "Week3/lecture.html#clt-in-action-nhanes-example",
    "href": "Week3/lecture.html#clt-in-action-nhanes-example",
    "title": "Probability, Sampling, and Experiments",
    "section": "CLT in Action: NHANES Example",
    "text": "CLT in Action: NHANES Example\n\nOriginal DistributionCode ExampleKey Insights\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Compare skewness\nlibrary(moments)\noriginal_skew &lt;- skewness(NHANES_clean$AlcoholYear)\nsampling_skew &lt;- skewness(samples_alc$mean_alcohol)\n\ncat(\"Original Distribution Skewness:\", round(original_skew, 2), \"\\n\")\ncat(\"Sampling Distribution Skewness:\", round(sampling_skew, 2))\n\n\n\n\nOriginal data is highly skewed\nSampling distribution is nearly normal\nCLT works even with:\n\nNon-normal data\nSkewed distributions\nDiscrete values\n\nSample size of 50 is sufficient\n\n\n\n\n\nLet’s work with the variable AlcoholYear from the NHANES dataset, which is highly skewed. This distribution is, for lack of a better word, funky – and definitely not normally distributed.\nNow let’s look at the sampling distribution of the mean for this variable. Despite the clear non-normality of the original data, the sampling distribution is remarkably close to the normal.\nThe Central Limit Theorem is important for statistics because it allows us to safely assume that the sampling distribution of the mean will be normal in most cases. This means that we can take advantage of statistical techniques that assume a normal distribution."
  },
  {
    "objectID": "Week3/lecture.html#summary-1",
    "href": "Week3/lecture.html#summary-1",
    "title": "Probability, Sampling, and Experiments",
    "section": "Summary",
    "text": "Summary\n\n\n\n\nSampling Fundamentals:\n\nPopulation vs Sample\nRepresentative sampling\nWith/without replacement\nSampling error\n\nStandard Error:\n\nMeasures sampling variability\nDecreases with √n\nGuides sample size decisions\nQuantifies precision\n\n\n\n\n\n\nCentral Limit Theorem:\n\nSampling distribution normality\nIndependent of original distribution\nEnables statistical inference\nFoundation for hypothesis testing\n\nApplications:\n\nPolitical polling\nClinical trials\nQuality control\nResearch design\n\n\n\n\n\nIn this lecture, we covered: - The fundamentals of statistical sampling and why it works - How to characterize sampling error and the sampling distribution - The standard error of the mean and its relationship with sample size - The Central Limit Theorem and its importance in statistical inference - Real-world applications and examples using the NHANES dataset"
  },
  {
    "objectID": "Week3/06-distributions.html",
    "href": "Week3/06-distributions.html",
    "title": "Probability Distributions",
    "section": "",
    "text": "Definition:\n\nDescribes all possible outcomes\nAssigns probability to each\nDifferent types for different data\nMathematical formulation\n\nExamples:\n\nBinomial (success/failure)\nNormal (continuous)\nPoisson (counts)\n\n\n\nCode# Create example distributions\nx &lt;- seq(-4, 4, length.out = 100)\nnormal_df &lt;- data.frame(\n  x = x,\n  y = dnorm(x),\n  type = \"Normal\"\n)\n\nx &lt;- 0:10\npoisson_df &lt;- data.frame(\n  x = x,\n  y = dpois(x, lambda = 3),\n  type = \"Poisson\"\n)\n\ncolors &lt;- c(\n  \"Normal\" = \"blue\",\n  \"Poisson\" = \"red\"\n)\n\n# Plot distributions\nggplot() +\n  geom_line(data = normal_df, aes(x = x, y = y, color = \"Normal\"), size = 1) +\n  geom_point(\n    data = poisson_df,\n    aes(x = x, y = y, color = \"Poisson\"),\n    size = 1.5\n  ) +\n  labs(\n    title = \"Example Distributions\",\n    x = \"Value\",\n    y = \"Probability\",\n    color = \"Legend\"\n  ) +\n  theme(legend.position = \"top\") +\n  scale_color_manual(values = colors) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nA probability distribution describes the probability of all of the possible outcomes in an experiment. Throughout this section we will encounter a number of these probability distributions, each of which is appropriate to describe different types of data.\n\n\n\n\n\nProperties:\n\nIndependent trials\nTwo outcomes\nFixed probability\nOrder doesn’t matter\n\nFormula:\nP(k; n,p) = \\binom{n}{k} p^k(1-p)^{n-k}\nWhere:\n\nk = successes\nn = trials\np = probability per trial\n\nBinomial Coefficient:\n\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\n\n\nCode# Create binomial distribution plot\nx &lt;- 0:10\nn &lt;- 10\np &lt;- 0.5\nbinom_df &lt;- data.frame(\n  x = x,\n  y = dbinom(x, size = n, prob = p)\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe binomial distribution provides a way to compute the probability of some number of successes out of a number of trials on which there is either success or failure and nothing in between (known as “Bernoulli trials”), given some known probability of success on each trial.\n\n\n\n\nScenario:\n\nSteph Curry hits 91% of his free throws\nIn a game in Jan, 2018, he hit only 2 out of 4 free throws\nIt seems pretty unlikely that he would hit only 50% of his free throws in a game, but exactly how unlikely is it?\n\n\nCalculation:\nP(2;4,0.91) = \\binom{4}{2} 0.91^2(1-0.91)^{2}\n= 6 * 0.8281 * 0.0081\n= 0.040\n\n\n\nInterpretation:\n\nVery unlikely (4%)\nYet it happened\nRare events do occur\nDon’t overinterpret\n\n\n\n\n\nOn Jan 20 2018, the basketball player Steph Curry hit only 2 out of 4 free throws in a game against the Houston Rockets. We know that Curry’s overall probability of hitting free throws across the entire season was 0.91, so it seems pretty unlikely that he would hit only 50% of his free throws in a game, but exactly how unlikely is it?\n\n\nOften we want to know not just how likely a specific value is, but how likely it is to find a value that is as extreme or more than a particular value?\nDefinition:\n\nProbability of value ≤ x\nAccumulates probabilities\nOften more useful\nImportant for testing\n\nExample:\nP(k\\le2)= P(k=2) + P(k=1) + P(k=0)\n\nOften we want to know not just how likely a specific value is, but how likely it is to find a value that is as extreme or more than a particular value?\n\n\n\n\n\nCode# curry_df &lt;- tibble(\n#   numSuccesses = seq(0, 4)\n# ) %&gt;%\n#   mutate(\n#     Probability = dbinom(numSuccesses, size = 4, prob = 0.91),\n#     CumulativeProbability = pbinom(numSuccesses, size = 4, prob = 0.91)\n#   )\n# Create data for Curry's free throw distributions\nn_throws &lt;- 4\ncurry_prob &lt;- 0.91\nx &lt;- 0:n_throws\n\ncurry_dist_df &lt;- data.frame(\n  x = x,\n  Simple = dbinom(x, size = n_throws, prob = curry_prob),\n  Cumulative = pbinom(x, size = n_throws, prob = curry_prob)\n)\n\nkable(\n  curry_dist_df,\n  caption = \"Simple and cumulative probability distributions\",\n  digits = 3\n)\n\n\nSimple and cumulative probability distributions\n\nx\nSimple\nCumulative\n\n\n\n0\n0.000\n0.000\n\n\n1\n0.003\n0.003\n\n\n2\n0.040\n0.043\n\n\n3\n0.271\n0.314\n\n\n4\n0.686\n1.000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe binomial distribution is a discrete probability distribution that describes the number of successes in a sequence of independent experiments, each of which has a constant probability of success. In this example, we are looking at the probability of Steph Curry making a certain number of free throws out of 4 attempts, given that his overall success rate is 91%.\nThis visualization shows both the probability of making exactly k free throws (blue bars) and the probability of making k or fewer free throws (red line) for Curry’s specific scenario of 4 attempts with a 91% success rate.\n\n\n\n\nCore Concepts:\n\nProbability measures uncertainty\nThree approaches:\n\nPersonal belief\nEmpirical frequency\nClassical probability\n\n\nFundamental rules:\n\nAddition\nMultiplication\nSubtraction\n\n\n\n\nAdvanced Topics:\n\nConditional probability\nIndependence\nBayes’ rule\nProbability distributions\n\nApplications:\n\nMedical screening\nData analysis\nDecision making\nStatistical inference\n\n\n\n\nThese concepts form the foundation for statistical inference, which we will explore in later chapters. Having read this chapter, you should be able to:\n\nDescribe the sample space for a selected random experiment\nCompute relative frequency and empirical probability\nCompute probabilities of single events, complementary events, and unions/intersections\nDescribe the law of large numbers\nUnderstand conditional probability and independence\nUse Bayes’ theorem",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Probability Distributions"
    ]
  },
  {
    "objectID": "Week3/06-distributions.html#what-is-a-probability-distribution",
    "href": "Week3/06-distributions.html#what-is-a-probability-distribution",
    "title": "Probability Distributions",
    "section": "",
    "text": "Definition:\n\nDescribes all possible outcomes\nAssigns probability to each\nDifferent types for different data\nMathematical formulation\n\nExamples:\n\nBinomial (success/failure)\nNormal (continuous)\nPoisson (counts)\n\n\n\nCode# Create example distributions\nx &lt;- seq(-4, 4, length.out = 100)\nnormal_df &lt;- data.frame(\n  x = x,\n  y = dnorm(x),\n  type = \"Normal\"\n)\n\nx &lt;- 0:10\npoisson_df &lt;- data.frame(\n  x = x,\n  y = dpois(x, lambda = 3),\n  type = \"Poisson\"\n)\n\ncolors &lt;- c(\n  \"Normal\" = \"blue\",\n  \"Poisson\" = \"red\"\n)\n\n# Plot distributions\nggplot() +\n  geom_line(data = normal_df, aes(x = x, y = y, color = \"Normal\"), size = 1) +\n  geom_point(\n    data = poisson_df,\n    aes(x = x, y = y, color = \"Poisson\"),\n    size = 1.5\n  ) +\n  labs(\n    title = \"Example Distributions\",\n    x = \"Value\",\n    y = \"Probability\",\n    color = \"Legend\"\n  ) +\n  theme(legend.position = \"top\") +\n  scale_color_manual(values = colors) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nA probability distribution describes the probability of all of the possible outcomes in an experiment. Throughout this section we will encounter a number of these probability distributions, each of which is appropriate to describe different types of data.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Probability Distributions"
    ]
  },
  {
    "objectID": "Week3/06-distributions.html#the-binomial-distribution",
    "href": "Week3/06-distributions.html#the-binomial-distribution",
    "title": "Probability Distributions",
    "section": "",
    "text": "Properties:\n\nIndependent trials\nTwo outcomes\nFixed probability\nOrder doesn’t matter\n\nFormula:\nP(k; n,p) = \\binom{n}{k} p^k(1-p)^{n-k}\nWhere:\n\nk = successes\nn = trials\np = probability per trial\n\nBinomial Coefficient:\n\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\n\n\nCode# Create binomial distribution plot\nx &lt;- 0:10\nn &lt;- 10\np &lt;- 0.5\nbinom_df &lt;- data.frame(\n  x = x,\n  y = dbinom(x, size = n, prob = p)\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe binomial distribution provides a way to compute the probability of some number of successes out of a number of trials on which there is either success or failure and nothing in between (known as “Bernoulli trials”), given some known probability of success on each trial.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Probability Distributions"
    ]
  },
  {
    "objectID": "Week3/06-distributions.html#example-steph-currys-free-throws",
    "href": "Week3/06-distributions.html#example-steph-currys-free-throws",
    "title": "Probability Distributions",
    "section": "",
    "text": "Scenario:\n\nSteph Curry hits 91% of his free throws\nIn a game in Jan, 2018, he hit only 2 out of 4 free throws\nIt seems pretty unlikely that he would hit only 50% of his free throws in a game, but exactly how unlikely is it?\n\n\nCalculation:\nP(2;4,0.91) = \\binom{4}{2} 0.91^2(1-0.91)^{2}\n= 6 * 0.8281 * 0.0081\n= 0.040\n\n\n\nInterpretation:\n\nVery unlikely (4%)\nYet it happened\nRare events do occur\nDon’t overinterpret\n\n\n\n\n\nOn Jan 20 2018, the basketball player Steph Curry hit only 2 out of 4 free throws in a game against the Houston Rockets. We know that Curry’s overall probability of hitting free throws across the entire season was 0.91, so it seems pretty unlikely that he would hit only 50% of his free throws in a game, but exactly how unlikely is it?",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Probability Distributions"
    ]
  },
  {
    "objectID": "Week3/06-distributions.html#cumulative-distributions",
    "href": "Week3/06-distributions.html#cumulative-distributions",
    "title": "Probability Distributions",
    "section": "",
    "text": "Often we want to know not just how likely a specific value is, but how likely it is to find a value that is as extreme or more than a particular value?\nDefinition:\n\nProbability of value ≤ x\nAccumulates probabilities\nOften more useful\nImportant for testing\n\nExample:\nP(k\\le2)= P(k=2) + P(k=1) + P(k=0)\n\nOften we want to know not just how likely a specific value is, but how likely it is to find a value that is as extreme or more than a particular value?",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Probability Distributions"
    ]
  },
  {
    "objectID": "Week3/06-distributions.html#cumulative-distributions-1",
    "href": "Week3/06-distributions.html#cumulative-distributions-1",
    "title": "Probability Distributions",
    "section": "",
    "text": "Code# curry_df &lt;- tibble(\n#   numSuccesses = seq(0, 4)\n# ) %&gt;%\n#   mutate(\n#     Probability = dbinom(numSuccesses, size = 4, prob = 0.91),\n#     CumulativeProbability = pbinom(numSuccesses, size = 4, prob = 0.91)\n#   )\n# Create data for Curry's free throw distributions\nn_throws &lt;- 4\ncurry_prob &lt;- 0.91\nx &lt;- 0:n_throws\n\ncurry_dist_df &lt;- data.frame(\n  x = x,\n  Simple = dbinom(x, size = n_throws, prob = curry_prob),\n  Cumulative = pbinom(x, size = n_throws, prob = curry_prob)\n)\n\nkable(\n  curry_dist_df,\n  caption = \"Simple and cumulative probability distributions\",\n  digits = 3\n)\n\n\nSimple and cumulative probability distributions\n\nx\nSimple\nCumulative\n\n\n\n0\n0.000\n0.000\n\n\n1\n0.003\n0.003\n\n\n2\n0.040\n0.043\n\n\n3\n0.271\n0.314\n\n\n4\n0.686\n1.000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe binomial distribution is a discrete probability distribution that describes the number of successes in a sequence of independent experiments, each of which has a constant probability of success. In this example, we are looking at the probability of Steph Curry making a certain number of free throws out of 4 attempts, given that his overall success rate is 91%.\nThis visualization shows both the probability of making exactly k free throws (blue bars) and the probability of making k or fewer free throws (red line) for Curry’s specific scenario of 4 attempts with a 91% success rate.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Probability Distributions"
    ]
  },
  {
    "objectID": "Week3/06-distributions.html#summary",
    "href": "Week3/06-distributions.html#summary",
    "title": "Probability Distributions",
    "section": "",
    "text": "Core Concepts:\n\nProbability measures uncertainty\nThree approaches:\n\nPersonal belief\nEmpirical frequency\nClassical probability\n\n\nFundamental rules:\n\nAddition\nMultiplication\nSubtraction\n\n\n\n\nAdvanced Topics:\n\nConditional probability\nIndependence\nBayes’ rule\nProbability distributions\n\nApplications:\n\nMedical screening\nData analysis\nDecision making\nStatistical inference\n\n\n\n\nThese concepts form the foundation for statistical inference, which we will explore in later chapters. Having read this chapter, you should be able to:\n\nDescribe the sample space for a selected random experiment\nCompute relative frequency and empirical probability\nCompute probabilities of single events, complementary events, and unions/intersections\nDescribe the law of large numbers\nUnderstand conditional probability and independence\nUse Bayes’ theorem",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Probability Distributions"
    ]
  },
  {
    "objectID": "Week3/04-conditional.html",
    "href": "Week3/04-conditional.html",
    "title": "Conditional Probability and Independence",
    "section": "",
    "text": "Definition:\n\nProbability of A given B occurred\nWritten as P(A|B)\n\nUpdates probability based on new information\n\nFormula:\nP(A|B) = \\frac{P(A \\cap B)}{P(B)}\n\n\n\n\n\n\n\n\n\n\n\n\nSo far we have limited ourselves to simple probabilities - that is, the probability of a single event or combination of events. However, we often wish to determine the probability of some event given that some other event has occurred, which are known as conditional probabilities.\n\n\n\n\nQuestion:\nWhat is P(diabetes|inactive)?\n\n\n\n\ntotal\ninactive\ndiabetes\ndiabetes_given_inactive\n\n\n5443\n0.454\n0.101\n0.141\n\n\n\n\n\nJoint Probabilities:\n\n\n\nJoint probabilities\n\nDiabetes\nPhysActive\nn\nprob\n\n\n\nNo\nNo\n2123\n0.3900423\n\n\nNo\nYes\n2770\n0.5089105\n\n\nYes\nNo\n349\n0.0641191\n\n\nYes\nYes\n201\n0.0369282\n\n\n\n\n\n\n\n\nWe can compute conditional probabilities directly from data. Let’s say that we are interested in the following question: What is the probability that someone has diabetes, given that they are not physically active? The NHANES dataset includes two variables that address the two parts of this question: Diabetes and PhysActive.\n\n\n\n\nStatistical Independence:\nP(A|B) = P(A)\nKey Points:\n\nB tells us nothing about A\nDifferent from everyday usage\nMust check with data\n\n\nExample: Jefferson State\n\nP(Jeffersonian) = 0.014\nP(Californian) = 0.986\nNot independent!\nMutually exclusive\n\n\n\n\nThe term “independent” has a very specific meaning in statistics, which is somewhat different from the common usage of the term. Statistical independence between two variables means that knowing the value of one variable doesn’t tell us anything about the value of the other.\nFor example, there is currently a move by a small group of California citizens to declare a new independent state called Jefferson. The new states might be politically independent, but they would not be statistically independent, because if we know that a person is Jeffersonian, then we can be sure they are not Californian!\n\n\n\n\nQuestion: Are physical and mental health independent?\nVariables:\n\nPhysActive: physically active?\nDaysMentHlthBad: bad mental health days\nThreshold: &gt;7 days = bad mental health\n\n\n\n\n\n\nPhysActive\nBad Mental Health\nGood Mental Health\nTotal\n\n\n\nNo\n629\n2510\n3139\n\n\nYes\n471\n3095\n3566\n\n\nTotal\n1100\n5605\n6705\n\n\n\n\n\n\n\n\nLet’s look at another example, using the NHANES data: Are physical health and mental health independent of one another? To determine whether mental health and physical activity are independent, we would compare the simple probability of bad mental health to the conditional probability of bad mental health given that one is physically active.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Conditional Probability and Independence"
    ]
  },
  {
    "objectID": "Week3/04-conditional.html#what-is-conditional-probability",
    "href": "Week3/04-conditional.html#what-is-conditional-probability",
    "title": "Conditional Probability and Independence",
    "section": "",
    "text": "Definition:\n\nProbability of A given B occurred\nWritten as P(A|B)\n\nUpdates probability based on new information\n\nFormula:\nP(A|B) = \\frac{P(A \\cap B)}{P(B)}\n\n\n\n\n\n\n\n\n\n\n\n\nSo far we have limited ourselves to simple probabilities - that is, the probability of a single event or combination of events. However, we often wish to determine the probability of some event given that some other event has occurred, which are known as conditional probabilities.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Conditional Probability and Independence"
    ]
  },
  {
    "objectID": "Week3/04-conditional.html#nhanes-example-physical-activity",
    "href": "Week3/04-conditional.html#nhanes-example-physical-activity",
    "title": "Conditional Probability and Independence",
    "section": "",
    "text": "Question:\nWhat is P(diabetes|inactive)?\n\n\n\n\ntotal\ninactive\ndiabetes\ndiabetes_given_inactive\n\n\n5443\n0.454\n0.101\n0.141\n\n\n\n\n\nJoint Probabilities:\n\n\n\nJoint probabilities\n\nDiabetes\nPhysActive\nn\nprob\n\n\n\nNo\nNo\n2123\n0.3900423\n\n\nNo\nYes\n2770\n0.5089105\n\n\nYes\nNo\n349\n0.0641191\n\n\nYes\nYes\n201\n0.0369282\n\n\n\n\n\n\n\n\nWe can compute conditional probabilities directly from data. Let’s say that we are interested in the following question: What is the probability that someone has diabetes, given that they are not physically active? The NHANES dataset includes two variables that address the two parts of this question: Diabetes and PhysActive.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Conditional Probability and Independence"
    ]
  },
  {
    "objectID": "Week3/04-conditional.html#independence",
    "href": "Week3/04-conditional.html#independence",
    "title": "Conditional Probability and Independence",
    "section": "",
    "text": "Statistical Independence:\nP(A|B) = P(A)\nKey Points:\n\nB tells us nothing about A\nDifferent from everyday usage\nMust check with data\n\n\nExample: Jefferson State\n\nP(Jeffersonian) = 0.014\nP(Californian) = 0.986\nNot independent!\nMutually exclusive\n\n\n\n\nThe term “independent” has a very specific meaning in statistics, which is somewhat different from the common usage of the term. Statistical independence between two variables means that knowing the value of one variable doesn’t tell us anything about the value of the other.\nFor example, there is currently a move by a small group of California citizens to declare a new independent state called Jefferson. The new states might be politically independent, but they would not be statistically independent, because if we know that a person is Jeffersonian, then we can be sure they are not Californian!",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Conditional Probability and Independence"
    ]
  },
  {
    "objectID": "Week3/04-conditional.html#mental-health-and-physical-activity",
    "href": "Week3/04-conditional.html#mental-health-and-physical-activity",
    "title": "Conditional Probability and Independence",
    "section": "",
    "text": "Question: Are physical and mental health independent?\nVariables:\n\nPhysActive: physically active?\nDaysMentHlthBad: bad mental health days\nThreshold: &gt;7 days = bad mental health\n\n\n\n\n\n\nPhysActive\nBad Mental Health\nGood Mental Health\nTotal\n\n\n\nNo\n629\n2510\n3139\n\n\nYes\n471\n3095\n3566\n\n\nTotal\n1100\n5605\n6705\n\n\n\n\n\n\n\n\nLet’s look at another example, using the NHANES data: Are physical health and mental health independent of one another? To determine whether mental health and physical activity are independent, we would compare the simple probability of bad mental health to the conditional probability of bad mental health given that one is physically active.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Conditional Probability and Independence"
    ]
  },
  {
    "objectID": "Week3/02-rules.html",
    "href": "Week3/02-rules.html",
    "title": "Probability Rules and Classical Probability",
    "section": "",
    "text": "Rule of Subtraction:\nP(\\neg A) = 1 - P(A)\nExample: P(not rolling a 1) = 1 - \\frac{1}{6} = \\frac{5}{6}\n\n\nIntersection Rule (independent events):\nP(A \\cap B) = P(A) * P(B)\nExample: P(six on both rolls) = \\frac{1}{6} * \\frac{1}{6} = \\frac{1}{36}\n\n\nAddition Rule:\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n\n\n\nTo understand de Méré’s error, we need to introduce some of the rules of probability theory:\n\nThe rule of subtraction says that the probability of some event A not happening is one minus the probability of the event happening\nFor independent events, we compute the probability of both occurring by multiplying their individual probabilities\nThe addition rule tells us that to obtain the probability of either of two events occurring, we add together the individual probabilities, but then subtract the likelihood of both occurring together\n\n\n\n\n\nKey Principles:\n\nEqual likelihood assumption\nBased on counting outcomes\nNo experiments needed\nCommon in games of chance\n\nBasic Formula:\nP(outcome_i) = \\frac{1}{\\text{number of possible outcomes}}\n\nExamples:\n\nFair coin: P(heads) = 1/2\nFair die: P(6) = 1/6\nTwo dice: P(double-six) = 1/36\n\n\n\n\nClassical probability arose from the study of games of chance such as dice and cards. In this approach, we compute the probability directly based on our knowledge of the situation.\nWe start with the assumption that all of the elementary events in the sample space are equally likely; that is, when you roll a die, each of the possible outcomes ({1,2,3,4,5,6}) is equally likely to occur.\n\n\n\n\nFrench gambler Chevalier de Méré played two games:\n\nBet on ≥1 six in 4 die rolls\nBet on ≥1 double-six in 24 rolls of two dice\n\nHe thought both had probability \\frac{2}{3} but…\n\nWon money on first bet\nLost money on second bet\n\n\nHis reasoning:\nFor first bet:\n4 * \\frac{1}{6} = \\frac{2}{3}\nFor second bet:\n24 * \\frac{1}{36} = \\frac{2}{3}\n\n\n\nA famous example arose from a problem encountered by a French gambler who went by the name of Chevalier de Méré. de Méré played two different dice games: In the first he bet on the chance of at least one six on four rolls of a six-sided die, while in the second he bet on the chance of at least one double-six on 24 rolls of two dice. He expected to win money on both of these gambles, but he found that while on average he won money on the first gamble, he actually lost money on average when he played the second gamble many times.\n\n\n\n\nMatrix of Outcomes:\n\n\n\n\n\n\n\n\n\nKey Points:\n\nRed cells: six on either throw\nTotal red cells: 11\nExplains \\frac{11}{36} probability\nShows de Méré’s error\n\n\n\n\nThis matrix represents all possible combinations of results across two throws, and highlights the cells that involve a six on either the first or second throw. If you count up the cells in red you will see that there are 11 such cells. This shows why the addition rule gives a different answer from de Méré’s; if we were to simply add together the probabilities for the two throws as he did, then we would count (6,6) towards both, when it should really only be counted once.\n\n\n\n\nFirst bet:\nP(\\text{no sixes}) = \\bigg(\\frac{5}{6}\\bigg)^4=0.482\nP(\\text{≥1 six}) = 1 - 0.482 = 0.517\nSecond bet:\nP(\\text{no double six}) = \\bigg(\\frac{35}{36}\\bigg)^{24}=0.509\nP(\\text{≥1 double six}) = 1 - 0.509 = 0.491\n\nKey Insights:\n\nEasier to compute complement\nFirst bet: P &gt; 0.5\nSecond bet: P &lt; 0.5\nExplains gambling results\n\n\n\n\nBlaise Pascal used the rules of probability to solve de Méré’s problem. First, he realized that computing the probability of at least one event out of a combination was tricky, whereas computing the probability that something does not occur across several events is relatively easy – it’s just the product of the probabilities of the individual events.\nThe first bet has probability &gt; 0.5, explaining why de Méré made money on this bet on average. The second bet has probability &lt; 0.5, explaining why de Méré lost money on average on this bet.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Probability Rules and Classical Probability"
    ]
  },
  {
    "objectID": "Week3/02-rules.html#basic-rules",
    "href": "Week3/02-rules.html#basic-rules",
    "title": "Probability Rules and Classical Probability",
    "section": "",
    "text": "Rule of Subtraction:\nP(\\neg A) = 1 - P(A)\nExample: P(not rolling a 1) = 1 - \\frac{1}{6} = \\frac{5}{6}\n\n\nIntersection Rule (independent events):\nP(A \\cap B) = P(A) * P(B)\nExample: P(six on both rolls) = \\frac{1}{6} * \\frac{1}{6} = \\frac{1}{36}\n\n\nAddition Rule:\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n\n\n\nTo understand de Méré’s error, we need to introduce some of the rules of probability theory:\n\nThe rule of subtraction says that the probability of some event A not happening is one minus the probability of the event happening\nFor independent events, we compute the probability of both occurring by multiplying their individual probabilities\nThe addition rule tells us that to obtain the probability of either of two events occurring, we add together the individual probabilities, but then subtract the likelihood of both occurring together",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Probability Rules and Classical Probability"
    ]
  },
  {
    "objectID": "Week3/02-rules.html#classical-probability",
    "href": "Week3/02-rules.html#classical-probability",
    "title": "Probability Rules and Classical Probability",
    "section": "",
    "text": "Key Principles:\n\nEqual likelihood assumption\nBased on counting outcomes\nNo experiments needed\nCommon in games of chance\n\nBasic Formula:\nP(outcome_i) = \\frac{1}{\\text{number of possible outcomes}}\n\nExamples:\n\nFair coin: P(heads) = 1/2\nFair die: P(6) = 1/6\nTwo dice: P(double-six) = 1/36\n\n\n\n\nClassical probability arose from the study of games of chance such as dice and cards. In this approach, we compute the probability directly based on our knowledge of the situation.\nWe start with the assumption that all of the elementary events in the sample space are equally likely; that is, when you roll a die, each of the possible outcomes ({1,2,3,4,5,6}) is equally likely to occur.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Probability Rules and Classical Probability"
    ]
  },
  {
    "objectID": "Week3/02-rules.html#de-mérés-problem",
    "href": "Week3/02-rules.html#de-mérés-problem",
    "title": "Probability Rules and Classical Probability",
    "section": "",
    "text": "French gambler Chevalier de Méré played two games:\n\nBet on ≥1 six in 4 die rolls\nBet on ≥1 double-six in 24 rolls of two dice\n\nHe thought both had probability \\frac{2}{3} but…\n\nWon money on first bet\nLost money on second bet\n\n\nHis reasoning:\nFor first bet:\n4 * \\frac{1}{6} = \\frac{2}{3}\nFor second bet:\n24 * \\frac{1}{36} = \\frac{2}{3}\n\n\n\nA famous example arose from a problem encountered by a French gambler who went by the name of Chevalier de Méré. de Méré played two different dice games: In the first he bet on the chance of at least one six on four rolls of a six-sided die, while in the second he bet on the chance of at least one double-six on 24 rolls of two dice. He expected to win money on both of these gambles, but he found that while on average he won money on the first gamble, he actually lost money on average when he played the second gamble many times.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Probability Rules and Classical Probability"
    ]
  },
  {
    "objectID": "Week3/02-rules.html#visualizing-multiple-events",
    "href": "Week3/02-rules.html#visualizing-multiple-events",
    "title": "Probability Rules and Classical Probability",
    "section": "",
    "text": "Matrix of Outcomes:\n\n\n\n\n\n\n\n\n\nKey Points:\n\nRed cells: six on either throw\nTotal red cells: 11\nExplains \\frac{11}{36} probability\nShows de Méré’s error\n\n\n\n\nThis matrix represents all possible combinations of results across two throws, and highlights the cells that involve a six on either the first or second throw. If you count up the cells in red you will see that there are 11 such cells. This shows why the addition rule gives a different answer from de Méré’s; if we were to simply add together the probabilities for the two throws as he did, then we would count (6,6) towards both, when it should really only be counted once.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Probability Rules and Classical Probability"
    ]
  },
  {
    "objectID": "Week3/02-rules.html#pascals-solution",
    "href": "Week3/02-rules.html#pascals-solution",
    "title": "Probability Rules and Classical Probability",
    "section": "",
    "text": "First bet:\nP(\\text{no sixes}) = \\bigg(\\frac{5}{6}\\bigg)^4=0.482\nP(\\text{≥1 six}) = 1 - 0.482 = 0.517\nSecond bet:\nP(\\text{no double six}) = \\bigg(\\frac{35}{36}\\bigg)^{24}=0.509\nP(\\text{≥1 double six}) = 1 - 0.509 = 0.491\n\nKey Insights:\n\nEasier to compute complement\nFirst bet: P &gt; 0.5\nSecond bet: P &lt; 0.5\nExplains gambling results\n\n\n\n\nBlaise Pascal used the rules of probability to solve de Méré’s problem. First, he realized that computing the probability of at least one event out of a combination was tricky, whereas computing the probability that something does not occur across several events is relatively easy – it’s just the product of the probabilities of the individual events.\nThe first bet has probability &gt; 0.5, explaining why de Méré made money on this bet on average. The second bet has probability &lt; 0.5, explaining why de Méré lost money on average on this bet.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Probability Rules and Classical Probability"
    ]
  },
  {
    "objectID": "Week2/notes.html",
    "href": "Week2/notes.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Slides\n\n Download PDF Slides \n\n\nor continue on to the next pages for lecture notes.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#this-weeks-lecture",
    "href": "Week2/notes.html#this-weeks-lecture",
    "title": "Data Visualization",
    "section": "",
    "text": "Slides\n\n Download PDF Slides \n\n\nor continue on to the next pages for lecture notes.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#introduction",
    "href": "Week2/notes.html#introduction",
    "title": "Data Visualization",
    "section": "Introduction",
    "text": "Introduction\n\n“The simple graph has brought more information to the data analyst’s mind than any other device.” — John Tukey\n\nR has several systems for making graphs, but ggplot2 is one of the most elegant and most versatile.\nggplot2 implements the grammar of graphics, a coherent system for describing and building graphs. With ggplot2, you can do more and faster by learning one system and applying it in many places.\n\n\nThis presentation is based on the Data Visualization chapter of the R for Data Science book by Hadley Wickham and Garrett Grolemund.\n\nIn this tutorial, we will create this plot:\n\n#| echo: false\n#| autorun: true\n#| fig-width: 10\n#| fig-height: 5\nintro_p &lt;- penguins |&gt;\n  drop_na() |&gt;\n  ggplot(mapping = aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(mapping = aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\", formula = y ~ x) +\n  labs(\n    title = \"Body Mass and Flipper Length\",\n    subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n    x = \"Flipper Length (mm)\",\n    y = \"Body Mass (g)\",\n    color = \"Species\",\n    shape = \"Species\"\n  )\n\nintro_p",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#the-grammar-of-graphics",
    "href": "Week2/notes.html#the-grammar-of-graphics",
    "title": "Data Visualization",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\n\n\nIn 1999, a statistician named Leland Wilkinson published the first edition of what has been the most influential work in data visualization, The Grammar of Graphics.\nThe most complete implementation of the grammar is found in an R package called ggplot2 by Hadley Wickham.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#the-grammar-of-graphics-1",
    "href": "Week2/notes.html#the-grammar-of-graphics-1",
    "title": "Data Visualization",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nA plot can be decomposed into three primary elements\n1. the data\n2. the aesthetic mapping of the variables in the data to visual cues\n3. the geometry used to encode the observations on the plot.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#getting-started",
    "href": "Week2/notes.html#getting-started",
    "title": "Data Visualization",
    "section": "Getting Started",
    "text": "Getting Started\nThroughout this lecture, we will be writing code together inside this webpage.\n\n#| exercise: getting-started\n#| min-lines: 5\n\n\n\n\n\n\n\n\n\nHints:\n\n\n\n\n\nYou can type code into the cells and run them by clicking the “Run” button.\n\n2 + 3",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#getting-started-1",
    "href": "Week2/notes.html#getting-started-1",
    "title": "Data Visualization",
    "section": "Getting Started",
    "text": "Getting Started\nPackages\n\n\nWe begin by loading the tidyverse and ggplot2 packages.\nWe almost always begin our work by loading the tidyverse package. Note that the terms “package” and “library” are used interchangeably but that there is no package() function. To load a package, you need to use library().\n\n\n\n#| label: getting-started\n#| min-lines: 3\n\n# Load the libraries",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#getting-started-2",
    "href": "Week2/notes.html#getting-started-2",
    "title": "Data Visualization",
    "section": "Getting Started",
    "text": "Getting Started\nLoading the Data\n\n\nLoad the palmerpenguins package using library().\nThis package contains the penguins dataset, which we will use for this tutorial.\n\n\n#| autorun: false\n#| min-lines: 2\nlibrary(palmerpenguins)",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#getting-started-3",
    "href": "Week2/notes.html#getting-started-3",
    "title": "Data Visualization",
    "section": "Getting Started",
    "text": "Getting Started\nGetting help\n\n\nIf you are unsure about how to use a function, you can use the ? operator to get help.\nFor a data package like palmerpenguins, you can use ?penguins to get help on the dataset.\n\n\n?penguins",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#the-grammar-of-graphics-2",
    "href": "Week2/notes.html#the-grammar-of-graphics-2",
    "title": "Data Visualization",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nThe Data\n- A variable is a quantity, quality, or property that you can measure.\n- A value is the state of a variable when you measure it. The value of a variable may change from measurement to measurement.\n- An observation is a set of measurements made under similar conditions. An observation will contain several values, each associated with a different variable. We’ll sometimes refer to an observation as a data point.\n- Tabular data is a set of values, each associated with a variable and an observation. Tabular data is tidy if each value is placed in its own “cell”, each variable in its own column, and each observation in its own row.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#the-grammar-of-graphics-3",
    "href": "Week2/notes.html#the-grammar-of-graphics-3",
    "title": "Data Visualization",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nThe Data\n\n\n\n\nspecies: a penguin’s species (Adelie, Chinstrap, or Gentoo).\n\nflipper_length_mm: length of a penguin’s flipper, in millimeters.\n\nbody_mass_g: body mass of a penguin, in grams.\n\n\n\n\n\npenguins\n\n\n\n\n\n#| fig-width: 6.5\n#| fig-height: 4.8\n#| warning: false\n#| echo: false\n#| autorun: true\nggplot(\n  data = penguins,\n  mapping = aes(x = bill_length_mm, y = bill_depth_mm, color = species)\n) +\n  geom_point()",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#formulating-our-research-questions",
    "href": "Week2/notes.html#formulating-our-research-questions",
    "title": "Data Visualization",
    "section": "Formulating our Research Question(s)",
    "text": "Formulating our Research Question(s)\n\n\nDo penguins with longer flippers weigh more or less than penguins with shorter flippers? You probably already have an answer, but try to make your answer precise.\nWhat does the relationship between flipper length and body mass look like? Is it positive? Negative? Linear? Nonlinear?\nDoes the relationship vary by the species of the penguin? How about by the island where the penguin lives?",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#building-up-a-plot",
    "href": "Week2/notes.html#building-up-a-plot",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nCreating a ggplot\n\n\nWith ggplot2, you begin a plot with the function ggplot(), defining a plot object that you then add layers to.\nThe first argument of ggplot() is the dataset to use in the graph and so ggplot(data = penguins) creates an empty graph that is primed to display the penguins data, but since we haven’t told it how to visualize it yet, for now it’s empty.\n\n\n#| exercise: empty-plot\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n#| edit: false\nggplot(data = penguins)\n\n\n\n\n\n\n\n\ninput ggplot(data = penguins)",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#building-up-a-plot-1",
    "href": "Week2/notes.html#building-up-a-plot-1",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nThis is not a very exciting plot, but you can think of it like an empty canvas you’ll paint the remaining layers of your plot onto.\nNext, we need to tell ggplot() how the information from our data will be visually represented. The mapping argument of the ggplot() function defines how variables in your dataset are mapped to visual properties (aesthetics) of your plot.\n. . .\nFor now, we will only map flipper length to the x aesthetic and body mass to the y aesthetic.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#the-grammar-of-graphics-4",
    "href": "Week2/notes.html#the-grammar-of-graphics-4",
    "title": "Data Visualization",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nAesthetics",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#building-up-a-plot-2",
    "href": "Week2/notes.html#building-up-a-plot-2",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nAesthetic mappings\n\n\nThe mapping argument is always defined in the aes() function, and the x and y arguments of aes() specify which variables to map to the x and y axes.\nFor now, we will only map flipper length to the x aesthetic and body mass to the y aesthetic. ggplot2 looks for the mapped variables in the data argument, in this case, penguins.\n\n\n#| exercise: aesthetic-mappings\n#| autorun: false\nggplot(\n  data = penguins,\n  mapping = ______________________\n)\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n#| edit: false\n#| autorun: false\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n)\n\n\n\n\n\n\n\n\nadd mapping = aes(x = flipper_length_mm, y = body_mass_g)\nOur empty canvas now has more structure – it’s clear where flipper lengths will be displayed (on the x-axis) and where body masses will be displayed (on the y-axis). But the penguins themselves are not yet on the plot. This is because we have not yet articulated, in our code, how to represent the observations from our data frame on our plot.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#building-up-a-plot-3",
    "href": "Week2/notes.html#building-up-a-plot-3",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdding layers\nWe need to define a geom: the geometrical object that a plot uses to represent data. These geometric objects are made available in ggplot2 with functions that start with geom_.\nPeople often describe plots by the type of geom that the plot uses:\n\n\nbar charts use bar geoms (geom_bar()),\nline charts use line geoms (geom_line()),\nboxplots use boxplot geoms (geom_boxplot()),\nscatterplots use point geoms (geom_point()), and so on.\n\n\nThe function geom_point() adds a layer of points to your plot, which creates a scatterplot.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#building-up-a-plot-4",
    "href": "Week2/notes.html#building-up-a-plot-4",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdd a scatter point layer to the plot:\n\n#| exercise: geom-point\n#| fig-width: 9\n#| fig-height: 5\n#| fig-alt: |\n#|   A scatterplot of flipper length (x-axis) and body mass (y-axis) for penguins. The plot\n#|   displays a positive, linear, and relative strong relationship between these two variables.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  __________________________\n\n\n\nSolution. \n\n\n\n\n\n\nTip\n\n\n\n\n\n\n#| edit: false\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()\n\n\n\n\n\n\nadd + geom_point()\nNow we have a scatterplot of flipper length (x-axis) and body mass (y-axis) for penguins. The plot displays a positive, linear, and relative strong relationship between these two variables.\nBefore we add more layers to this plot, let’s pause for a moment and review the warning message we got:\nWe’re seeing this message because there are two penguins in our dataset with missing body mass and/or flipper length values and ggplot2 has no way of representing them on the plot without both of these values. Like R, ggplot2 subscribes to the philosophy that missing values should never silently go missing. This type of warning is probably one of the most common types of warnings you will see when working with real data – missing values are a very common issue and you’ll learn more about them later. For the remaining plots we will suppress this warning so it’s not printed alongside every single plot we make.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#building-up-a-plot-5",
    "href": "Week2/notes.html#building-up-a-plot-5",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdding aesthetics\n\n\n\n\nIt’s always a good idea to be skeptical of any apparent relationship between two variables and ask if there may be other variables that explain or change the nature of this apparent relationship.\n\n\nFor example, does the relationship between flipper length and body mass differ by species?\nWhen exploring relationships between variables, it’s important to consider other variables that might affect the relationship. Let’s incorporate species into our plot using color:\n\n\n\n\n#| exercise: add-color\n#| warning: false\n#| fig-alt: |\n#|   A scatterplot of body mass vs. flipper length of penguins, with points\n#|   colored by species.\nggplot(\n  data = penguins,\n  mapping = aes(\n    x = flipper_length_mm,\n    y = body_mass_g,\n    ______________________\n  )\n) +\n  geom_point()\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n#| edit: false\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point()\n\n\n\n\n\n\n\n\nScatterplots are useful for displaying the relationship between two numerical variables, but it’s always a good idea to be skeptical of any apparent relationship between two variables and ask if there may be other variables that explain or change the nature of this apparent relationship.\nFor example, does the relationship between flipper length and body mass differ by species? Let’s incorporate species into our plot and see if this reveals any additional insights into the apparent relationship between these variables. We will do this by representing species with different colored points.\nTo achieve this, will we need to modify the aesthetic or the geom? If you guessed “in the aesthetic mapping, inside of aes()”, you’re already getting the hang of creating data visualizations with ggplot2! And if not, don’t worry.\nAdd color = species to the aesthetic mapping. This tells ggplot2 to color the points by species.\nWhen we map a categorical variable to an aesthetic, ggplot2 automatically: - Assigns a unique value to each level (here, a unique color for each species) - Adds a legend explaining the mapping",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#building-up-a-plot-6",
    "href": "Week2/notes.html#building-up-a-plot-6",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdd a trend line to see the relationship more clearly using geom_smooth()\n\n\n#| exercise: add-lm\n#| warning: false\n#| fig-alt: |\n#|   A scatterplot of body mass vs. flipper length with trend lines by species.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point() +\n  _______________\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nAdd a trendline (geom_smooth(method = \"lm\")) layer to the plot.\n\n#| edit: false\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\nNow let’s add one more layer: a smooth curve displaying the relationship between body mass and flipper length. Before you proceed, refer back to the code above, and think about how we can add this to our existing plot.\nSince this is a new geometric object representing our data, we will add a new geom as a layer on top of our point geom: geom_smooth(). And we will specify that we want to draw the line of best fit based on a linear model with method = \"lm\".\nAdd geom_smooth(method = \"lm\") to the plot.\nThe method = “lm” argument tells geom_smooth() to use a linear model. Notice how the color aesthetic is inherited by both geoms, creating separate trend lines for each species.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#building-up-a-plot-7",
    "href": "Week2/notes.html#building-up-a-plot-7",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdding smooth curves\nIt’s important to recognise how the color aesthetic is inherited by both geoms, creating separate trend lines for each species.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\nWe have successfully added lines, but this plot doesn’t look like our ultimate goal plot, which only has one line for the entire dataset as opposed to separate lines for each of the penguin species.\nWhen aesthetic mappings are defined in ggplot(), at the global level, they’re passed down to each of the subsequent geom layers of the plot. However, each geom function in ggplot2 can also take a mapping argument, which allows for aesthetic mappings at the local level that are added to those inherited from the global level. Since we want points to be colored based on species but don’t want the lines to be separated out for them, we should specify color = species for geom_point() only.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#building-up-a-plot-8",
    "href": "Week2/notes.html#building-up-a-plot-8",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdding smooth curves\nIt’s important to recognise how the color aesthetic is inherited by both geoms, creating separate trend lines for each species.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, \n  color = species)\n) +\n  geom_point( &lt;color = species&gt; ) +\n  geom_smooth(method = \"lm\")",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#building-up-a-plot-9",
    "href": "Week2/notes.html#building-up-a-plot-9",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdding smooth curves\nIt’s important to recognise how the color aesthetic is inherited by both geoms, creating separate trend lines for each species.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, \n  color = species)\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\", &lt;color = species&gt;)\nPay attention to how the aesthetic mappings propagate through the layers of the plot.\nThis can be useful for creating complex plots with multiple layers, but it can also lead to unexpected results if you’re not careful.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#building-up-plots",
    "href": "Week2/notes.html#building-up-plots",
    "title": "Data Visualization",
    "section": "Building up plots",
    "text": "Building up plots\nGlobal vs Local aesthetics\n\n\nIn the previous plot, the color aesthetic was defined in the global mapping. This means that it applies to all geoms in the plot.\nTo get a single trend line while keeping colored points, we move the color aesthetic to geom_point():\n\n\n#| exercise: local-aesthetics\n#| warning: false\n#| fig-alt: |\n#|   A scatterplot with colored points by species and a single trend line.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(___________________________) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n#| edit: false\n#| warning: false\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species)) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nAesthetic mappings can be defined at the global level (in ggplot()) or at the local level (in individual geoms). Local aesthetics override global ones.\nVoila! We have something that looks very much like our ultimate goal, though it’s not yet perfect. We still need to use different shapes for each species of penguins and improve labels.\nIt’s generally not a good idea to represent information using only colors on a plot, as people perceive colors differently due to color blindness or other color vision differences. Therefore, in addition to color, we can also map species to the shape aesthetic.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#building-up-a-plot-10",
    "href": "Week2/notes.html#building-up-a-plot-10",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nOther aesthetics - shapes\n\n\nIn addition to color, we can also map out variables to other aesthetic elements.\nHere, we map species to the shape aesthetic.\n\n\n\n#| exercise: add-shape\n#| warning: false\n#| fig-width: 10\n#| fig-height: 5\n#| fig-alt: |\n#|   A scatterplot of body mass vs. flipper length of penguins. Overlaid \n#|   on the scatterplot is a single line of best fit displaying the \n#|   relationship between these variables for each species (Adelie, \n#|   Chinstrap, and Gentoo). Different penguin species are plotted in \n#|   different colors and shapes for the points only.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(\n    mapping = aes(color = species, ____________________)\n    ) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n#| edit: false\n#| warning: false\n#| fig-width: 10\n#| fig-height: 5\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nIt’s generally not a good idea to represent information using only colors on a plot, as people perceive colors differently due to color blindness or other color vision differences. Therefore, in addition to color, we can also map species to the shape aesthetic.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#building-up-a-plot-11",
    "href": "Week2/notes.html#building-up-a-plot-11",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nFinal touches\nThe data portions of our plot are now complete. But data visualization is not just about the data – it’s also about the visual elements that make the plot accessible and informative.\nWe also need the plot itself to communicate:\n\n\nWhat the plot is about (title)\nWhat the axes represent, including units (labels)\nWhat the colors and shapes represent (legends)\nAdditional context such as the source of the data (subtitle or caption)",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#building-up-a-plot-12",
    "href": "Week2/notes.html#building-up-a-plot-12",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nWe can now add this information to our plot\n\n#| warning: false\n#| exercise: final-plot\n#| fig-width: 12\n#| fig-height: 6\n#| fig-alt: |\n#|   The final version of our plot with proper labels and both color and shape aesthetics.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\") +\n  labs(\n    _____ = ________________, \n    color = \"Species\",\n    shape = \"Species\"\n  ) +\n  scale_color_colorblind()\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n#| edit: false\n#| warning: false\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Body Mass and Flipper Length\",\n    subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n    x = \"Flipper Length (mm)\", \n    y = \"Body Mass (g)\",\n    color = \"Species\", \n    shape = \"Species\"\n  ) +\n  scale_color_colorblind()\n\n\n\n\n\n\nTo add:\ntitle = \"Body Mass and Flipper Length\",\nsubtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\nx = \"Flipper Length (mm)\", \ny = \"Body Mass (g)\",\ncolor = \"Species\", \nshape = \"Species\"\nAnd finally, we can improve the labels of our plot using the labs() function in a new layer. Some of the arguments to labs() might be self explanatory:\n\n\ntitle adds a title and subtitle adds a subtitle to the plot\n\nOther arguments match the aesthetic mappings:\n\n\nx is the x-axis label\n\ny is the y-axis label\n\ncolor and shape define the label for the legend.\n\nIn addition, we can improve the color palette to be colorblind safe with the scale_color_colorblind() function from the ggthemes package.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#some-notes-on-ggplot-calls",
    "href": "Week2/notes.html#some-notes-on-ggplot-calls",
    "title": "Data Visualization",
    "section": "Some notes on ggplot() calls",
    "text": "Some notes on ggplot() calls\n\n\nSo far, we’ve written the code in a very explicit way, with each argument named. This is a good practice when you’re learning, but it can be a bit verbose.\n\n\nTypically, the first one or two arguments to a function are so important that you should know them by heart. The first two arguments to ggplot() are data and mapping.\n\nYou’ll often see them left out. This is true for other functions as well.\nWhen leaving the names out, the order of the arguments matters.\n\n\n\n\n\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()\n\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()\n\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()\n\n\n\n\n#| edit: false\n?ggplot",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#some-notes-on-ggplot-calls-1",
    "href": "Week2/notes.html#some-notes-on-ggplot-calls-1",
    "title": "Data Visualization",
    "section": "Some notes on ggplot() calls",
    "text": "Some notes on ggplot() calls\nIn the future, you’ll also learn about the pipe, |&gt;, which operates similarly to the + operator in ggplot2.\nIt lets you chain together a series of operations, passing the output of one function to the input of the next.\npenguins |&gt; \n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#some-notes-on-ggplot-calls-2",
    "href": "Week2/notes.html#some-notes-on-ggplot-calls-2",
    "title": "Data Visualization",
    "section": "Some notes on ggplot() calls",
    "text": "Some notes on ggplot() calls\nIn the future, you’ll also learn about the pipe, |&gt;, which operates similarly to the + operator in ggplot2.\nIt lets you chain together a series of operations, passing the output of one function to the input of the next.\npenguins |&gt; \n  ggplot(&lt;penguins&gt;, aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()\n\nDon’t worry if you don’t understand this yet. It’s just a sneak peek at what’s to come.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#summary",
    "href": "Week2/notes.html#summary",
    "title": "Data Visualization",
    "section": "Summary",
    "text": "Summary\nThe basic idea that underpins ggplot2: a visualization is a mapping from variables in your data to aesthetic properties like position, color, size and shape.\n\nThe grammar of graphics provides a systematic way to build visualizations\nStart with data and aesthetic mappings\nAdd layers with geoms\nUse different geoms for different types of variables\nEnhance plots with labels, colors, and facets\nMake sure your plots are clear and honest\n\n\nKey takeaways: - Build plots layer by layer - Choose appropriate visualizations for your variable types - Consider your audience when making design choices - Use aesthetics and facets to show additional variables - Always aim for clear and accessible visualizations",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#thats-it",
    "href": "Week2/notes.html#thats-it",
    "title": "Data Visualization",
    "section": "That’s it!",
    "text": "That’s it!\nWith our remaining time, I’d like you to practice with ggplot2 using the DataAnalytics exercise. You should have already installed DataAnalytics with:\ndevtools::install_github(\"antoinevernet/DataAnalytics\")\nYou can then run the following code to get started:\nlearnr::run_tutorial(\"02-Visualisation\", package = \"DataAnalytics\")",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#continuous-module-dialogue",
    "href": "Week2/notes.html#continuous-module-dialogue",
    "title": "Data Visualization",
    "section": "Continuous Module Dialogue",
    "text": "Continuous Module Dialogue\nMenti Survey\n\n\nhttps://www.menti.com/al7gkr8qhntz",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#visualizing-distributions",
    "href": "Week2/notes.html#visualizing-distributions",
    "title": "Data Visualization",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nThe following are some additional slides on dealing with other data types and visualizing distributions in ggplot2. Explore them at your own pace.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#visualizing-distributions-1",
    "href": "Week2/notes.html#visualizing-distributions-1",
    "title": "Data Visualization",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nCategorical variables\nFor categorical variables like species, we use bar charts:\n\n#| fig-alt: |\n#|   A bar chart showing the frequency of each penguin species.\nggplot(penguins, aes(x = species)) +\n  geom_bar()\n\n\nA bar chart shows: - Categories on one axis - Counts (frequencies) on the other axis - Height of bars represents number of observations in each category",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#visualizing-distributions-2",
    "href": "Week2/notes.html#visualizing-distributions-2",
    "title": "Data Visualization",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nImproving categorical plots\nWe can reorder bars by frequency for better visualization:\n\n#| fig-alt: |\n#|   A bar chart with species ordered by frequency.\nggplot(penguins, aes(x = fct_infreq(species))) +\n  geom_bar()\n\n\nfct_infreq() is a function from forcats package that reorders factor levels by their frequencies. This makes the plot easier to read and interpret patterns.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#visualizing-distributions-3",
    "href": "Week2/notes.html#visualizing-distributions-3",
    "title": "Data Visualization",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nNumerical variables\nFor numerical variables like body_mass_g, we use histograms:\n\n#| warning: false\n#| fig-alt: |\n#|   A histogram showing the distribution of penguin body mass.\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 200)\n\n\nA histogram: - Divides the x-axis into bins - Height shows number of observations in each bin - binwidth controls the size of the bins - Different binwidths can reveal different patterns",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#visualizing-distributions-4",
    "href": "Week2/notes.html#visualizing-distributions-4",
    "title": "Data Visualization",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nExploring binwidth\nThe choice of binwidth affects what patterns we can see:\n\n\n\n\n\n\n#| warning: false\n#| layout-ncol: 2\n#| fig-width: 5\n#| fig-alt: |\n#|   Two histograms with different binwidths showing how binwidth affects visualization.\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 20)\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 2000)\n\n\n\n\n\nToo small binwidth: too many bars, noisy pattern\nToo large binwidth: too few bars, loses detail\nNeed to experiment to find the right balance",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#visualizing-distributions-5",
    "href": "Week2/notes.html#visualizing-distributions-5",
    "title": "Data Visualization",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nDensity plots\nAn alternative to histograms is the density plot:\n\n#| fig-alt: |\n#|   A density plot showing the distribution of penguin body mass.\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_density()\n\n\nDensity plots: - Show the shape of the distribution smoothly - Easier to compare multiple groups - Like a smoothed histogram - Think of it as draping a string over a histogram",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#visualizing-relationships",
    "href": "Week2/notes.html#visualizing-relationships",
    "title": "Data Visualization",
    "section": "Visualizing relationships",
    "text": "Visualizing relationships\nNumerical and categorical variables\nTo compare a numerical variable across categories, use boxplots:\n\n#| warning: false\n#| fig-alt: |\n#|   Box plots showing body mass distribution by species.\nggplot(penguins, aes(x = species, y = body_mass_g)) +\n  geom_boxplot()\n\n\nA boxplot shows: - Median (middle line) - IQR (box) - Whiskers (extend to most extreme non-outlier points) - Individual points for outliers",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#visualizing-relationships-1",
    "href": "Week2/notes.html#visualizing-relationships-1",
    "title": "Data Visualization",
    "section": "Visualizing relationships",
    "text": "Visualizing relationships\nAlternative views\nWe can also use density plots to compare distributions:\n\n#| warning: false\n#| fig-alt: |\n#|   Density plots of body mass by species.\nggplot(penguins, aes(x = body_mass_g, color = species, fill = species)) +\n  geom_density(alpha = 0.5)\n\n\n\ncolor and fill aesthetics distinguish species\nalpha controls transparency\nOverlapping distributions show how groups compare",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#visualizing-relationships-2",
    "href": "Week2/notes.html#visualizing-relationships-2",
    "title": "Data Visualization",
    "section": "Visualizing relationships",
    "text": "Visualizing relationships\nTwo categorical variables\nFor two categorical variables, use stacked bar plots:\n\n#| fig-alt: |\n#|   A stacked bar plot showing species distribution across islands.\nggplot(penguins, aes(x = island, fill = species)) +\n  geom_bar(position = \"fill\")\n\n\nposition = “fill”: - Standardizes bars to same height - Shows proportions instead of counts - Better for comparing distributions across groups",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/notes.html#visualizing-relationships-3",
    "href": "Week2/notes.html#visualizing-relationships-3",
    "title": "Data Visualization",
    "section": "Visualizing relationships",
    "text": "Visualizing relationships\nThree or more variables\nUse facets to split plots by a categorical variable:\n\n#| warning: false\n#| fig-width: 10\n#| fig-height: 3\n#| fig-alt: |\n#|   A faceted plot showing the relationship between body mass and flipper length for each island.\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species, shape = species)) +\n  facet_wrap(~island)\n\n\nfacet_wrap(): - Creates separate plots for each category - Maintains same scales across plots - Useful for seeing patterns within groups - Alternative to mapping variables to aesthetics",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 2",
      "Data Visualization"
    ]
  },
  {
    "objectID": "Week2/01-dataviz.html",
    "href": "Week2/01-dataviz.html",
    "title": "BSSC0021",
    "section": "",
    "text": "#| include: false\n#| autorun: true\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(palmerpenguins)\nlibrary(ggthemes)\n\n\n“The simple graph has brought more information to the data analyst’s mind than any other device.” — John Tukey\n\nR has several systems for making graphs, but ggplot2 is one of the most elegant and most versatile.\nggplot2 implements the grammar of graphics, a coherent system for describing and building graphs. With ggplot2, you can do more and faster by learning one system and applying it in many places.\n\n\nThis presentation is based on the Data Visualization chapter of the R for Data Science book by Hadley Wickham and Garrett Grolemund.\n\nIn this tutorial, we will create this plot:\n\n#| echo: false\n#| autorun: true\n#| fig-width: 10\n#| fig-height: 5\nintro_p &lt;- penguins |&gt;\n  drop_na() |&gt;\n  ggplot(mapping = aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(mapping = aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\", formula = y ~ x) +\n  labs(\n    title = \"Body Mass and Flipper Length\",\n    subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n    x = \"Flipper Length (mm)\",\n    y = \"Body Mass (g)\",\n    color = \"Species\",\n    shape = \"Species\"\n  )\n\nintro_p"
  },
  {
    "objectID": "Week2/01-dataviz.html#introduction",
    "href": "Week2/01-dataviz.html#introduction",
    "title": "BSSC0021",
    "section": "",
    "text": "#| include: false\n#| autorun: true\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(palmerpenguins)\nlibrary(ggthemes)\n\n\n“The simple graph has brought more information to the data analyst’s mind than any other device.” — John Tukey\n\nR has several systems for making graphs, but ggplot2 is one of the most elegant and most versatile.\nggplot2 implements the grammar of graphics, a coherent system for describing and building graphs. With ggplot2, you can do more and faster by learning one system and applying it in many places.\n\n\nThis presentation is based on the Data Visualization chapter of the R for Data Science book by Hadley Wickham and Garrett Grolemund.\n\nIn this tutorial, we will create this plot:\n\n#| echo: false\n#| autorun: true\n#| fig-width: 10\n#| fig-height: 5\nintro_p &lt;- penguins |&gt;\n  drop_na() |&gt;\n  ggplot(mapping = aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(mapping = aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\", formula = y ~ x) +\n  labs(\n    title = \"Body Mass and Flipper Length\",\n    subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n    x = \"Flipper Length (mm)\",\n    y = \"Body Mass (g)\",\n    color = \"Species\",\n    shape = \"Species\"\n  )\n\nintro_p"
  },
  {
    "objectID": "Week2/01-dataviz.html#the-grammar-of-graphics",
    "href": "Week2/01-dataviz.html#the-grammar-of-graphics",
    "title": "BSSC0021",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\n\n\nIn 1999, a statistician named Leland Wilkinson published the first edition of what has been the most influential work in data visualization, The Grammar of Graphics.\nThe most complete implementation of the grammar is found in an R package called ggplot2 by Hadley Wickham."
  },
  {
    "objectID": "Week2/01-dataviz.html#the-grammar-of-graphics-1",
    "href": "Week2/01-dataviz.html#the-grammar-of-graphics-1",
    "title": "BSSC0021",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nA plot can be decomposed into three primary elements\n1. the data\n2. the aesthetic mapping of the variables in the data to visual cues\n3. the geometry used to encode the observations on the plot."
  },
  {
    "objectID": "Week2/01-dataviz.html#getting-started",
    "href": "Week2/01-dataviz.html#getting-started",
    "title": "BSSC0021",
    "section": "Getting Started",
    "text": "Getting Started\nThroughout this lecture, we will be writing code together inside this webpage.\n\n#| exercise: getting-started\n#| min-lines: 5\n\n\n\n\n\n\n\n\n\nHints:\n\n\n\n\n\nYou can type code into the cells and run them by clicking the “Run” button.\n\n2 + 3"
  },
  {
    "objectID": "Week2/01-dataviz.html#getting-started-1",
    "href": "Week2/01-dataviz.html#getting-started-1",
    "title": "BSSC0021",
    "section": "Getting Started",
    "text": "Getting Started\nPackages\n\n\nWe begin by loading the tidyverse and ggplot2 packages.\nWe almost always begin our work by loading the tidyverse package. Note that the terms “package” and “library” are used interchangeably but that there is no package() function. To load a package, you need to use library().\n\n\n\n#| label: getting-started\n#| min-lines: 3\n\n# Load the libraries"
  },
  {
    "objectID": "Week2/01-dataviz.html#getting-started-2",
    "href": "Week2/01-dataviz.html#getting-started-2",
    "title": "BSSC0021",
    "section": "Getting Started",
    "text": "Getting Started\nLoading the Data\n\n\nLoad the palmerpenguins package using library().\nThis package contains the penguins dataset, which we will use for this tutorial.\n\n\n#| autorun: false\n#| min-lines: 2\nlibrary(palmerpenguins)"
  },
  {
    "objectID": "Week2/01-dataviz.html#getting-started-3",
    "href": "Week2/01-dataviz.html#getting-started-3",
    "title": "BSSC0021",
    "section": "Getting Started",
    "text": "Getting Started\nGetting help\n\n\nIf you are unsure about how to use a function, you can use the ? operator to get help.\nFor a data package like palmerpenguins, you can use ?penguins to get help on the dataset.\n\n\n?penguins"
  },
  {
    "objectID": "Week2/01-dataviz.html#the-grammar-of-graphics-2",
    "href": "Week2/01-dataviz.html#the-grammar-of-graphics-2",
    "title": "BSSC0021",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nThe Data\n- A variable is a quantity, quality, or property that you can measure.\n- A value is the state of a variable when you measure it. The value of a variable may change from measurement to measurement.\n- An observation is a set of measurements made under similar conditions. An observation will contain several values, each associated with a different variable. We’ll sometimes refer to an observation as a data point.\n- Tabular data is a set of values, each associated with a variable and an observation. Tabular data is tidy if each value is placed in its own “cell”, each variable in its own column, and each observation in its own row."
  },
  {
    "objectID": "Week2/01-dataviz.html#the-grammar-of-graphics-3",
    "href": "Week2/01-dataviz.html#the-grammar-of-graphics-3",
    "title": "BSSC0021",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nThe Data\n\n\n\n\nspecies: a penguin’s species (Adelie, Chinstrap, or Gentoo).\n\nflipper_length_mm: length of a penguin’s flipper, in millimeters.\n\nbody_mass_g: body mass of a penguin, in grams.\n\n\n\n\n\npenguins\n\n\n\n\n\n#| fig-width: 6.5\n#| fig-height: 4.8\n#| warning: false\n#| echo: false\n#| autorun: true\nggplot(\n  data = penguins,\n  mapping = aes(x = bill_length_mm, y = bill_depth_mm, color = species)\n) +\n  geom_point()"
  },
  {
    "objectID": "Week2/01-dataviz.html#formulating-our-research-questions",
    "href": "Week2/01-dataviz.html#formulating-our-research-questions",
    "title": "BSSC0021",
    "section": "Formulating our Research Question(s)",
    "text": "Formulating our Research Question(s)\n\n\nDo penguins with longer flippers weigh more or less than penguins with shorter flippers? You probably already have an answer, but try to make your answer precise.\nWhat does the relationship between flipper length and body mass look like? Is it positive? Negative? Linear? Nonlinear?\nDoes the relationship vary by the species of the penguin? How about by the island where the penguin lives?"
  },
  {
    "objectID": "Week2/01-dataviz.html#building-up-a-plot",
    "href": "Week2/01-dataviz.html#building-up-a-plot",
    "title": "BSSC0021",
    "section": "Building up a plot",
    "text": "Building up a plot\nCreating a ggplot\n\n\nWith ggplot2, you begin a plot with the function ggplot(), defining a plot object that you then add layers to.\nThe first argument of ggplot() is the dataset to use in the graph and so ggplot(data = penguins) creates an empty graph that is primed to display the penguins data, but since we haven’t told it how to visualize it yet, for now it’s empty.\n\n\n#| exercise: empty-plot\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n#| edit: false\nggplot(data = penguins)\n\n\n\n\n\n\n\n\ninput ggplot(data = penguins)"
  },
  {
    "objectID": "Week2/01-dataviz.html#building-up-a-plot-1",
    "href": "Week2/01-dataviz.html#building-up-a-plot-1",
    "title": "BSSC0021",
    "section": "Building up a plot",
    "text": "Building up a plot\nThis is not a very exciting plot, but you can think of it like an empty canvas you’ll paint the remaining layers of your plot onto.\nNext, we need to tell ggplot() how the information from our data will be visually represented. The mapping argument of the ggplot() function defines how variables in your dataset are mapped to visual properties (aesthetics) of your plot.\n. . .\nFor now, we will only map flipper length to the x aesthetic and body mass to the y aesthetic."
  },
  {
    "objectID": "Week2/01-dataviz.html#the-grammar-of-graphics-4",
    "href": "Week2/01-dataviz.html#the-grammar-of-graphics-4",
    "title": "BSSC0021",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nAesthetics"
  },
  {
    "objectID": "Week2/01-dataviz.html#building-up-a-plot-2",
    "href": "Week2/01-dataviz.html#building-up-a-plot-2",
    "title": "BSSC0021",
    "section": "Building up a plot",
    "text": "Building up a plot\nAesthetic mappings\n\n\nThe mapping argument is always defined in the aes() function, and the x and y arguments of aes() specify which variables to map to the x and y axes.\nFor now, we will only map flipper length to the x aesthetic and body mass to the y aesthetic. ggplot2 looks for the mapped variables in the data argument, in this case, penguins.\n\n\n#| exercise: aesthetic-mappings\n#| autorun: false\nggplot(\n  data = penguins,\n  mapping = ______________________\n)\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n#| edit: false\n#| autorun: false\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n)\n\n\n\n\n\n\n\n\nadd mapping = aes(x = flipper_length_mm, y = body_mass_g)\nOur empty canvas now has more structure – it’s clear where flipper lengths will be displayed (on the x-axis) and where body masses will be displayed (on the y-axis). But the penguins themselves are not yet on the plot. This is because we have not yet articulated, in our code, how to represent the observations from our data frame on our plot."
  },
  {
    "objectID": "Week2/01-dataviz.html#building-up-a-plot-3",
    "href": "Week2/01-dataviz.html#building-up-a-plot-3",
    "title": "BSSC0021",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdding layers\nWe need to define a geom: the geometrical object that a plot uses to represent data. These geometric objects are made available in ggplot2 with functions that start with geom_.\nPeople often describe plots by the type of geom that the plot uses:\n\n\nbar charts use bar geoms (geom_bar()),\nline charts use line geoms (geom_line()),\nboxplots use boxplot geoms (geom_boxplot()),\nscatterplots use point geoms (geom_point()), and so on.\n\n\nThe function geom_point() adds a layer of points to your plot, which creates a scatterplot."
  },
  {
    "objectID": "Week2/01-dataviz.html#building-up-a-plot-4",
    "href": "Week2/01-dataviz.html#building-up-a-plot-4",
    "title": "BSSC0021",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdd a scatter point layer to the plot:\n\n#| exercise: geom-point\n#| fig-width: 9\n#| fig-height: 5\n#| fig-alt: |\n#|   A scatterplot of flipper length (x-axis) and body mass (y-axis) for penguins. The plot\n#|   displays a positive, linear, and relative strong relationship between these two variables.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  __________________________\n\n\n\nSolution. \n\n\n\n\n\n\nTip\n\n\n\n\n\n\n#| edit: false\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()\n\n\n\n\n\n\nadd + geom_point()\nNow we have a scatterplot of flipper length (x-axis) and body mass (y-axis) for penguins. The plot displays a positive, linear, and relative strong relationship between these two variables.\nBefore we add more layers to this plot, let’s pause for a moment and review the warning message we got:\nWe’re seeing this message because there are two penguins in our dataset with missing body mass and/or flipper length values and ggplot2 has no way of representing them on the plot without both of these values. Like R, ggplot2 subscribes to the philosophy that missing values should never silently go missing. This type of warning is probably one of the most common types of warnings you will see when working with real data – missing values are a very common issue and you’ll learn more about them later. For the remaining plots we will suppress this warning so it’s not printed alongside every single plot we make."
  },
  {
    "objectID": "Week2/01-dataviz.html#building-up-a-plot-5",
    "href": "Week2/01-dataviz.html#building-up-a-plot-5",
    "title": "BSSC0021",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdding aesthetics\n\n\n\n\nIt’s always a good idea to be skeptical of any apparent relationship between two variables and ask if there may be other variables that explain or change the nature of this apparent relationship.\n\n\nFor example, does the relationship between flipper length and body mass differ by species?\nWhen exploring relationships between variables, it’s important to consider other variables that might affect the relationship. Let’s incorporate species into our plot using color:\n\n\n\n\n#| exercise: add-color\n#| warning: false\n#| fig-alt: |\n#|   A scatterplot of body mass vs. flipper length of penguins, with points\n#|   colored by species.\nggplot(\n  data = penguins,\n  mapping = aes(\n    x = flipper_length_mm,\n    y = body_mass_g,\n    ______________________\n  )\n) +\n  geom_point()\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n#| edit: false\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point()\n\n\n\n\n\n\n\n\nScatterplots are useful for displaying the relationship between two numerical variables, but it’s always a good idea to be skeptical of any apparent relationship between two variables and ask if there may be other variables that explain or change the nature of this apparent relationship.\nFor example, does the relationship between flipper length and body mass differ by species? Let’s incorporate species into our plot and see if this reveals any additional insights into the apparent relationship between these variables. We will do this by representing species with different colored points.\nTo achieve this, will we need to modify the aesthetic or the geom? If you guessed “in the aesthetic mapping, inside of aes()”, you’re already getting the hang of creating data visualizations with ggplot2! And if not, don’t worry.\nAdd color = species to the aesthetic mapping. This tells ggplot2 to color the points by species.\nWhen we map a categorical variable to an aesthetic, ggplot2 automatically: - Assigns a unique value to each level (here, a unique color for each species) - Adds a legend explaining the mapping"
  },
  {
    "objectID": "Week2/01-dataviz.html#building-up-a-plot-6",
    "href": "Week2/01-dataviz.html#building-up-a-plot-6",
    "title": "BSSC0021",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdd a trend line to see the relationship more clearly using geom_smooth()\n\n\n#| exercise: add-lm\n#| warning: false\n#| fig-alt: |\n#|   A scatterplot of body mass vs. flipper length with trend lines by species.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point() +\n  _______________\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\nAdd a trendline (geom_smooth(method = \"lm\")) layer to the plot.\n\n#| edit: false\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\nNow let’s add one more layer: a smooth curve displaying the relationship between body mass and flipper length. Before you proceed, refer back to the code above, and think about how we can add this to our existing plot.\nSince this is a new geometric object representing our data, we will add a new geom as a layer on top of our point geom: geom_smooth(). And we will specify that we want to draw the line of best fit based on a linear model with method = \"lm\".\nAdd geom_smooth(method = \"lm\") to the plot.\nThe method = “lm” argument tells geom_smooth() to use a linear model. Notice how the color aesthetic is inherited by both geoms, creating separate trend lines for each species."
  },
  {
    "objectID": "Week2/01-dataviz.html#building-up-a-plot-7",
    "href": "Week2/01-dataviz.html#building-up-a-plot-7",
    "title": "BSSC0021",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdding smooth curves\nIt’s important to recognise how the color aesthetic is inherited by both geoms, creating separate trend lines for each species.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\nWe have successfully added lines, but this plot doesn’t look like our ultimate goal plot, which only has one line for the entire dataset as opposed to separate lines for each of the penguin species.\nWhen aesthetic mappings are defined in ggplot(), at the global level, they’re passed down to each of the subsequent geom layers of the plot. However, each geom function in ggplot2 can also take a mapping argument, which allows for aesthetic mappings at the local level that are added to those inherited from the global level. Since we want points to be colored based on species but don’t want the lines to be separated out for them, we should specify color = species for geom_point() only."
  },
  {
    "objectID": "Week2/01-dataviz.html#building-up-a-plot-8",
    "href": "Week2/01-dataviz.html#building-up-a-plot-8",
    "title": "BSSC0021",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdding smooth curves\nIt’s important to recognise how the color aesthetic is inherited by both geoms, creating separate trend lines for each species.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, \n  color = species)\n) +\n  geom_point( &lt;color = species&gt; ) +\n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "Week2/01-dataviz.html#building-up-a-plot-9",
    "href": "Week2/01-dataviz.html#building-up-a-plot-9",
    "title": "BSSC0021",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdding smooth curves\nIt’s important to recognise how the color aesthetic is inherited by both geoms, creating separate trend lines for each species.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, \n  color = species)\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\", &lt;color = species&gt;)\nPay attention to how the aesthetic mappings propagate through the layers of the plot.\nThis can be useful for creating complex plots with multiple layers, but it can also lead to unexpected results if you’re not careful."
  },
  {
    "objectID": "Week2/01-dataviz.html#building-up-plots",
    "href": "Week2/01-dataviz.html#building-up-plots",
    "title": "BSSC0021",
    "section": "Building up plots",
    "text": "Building up plots\nGlobal vs Local aesthetics\n\n\nIn the previous plot, the color aesthetic was defined in the global mapping. This means that it applies to all geoms in the plot.\nTo get a single trend line while keeping colored points, we move the color aesthetic to geom_point():\n\n\n#| exercise: local-aesthetics\n#| warning: false\n#| fig-alt: |\n#|   A scatterplot with colored points by species and a single trend line.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(___________________________) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n#| edit: false\n#| warning: false\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species)) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nAesthetic mappings can be defined at the global level (in ggplot()) or at the local level (in individual geoms). Local aesthetics override global ones.\nVoila! We have something that looks very much like our ultimate goal, though it’s not yet perfect. We still need to use different shapes for each species of penguins and improve labels.\nIt’s generally not a good idea to represent information using only colors on a plot, as people perceive colors differently due to color blindness or other color vision differences. Therefore, in addition to color, we can also map species to the shape aesthetic."
  },
  {
    "objectID": "Week2/01-dataviz.html#building-up-a-plot-10",
    "href": "Week2/01-dataviz.html#building-up-a-plot-10",
    "title": "BSSC0021",
    "section": "Building up a plot",
    "text": "Building up a plot\nOther aesthetics - shapes\n\n\nIn addition to color, we can also map out variables to other aesthetic elements.\nHere, we map species to the shape aesthetic.\n\n\n\n#| exercise: add-shape\n#| warning: false\n#| fig-width: 10\n#| fig-height: 5\n#| fig-alt: |\n#|   A scatterplot of body mass vs. flipper length of penguins. Overlaid \n#|   on the scatterplot is a single line of best fit displaying the \n#|   relationship between these variables for each species (Adelie, \n#|   Chinstrap, and Gentoo). Different penguin species are plotted in \n#|   different colors and shapes for the points only.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(\n    mapping = aes(color = species, ____________________)\n    ) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n#| edit: false\n#| warning: false\n#| fig-width: 10\n#| fig-height: 5\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(mapping = aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\nIt’s generally not a good idea to represent information using only colors on a plot, as people perceive colors differently due to color blindness or other color vision differences. Therefore, in addition to color, we can also map species to the shape aesthetic."
  },
  {
    "objectID": "Week2/01-dataviz.html#building-up-a-plot-11",
    "href": "Week2/01-dataviz.html#building-up-a-plot-11",
    "title": "BSSC0021",
    "section": "Building up a plot",
    "text": "Building up a plot\nFinal touches\nThe data portions of our plot are now complete. But data visualization is not just about the data – it’s also about the visual elements that make the plot accessible and informative.\nWe also need the plot itself to communicate:\n\n\nWhat the plot is about (title)\nWhat the axes represent, including units (labels)\nWhat the colors and shapes represent (legends)\nAdditional context such as the source of the data (subtitle or caption)"
  },
  {
    "objectID": "Week2/01-dataviz.html#building-up-a-plot-12",
    "href": "Week2/01-dataviz.html#building-up-a-plot-12",
    "title": "BSSC0021",
    "section": "Building up a plot",
    "text": "Building up a plot\nWe can now add this information to our plot\n\n#| warning: false\n#| exercise: final-plot\n#| fig-width: 12\n#| fig-height: 6\n#| fig-alt: |\n#|   The final version of our plot with proper labels and both color and shape aesthetics.\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\") +\n  labs(\n    _____ = ________________, \n    color = \"Species\",\n    shape = \"Species\"\n  ) +\n  scale_color_colorblind()\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n#| edit: false\n#| warning: false\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point(aes(color = species, shape = species)) +\n  geom_smooth(method = \"lm\") +\n  labs(\n    title = \"Body Mass and Flipper Length\",\n    subtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\n    x = \"Flipper Length (mm)\", \n    y = \"Body Mass (g)\",\n    color = \"Species\", \n    shape = \"Species\"\n  ) +\n  scale_color_colorblind()\n\n\n\n\n\n\nTo add:\ntitle = \"Body Mass and Flipper Length\",\nsubtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\nx = \"Flipper Length (mm)\", \ny = \"Body Mass (g)\",\ncolor = \"Species\", \nshape = \"Species\"\nAnd finally, we can improve the labels of our plot using the labs() function in a new layer. Some of the arguments to labs() might be self explanatory:\n\n\ntitle adds a title and subtitle adds a subtitle to the plot\n\nOther arguments match the aesthetic mappings:\n\n\nx is the x-axis label\n\ny is the y-axis label\n\ncolor and shape define the label for the legend.\n\nIn addition, we can improve the color palette to be colorblind safe with the scale_color_colorblind() function from the ggthemes package."
  },
  {
    "objectID": "Week2/01-dataviz.html#some-notes-on-ggplot-calls",
    "href": "Week2/01-dataviz.html#some-notes-on-ggplot-calls",
    "title": "BSSC0021",
    "section": "Some notes on ggplot() calls",
    "text": "Some notes on ggplot() calls\n\n\nSo far, we’ve written the code in a very explicit way, with each argument named. This is a good practice when you’re learning, but it can be a bit verbose.\n\n\nTypically, the first one or two arguments to a function are so important that you should know them by heart. The first two arguments to ggplot() are data and mapping.\n\nYou’ll often see them left out. This is true for other functions as well.\nWhen leaving the names out, the order of the arguments matters.\n\n\n\n\n\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()\n\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()\n\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()\n\n\n\n\n#| edit: false\n?ggplot"
  },
  {
    "objectID": "Week2/01-dataviz.html#some-notes-on-ggplot-calls-1",
    "href": "Week2/01-dataviz.html#some-notes-on-ggplot-calls-1",
    "title": "BSSC0021",
    "section": "Some notes on ggplot() calls",
    "text": "Some notes on ggplot() calls\nIn the future, you’ll also learn about the pipe, |&gt;, which operates similarly to the + operator in ggplot2.\nIt lets you chain together a series of operations, passing the output of one function to the input of the next.\npenguins |&gt; \n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()"
  },
  {
    "objectID": "Week2/01-dataviz.html#some-notes-on-ggplot-calls-2",
    "href": "Week2/01-dataviz.html#some-notes-on-ggplot-calls-2",
    "title": "BSSC0021",
    "section": "Some notes on ggplot() calls",
    "text": "Some notes on ggplot() calls\nIn the future, you’ll also learn about the pipe, |&gt;, which operates similarly to the + operator in ggplot2.\nIt lets you chain together a series of operations, passing the output of one function to the input of the next.\npenguins |&gt; \n  ggplot(&lt;penguins&gt;, aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()\n\nDon’t worry if you don’t understand this yet. It’s just a sneak peek at what’s to come."
  },
  {
    "objectID": "Week2/01-dataviz.html#summary",
    "href": "Week2/01-dataviz.html#summary",
    "title": "BSSC0021",
    "section": "Summary",
    "text": "Summary\nThe basic idea that underpins ggplot2: a visualization is a mapping from variables in your data to aesthetic properties like position, color, size and shape.\n\nThe grammar of graphics provides a systematic way to build visualizations\nStart with data and aesthetic mappings\nAdd layers with geoms\nUse different geoms for different types of variables\nEnhance plots with labels, colors, and facets\nMake sure your plots are clear and honest\n\n\nKey takeaways: - Build plots layer by layer - Choose appropriate visualizations for your variable types - Consider your audience when making design choices - Use aesthetics and facets to show additional variables - Always aim for clear and accessible visualizations"
  },
  {
    "objectID": "Week2/01-dataviz.html#thats-it",
    "href": "Week2/01-dataviz.html#thats-it",
    "title": "BSSC0021",
    "section": "That’s it!",
    "text": "That’s it!\nWith our remaining time, I’d like you to practice with ggplot2 using the DataAnalytics exercise. You should have already installed DataAnalytics with:\ndevtools::install_github(\"antoinevernet/DataAnalytics\")\nYou can then run the following code to get started:\nlearnr::run_tutorial(\"02-Visualisation\", package = \"DataAnalytics\")"
  },
  {
    "objectID": "Week2/01-dataviz.html#continuous-module-dialogue",
    "href": "Week2/01-dataviz.html#continuous-module-dialogue",
    "title": "BSSC0021",
    "section": "Continuous Module Dialogue",
    "text": "Continuous Module Dialogue\nMenti Survey\n\n\nhttps://www.menti.com/al7gkr8qhntz"
  },
  {
    "objectID": "Week2/01-dataviz.html#visualizing-distributions",
    "href": "Week2/01-dataviz.html#visualizing-distributions",
    "title": "BSSC0021",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nThe following are some additional slides on dealing with other data types and visualizing distributions in ggplot2. Explore them at your own pace."
  },
  {
    "objectID": "Week2/01-dataviz.html#visualizing-distributions-1",
    "href": "Week2/01-dataviz.html#visualizing-distributions-1",
    "title": "BSSC0021",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nCategorical variables\nFor categorical variables like species, we use bar charts:\n\n#| fig-alt: |\n#|   A bar chart showing the frequency of each penguin species.\nggplot(penguins, aes(x = species)) +\n  geom_bar()\n\n\nA bar chart shows: - Categories on one axis - Counts (frequencies) on the other axis - Height of bars represents number of observations in each category"
  },
  {
    "objectID": "Week2/01-dataviz.html#visualizing-distributions-2",
    "href": "Week2/01-dataviz.html#visualizing-distributions-2",
    "title": "BSSC0021",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nImproving categorical plots\nWe can reorder bars by frequency for better visualization:\n\n#| fig-alt: |\n#|   A bar chart with species ordered by frequency.\nggplot(penguins, aes(x = fct_infreq(species))) +\n  geom_bar()\n\n\nfct_infreq() is a function from forcats package that reorders factor levels by their frequencies. This makes the plot easier to read and interpret patterns."
  },
  {
    "objectID": "Week2/01-dataviz.html#visualizing-distributions-3",
    "href": "Week2/01-dataviz.html#visualizing-distributions-3",
    "title": "BSSC0021",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nNumerical variables\nFor numerical variables like body_mass_g, we use histograms:\n\n#| warning: false\n#| fig-alt: |\n#|   A histogram showing the distribution of penguin body mass.\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 200)\n\n\nA histogram: - Divides the x-axis into bins - Height shows number of observations in each bin - binwidth controls the size of the bins - Different binwidths can reveal different patterns"
  },
  {
    "objectID": "Week2/01-dataviz.html#visualizing-distributions-4",
    "href": "Week2/01-dataviz.html#visualizing-distributions-4",
    "title": "BSSC0021",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nExploring binwidth\nThe choice of binwidth affects what patterns we can see:\n\n\n\n\n\n\n#| warning: false\n#| layout-ncol: 2\n#| fig-width: 5\n#| fig-alt: |\n#|   Two histograms with different binwidths showing how binwidth affects visualization.\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 20)\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 2000)\n\n\n\n\n\nToo small binwidth: too many bars, noisy pattern\nToo large binwidth: too few bars, loses detail\nNeed to experiment to find the right balance"
  },
  {
    "objectID": "Week2/01-dataviz.html#visualizing-distributions-5",
    "href": "Week2/01-dataviz.html#visualizing-distributions-5",
    "title": "BSSC0021",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nDensity plots\nAn alternative to histograms is the density plot:\n\n#| fig-alt: |\n#|   A density plot showing the distribution of penguin body mass.\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_density()\n\n\nDensity plots: - Show the shape of the distribution smoothly - Easier to compare multiple groups - Like a smoothed histogram - Think of it as draping a string over a histogram"
  },
  {
    "objectID": "Week2/01-dataviz.html#visualizing-relationships",
    "href": "Week2/01-dataviz.html#visualizing-relationships",
    "title": "BSSC0021",
    "section": "Visualizing relationships",
    "text": "Visualizing relationships\nNumerical and categorical variables\nTo compare a numerical variable across categories, use boxplots:\n\n#| warning: false\n#| fig-alt: |\n#|   Box plots showing body mass distribution by species.\nggplot(penguins, aes(x = species, y = body_mass_g)) +\n  geom_boxplot()\n\n\nA boxplot shows: - Median (middle line) - IQR (box) - Whiskers (extend to most extreme non-outlier points) - Individual points for outliers"
  },
  {
    "objectID": "Week2/01-dataviz.html#visualizing-relationships-1",
    "href": "Week2/01-dataviz.html#visualizing-relationships-1",
    "title": "BSSC0021",
    "section": "Visualizing relationships",
    "text": "Visualizing relationships\nAlternative views\nWe can also use density plots to compare distributions:\n\n#| warning: false\n#| fig-alt: |\n#|   Density plots of body mass by species.\nggplot(penguins, aes(x = body_mass_g, color = species, fill = species)) +\n  geom_density(alpha = 0.5)\n\n\n\ncolor and fill aesthetics distinguish species\nalpha controls transparency\nOverlapping distributions show how groups compare"
  },
  {
    "objectID": "Week2/01-dataviz.html#visualizing-relationships-2",
    "href": "Week2/01-dataviz.html#visualizing-relationships-2",
    "title": "BSSC0021",
    "section": "Visualizing relationships",
    "text": "Visualizing relationships\nTwo categorical variables\nFor two categorical variables, use stacked bar plots:\n\n#| fig-alt: |\n#|   A stacked bar plot showing species distribution across islands.\nggplot(penguins, aes(x = island, fill = species)) +\n  geom_bar(position = \"fill\")\n\n\nposition = “fill”: - Standardizes bars to same height - Shows proportions instead of counts - Better for comparing distributions across groups"
  },
  {
    "objectID": "Week2/01-dataviz.html#visualizing-relationships-3",
    "href": "Week2/01-dataviz.html#visualizing-relationships-3",
    "title": "BSSC0021",
    "section": "Visualizing relationships",
    "text": "Visualizing relationships\nThree or more variables\nUse facets to split plots by a categorical variable:\n\n#| warning: false\n#| fig-width: 10\n#| fig-height: 3\n#| fig-alt: |\n#|   A faceted plot showing the relationship between body mass and flipper length for each island.\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species, shape = species)) +\n  facet_wrap(~island)\n\n\nfacet_wrap(): - Creates separate plots for each category - Maintains same scales across plots - Useful for seeing patterns within groups - Alternative to mapping variables to aesthetics"
  },
  {
    "objectID": "Week2/lecture.html#introduction",
    "href": "Week2/lecture.html#introduction",
    "title": "Data Visualization",
    "section": "Introduction",
    "text": "Introduction\n\n“The simple graph has brought more information to the data analyst’s mind than any other device.” — John Tukey\n\nR has several systems for making graphs, but ggplot2 is one of the most elegant and most versatile.\nggplot2 implements the grammar of graphics, a coherent system for describing and building graphs. With ggplot2, you can do more and faster by learning one system and applying it in many places.\n\n\nThis presentation is based on the Data Visualization chapter of the R for Data Science book by Hadley Wickham and Garrett Grolemund."
  },
  {
    "objectID": "Week2/lecture.html#the-grammar-of-graphics",
    "href": "Week2/lecture.html#the-grammar-of-graphics",
    "title": "Data Visualization",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\n\n\nIn 1999, a statistician named Leland Wilkinson published the first edition of what has been the most influential work in data visualization, The Grammar of Graphics.\nThe most complete implementation of the grammar is found in an R package called ggplot2 by Hadley Wickham."
  },
  {
    "objectID": "Week2/lecture.html#the-grammar-of-graphics-1",
    "href": "Week2/lecture.html#the-grammar-of-graphics-1",
    "title": "Data Visualization",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nA plot can be decomposed into three primary elements\n1. the data\n2. the aesthetic mapping of the variables in the data to visual cues\n3. the geometry used to encode the observations on the plot."
  },
  {
    "objectID": "Week2/lecture.html#getting-started",
    "href": "Week2/lecture.html#getting-started",
    "title": "Data Visualization",
    "section": "Getting Started",
    "text": "Getting Started\nThroughout this lecture, we will be writing code together inside this webpage.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHints:\n\n\nYou can type code into the cells and run them by clicking the “Run” button."
  },
  {
    "objectID": "Week2/lecture.html#getting-started-1",
    "href": "Week2/lecture.html#getting-started-1",
    "title": "Data Visualization",
    "section": "Getting Started",
    "text": "Getting Started\nPackages\n\n\nWe begin by loading the tidyverse and ggplot2 packages.\n\nWe almost always begin our work by loading the tidyverse package. Note that the terms “package” and “library” are used interchangeably but that there is no package() function. To load a package, you need to use library()."
  },
  {
    "objectID": "Week2/lecture.html#getting-started-2",
    "href": "Week2/lecture.html#getting-started-2",
    "title": "Data Visualization",
    "section": "Getting Started",
    "text": "Getting Started\nLoading the Data\n\n\nLoad the palmerpenguins package using library().\nThis package contains the penguins dataset, which we will use for this tutorial."
  },
  {
    "objectID": "Week2/lecture.html#getting-started-3",
    "href": "Week2/lecture.html#getting-started-3",
    "title": "Data Visualization",
    "section": "Getting Started",
    "text": "Getting Started\nGetting help\n\n\nIf you are unsure about how to use a function, you can use the ? operator to get help.\nFor a data package like palmerpenguins, you can use ?penguins to get help on the dataset."
  },
  {
    "objectID": "Week2/lecture.html#the-grammar-of-graphics-2",
    "href": "Week2/lecture.html#the-grammar-of-graphics-2",
    "title": "Data Visualization",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nThe Data\n- A variable is a quantity, quality, or property that you can measure.\n- A value is the state of a variable when you measure it. The value of a variable may change from measurement to measurement.\n- An observation is a set of measurements made under similar conditions. An observation will contain several values, each associated with a different variable. We’ll sometimes refer to an observation as a data point.\n- Tabular data is a set of values, each associated with a variable and an observation. Tabular data is tidy if each value is placed in its own “cell”, each variable in its own column, and each observation in its own row."
  },
  {
    "objectID": "Week2/lecture.html#the-grammar-of-graphics-3",
    "href": "Week2/lecture.html#the-grammar-of-graphics-3",
    "title": "Data Visualization",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nThe Data\n\n\n\nspecies: a penguin’s species (Adelie, Chinstrap, or Gentoo).\nflipper_length_mm: length of a penguin’s flipper, in millimeters.\nbody_mass_g: body mass of a penguin, in grams."
  },
  {
    "objectID": "Week2/lecture.html#formulating-our-research-questions",
    "href": "Week2/lecture.html#formulating-our-research-questions",
    "title": "Data Visualization",
    "section": "Formulating our Research Question(s)",
    "text": "Formulating our Research Question(s)\n\n\nDo penguins with longer flippers weigh more or less than penguins with shorter flippers? You probably already have an answer, but try to make your answer precise.\nWhat does the relationship between flipper length and body mass look like? Is it positive? Negative? Linear? Nonlinear?\nDoes the relationship vary by the species of the penguin? How about by the island where the penguin lives?"
  },
  {
    "objectID": "Week2/lecture.html#building-up-a-plot",
    "href": "Week2/lecture.html#building-up-a-plot",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nCreating a ggplot\n\n\nWith ggplot2, you begin a plot with the function ggplot(), defining a plot object that you then add layers to.\n\nThe first argument of ggplot() is the dataset to use in the graph and so ggplot(data = penguins) creates an empty graph that is primed to display the penguins data, but since we haven’t told it how to visualize it yet, for now it’s empty.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninput ggplot(data = penguins)"
  },
  {
    "objectID": "Week2/lecture.html#building-up-a-plot-1",
    "href": "Week2/lecture.html#building-up-a-plot-1",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nThis is not a very exciting plot, but you can think of it like an empty canvas you’ll paint the remaining layers of your plot onto.\n\nNext, we need to tell ggplot() how the information from our data will be visually represented. The mapping argument of the ggplot() function defines how variables in your dataset are mapped to visual properties (aesthetics) of your plot.\n\n\nFor now, we will only map flipper length to the x aesthetic and body mass to the y aesthetic."
  },
  {
    "objectID": "Week2/lecture.html#the-grammar-of-graphics-4",
    "href": "Week2/lecture.html#the-grammar-of-graphics-4",
    "title": "Data Visualization",
    "section": "The Grammar of Graphics",
    "text": "The Grammar of Graphics\nAesthetics"
  },
  {
    "objectID": "Week2/lecture.html#building-up-a-plot-2",
    "href": "Week2/lecture.html#building-up-a-plot-2",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nAesthetic mappings\n\n\nThe mapping argument is always defined in the aes() function, and the x and y arguments of aes() specify which variables to map to the x and y axes.\n\nFor now, we will only map flipper length to the x aesthetic and body mass to the y aesthetic. ggplot2 looks for the mapped variables in the data argument, in this case, penguins.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nadd mapping = aes(x = flipper_length_mm, y = body_mass_g)\nOur empty canvas now has more structure – it’s clear where flipper lengths will be displayed (on the x-axis) and where body masses will be displayed (on the y-axis). But the penguins themselves are not yet on the plot. This is because we have not yet articulated, in our code, how to represent the observations from our data frame on our plot."
  },
  {
    "objectID": "Week2/lecture.html#building-up-a-plot-3",
    "href": "Week2/lecture.html#building-up-a-plot-3",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdding layers\nWe need to define a geom: the geometrical object that a plot uses to represent data. These geometric objects are made available in ggplot2 with functions that start with geom_.\n\nPeople often describe plots by the type of geom that the plot uses:\n\n\nbar charts use bar geoms (geom_bar()),\nline charts use line geoms (geom_line()),\nboxplots use boxplot geoms (geom_boxplot()),\nscatterplots use point geoms (geom_point()), and so on.\n\n\nThe function geom_point() adds a layer of points to your plot, which creates a scatterplot."
  },
  {
    "objectID": "Week2/lecture.html#building-up-a-plot-4",
    "href": "Week2/lecture.html#building-up-a-plot-4",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdd a scatter point layer to the plot:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nadd + geom_point()\nNow we have a scatterplot of flipper length (x-axis) and body mass (y-axis) for penguins. The plot displays a positive, linear, and relative strong relationship between these two variables.\nBefore we add more layers to this plot, let’s pause for a moment and review the warning message we got:\nWe’re seeing this message because there are two penguins in our dataset with missing body mass and/or flipper length values and ggplot2 has no way of representing them on the plot without both of these values. Like R, ggplot2 subscribes to the philosophy that missing values should never silently go missing. This type of warning is probably one of the most common types of warnings you will see when working with real data – missing values are a very common issue and you’ll learn more about them later. For the remaining plots we will suppress this warning so it’s not printed alongside every single plot we make."
  },
  {
    "objectID": "Week2/lecture.html#building-up-a-plot-5",
    "href": "Week2/lecture.html#building-up-a-plot-5",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdding aesthetics\n\n\n\n\nIt’s always a good idea to be skeptical of any apparent relationship between two variables and ask if there may be other variables that explain or change the nature of this apparent relationship.\n\n\nFor example, does the relationship between flipper length and body mass differ by species?\nWhen exploring relationships between variables, it’s important to consider other variables that might affect the relationship. Let’s incorporate species into our plot using color:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScatterplots are useful for displaying the relationship between two numerical variables, but it’s always a good idea to be skeptical of any apparent relationship between two variables and ask if there may be other variables that explain or change the nature of this apparent relationship.\nFor example, does the relationship between flipper length and body mass differ by species? Let’s incorporate species into our plot and see if this reveals any additional insights into the apparent relationship between these variables. We will do this by representing species with different colored points.\nTo achieve this, will we need to modify the aesthetic or the geom? If you guessed “in the aesthetic mapping, inside of aes()”, you’re already getting the hang of creating data visualizations with ggplot2! And if not, don’t worry.\nAdd color = species to the aesthetic mapping. This tells ggplot2 to color the points by species.\nWhen we map a categorical variable to an aesthetic, ggplot2 automatically: - Assigns a unique value to each level (here, a unique color for each species) - Adds a legend explaining the mapping"
  },
  {
    "objectID": "Week2/lecture.html#building-up-a-plot-6",
    "href": "Week2/lecture.html#building-up-a-plot-6",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdd a trend line to see the relationship more clearly using geom_smooth()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nAdd a trendline (geom_smooth(method = \"lm\")) layer to the plot.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow let’s add one more layer: a smooth curve displaying the relationship between body mass and flipper length. Before you proceed, refer back to the code above, and think about how we can add this to our existing plot.\nSince this is a new geometric object representing our data, we will add a new geom as a layer on top of our point geom: geom_smooth(). And we will specify that we want to draw the line of best fit based on a linear model with method = \"lm\".\nAdd geom_smooth(method = \"lm\") to the plot.\nThe method = “lm” argument tells geom_smooth() to use a linear model. Notice how the color aesthetic is inherited by both geoms, creating separate trend lines for each species."
  },
  {
    "objectID": "Week2/lecture.html#building-up-a-plot-7",
    "href": "Week2/lecture.html#building-up-a-plot-7",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdding smooth curves\nIt’s important to recognise how the color aesthetic is inherited by both geoms, creating separate trend lines for each species.\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n\nWe have successfully added lines, but this plot doesn’t look like our ultimate goal plot, which only has one line for the entire dataset as opposed to separate lines for each of the penguin species.\nWhen aesthetic mappings are defined in ggplot(), at the global level, they’re passed down to each of the subsequent geom layers of the plot. However, each geom function in ggplot2 can also take a mapping argument, which allows for aesthetic mappings at the local level that are added to those inherited from the global level. Since we want points to be colored based on species but don’t want the lines to be separated out for them, we should specify color = species for geom_point() only."
  },
  {
    "objectID": "Week2/lecture.html#building-up-a-plot-8",
    "href": "Week2/lecture.html#building-up-a-plot-8",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdding smooth curves\nIt’s important to recognise how the color aesthetic is inherited by both geoms, creating separate trend lines for each species.\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, \n  color = species)\n) +\n  geom_point( &lt;color = species&gt; ) +\n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "Week2/lecture.html#building-up-a-plot-9",
    "href": "Week2/lecture.html#building-up-a-plot-9",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nAdding smooth curves\nIt’s important to recognise how the color aesthetic is inherited by both geoms, creating separate trend lines for each species.\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g, \n  color = species)\n) +\n  geom_point() +\n  geom_smooth(method = \"lm\", &lt;color = species&gt;)\nPay attention to how the aesthetic mappings propagate through the layers of the plot.\nThis can be useful for creating complex plots with multiple layers, but it can also lead to unexpected results if you’re not careful."
  },
  {
    "objectID": "Week2/lecture.html#building-up-plots",
    "href": "Week2/lecture.html#building-up-plots",
    "title": "Data Visualization",
    "section": "Building up plots",
    "text": "Building up plots\nGlobal vs Local aesthetics\n\n\nIn the previous plot, the color aesthetic was defined in the global mapping. This means that it applies to all geoms in the plot.\nTo get a single trend line while keeping colored points, we move the color aesthetic to geom_point():\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAesthetic mappings can be defined at the global level (in ggplot()) or at the local level (in individual geoms). Local aesthetics override global ones.\nVoila! We have something that looks very much like our ultimate goal, though it’s not yet perfect. We still need to use different shapes for each species of penguins and improve labels.\nIt’s generally not a good idea to represent information using only colors on a plot, as people perceive colors differently due to color blindness or other color vision differences. Therefore, in addition to color, we can also map species to the shape aesthetic."
  },
  {
    "objectID": "Week2/lecture.html#building-up-a-plot-10",
    "href": "Week2/lecture.html#building-up-a-plot-10",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nOther aesthetics - shapes\n\n\nIn addition to color, we can also map out variables to other aesthetic elements.\nHere, we map species to the shape aesthetic.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt’s generally not a good idea to represent information using only colors on a plot, as people perceive colors differently due to color blindness or other color vision differences. Therefore, in addition to color, we can also map species to the shape aesthetic."
  },
  {
    "objectID": "Week2/lecture.html#building-up-a-plot-11",
    "href": "Week2/lecture.html#building-up-a-plot-11",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nFinal touches\nThe data portions of our plot are now complete. But data visualization is not just about the data – it’s also about the visual elements that make the plot accessible and informative.\n\nWe also need the plot itself to communicate:\n\n\nWhat the plot is about (title)\nWhat the axes represent, including units (labels)\nWhat the colors and shapes represent (legends)\nAdditional context such as the source of the data (subtitle or caption)"
  },
  {
    "objectID": "Week2/lecture.html#building-up-a-plot-12",
    "href": "Week2/lecture.html#building-up-a-plot-12",
    "title": "Data Visualization",
    "section": "Building up a plot",
    "text": "Building up a plot\nWe can now add this information to our plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo add:\ntitle = \"Body Mass and Flipper Length\",\nsubtitle = \"Dimensions for Adelie, Chinstrap, and Gentoo Penguins\",\nx = \"Flipper Length (mm)\", \ny = \"Body Mass (g)\",\ncolor = \"Species\", \nshape = \"Species\"\nAnd finally, we can improve the labels of our plot using the labs() function in a new layer. Some of the arguments to labs() might be self explanatory:\n\ntitle adds a title and subtitle adds a subtitle to the plot\n\nOther arguments match the aesthetic mappings:\n\nx is the x-axis label\ny is the y-axis label\ncolor and shape define the label for the legend.\n\nIn addition, we can improve the color palette to be colorblind safe with the scale_color_colorblind() function from the ggthemes package."
  },
  {
    "objectID": "Week2/lecture.html#some-notes-on-ggplot-calls",
    "href": "Week2/lecture.html#some-notes-on-ggplot-calls",
    "title": "Data Visualization",
    "section": "Some notes on ggplot() calls",
    "text": "Some notes on ggplot() calls\n\n\nSo far, we’ve written the code in a very explicit way, with each argument named. This is a good practice when you’re learning, but it can be a bit verbose.\n\n\n\nTypically, the first one or two arguments to a function are so important that you should know them by heart. The first two arguments to ggplot() are data and mapping.\n\nYou’ll often see them left out. This is true for other functions as well.\nWhen leaving the names out, the order of the arguments matters.\n\n\n\n\n\n\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()\n\n\nggplot(\n  data = penguins,\n  mapping = aes(x = flipper_length_mm, y = body_mass_g)\n) +\n  geom_point()\n\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()"
  },
  {
    "objectID": "Week2/lecture.html#some-notes-on-ggplot-calls-1",
    "href": "Week2/lecture.html#some-notes-on-ggplot-calls-1",
    "title": "Data Visualization",
    "section": "Some notes on ggplot() calls",
    "text": "Some notes on ggplot() calls\nIn the future, you’ll also learn about the pipe, |&gt;, which operates similarly to the + operator in ggplot2.\nIt lets you chain together a series of operations, passing the output of one function to the input of the next.\n\npenguins |&gt; \n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()"
  },
  {
    "objectID": "Week2/lecture.html#some-notes-on-ggplot-calls-2",
    "href": "Week2/lecture.html#some-notes-on-ggplot-calls-2",
    "title": "Data Visualization",
    "section": "Some notes on ggplot() calls",
    "text": "Some notes on ggplot() calls\nIn the future, you’ll also learn about the pipe, |&gt;, which operates similarly to the + operator in ggplot2.\nIt lets you chain together a series of operations, passing the output of one function to the input of the next.\n\npenguins |&gt; \n  ggplot(&lt;penguins&gt;, aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()\n\nDon’t worry if you don’t understand this yet. It’s just a sneak peek at what’s to come."
  },
  {
    "objectID": "Week2/lecture.html#summary",
    "href": "Week2/lecture.html#summary",
    "title": "Data Visualization",
    "section": "Summary",
    "text": "Summary\nThe basic idea that underpins ggplot2: a visualization is a mapping from variables in your data to aesthetic properties like position, color, size and shape.\n\nThe grammar of graphics provides a systematic way to build visualizations\nStart with data and aesthetic mappings\nAdd layers with geoms\nUse different geoms for different types of variables\nEnhance plots with labels, colors, and facets\nMake sure your plots are clear and honest\n\n\nKey takeaways: - Build plots layer by layer - Choose appropriate visualizations for your variable types - Consider your audience when making design choices - Use aesthetics and facets to show additional variables - Always aim for clear and accessible visualizations"
  },
  {
    "objectID": "Week2/lecture.html#thats-it",
    "href": "Week2/lecture.html#thats-it",
    "title": "Data Visualization",
    "section": "That’s it!",
    "text": "That’s it!\nWith our remaining time, I’d like you to practice with ggplot2 using the DataAnalytics exercise. You should have already installed DataAnalytics with:\ndevtools::install_github(\"antoinevernet/DataAnalytics\")\nYou can then run the following code to get started:\nlearnr::run_tutorial(\"02-Visualisation\", package = \"DataAnalytics\")"
  },
  {
    "objectID": "Week2/lecture.html#continuous-module-dialogue",
    "href": "Week2/lecture.html#continuous-module-dialogue",
    "title": "Data Visualization",
    "section": "Continuous Module Dialogue",
    "text": "Continuous Module Dialogue\nMenti Survey\n\nhttps://www.menti.com/al7gkr8qhntz"
  },
  {
    "objectID": "Week2/lecture.html#visualizing-distributions",
    "href": "Week2/lecture.html#visualizing-distributions",
    "title": "Data Visualization",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nThe following are some additional slides on dealing with other data types and visualizing distributions in ggplot2. Explore them at your own pace."
  },
  {
    "objectID": "Week2/lecture.html#visualizing-distributions-1",
    "href": "Week2/lecture.html#visualizing-distributions-1",
    "title": "Data Visualization",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nCategorical variables\nFor categorical variables like species, we use bar charts:\n\n\n\n\n\n\n\n\n\nA bar chart shows: - Categories on one axis - Counts (frequencies) on the other axis - Height of bars represents number of observations in each category"
  },
  {
    "objectID": "Week2/lecture.html#visualizing-distributions-3",
    "href": "Week2/lecture.html#visualizing-distributions-3",
    "title": "Data Visualization",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nNumerical variables\nFor numerical variables like body_mass_g, we use histograms:\n\n\n\n\n\n\n\n\n\nA histogram: - Divides the x-axis into bins - Height shows number of observations in each bin - binwidth controls the size of the bins - Different binwidths can reveal different patterns"
  },
  {
    "objectID": "Week2/lecture.html#visualizing-distributions-4",
    "href": "Week2/lecture.html#visualizing-distributions-4",
    "title": "Data Visualization",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nExploring binwidth\nThe choice of binwidth affects what patterns we can see:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nToo small binwidth: too many bars, noisy pattern\nToo large binwidth: too few bars, loses detail\nNeed to experiment to find the right balance"
  },
  {
    "objectID": "Week2/lecture.html#visualizing-distributions-5",
    "href": "Week2/lecture.html#visualizing-distributions-5",
    "title": "Data Visualization",
    "section": "Visualizing distributions",
    "text": "Visualizing distributions\nDensity plots\nAn alternative to histograms is the density plot:\n\n\n\n\n\n\n\n\n\nDensity plots: - Show the shape of the distribution smoothly - Easier to compare multiple groups - Like a smoothed histogram - Think of it as draping a string over a histogram"
  },
  {
    "objectID": "Week2/lecture.html#visualizing-relationships",
    "href": "Week2/lecture.html#visualizing-relationships",
    "title": "Data Visualization",
    "section": "Visualizing relationships",
    "text": "Visualizing relationships\nNumerical and categorical variables\nTo compare a numerical variable across categories, use boxplots:\n\n\n\n\n\n\n\n\n\nA boxplot shows: - Median (middle line) - IQR (box) - Whiskers (extend to most extreme non-outlier points) - Individual points for outliers"
  },
  {
    "objectID": "Week2/lecture.html#visualizing-relationships-1",
    "href": "Week2/lecture.html#visualizing-relationships-1",
    "title": "Data Visualization",
    "section": "Visualizing relationships",
    "text": "Visualizing relationships\nAlternative views\nWe can also use density plots to compare distributions:\n\n\n\n\n\n\n\n\n\n\ncolor and fill aesthetics distinguish species\nalpha controls transparency\nOverlapping distributions show how groups compare"
  },
  {
    "objectID": "Week2/lecture.html#visualizing-relationships-2",
    "href": "Week2/lecture.html#visualizing-relationships-2",
    "title": "Data Visualization",
    "section": "Visualizing relationships",
    "text": "Visualizing relationships\nTwo categorical variables\nFor two categorical variables, use stacked bar plots:\n\n\n\n\n\n\n\n\n\nposition = “fill”: - Standardizes bars to same height - Shows proportions instead of counts - Better for comparing distributions across groups"
  },
  {
    "objectID": "Week2/lecture.html#visualizing-relationships-3",
    "href": "Week2/lecture.html#visualizing-relationships-3",
    "title": "Data Visualization",
    "section": "Visualizing relationships",
    "text": "Visualizing relationships\nThree or more variables\nUse facets to split plots by a categorical variable:\n\n\n\n\n\n\n\n\n\nfacet_wrap(): - Creates separate plots for each category - Maintains same scales across plots - Useful for seeing patterns within groups - Alternative to mapping variables to aesthetics"
  },
  {
    "objectID": "Week3/01-intro.html",
    "href": "Week3/01-intro.html",
    "title": "Part 1: Introduction to Probability",
    "section": "",
    "text": "Branch of mathematics dealing with chance and uncertainty\nFoundation for statistics\nProvides tools to describe uncertain events\nHistorical origins in games of chance\nDeep questions about meaning and interpretation\n\n\nProbability theory is the branch of mathematics that deals with chance and uncertainty. It forms an important part of the foundation for statistics, because it provides us with the mathematical tools to describe uncertain events.\nThe study of probability arose in part due to interest in understanding games of chance, like cards or dice. These games provide useful examples of many statistical concepts, because when we repeat these games the likelihood of different outcomes remains (mostly) the same. However, there are deep questions about the meaning of probability that we will not address here.\n\n\n\n\nAn experiment is any activity that produces or observes an outcome. Examples are flipping a coin, rolling a 6-sided die, or trying a new route to work to see if it’s faster than the old route.\n\nCoin flip: {heads, tails}\nDie roll: {1,2,3,4,5,6}\nTravel time: (0,∞)\n\n\nThe sample space is the set of possible outcomes for an experiment. We represent these by listing them within a set of squiggly brackets.\nAn event is a subset of the sample space. In principle it could be one or more of possible outcomes in the sample space, but here we will focus primarily on elementary events which consist of exactly one possible outcome.\n\nSubset of sample space\nCan be elementary or compound\nExample: rolling a 4\n\n\n\n\n\nTo formalize probability theory, we first need to define a few terms:\n\nAn experiment is any activity that produces or observes an outcome. Examples are flipping a coin, rolling a 6-sided die, or trying a new route to work to see if it’s faster than the old route.\nThe sample space is the set of possible outcomes for an experiment. We represent these by listing them within a set of squiggly brackets.\nAn event is a subset of the sample space. In principle it could be one or more of possible outcomes in the sample space, but here we will focus primarily on elementary events which consist of exactly one possible outcome.\n\n\n\nFor events {E_1, E_2, ... , E_N} and random variable X:\n\n\n\n\nNon-negativity:\nP(X=E_i) \\ge 0\n\n\nNormalization:\n\\sum_{i=1}^N{P(X=E_i)} = 1\n\n\nBoundedness:\nP(X=E_i)\\le 1\n\n\n\n\nImplications:\n\nAll probabilities are between 0 and 1\nTotal probability must sum to 1\nIndividual probabilities ≤ 1\n\n\n\n\n\nThese are the features that a value has to have if it is going to be a probability, which were first defined by the Russian mathematician Andrei Kolmogorov.\nThe summation is interpreted as saying “Take all of the N elementary events, which we have labeled from 1 to N, and add up their probabilities. These must sum to one.”\nThe third point is implied by the previous points; since they must sum to one, and they can’t be negative, then any particular probability cannot exceed one.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Part 1: Introduction to Probability"
    ]
  },
  {
    "objectID": "Week3/01-intro.html#what-is-probability-theory",
    "href": "Week3/01-intro.html#what-is-probability-theory",
    "title": "Part 1: Introduction to Probability",
    "section": "",
    "text": "Branch of mathematics dealing with chance and uncertainty\nFoundation for statistics\nProvides tools to describe uncertain events\nHistorical origins in games of chance\nDeep questions about meaning and interpretation\n\n\nProbability theory is the branch of mathematics that deals with chance and uncertainty. It forms an important part of the foundation for statistics, because it provides us with the mathematical tools to describe uncertain events.\nThe study of probability arose in part due to interest in understanding games of chance, like cards or dice. These games provide useful examples of many statistical concepts, because when we repeat these games the likelihood of different outcomes remains (mostly) the same. However, there are deep questions about the meaning of probability that we will not address here.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Part 1: Introduction to Probability"
    ]
  },
  {
    "objectID": "Week3/01-intro.html#experiment-sample-space-events",
    "href": "Week3/01-intro.html#experiment-sample-space-events",
    "title": "Part 1: Introduction to Probability",
    "section": "",
    "text": "An experiment is any activity that produces or observes an outcome. Examples are flipping a coin, rolling a 6-sided die, or trying a new route to work to see if it’s faster than the old route.\n\nCoin flip: {heads, tails}\nDie roll: {1,2,3,4,5,6}\nTravel time: (0,∞)\n\n\nThe sample space is the set of possible outcomes for an experiment. We represent these by listing them within a set of squiggly brackets.\nAn event is a subset of the sample space. In principle it could be one or more of possible outcomes in the sample space, but here we will focus primarily on elementary events which consist of exactly one possible outcome.\n\nSubset of sample space\nCan be elementary or compound\nExample: rolling a 4\n\n\n\n\n\nTo formalize probability theory, we first need to define a few terms:\n\nAn experiment is any activity that produces or observes an outcome. Examples are flipping a coin, rolling a 6-sided die, or trying a new route to work to see if it’s faster than the old route.\nThe sample space is the set of possible outcomes for an experiment. We represent these by listing them within a set of squiggly brackets.\nAn event is a subset of the sample space. In principle it could be one or more of possible outcomes in the sample space, but here we will focus primarily on elementary events which consist of exactly one possible outcome.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Part 1: Introduction to Probability"
    ]
  },
  {
    "objectID": "Week3/01-intro.html#kolmogorovs-axioms",
    "href": "Week3/01-intro.html#kolmogorovs-axioms",
    "title": "Part 1: Introduction to Probability",
    "section": "",
    "text": "For events {E_1, E_2, ... , E_N} and random variable X:\n\n\n\n\nNon-negativity:\nP(X=E_i) \\ge 0\n\n\nNormalization:\n\\sum_{i=1}^N{P(X=E_i)} = 1\n\n\nBoundedness:\nP(X=E_i)\\le 1\n\n\n\n\nImplications:\n\nAll probabilities are between 0 and 1\nTotal probability must sum to 1\nIndividual probabilities ≤ 1\n\n\n\n\n\nThese are the features that a value has to have if it is going to be a probability, which were first defined by the Russian mathematician Andrei Kolmogorov.\nThe summation is interpreted as saying “Take all of the N elementary events, which we have labeled from 1 to N, and add up their probabilities. These must sum to one.”\nThe third point is implied by the previous points; since they must sum to one, and they can’t be negative, then any particular probability cannot exceed one.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Part 1: Introduction to Probability"
    ]
  },
  {
    "objectID": "Week3/03-empirical.html",
    "href": "Week3/03-empirical.html",
    "title": "Determining Probabilities",
    "section": "",
    "text": "Personal Belief\n\nSubjective assessment\nBased on knowledge/experience\nLimited scientific validity\nOften only available approach\n\n\n\nEmpirical Frequency\n\nBased on repeated experiments\nLaw of large numbers\nReal-world data collection\n\n\n\n\n\n\nClassical Probability\n\nBased on equally likely outcomes\nMathematical approach\nCommon in games of chance\nNo experiments needed\n\n\n\n\n\n\nNow that we know what a probability is, how do we actually figure out what the probability is for any particular event? There are three main approaches, each with their own strengths and limitations.\n\n\n\n\nExample Question:\nWhat was the probability that Bernie Sanders would have won the 2016 presidential election if he had been the democratic nominee?\nKey Points:\n\nCan’t run this experiment\nPeople can still estimate based on knowledge\nNot scientifically satisfying\nOften the only available approach\n\n\nOther Examples:\n\nWeather forecasts\nSports predictions\nEconomic forecasts\nPersonal decisions\n\n\n\n\nLet’s say that I asked you what the probability was that Bernie Sanders would have won the 2016 presidential election if he had been the democratic nominee instead of Hilary Clinton? We can’t actually do the experiment to find the outcome. However, most people with knowledge of American politics would be willing to at least offer a guess at the probability of this event. In many cases personal knowledge and/or opinion is the only guide we have determining the probability of an event, but this is not very scientifically satisfying.\n\n\n\n\nSan Francisco Rain Example:\n\nTotal days in 2017: 365\nRainy days: 73\nP(rain in SF) = 73/365 = 0.2\n\nKey Steps:\n\nDefine experiment clearly\nCount occurrences\nDivide by total trials\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnother way to determine the probability of an event is to do the experiment many times and count how often each event happens. From the relative frequency of the different outcomes, we can compute the probability of each outcome. For example, let’s say that we are interested in knowing the probability of rain in San Francisco. We first have to define the experiment — let’s say that we will look at the National Weather Service data for each day in 2017 and determine whether there was any rain at the downtown San Francisco weather station. According to these data, in 2017 there were 73 rainy days. To compute the probability of rain in San Francisco, we simply divide the number of rainy days by the number of days counted (365), giving P(rain in SF in 2017) = 0.2.\nThe graph shows how the empirical probability of rain converges to 0.2 as we accumulate more days of data throughout the year.\n\n\n\n\nCoin Flip Example:\n\nTrue probability of heads = 0.5\nSmall samples vary widely\nMore flips = better estimate\nConverges to true probability\n“Law of small numbers” fallacy\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe graph shows how early results from coin flips can be highly variable and unrepresentative of the true value. Even though we know a fair coin has a probability of 0.5 for heads, small samples can give very different results. This demonstrates how small samples can give misleading results.\nThis was referred to as the “law of small numbers” by psychologists Danny Kahneman and Amos Tversky, who showed that people (even trained researchers) often behave as if the law of large numbers applies even to small samples, leading them to draw strong conclusions from insufficient data.\n\n\n\n\n2017 Senate Race:\n\nRoy Moore vs Doug Jones\nEarly results volatile\nFinal outcome different\nSmall sample warning\n\n\n\n\n\n\n\n\n\n\n\n\n\nA real-world example of this was seen in the 2017 special election for the US Senate in Alabama. Early in the evening the vote counts were especially volatile, swinging from a large initial lead for Jones to a long period where Moore had the lead, until finally Jones took the lead to win the race.\nThis demonstrates how small samples can give misleading results. Unfortunately, many people forget this and overinterpret results from small samples.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Determining Probabilities"
    ]
  },
  {
    "objectID": "Week3/03-empirical.html#three-approaches",
    "href": "Week3/03-empirical.html#three-approaches",
    "title": "Determining Probabilities",
    "section": "",
    "text": "Personal Belief\n\nSubjective assessment\nBased on knowledge/experience\nLimited scientific validity\nOften only available approach\n\n\n\nEmpirical Frequency\n\nBased on repeated experiments\nLaw of large numbers\nReal-world data collection\n\n\n\n\n\n\nClassical Probability\n\nBased on equally likely outcomes\nMathematical approach\nCommon in games of chance\nNo experiments needed\n\n\n\n\n\n\nNow that we know what a probability is, how do we actually figure out what the probability is for any particular event? There are three main approaches, each with their own strengths and limitations.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Determining Probabilities"
    ]
  },
  {
    "objectID": "Week3/03-empirical.html#personal-belief",
    "href": "Week3/03-empirical.html#personal-belief",
    "title": "Determining Probabilities",
    "section": "",
    "text": "Example Question:\nWhat was the probability that Bernie Sanders would have won the 2016 presidential election if he had been the democratic nominee?\nKey Points:\n\nCan’t run this experiment\nPeople can still estimate based on knowledge\nNot scientifically satisfying\nOften the only available approach\n\n\nOther Examples:\n\nWeather forecasts\nSports predictions\nEconomic forecasts\nPersonal decisions\n\n\n\n\nLet’s say that I asked you what the probability was that Bernie Sanders would have won the 2016 presidential election if he had been the democratic nominee instead of Hilary Clinton? We can’t actually do the experiment to find the outcome. However, most people with knowledge of American politics would be willing to at least offer a guess at the probability of this event. In many cases personal knowledge and/or opinion is the only guide we have determining the probability of an event, but this is not very scientifically satisfying.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Determining Probabilities"
    ]
  },
  {
    "objectID": "Week3/03-empirical.html#empirical-frequency",
    "href": "Week3/03-empirical.html#empirical-frequency",
    "title": "Determining Probabilities",
    "section": "",
    "text": "San Francisco Rain Example:\n\nTotal days in 2017: 365\nRainy days: 73\nP(rain in SF) = 73/365 = 0.2\n\nKey Steps:\n\nDefine experiment clearly\nCount occurrences\nDivide by total trials\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnother way to determine the probability of an event is to do the experiment many times and count how often each event happens. From the relative frequency of the different outcomes, we can compute the probability of each outcome. For example, let’s say that we are interested in knowing the probability of rain in San Francisco. We first have to define the experiment — let’s say that we will look at the National Weather Service data for each day in 2017 and determine whether there was any rain at the downtown San Francisco weather station. According to these data, in 2017 there were 73 rainy days. To compute the probability of rain in San Francisco, we simply divide the number of rainy days by the number of days counted (365), giving P(rain in SF in 2017) = 0.2.\nThe graph shows how the empirical probability of rain converges to 0.2 as we accumulate more days of data throughout the year.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Determining Probabilities"
    ]
  },
  {
    "objectID": "Week3/03-empirical.html#law-of-large-numbers",
    "href": "Week3/03-empirical.html#law-of-large-numbers",
    "title": "Determining Probabilities",
    "section": "",
    "text": "Coin Flip Example:\n\nTrue probability of heads = 0.5\nSmall samples vary widely\nMore flips = better estimate\nConverges to true probability\n“Law of small numbers” fallacy\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe graph shows how early results from coin flips can be highly variable and unrepresentative of the true value. Even though we know a fair coin has a probability of 0.5 for heads, small samples can give very different results. This demonstrates how small samples can give misleading results.\nThis was referred to as the “law of small numbers” by psychologists Danny Kahneman and Amos Tversky, who showed that people (even trained researchers) often behave as if the law of large numbers applies even to small samples, leading them to draw strong conclusions from insufficient data.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Determining Probabilities"
    ]
  },
  {
    "objectID": "Week3/03-empirical.html#real-world-example-alabama-election",
    "href": "Week3/03-empirical.html#real-world-example-alabama-election",
    "title": "Determining Probabilities",
    "section": "",
    "text": "2017 Senate Race:\n\nRoy Moore vs Doug Jones\nEarly results volatile\nFinal outcome different\nSmall sample warning\n\n\n\n\n\n\n\n\n\n\n\n\n\nA real-world example of this was seen in the 2017 special election for the US Senate in Alabama. Early in the evening the vote counts were especially volatile, swinging from a large initial lead for Jones to a long period where Moore had the lead, until finally Jones took the lead to win the race.\nThis demonstrates how small samples can give misleading results. Unfortunately, many people forget this and overinterpret results from small samples.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Determining Probabilities"
    ]
  },
  {
    "objectID": "Week3/05-bayes.html",
    "href": "Week3/05-bayes.html",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "",
    "text": "When we know P(A|B) but want P(B|A):\nP(B|A) = \\frac{P(A|B)*P(B)}{P(A)}\nAlternative Form:\nP(B|A) = \\frac{P(A|B)*P(B)}{P(A|B)*P(B) + P(A|\\neg B)*P(\\neg B)}\n\n\nComponents:\n\nPrior: P(B)\n\nLikelihood: P(A|B)\n\nMarginal likelihood: P(A)\n\nPosterior: P(B|A)\n\n\n\n\n\n\nIn many cases, we know P(A|B) but we really want to know P(B|A). This commonly occurs in medical screening, where we know P(positive test result| disease) but what we want to know is P(disease|positive test result).\nIf we have only two outcomes, we can express Bayes’ rule in a somewhat clearer way, using the sum rule to redefine P(A).\n\n\n\nA major construction company conducts mandatory random drug and alcohol screening using rapid saliva tests. Consider the following scenario:\n\n\nIn the UK construction industry during 2023, the prevalence of substance use affecting workplace safety was estimated at 2.5% of the workforce\nThe rapid saliva test used has a sensitivity (true positive rate) of 85% when conducted according to protocol\nThe specificity (true negative rate) of these tests is 99.2%\n\n\n\n\nLet’s consider a specific example. Suppose that a worker is selected for a random drug screening. The test result is positive. What is the probability that this worker is actually positive for substances?\n\n\n\nLet’s consider a specific example. Suppose that a worker is selected for a random drug screening. The test result is positive. What is the probability that this worker is actually positive for substances?\n\nContext: The company’s current policy is immediate suspension without pay following a positive test result, pending a more accurate laboratory confirmation test that takes 48 hours.\n\nMandatory screening\nRapid saliva test\nSafety-critical roles\nImmediate consequences\n\n\n\n\nConstruction Site Testing:\n\nSensitivity: P(positive|substance) = 0.85\nSpecificity: P(negative|no substance) = 0.992\nBase rate: P(substance) = 0.025\n\n\nKey Values:\n\nP(S) = 0.025 (prevalence)\nP(P|S) = 0.85 (sensitivity)\nP(P|not S) = 0.008 (1 - specificity)\n\n\n\nA major construction company conducts mandatory random drug and alcohol screening using rapid saliva tests. In the UK construction industry during 2023, the prevalence of substance use affecting workplace safety was estimated at 2.5% of the workforce. The rapid saliva test used has a sensitivity of 85% when conducted according to protocol, and a specificity of 99.2%.\n\n\nUsing Bayes’ Theorem, calculate the probability that this worker is actually positive for substances given their positive test result.\n\nP(S) = 0.025 (prevalence)\nP(P|S) = 0.85 (sensitivity)\nP(P|not S) = 0.008 (1 - specificity)\n\n\nA construction worker is randomly selected for testing at the start of their shift. Their saliva test comes back positive. Using Bayes’ Theorem, calculate the probability that this worker is actually positive for substances given their positive test result.\n\n\n\n\nCalculate P(substance|positive):\n\n\\begin{align*}\nP(P) &= P(P|S) \\times P(S) + P(P|not S) \\times P(not S) \\\\\n&= (0.85 \\times 0.025) + (0.008 \\times 0.975) \\\\\n&= 0.02125 + 0.0078 \\\\\n&= 0.02905\n\\end{align*}\n\n\n\\begin{align*}\nP(S|P) &= \\frac{P(P|S) \\times P(S)}{P(P)} \\\\\n&= \\frac{0.85 \\times 0.025}{0.02905} \\\\\n&= 0.7314974 \\text{ or } 73.1\\%\n\\end{align*}\n\n\n\nInterpretation:\n\n~73.1% chance true positive\n~26.9% chance false positive\nMuch higher than 2.5% base rate\nStill significant uncertainty\n\n\n\n\n\nUsing Bayes’ Theorem, we find that given a positive test result, there is a 73.1% probability that the worker actually has substances present. This is much higher than the base rate of 2.5%, but still leaves significant uncertainty with a 26.9% false positive rate.\n\n\nThe company’s current policy is immediate suspension without pay following a positive test result.\nWhat do these results mean for this business policy? Is it fair to immediately suspend workers without pay for a positive test result?\n\nThe company’s current policy is immediate suspension without pay following a positive test result, pending a more accurate laboratory confirmation test that takes 48 hours.\nGiven that approximately 26.9% of positive test results may be false positives, an immediate suspension without pay could unfairly penalize innocent workers; however, the high stakes of construction safety and the 73.1% probability of a true positive suggest that temporary removal from safety-critical roles is prudent while awaiting confirmation.\n\n\n\n\nBayes’ Rule as Learning:\nP(B|A) = \\frac{P(A|B)}{P(A)}*P(B)\nComponents:\n\nPrior belief: P(B)\n\nEvidence strength: \\frac{P(A|B)}{P(A)}\n\nUpdated belief: P(B|A)\n\n\n\nKey Insights:\n\nUpdates prior knowledge\nEvidence can strengthen/weaken\nSystematic way to learn\nCombines knowledge & data\n\n\n\n\nAnother way to think of Bayes’ rule is as a way to update our beliefs on the basis of data. The different parts of Bayes’ rule have specific names, that relate to their role in using Bayes’ rule to update our beliefs.\nThe part on the left tells us how much more or less likely the data A are given B, relative to the overall likelihood of the data, while the part on the right side tells us how likely we thought B was before we knew anything about the data.\n\n\n\n\nConverting to Odds:\n\\text{odds of A} = \\frac{P(A)}{P(\\neg A)}\nExample:\nDrug test odds:\n\nPrior: \\frac{0.025}{0.975} = 0.026\n\nPosterior: \\frac{0.7314974}{0.2685026} = 2.724\n\n\n\nOdds Ratio:\n\\frac{\\text{posterior odds}}{\\text{prior odds}} = \\frac{2.724}{0.026} = 106.25\nInterpretation:\n\nOdds increased 105×\nMuch stronger evidence\nShows test’s power\nDespite false positives\n\n\n\n\nWe can convert probabilities into odds which express the relative likelihood of something happening or not. An odds ratio is an example of what we will later call an effect size, which is a way of quantifying how relatively large any particular statistical effect is.\nFirst, remember the rule for computing a conditional probability. We can rearrange this to get the formula to compute the joint probability using the conditional. Using this we can compute the inverse probability.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Bayes' Rule and Learning from Data"
    ]
  },
  {
    "objectID": "Week3/05-bayes.html#the-basic-formula",
    "href": "Week3/05-bayes.html#the-basic-formula",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "",
    "text": "When we know P(A|B) but want P(B|A):\nP(B|A) = \\frac{P(A|B)*P(B)}{P(A)}\nAlternative Form:\nP(B|A) = \\frac{P(A|B)*P(B)}{P(A|B)*P(B) + P(A|\\neg B)*P(\\neg B)}\n\n\nComponents:\n\nPrior: P(B)\n\nLikelihood: P(A|B)\n\nMarginal likelihood: P(A)\n\nPosterior: P(B|A)\n\n\n\n\n\n\nIn many cases, we know P(A|B) but we really want to know P(B|A). This commonly occurs in medical screening, where we know P(positive test result| disease) but what we want to know is P(disease|positive test result).\nIf we have only two outcomes, we can express Bayes’ rule in a somewhat clearer way, using the sum rule to redefine P(A).",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Bayes' Rule and Learning from Data"
    ]
  },
  {
    "objectID": "Week3/05-bayes.html#putting-bayes-into-practice",
    "href": "Week3/05-bayes.html#putting-bayes-into-practice",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "",
    "text": "A major construction company conducts mandatory random drug and alcohol screening using rapid saliva tests. Consider the following scenario:\n\n\nIn the UK construction industry during 2023, the prevalence of substance use affecting workplace safety was estimated at 2.5% of the workforce\nThe rapid saliva test used has a sensitivity (true positive rate) of 85% when conducted according to protocol\nThe specificity (true negative rate) of these tests is 99.2%\n\n\n\n\nLet’s consider a specific example. Suppose that a worker is selected for a random drug screening. The test result is positive. What is the probability that this worker is actually positive for substances?",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Bayes' Rule and Learning from Data"
    ]
  },
  {
    "objectID": "Week3/05-bayes.html#putting-bayes-into-practice-1",
    "href": "Week3/05-bayes.html#putting-bayes-into-practice-1",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "",
    "text": "Let’s consider a specific example. Suppose that a worker is selected for a random drug screening. The test result is positive. What is the probability that this worker is actually positive for substances?\n\nContext: The company’s current policy is immediate suspension without pay following a positive test result, pending a more accurate laboratory confirmation test that takes 48 hours.\n\nMandatory screening\nRapid saliva test\nSafety-critical roles\nImmediate consequences",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Bayes' Rule and Learning from Data"
    ]
  },
  {
    "objectID": "Week3/05-bayes.html#putting-bayes-into-practice-2",
    "href": "Week3/05-bayes.html#putting-bayes-into-practice-2",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "",
    "text": "Construction Site Testing:\n\nSensitivity: P(positive|substance) = 0.85\nSpecificity: P(negative|no substance) = 0.992\nBase rate: P(substance) = 0.025\n\n\nKey Values:\n\nP(S) = 0.025 (prevalence)\nP(P|S) = 0.85 (sensitivity)\nP(P|not S) = 0.008 (1 - specificity)\n\n\n\nA major construction company conducts mandatory random drug and alcohol screening using rapid saliva tests. In the UK construction industry during 2023, the prevalence of substance use affecting workplace safety was estimated at 2.5% of the workforce. The rapid saliva test used has a sensitivity of 85% when conducted according to protocol, and a specificity of 99.2%.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Bayes' Rule and Learning from Data"
    ]
  },
  {
    "objectID": "Week3/05-bayes.html#lets-work-through-it",
    "href": "Week3/05-bayes.html#lets-work-through-it",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "",
    "text": "Using Bayes’ Theorem, calculate the probability that this worker is actually positive for substances given their positive test result.\n\nP(S) = 0.025 (prevalence)\nP(P|S) = 0.85 (sensitivity)\nP(P|not S) = 0.008 (1 - specificity)\n\n\nA construction worker is randomly selected for testing at the start of their shift. Their saliva test comes back positive. Using Bayes’ Theorem, calculate the probability that this worker is actually positive for substances given their positive test result.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Bayes' Rule and Learning from Data"
    ]
  },
  {
    "objectID": "Week3/05-bayes.html#solution",
    "href": "Week3/05-bayes.html#solution",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "",
    "text": "Calculate P(substance|positive):\n\n\\begin{align*}\nP(P) &= P(P|S) \\times P(S) + P(P|not S) \\times P(not S) \\\\\n&= (0.85 \\times 0.025) + (0.008 \\times 0.975) \\\\\n&= 0.02125 + 0.0078 \\\\\n&= 0.02905\n\\end{align*}\n\n\n\\begin{align*}\nP(S|P) &= \\frac{P(P|S) \\times P(S)}{P(P)} \\\\\n&= \\frac{0.85 \\times 0.025}{0.02905} \\\\\n&= 0.7314974 \\text{ or } 73.1\\%\n\\end{align*}\n\n\n\nInterpretation:\n\n~73.1% chance true positive\n~26.9% chance false positive\nMuch higher than 2.5% base rate\nStill significant uncertainty\n\n\n\n\n\nUsing Bayes’ Theorem, we find that given a positive test result, there is a 73.1% probability that the worker actually has substances present. This is much higher than the base rate of 2.5%, but still leaves significant uncertainty with a 26.9% false positive rate.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Bayes' Rule and Learning from Data"
    ]
  },
  {
    "objectID": "Week3/05-bayes.html#discussion-the-real-world-implications",
    "href": "Week3/05-bayes.html#discussion-the-real-world-implications",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "",
    "text": "The company’s current policy is immediate suspension without pay following a positive test result.\nWhat do these results mean for this business policy? Is it fair to immediately suspend workers without pay for a positive test result?\n\nThe company’s current policy is immediate suspension without pay following a positive test result, pending a more accurate laboratory confirmation test that takes 48 hours.\nGiven that approximately 26.9% of positive test results may be false positives, an immediate suspension without pay could unfairly penalize innocent workers; however, the high stakes of construction safety and the 73.1% probability of a true positive suggest that temporary removal from safety-critical roles is prudent while awaiting confirmation.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Bayes' Rule and Learning from Data"
    ]
  },
  {
    "objectID": "Week3/05-bayes.html#learning-from-data",
    "href": "Week3/05-bayes.html#learning-from-data",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "",
    "text": "Bayes’ Rule as Learning:\nP(B|A) = \\frac{P(A|B)}{P(A)}*P(B)\nComponents:\n\nPrior belief: P(B)\n\nEvidence strength: \\frac{P(A|B)}{P(A)}\n\nUpdated belief: P(B|A)\n\n\n\nKey Insights:\n\nUpdates prior knowledge\nEvidence can strengthen/weaken\nSystematic way to learn\nCombines knowledge & data\n\n\n\n\nAnother way to think of Bayes’ rule is as a way to update our beliefs on the basis of data. The different parts of Bayes’ rule have specific names, that relate to their role in using Bayes’ rule to update our beliefs.\nThe part on the left tells us how much more or less likely the data A are given B, relative to the overall likelihood of the data, while the part on the right side tells us how likely we thought B was before we knew anything about the data.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Bayes' Rule and Learning from Data"
    ]
  },
  {
    "objectID": "Week3/05-bayes.html#odds-and-odds-ratios",
    "href": "Week3/05-bayes.html#odds-and-odds-ratios",
    "title": "Bayes’ Rule and Learning from Data",
    "section": "",
    "text": "Converting to Odds:\n\\text{odds of A} = \\frac{P(A)}{P(\\neg A)}\nExample:\nDrug test odds:\n\nPrior: \\frac{0.025}{0.975} = 0.026\n\nPosterior: \\frac{0.7314974}{0.2685026} = 2.724\n\n\n\nOdds Ratio:\n\\frac{\\text{posterior odds}}{\\text{prior odds}} = \\frac{2.724}{0.026} = 106.25\nInterpretation:\n\nOdds increased 105×\nMuch stronger evidence\nShows test’s power\nDespite false positives\n\n\n\n\nWe can convert probabilities into odds which express the relative likelihood of something happening or not. An odds ratio is an example of what we will later call an effect size, which is a way of quantifying how relatively large any particular statistical effect is.\nFirst, remember the rule for computing a conditional probability. We can rearrange this to get the formula to compute the joint probability using the conditional. Using this we can compute the inverse probability.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Bayes' Rule and Learning from Data"
    ]
  },
  {
    "objectID": "Week3/07-sampling.html",
    "href": "Week3/07-sampling.html",
    "title": "Part 2: Statistical Sampling",
    "section": "",
    "text": "Making inferences about populations from samples\n\n\n\nThe Power of Sampling:\nNate Silver’s 2012 Election Prediction:\n\nCorrectly predicted all 50 states\nUsed only 21,000 people\nTo predict 125 million votes\nCombined data from 21 polls\n\n\nKey Insights:\n\nSmall samples can be powerful\nProper methodology is crucial\nCombining data improves accuracy\nStatistical rigor matters\n\n\n\n\nOne of the foundational ideas in statistics is that we can make inferences about an entire population based on a relatively small sample of individuals from that population.\nAnyone living in the United States will be familiar with the concept of sampling from the political polls that have become a central part of our electoral process. In some cases, these polls can be incredibly accurate at predicting the outcomes of elections. The best known example comes from the 2008 and 2012 US Presidential elections, when the pollster Nate Silver correctly predicted electoral outcomes for 49/50 states in 2008 and for all 50 states in 2012.\nSilver did this by combining data from 21 different polls, which vary in the degree to which they tend to lean towards either the Republican or Democratic side. Each of these polls included data from about 1000 likely voters – meaning that Silver was able to almost perfectly predict the pattern of votes of more than 125 million voters using data from only about 21,000 people, along with other knowledge.\n\n\n\n\n\n\n\nPopulation vs Sample:\n\nPopulation: Entire group of interest\nSample: Subset used for measurement\nGoal: Infer population parameters from sample statistics\n\n\n\nRepresentative Sampling:\n\nEqual chance of selection\nAvoid systematic bias\nRandom selection crucial\n\n\n\n\n\n\n\n\nTypes of Sampling:\n\nWith replacement: Items can be selected multiple times\nWithout replacement: Items selected only once\nChoice affects probability calculations\n\n\n\nKey Terms:\n\nParameter: Population value (usually unknown)\nStatistic: Sample value (our estimate)\nSampling Error: Difference between statistic and parameter\n\n\n\n\n\n\n\nOur goal in sampling is to determine the value of a statistic for an entire population of interest, using just a small subset of the population. We do this primarily to save time and effort – why go to the trouble of measuring every individual in the population when just a small sample is sufficient to accurately estimate the statistic of interest?\nIn the election example, the population is all registered voters in the region being polled, and the sample is the set of 1000 individuals selected by the polling organization. The way in which we select the sample is critical to ensuring that the sample is representative of the entire population, which is a main goal of statistical sampling.\nIt’s important to also distinguish between two different ways of sampling: with replacement versus without replacement. In sampling with replacement, after a member of the population has been sampled, they are put back into the pool so that they can potentially be sampled again. In sampling without replacement, once a member has been sampled they are not eligible to be sampled again.\n\n\n\nWhat is Sampling Error?\n\nDifference between sample and population\nVaries across samples\nAffects measurement quality\nCan be quantified\n\n\n\n\n\n# Take 5 samples of 50 adults each\nset.seed(123)\nsamples &lt;- map_df(\n  1:5,\n  ~{\n    NHANES_adult |&gt;\n      sample_n(50) |&gt;\n      summarise(\n        mean_height = mean(Height),\n        sd_height = sd(Height),\n      )\n  }\n)\nsamples\n\n# A tibble: 5 × 2\n  mean_height sd_height\n        &lt;dbl&gt;     &lt;dbl&gt;\n1        169.     11.6 \n2        167.      9.13\n3        169.     11.2 \n4        166.      9.62\n5        169.     11.0 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegardless of how representative our sample is, it’s likely that the statistic that we compute from the sample is going to differ at least slightly from the population parameter. We refer to this as sampling error. If we take multiple samples, the value of our statistical estimate will also vary from sample to sample; we refer to this distribution of our statistic across samples as the sampling distribution.\nSampling error is directly related to the quality of our measurement of the population. Clearly we want the estimates obtained from our sample to be as close as possible to the true value of the population parameter. However, even if our statistic is unbiased (that is, we expect it to have the same value as the population parameter), the value for any particular estimate will differ from the population value, and those differences will be greater when the sampling error is greater.\nThe visualization shows how sample means distribute around the true population mean (red line) when we take many samples.\n\n\n\n\nDefinition:\nSEM = \\frac{\\hat{\\sigma}}{\\sqrt{n}}\nWhere:\n\n\n\\hat{\\sigma} is estimated standard deviation\n\nn is sample size\n\nKey Properties:\n\nMeasures sampling distribution variability\nDecreases with larger samples\nIncreases with population variability\n\n\nExample with NHANES:\n\nCode# Population SEM\npop_sd &lt;- sd(NHANES_adult$Height)\nn &lt;- 50\nsem_theoretical &lt;- pop_sd / sqrt(n)\n\n# Observed SEM from samples\nsem_observed &lt;- sd(samples_large$mean_height)\n\ncat(\"Theoretical SEM:\", round(sem_theoretical, 2), \"\\n\")\n\nTheoretical SEM: 1.44 \n\nCodecat(\"Observed SEM:\", round(sem_observed, 2))\n\nObserved SEM: 1.42\n\n\n\n\n\nLater in the course it will become essential to be able to characterize how variable our samples are, in order to make inferences about the sample statistics. For the mean, we do this using a quantity called the standard error of the mean (SEM), which one can think of as the standard deviation of the sampling distribution of the mean.\nThe formula for the standard error of the mean implies that the quality of our measurement involves two quantities: the population variability, and the size of our sample. Because the sample size is the denominator in the formula for SEM, a larger sample size will yield a smaller SEM when holding the population variability constant.\nWe have no control over the population variability, but we do have control over the sample size. Thus, if we wish to improve our sample statistics (by reducing their sampling variability) then we should use larger samples. However, the formula also tells us something very fundamental about statistical sampling – namely, that the utility of larger samples diminishes with the square root of the sample size.\n\n\n\n\nTheory\nVisualization\nCode\n\n\n\nImpact of Sample Size:\n\nLarger n → Smaller SEM\nRelationship is not linear\nDiminishing returns\nSquare root relationship\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode# Compare SEM for different sample sizes\nn1 &lt;- 50\nn2 &lt;- 200 # 4 times larger\n\nsem1 &lt;- pop_sd / sqrt(n1)\nsem2 &lt;- pop_sd / sqrt(n2)\n\n# Improvement factor\nimprovement &lt;- sem1 / sem2\ncat(\"Improvement factor:\", round(improvement, 2))\n\n\n\n\n\n\nThe relationship between sample size and standard error is not linear. Doubling the sample size will not double the quality of the statistics; rather, it will improve it by a factor of √2. This has important implications for study design and resource allocation.\nThe visualization shows how the standard error decreases as sample size increases, but with diminishing returns. This means that after a certain point, increasing sample size may not be worth the additional cost and effort.\nThis relationship is fundamental to statistical power, which we will discuss in later sections. Understanding this relationship helps researchers make informed decisions about sample size requirements for their studies.\n\n\nKey Points:\n\nAs sample size increases:\n\nSampling distribution becomes normal\nRegardless of population distribution\nMean approaches population mean\nVariance decreases\n\n\nImplications:\n\nEnables statistical inference\nJustifies normal approximation\nExplains real-world patterns\n\n\n\nThe Central Limit Theorem tells us that as sample sizes get larger, the sampling distribution of the mean will become normally distributed, even if the data within each sample are not normally distributed. This is a powerful result that allows us to make inferences about population parameters based on sample statistics.\n\n\n\n\n\nBell-shaped curve\nDefined by mean (\\mu) and SD (\\sigma)\nSymmetric around mean\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Central Limit Theorem tells us that as sample sizes get larger, the sampling distribution of the mean will become normally distributed, even if the data within each sample are not normally distributed.\nThe normal distribution is described in terms of two parameters: the mean (which you can think of as the location of the peak), and the standard deviation (which specifies the width of the distribution). The bell-like shape of the distribution never changes, only its location and width.\nThe normal distribution is commonly observed in data collected in the real world – and the central limit theorem gives us some insight into why that occurs. For example, the height of any adult depends on a complex mixture of their genetics and experience; even if those individual contributions may not be normally distributed, when we combine them the result is a normal distribution.\n\n\n\n\nOriginal Distribution\nCode Example\nKey Insights\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode# Compare skewness\nlibrary(moments)\noriginal_skew &lt;- skewness(NHANES_clean$AlcoholYear)\nsampling_skew &lt;- skewness(samples_alc$mean_alcohol)\n\ncat(\"Original Distribution Skewness:\", round(original_skew, 2), \"\\n\")\ncat(\"Sampling Distribution Skewness:\", round(sampling_skew, 2))\n\n\n\n\n\nOriginal data is highly skewed\nSampling distribution is nearly normal\nCLT works even with:\n\nNon-normal data\nSkewed distributions\nDiscrete values\n\n\nSample size of 50 is sufficient\n\n\n\n\n\nLet’s work with the variable AlcoholYear from the NHANES dataset, which is highly skewed. This distribution is, for lack of a better word, funky – and definitely not normally distributed.\nNow let’s look at the sampling distribution of the mean for this variable. Despite the clear non-normality of the original data, the sampling distribution is remarkably close to the normal.\nThe Central Limit Theorem is important for statistics because it allows us to safely assume that the sampling distribution of the mean will be normal in most cases. This means that we can take advantage of statistical techniques that assume a normal distribution.\n\n\n\n\n\n\n\nSampling Fundamentals:\n\nPopulation vs Sample\nRepresentative sampling\nWith/without replacement\nSampling error\n\n\n\nStandard Error:\n\nMeasures sampling variability\nDecreases with √n\nGuides sample size decisions\nQuantifies precision\n\n\n\n\n\n\n\n\nCentral Limit Theorem:\n\nSampling distribution normality\nIndependent of original distribution\nEnables statistical inference\nFoundation for hypothesis testing\n\n\n\nApplications:\n\nPolitical polling\nClinical trials\nQuality control\nResearch design\n\n\n\n\n\n\n\nIn this lecture, we covered: - The fundamentals of statistical sampling and why it works - How to characterize sampling error and the sampling distribution - The standard error of the mean and its relationship with sample size - The Central Limit Theorem and its importance in statistical inference - Real-world applications and examples using the NHANES dataset",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Part 2: Statistical Sampling"
    ]
  },
  {
    "objectID": "Week3/07-sampling.html#why-study-sampling",
    "href": "Week3/07-sampling.html#why-study-sampling",
    "title": "Part 2: Statistical Sampling",
    "section": "",
    "text": "The Power of Sampling:\nNate Silver’s 2012 Election Prediction:\n\nCorrectly predicted all 50 states\nUsed only 21,000 people\nTo predict 125 million votes\nCombined data from 21 polls\n\n\nKey Insights:\n\nSmall samples can be powerful\nProper methodology is crucial\nCombining data improves accuracy\nStatistical rigor matters\n\n\n\n\nOne of the foundational ideas in statistics is that we can make inferences about an entire population based on a relatively small sample of individuals from that population.\nAnyone living in the United States will be familiar with the concept of sampling from the political polls that have become a central part of our electoral process. In some cases, these polls can be incredibly accurate at predicting the outcomes of elections. The best known example comes from the 2008 and 2012 US Presidential elections, when the pollster Nate Silver correctly predicted electoral outcomes for 49/50 states in 2008 and for all 50 states in 2012.\nSilver did this by combining data from 21 different polls, which vary in the degree to which they tend to lean towards either the Republican or Democratic side. Each of these polls included data from about 1000 likely voters – meaning that Silver was able to almost perfectly predict the pattern of votes of more than 125 million voters using data from only about 21,000 people, along with other knowledge.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Part 2: Statistical Sampling"
    ]
  },
  {
    "objectID": "Week3/07-sampling.html#sampling-fundamentals",
    "href": "Week3/07-sampling.html#sampling-fundamentals",
    "title": "Part 2: Statistical Sampling",
    "section": "",
    "text": "Population vs Sample:\n\nPopulation: Entire group of interest\nSample: Subset used for measurement\nGoal: Infer population parameters from sample statistics\n\n\n\nRepresentative Sampling:\n\nEqual chance of selection\nAvoid systematic bias\nRandom selection crucial\n\n\n\n\n\n\n\n\nTypes of Sampling:\n\nWith replacement: Items can be selected multiple times\nWithout replacement: Items selected only once\nChoice affects probability calculations\n\n\n\nKey Terms:\n\nParameter: Population value (usually unknown)\nStatistic: Sample value (our estimate)\nSampling Error: Difference between statistic and parameter\n\n\n\n\n\n\n\nOur goal in sampling is to determine the value of a statistic for an entire population of interest, using just a small subset of the population. We do this primarily to save time and effort – why go to the trouble of measuring every individual in the population when just a small sample is sufficient to accurately estimate the statistic of interest?\nIn the election example, the population is all registered voters in the region being polled, and the sample is the set of 1000 individuals selected by the polling organization. The way in which we select the sample is critical to ensuring that the sample is representative of the entire population, which is a main goal of statistical sampling.\nIt’s important to also distinguish between two different ways of sampling: with replacement versus without replacement. In sampling with replacement, after a member of the population has been sampled, they are put back into the pool so that they can potentially be sampled again. In sampling without replacement, once a member has been sampled they are not eligible to be sampled again.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Part 2: Statistical Sampling"
    ]
  },
  {
    "objectID": "Week3/07-sampling.html#sampling-error-distribution",
    "href": "Week3/07-sampling.html#sampling-error-distribution",
    "title": "Part 2: Statistical Sampling",
    "section": "",
    "text": "What is Sampling Error?\n\nDifference between sample and population\nVaries across samples\nAffects measurement quality\nCan be quantified",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Part 2: Statistical Sampling"
    ]
  },
  {
    "objectID": "Week3/07-sampling.html#sampling-error-distribution-1",
    "href": "Week3/07-sampling.html#sampling-error-distribution-1",
    "title": "Part 2: Statistical Sampling",
    "section": "",
    "text": "# Take 5 samples of 50 adults each\nset.seed(123)\nsamples &lt;- map_df(\n  1:5,\n  ~{\n    NHANES_adult |&gt;\n      sample_n(50) |&gt;\n      summarise(\n        mean_height = mean(Height),\n        sd_height = sd(Height),\n      )\n  }\n)\nsamples\n\n# A tibble: 5 × 2\n  mean_height sd_height\n        &lt;dbl&gt;     &lt;dbl&gt;\n1        169.     11.6 \n2        167.      9.13\n3        169.     11.2 \n4        166.      9.62\n5        169.     11.0 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegardless of how representative our sample is, it’s likely that the statistic that we compute from the sample is going to differ at least slightly from the population parameter. We refer to this as sampling error. If we take multiple samples, the value of our statistical estimate will also vary from sample to sample; we refer to this distribution of our statistic across samples as the sampling distribution.\nSampling error is directly related to the quality of our measurement of the population. Clearly we want the estimates obtained from our sample to be as close as possible to the true value of the population parameter. However, even if our statistic is unbiased (that is, we expect it to have the same value as the population parameter), the value for any particular estimate will differ from the population value, and those differences will be greater when the sampling error is greater.\nThe visualization shows how sample means distribute around the true population mean (red line) when we take many samples.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Part 2: Statistical Sampling"
    ]
  },
  {
    "objectID": "Week3/07-sampling.html#standard-error-of-the-mean",
    "href": "Week3/07-sampling.html#standard-error-of-the-mean",
    "title": "Part 2: Statistical Sampling",
    "section": "",
    "text": "Definition:\nSEM = \\frac{\\hat{\\sigma}}{\\sqrt{n}}\nWhere:\n\n\n\\hat{\\sigma} is estimated standard deviation\n\nn is sample size\n\nKey Properties:\n\nMeasures sampling distribution variability\nDecreases with larger samples\nIncreases with population variability\n\n\nExample with NHANES:\n\nCode# Population SEM\npop_sd &lt;- sd(NHANES_adult$Height)\nn &lt;- 50\nsem_theoretical &lt;- pop_sd / sqrt(n)\n\n# Observed SEM from samples\nsem_observed &lt;- sd(samples_large$mean_height)\n\ncat(\"Theoretical SEM:\", round(sem_theoretical, 2), \"\\n\")\n\nTheoretical SEM: 1.44 \n\nCodecat(\"Observed SEM:\", round(sem_observed, 2))\n\nObserved SEM: 1.42\n\n\n\n\n\nLater in the course it will become essential to be able to characterize how variable our samples are, in order to make inferences about the sample statistics. For the mean, we do this using a quantity called the standard error of the mean (SEM), which one can think of as the standard deviation of the sampling distribution of the mean.\nThe formula for the standard error of the mean implies that the quality of our measurement involves two quantities: the population variability, and the size of our sample. Because the sample size is the denominator in the formula for SEM, a larger sample size will yield a smaller SEM when holding the population variability constant.\nWe have no control over the population variability, but we do have control over the sample size. Thus, if we wish to improve our sample statistics (by reducing their sampling variability) then we should use larger samples. However, the formula also tells us something very fundamental about statistical sampling – namely, that the utility of larger samples diminishes with the square root of the sample size.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Part 2: Statistical Sampling"
    ]
  },
  {
    "objectID": "Week3/07-sampling.html#sample-size-effects",
    "href": "Week3/07-sampling.html#sample-size-effects",
    "title": "Part 2: Statistical Sampling",
    "section": "",
    "text": "Theory\nVisualization\nCode\n\n\n\nImpact of Sample Size:\n\nLarger n → Smaller SEM\nRelationship is not linear\nDiminishing returns\nSquare root relationship\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode# Compare SEM for different sample sizes\nn1 &lt;- 50\nn2 &lt;- 200 # 4 times larger\n\nsem1 &lt;- pop_sd / sqrt(n1)\nsem2 &lt;- pop_sd / sqrt(n2)\n\n# Improvement factor\nimprovement &lt;- sem1 / sem2\ncat(\"Improvement factor:\", round(improvement, 2))\n\n\n\n\n\n\nThe relationship between sample size and standard error is not linear. Doubling the sample size will not double the quality of the statistics; rather, it will improve it by a factor of √2. This has important implications for study design and resource allocation.\nThe visualization shows how the standard error decreases as sample size increases, but with diminishing returns. This means that after a certain point, increasing sample size may not be worth the additional cost and effort.\nThis relationship is fundamental to statistical power, which we will discuss in later sections. Understanding this relationship helps researchers make informed decisions about sample size requirements for their studies.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Part 2: Statistical Sampling"
    ]
  },
  {
    "objectID": "Week3/07-sampling.html#the-central-limit-theorem",
    "href": "Week3/07-sampling.html#the-central-limit-theorem",
    "title": "Part 2: Statistical Sampling",
    "section": "",
    "text": "Key Points:\n\nAs sample size increases:\n\nSampling distribution becomes normal\nRegardless of population distribution\nMean approaches population mean\nVariance decreases\n\n\nImplications:\n\nEnables statistical inference\nJustifies normal approximation\nExplains real-world patterns\n\n\n\nThe Central Limit Theorem tells us that as sample sizes get larger, the sampling distribution of the mean will become normally distributed, even if the data within each sample are not normally distributed. This is a powerful result that allows us to make inferences about population parameters based on sample statistics.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Part 2: Statistical Sampling"
    ]
  },
  {
    "objectID": "Week3/07-sampling.html#the-central-limit-theorem-1",
    "href": "Week3/07-sampling.html#the-central-limit-theorem-1",
    "title": "Part 2: Statistical Sampling",
    "section": "",
    "text": "Bell-shaped curve\nDefined by mean (\\mu) and SD (\\sigma)\nSymmetric around mean\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Central Limit Theorem tells us that as sample sizes get larger, the sampling distribution of the mean will become normally distributed, even if the data within each sample are not normally distributed.\nThe normal distribution is described in terms of two parameters: the mean (which you can think of as the location of the peak), and the standard deviation (which specifies the width of the distribution). The bell-like shape of the distribution never changes, only its location and width.\nThe normal distribution is commonly observed in data collected in the real world – and the central limit theorem gives us some insight into why that occurs. For example, the height of any adult depends on a complex mixture of their genetics and experience; even if those individual contributions may not be normally distributed, when we combine them the result is a normal distribution.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Part 2: Statistical Sampling"
    ]
  },
  {
    "objectID": "Week3/07-sampling.html#clt-in-action-nhanes-example",
    "href": "Week3/07-sampling.html#clt-in-action-nhanes-example",
    "title": "Part 2: Statistical Sampling",
    "section": "",
    "text": "Original Distribution\nCode Example\nKey Insights\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode# Compare skewness\nlibrary(moments)\noriginal_skew &lt;- skewness(NHANES_clean$AlcoholYear)\nsampling_skew &lt;- skewness(samples_alc$mean_alcohol)\n\ncat(\"Original Distribution Skewness:\", round(original_skew, 2), \"\\n\")\ncat(\"Sampling Distribution Skewness:\", round(sampling_skew, 2))\n\n\n\n\n\nOriginal data is highly skewed\nSampling distribution is nearly normal\nCLT works even with:\n\nNon-normal data\nSkewed distributions\nDiscrete values\n\n\nSample size of 50 is sufficient\n\n\n\n\n\nLet’s work with the variable AlcoholYear from the NHANES dataset, which is highly skewed. This distribution is, for lack of a better word, funky – and definitely not normally distributed.\nNow let’s look at the sampling distribution of the mean for this variable. Despite the clear non-normality of the original data, the sampling distribution is remarkably close to the normal.\nThe Central Limit Theorem is important for statistics because it allows us to safely assume that the sampling distribution of the mean will be normal in most cases. This means that we can take advantage of statistical techniques that assume a normal distribution.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Part 2: Statistical Sampling"
    ]
  },
  {
    "objectID": "Week3/07-sampling.html#summary",
    "href": "Week3/07-sampling.html#summary",
    "title": "Part 2: Statistical Sampling",
    "section": "",
    "text": "Sampling Fundamentals:\n\nPopulation vs Sample\nRepresentative sampling\nWith/without replacement\nSampling error\n\n\n\nStandard Error:\n\nMeasures sampling variability\nDecreases with √n\nGuides sample size decisions\nQuantifies precision\n\n\n\n\n\n\n\n\nCentral Limit Theorem:\n\nSampling distribution normality\nIndependent of original distribution\nEnables statistical inference\nFoundation for hypothesis testing\n\n\n\nApplications:\n\nPolitical polling\nClinical trials\nQuality control\nResearch design\n\n\n\n\n\n\n\nIn this lecture, we covered: - The fundamentals of statistical sampling and why it works - How to characterize sampling error and the sampling distribution - The standard error of the mean and its relationship with sample size - The Central Limit Theorem and its importance in statistical inference - Real-world applications and examples using the NHANES dataset",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Part 2: Statistical Sampling"
    ]
  },
  {
    "objectID": "Week3/notes.html",
    "href": "Week3/notes.html",
    "title": "Week 3",
    "section": "",
    "text": "Slides\n\n Download PDF Slides \n\n\nor continue on to the next pages for lecture notes.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Week 3"
    ]
  },
  {
    "objectID": "Week3/notes.html#this-weeks-lecture",
    "href": "Week3/notes.html#this-weeks-lecture",
    "title": "Week 3",
    "section": "",
    "text": "Slides\n\n Download PDF Slides \n\n\nor continue on to the next pages for lecture notes.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Week 3"
    ]
  },
  {
    "objectID": "Week3/notes.html#suggested-readings",
    "href": "Week3/notes.html#suggested-readings",
    "title": "Week 3",
    "section": "Suggested Readings",
    "text": "Suggested Readings\nThis week’s lectures are adapted from Chapter 6 and Chapter 7 of Statistical Thinking by Russell Poldrack.\nAdditional resources for further exploration:\n\nProbability Lecture Notes from the Stat20 course at Berkeley.\nA Student’s Guide to Bayesian Statistics by Ben Lambert\nThe Drunkard’s Walk: How Randomness Rules Our Lives by Leonard Mlodinow\nTen Great Ideas about Chance by Persi Diaconis and Brian Skyrms\n\n\nThese books provide excellent additional resources for understanding probability theory and its applications.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 3",
      "Week 3"
    ]
  },
  {
    "objectID": "Week4/02-samp-dist.html",
    "href": "Week4/02-samp-dist.html",
    "title": "Sampling Distribution of the Mean",
    "section": "",
    "text": "Connecting sampling distributions with Standard Error, Confidence Intervals, and Hypothesis Testing\n\n\nThe central limit theorem (CLT) is one of the most powerful and useful ideas in all of statistics. There are two alternative forms of the theorem, and both alternatives are concerned with drawing finite samples size n from a population with a known mean, \\mu, and a known standard deviation, \\sigma. The first alternative says that if we collect samples of size n with a “large enough n,” then the resulting distribution can be approximated by the normal distribution.\nApplying the law of large numbers here, we could say that if you take larger and larger samples from a population, then the mean {x} of the sample tends to get closer and closer to \\mu. From the central limit theorem, we know that as n gets larger and larger, the sample means follow a normal distribution. The larger n gets, the smaller the standard deviation gets. (Remember that the standard deviation for {x} is .) This means that the sample mean {x} must be close to the population mean \\mu. We can say that \\mu is the value that the sample means approach as n gets larger. The central limit theorem illustrates the law of large numbers.\nThe size of the sample, n, that is required in order to be “large enough” depends on the original population from which the samples are drawn (the sample size should be at least 30 or the data should come from a normal distribution). If the original population is far from normal, then more observations are needed for the sample means or sums to be normal. Sampling is done with replacement.\n\nThe CLT means says that if you keep drawing larger and larger samples and calculating their means, the sample means form their own normal distribution (the sampling distribution).\nThe sampling distribution of the mean is generated by repeated sampling from the same population and recording the sample mean per sample. This forms a distribution of different means, and this distribution has its own mean and variance.\nThe normal distribution has the same mean as the original distribution and a variance that equals the original variance divided by the sample size.\n\n\nWe have established that different samples yield different statistics due to sampling variability . These statistics have their own distributions, called sampling distributions, that reflect this as a random variable. The sampling distribution of a sample statistic is the distribution of the point estimates based on samples of a fixed size, n, from a certain population. It is useful to think of a particular point estimate as being drawn from a sampling distribution.\nRecall the sample mean weight calculated from a previous sample of 173.3 lbs. Suppose another random sample of 60 participants might produce a different value of x, such as 169.5 lbs. Repeated random sampling could result in additional different values, perhaps 172.1 lbs, 168.5 lbs, and so on. Each sample mean can be thought of as a single observation from a random variable X. The distribution of X is called the sampling distribution of the sample mean, and has its own mean and standard deviation like the random variables discussed previously. We will simulate the concept of a sampling distribution using technology to repeatedly sample, calculate statistics, and graph them. However, the actual sampling distribution would only be attainable if we could theoretically take an infinite amount of samples.\nEach of the point estimates in the table above have their own unique sampling distributions which we will look at in the future\n\n\n\n\n\n\nWe are drawing a random sample of people from the dataset and calculating the mean weight for that sample. Sample size is the number of data points we pull. We then repeat this 5000 times (n_samples) to build up the sampling distribution.\n\n\n\n\n\nRecall: The Standard Error is the standard deviation of the sampling distribution.\n\nSEM = \\sigma_{\\bar{x}\\ (means)}\n\n\n\nRecall: The Standard Error is the standard deviation of the sampling distribution. This is also equal to the standard deviation \\sigma of the population divided by the square root of the sample size.\n\nSEM = \\sigma_{\\bar{x}\\ (means)} = \\frac{\\sigma}{\\sqrt{n}} \\approx \\frac{\\sigma_{x}}{\\sqrt{n}} \\left[ i.e. \\frac{\\text{Est. Std Dev of the sample}}{\\sqrt{\\text{Sample size}}} \\right]\n\n\nIn other words:\n\nIf you draw random samples of size n, the distribution of the random variable \\bar{X}, which consists of sample means, is called the sampling distribution of the sample mean. The sampling distribution of the mean approaches a normal distribution as n, the sample size, increases.\n\n\n\nIn the SEM formula, remember the sampling distribution is the distribution of multiple means - not the distribution of our sample.\nQuote from https://pressbooks.lib.vt.edu/introstatistics/chapter/the-central-limit-theorem-for-sample-means-averages/\n\n\n\n\nA sampling distribution is what we get by simulating multiple samples from a population.\nThe Standard Error is the standard deviation \\sigma_{\\bar{x}} of the sampling distribution.\nThe SE decreases as the sample size n increases.\nBecause of this relationship - we can estimate the SE from a single sample \\frac{\\sigma_x}{\\sqrt{n}}\n\n\n\nSEM = \\sigma_{\\bar{x}\\ (means)} = \\frac{\\sigma}{\\sqrt{n}} \\approx \\frac{\\sigma_{x}}{\\sqrt{n}} \\left[ i.e. \\frac{\\text{Est. Std Dev of the sample}}{\\sqrt{\\text{Sample size}}} \\right]",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Sampling Distribution of the Mean"
    ]
  },
  {
    "objectID": "Week4/02-samp-dist.html#central-limit-theorem",
    "href": "Week4/02-samp-dist.html#central-limit-theorem",
    "title": "Sampling Distribution of the Mean",
    "section": "",
    "text": "The central limit theorem (CLT) is one of the most powerful and useful ideas in all of statistics. There are two alternative forms of the theorem, and both alternatives are concerned with drawing finite samples size n from a population with a known mean, \\mu, and a known standard deviation, \\sigma. The first alternative says that if we collect samples of size n with a “large enough n,” then the resulting distribution can be approximated by the normal distribution.\nApplying the law of large numbers here, we could say that if you take larger and larger samples from a population, then the mean {x} of the sample tends to get closer and closer to \\mu. From the central limit theorem, we know that as n gets larger and larger, the sample means follow a normal distribution. The larger n gets, the smaller the standard deviation gets. (Remember that the standard deviation for {x} is .) This means that the sample mean {x} must be close to the population mean \\mu. We can say that \\mu is the value that the sample means approach as n gets larger. The central limit theorem illustrates the law of large numbers.\nThe size of the sample, n, that is required in order to be “large enough” depends on the original population from which the samples are drawn (the sample size should be at least 30 or the data should come from a normal distribution). If the original population is far from normal, then more observations are needed for the sample means or sums to be normal. Sampling is done with replacement.\n\nThe CLT means says that if you keep drawing larger and larger samples and calculating their means, the sample means form their own normal distribution (the sampling distribution).\nThe sampling distribution of the mean is generated by repeated sampling from the same population and recording the sample mean per sample. This forms a distribution of different means, and this distribution has its own mean and variance.\nThe normal distribution has the same mean as the original distribution and a variance that equals the original variance divided by the sample size.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Sampling Distribution of the Mean"
    ]
  },
  {
    "objectID": "Week4/02-samp-dist.html#drawing-samples-of-peoples-weight-from-the-nhanes-dataset.",
    "href": "Week4/02-samp-dist.html#drawing-samples-of-peoples-weight-from-the-nhanes-dataset.",
    "title": "Sampling Distribution of the Mean",
    "section": "",
    "text": "We have established that different samples yield different statistics due to sampling variability . These statistics have their own distributions, called sampling distributions, that reflect this as a random variable. The sampling distribution of a sample statistic is the distribution of the point estimates based on samples of a fixed size, n, from a certain population. It is useful to think of a particular point estimate as being drawn from a sampling distribution.\nRecall the sample mean weight calculated from a previous sample of 173.3 lbs. Suppose another random sample of 60 participants might produce a different value of x, such as 169.5 lbs. Repeated random sampling could result in additional different values, perhaps 172.1 lbs, 168.5 lbs, and so on. Each sample mean can be thought of as a single observation from a random variable X. The distribution of X is called the sampling distribution of the sample mean, and has its own mean and standard deviation like the random variables discussed previously. We will simulate the concept of a sampling distribution using technology to repeatedly sample, calculate statistics, and graph them. However, the actual sampling distribution would only be attainable if we could theoretically take an infinite amount of samples.\nEach of the point estimates in the table above have their own unique sampling distributions which we will look at in the future\n\n\n\n\n\n\nWe are drawing a random sample of people from the dataset and calculating the mean weight for that sample. Sample size is the number of data points we pull. We then repeat this 5000 times (n_samples) to build up the sampling distribution.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Sampling Distribution of the Mean"
    ]
  },
  {
    "objectID": "Week4/02-samp-dist.html#standard-error",
    "href": "Week4/02-samp-dist.html#standard-error",
    "title": "Sampling Distribution of the Mean",
    "section": "",
    "text": "Recall: The Standard Error is the standard deviation of the sampling distribution.\n\nSEM = \\sigma_{\\bar{x}\\ (means)}",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Sampling Distribution of the Mean"
    ]
  },
  {
    "objectID": "Week4/02-samp-dist.html#standard-error-1",
    "href": "Week4/02-samp-dist.html#standard-error-1",
    "title": "Sampling Distribution of the Mean",
    "section": "",
    "text": "Recall: The Standard Error is the standard deviation of the sampling distribution. This is also equal to the standard deviation \\sigma of the population divided by the square root of the sample size.\n\nSEM = \\sigma_{\\bar{x}\\ (means)} = \\frac{\\sigma}{\\sqrt{n}} \\approx \\frac{\\sigma_{x}}{\\sqrt{n}} \\left[ i.e. \\frac{\\text{Est. Std Dev of the sample}}{\\sqrt{\\text{Sample size}}} \\right]\n\n\nIn other words:\n\nIf you draw random samples of size n, the distribution of the random variable \\bar{X}, which consists of sample means, is called the sampling distribution of the sample mean. The sampling distribution of the mean approaches a normal distribution as n, the sample size, increases.\n\n\n\nIn the SEM formula, remember the sampling distribution is the distribution of multiple means - not the distribution of our sample.\nQuote from https://pressbooks.lib.vt.edu/introstatistics/chapter/the-central-limit-theorem-for-sample-means-averages/",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Sampling Distribution of the Mean"
    ]
  },
  {
    "objectID": "Week4/02-samp-dist.html#standard-error-2",
    "href": "Week4/02-samp-dist.html#standard-error-2",
    "title": "Sampling Distribution of the Mean",
    "section": "",
    "text": "A sampling distribution is what we get by simulating multiple samples from a population.\nThe Standard Error is the standard deviation \\sigma_{\\bar{x}} of the sampling distribution.\nThe SE decreases as the sample size n increases.\nBecause of this relationship - we can estimate the SE from a single sample \\frac{\\sigma_x}{\\sqrt{n}}\n\n\n\nSEM = \\sigma_{\\bar{x}\\ (means)} = \\frac{\\sigma}{\\sqrt{n}} \\approx \\frac{\\sigma_{x}}{\\sqrt{n}} \\left[ i.e. \\frac{\\text{Est. Std Dev of the sample}}{\\sqrt{\\text{Sample size}}} \\right]",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Sampling Distribution of the Mean"
    ]
  },
  {
    "objectID": "Week4/04-inference.html",
    "href": "Week4/04-inference.html",
    "title": "Statistical Inference",
    "section": "",
    "text": "Using a sample to generalize (or infer) about the population.\n\nWe know how to make a point estimate of a population - what else do we need in order to make a decision?\nHow confident are we that our estimate can generalize to the rest of the population?\nWe need to determine the uncertainty in our estimate\n\n\nWe use inferential statistics to make generalizations about an unknown population. The simplest way of doing this is to use the sample data help us to make a point estimate of a population parameter. We realize that due to sampling variability the point estimate is most likely not the exact value of the population parameter, but should be close to it. After calculating point estimates, we can build off of them to construct interval estimates, called confidence intervals.\n\n\nA confidence interval is another type of estimate, but instead of being just one number, it is an interval of numbers\n\nProvides a range a range of reasonable values where we expect the true population parameter to fall.\nPoint estimate (statistic) has some variability and uncertainty since we estimate it based on a sample.\nWe want to quantify and communicate this uncertainty.\n\n\nThere is no guarantee that a given confidence interval does capture the parameter, but there is a predictable probability of success. It is important to keep in mind that the confidence interval itself is a random variable, while the population parameter is fixed.\n\n\n\nYou work in the marketing department of a music company. You want to know the mean number of songs a consumer streams per month.\n\nYou conduct a survey of 100 customers and calculate the sample mean (\\bar{x}) and use it as the point estimate for the population mean (\\mu)\nSuppose we know that the standard deviation \\sigma = 100.\nFollowing the Central Limit Theorem, the Standard Error is:\n\n\n\n\\frac{\\sigma}{\\sqrt{n}} = \\frac{100}{\\sqrt{100}} = 10\n\n\n\nIf you worked in the marketing department of an entertainment company, you might be interested in the mean number of songs a consumer downloads a month from iTunes. If so, you could conduct a survey and calculate the sample mean, \\bar{x}. You would use \\bar{x} to estimate the population mean. The sample mean, \\bar{x}, is the point estimate for the population mean, μ.\nSuppose, for the iTunes example, we do not know the population mean μ, but we do know that the population standard deviation is \\sigma = 100 and our sample size is 100. Then, by the central limit theorem, the standard deviation for the sample mean is \\frac{\\sigma }{\\sqrt{n}}=\\frac{100}{\\sqrt{100}}=10.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Empirical Rule says that in approximately 95% of the samples, the sample mean, \\bar{x}, will be within two standard deviations of the population mean \\mu .\nFor our example, two standard deviations is (2)(10) = 20. The sample mean \\bar{x} is likely to be within 20 units of \\mu.\nBecause \\bar{x} is within 20 units of \\mu, which is unknown, then \\mu is likely to be within 20 units of \\bar{x} in 95% of the samples.\n\n\n\n\nBecause \\bar{x} is within 0.2 units of μ, which is unknown, then μ is likely to be within 0.2 units of \\bar{x} in 95% of the samples. The population mean μ is contained in an interval whose lower number is calculated by taking the sample mean and subtracting two standard deviations (2)(0.1) and whose upper number is calculated by taking the sample mean and adding two standard deviations. In other words, μ is between \\bar{x}\\text{ }-\\text{ 0}\\text{.2} and \\bar{x}\\text{ }+\\text{ 0}\\text{.2} in 95% of all the samples.\n\n\nWe want to calculate the range of values which the true mean is likely to fall within 95% of the time, given our sample.\n\nFor the streaming example, suppose that a sample produced a sample mean \\bar{x} = 200. Then the unknown population mean \\mu is between \\bar{x}-20=200-20=180 and \\bar{x}+20=200+20=220 songs per month.\n\n\nWe can say that we are about 95% confident that the unknown population mean number of songs streamed per month is between 180 and 220. The approximate 95% confidence interval is (1.8, 2.2).\n\n\nConfidence Interval: (Point Estimate \\pm Margin of error) = 200 \\pm 20 \\text{ songs}\n\n\n\nBased on our sample, we can say two things. Either:\n\nThe interval (180, 220) contains the true mean \\mu, or…\nOur sample prodcued an \\bar{x} that is not within 20 units of the true mean \\mu. This would only happen for 5% of the samples.\n\n\n\nWe can say that we are about 95% confident that the unknown population mean number of songs downloaded from iTunes per month is between 1.8 and 2.2. The approximate 95% confidence interval is (1.8, 2.2). This approximate 95% confidence interval implies two possibilities. Either the interval (1.8, 2.2) contains the true mean μ or our sample produced an that is not within 0.2 units of the true mean μ. The second possibility happens for only 5% of all the samples (95–100%).\nRemember that a confidence intervals are created for an unknown population parameter. Confidence intervals for most parameters have the form: (Point Estimate ± Margin of Error) = (Point Estimate – Margin of Error, Point Estimate + Margin of Error) The margin of error (MoE) depends on the confidence level or percentage of confidence and the standard error of the mean. When you read newspapers and journals, some reports will use the phrase “margin of error.” Other reports will not use that phrase, but include a confidence interval as the point estimate plus or minus the margin of error. These are two ways of expressing the same concept. A confidence interval for a population mean with a known standard deviation is based on the fact that the sample means follow an approximately normal distribution. Suppose that our sample has a mean of and we have constructed the 90% confidence interval (5, 15) where MoE = 5.\n\n\n\nThe interpretation should clearly state the confidence level (CL), explain what population parameter is being estimated (here the population mean), and state the confidence interval (both endpoints).\n“We can be _______ % confident that the interval we created, _______ to ________ captures the true population mean (include the context of the problem and appropriate units).”\nWe state the range within which our evidence indicates contains the population mean.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Week4/04-inference.html#statistical-inference-1",
    "href": "Week4/04-inference.html#statistical-inference-1",
    "title": "Statistical Inference",
    "section": "",
    "text": "Using a sample to generalize (or infer) about the population.\n\nWe know how to make a point estimate of a population - what else do we need in order to make a decision?\nHow confident are we that our estimate can generalize to the rest of the population?\nWe need to determine the uncertainty in our estimate\n\n\nWe use inferential statistics to make generalizations about an unknown population. The simplest way of doing this is to use the sample data help us to make a point estimate of a population parameter. We realize that due to sampling variability the point estimate is most likely not the exact value of the population parameter, but should be close to it. After calculating point estimates, we can build off of them to construct interval estimates, called confidence intervals.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Week4/04-inference.html#confidence-intervals",
    "href": "Week4/04-inference.html#confidence-intervals",
    "title": "Statistical Inference",
    "section": "",
    "text": "A confidence interval is another type of estimate, but instead of being just one number, it is an interval of numbers\n\nProvides a range a range of reasonable values where we expect the true population parameter to fall.\nPoint estimate (statistic) has some variability and uncertainty since we estimate it based on a sample.\nWe want to quantify and communicate this uncertainty.\n\n\nThere is no guarantee that a given confidence interval does capture the parameter, but there is a predictable probability of success. It is important to keep in mind that the confidence interval itself is a random variable, while the population parameter is fixed.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Week4/04-inference.html#business-example",
    "href": "Week4/04-inference.html#business-example",
    "title": "Statistical Inference",
    "section": "",
    "text": "You work in the marketing department of a music company. You want to know the mean number of songs a consumer streams per month.\n\nYou conduct a survey of 100 customers and calculate the sample mean (\\bar{x}) and use it as the point estimate for the population mean (\\mu)\nSuppose we know that the standard deviation \\sigma = 100.\nFollowing the Central Limit Theorem, the Standard Error is:\n\n\n\n\\frac{\\sigma}{\\sqrt{n}} = \\frac{100}{\\sqrt{100}} = 10\n\n\n\nIf you worked in the marketing department of an entertainment company, you might be interested in the mean number of songs a consumer downloads a month from iTunes. If so, you could conduct a survey and calculate the sample mean, \\bar{x}. You would use \\bar{x} to estimate the population mean. The sample mean, \\bar{x}, is the point estimate for the population mean, μ.\nSuppose, for the iTunes example, we do not know the population mean μ, but we do know that the population standard deviation is \\sigma = 100 and our sample size is 100. Then, by the central limit theorem, the standard deviation for the sample mean is \\frac{\\sigma }{\\sqrt{n}}=\\frac{100}{\\sqrt{100}}=10.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Week4/04-inference.html#what-is-the-probability-of-sampling-a-certain-mean-value",
    "href": "Week4/04-inference.html#what-is-the-probability-of-sampling-a-certain-mean-value",
    "title": "Statistical Inference",
    "section": "",
    "text": "The Empirical Rule says that in approximately 95% of the samples, the sample mean, \\bar{x}, will be within two standard deviations of the population mean \\mu .\nFor our example, two standard deviations is (2)(10) = 20. The sample mean \\bar{x} is likely to be within 20 units of \\mu.\nBecause \\bar{x} is within 20 units of \\mu, which is unknown, then \\mu is likely to be within 20 units of \\bar{x} in 95% of the samples.\n\n\n\n\nBecause \\bar{x} is within 0.2 units of μ, which is unknown, then μ is likely to be within 0.2 units of \\bar{x} in 95% of the samples. The population mean μ is contained in an interval whose lower number is calculated by taking the sample mean and subtracting two standard deviations (2)(0.1) and whose upper number is calculated by taking the sample mean and adding two standard deviations. In other words, μ is between \\bar{x}\\text{ }-\\text{ 0}\\text{.2} and \\bar{x}\\text{ }+\\text{ 0}\\text{.2} in 95% of all the samples.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Week4/04-inference.html#calculate-the-confidence-interval",
    "href": "Week4/04-inference.html#calculate-the-confidence-interval",
    "title": "Statistical Inference",
    "section": "",
    "text": "We want to calculate the range of values which the true mean is likely to fall within 95% of the time, given our sample.\n\nFor the streaming example, suppose that a sample produced a sample mean \\bar{x} = 200. Then the unknown population mean \\mu is between \\bar{x}-20=200-20=180 and \\bar{x}+20=200+20=220 songs per month.\n\n\nWe can say that we are about 95% confident that the unknown population mean number of songs streamed per month is between 180 and 220. The approximate 95% confidence interval is (1.8, 2.2).\n\n\nConfidence Interval: (Point Estimate \\pm Margin of error) = 200 \\pm 20 \\text{ songs}",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Week4/04-inference.html#calculate-the-confidence-interval-1",
    "href": "Week4/04-inference.html#calculate-the-confidence-interval-1",
    "title": "Statistical Inference",
    "section": "",
    "text": "Based on our sample, we can say two things. Either:\n\nThe interval (180, 220) contains the true mean \\mu, or…\nOur sample prodcued an \\bar{x} that is not within 20 units of the true mean \\mu. This would only happen for 5% of the samples.\n\n\n\nWe can say that we are about 95% confident that the unknown population mean number of songs downloaded from iTunes per month is between 1.8 and 2.2. The approximate 95% confidence interval is (1.8, 2.2). This approximate 95% confidence interval implies two possibilities. Either the interval (1.8, 2.2) contains the true mean μ or our sample produced an that is not within 0.2 units of the true mean μ. The second possibility happens for only 5% of all the samples (95–100%).\nRemember that a confidence intervals are created for an unknown population parameter. Confidence intervals for most parameters have the form: (Point Estimate ± Margin of Error) = (Point Estimate – Margin of Error, Point Estimate + Margin of Error) The margin of error (MoE) depends on the confidence level or percentage of confidence and the standard error of the mean. When you read newspapers and journals, some reports will use the phrase “margin of error.” Other reports will not use that phrase, but include a confidence interval as the point estimate plus or minus the margin of error. These are two ways of expressing the same concept. A confidence interval for a population mean with a known standard deviation is based on the fact that the sample means follow an approximately normal distribution. Suppose that our sample has a mean of and we have constructed the 90% confidence interval (5, 15) where MoE = 5.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Week4/04-inference.html#communicating-confidence-intervals",
    "href": "Week4/04-inference.html#communicating-confidence-intervals",
    "title": "Statistical Inference",
    "section": "",
    "text": "The interpretation should clearly state the confidence level (CL), explain what population parameter is being estimated (here the population mean), and state the confidence interval (both endpoints).\n“We can be _______ % confident that the interval we created, _______ to ________ captures the true population mean (include the context of the problem and appropriate units).”\nWe state the range within which our evidence indicates contains the population mean.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Week4/notes.html",
    "href": "Week4/notes.html",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "",
    "text": "Slides\n\n Download PDF Slides \n\n\nor continue on to the next pages for lecture notes.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference and Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week4/notes.html#this-weeks-lecture",
    "href": "Week4/notes.html#this-weeks-lecture",
    "title": "Statistical Inference and Hypothesis Testing",
    "section": "",
    "text": "Slides\n\n Download PDF Slides \n\n\nor continue on to the next pages for lecture notes.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 4",
      "Statistical Inference and Hypothesis Testing"
    ]
  },
  {
    "objectID": "Week5/1-content.html",
    "href": "Week5/1-content.html",
    "title": "Energy Consumption Prediction",
    "section": "",
    "text": "Climate change is a globally relevant, urgent, and multi-faceted issue heavily impacted by energy policy and infrastructure. Addressing climate change involves mitigation (i.e. mitigating greenhouse gas emissions) and adaptation (i.e. preparing for unavoidable consequences). Mitigation of GHG emissions requires changes to electricity systems, transportation, buildings, industry, and land use.\n\n\nAccording to a report issued by the International Energy Agency (IEA), the lifecycle of buildings from construction to demolition were responsible for 37% of global energy-related CO2 emissions in 2020. Yet it is possible to drastically reduct the energy consumption of buildings by a combination of easy-to-implement fixes and state-of-the-art strategies. For example, retrofitted buildings can reduce heating and colling energy requirements by 50-90 percent. Many of these energy efficiency measures also result in overall cost savings and yield other benefits, such as cleaner air for occupants. This potential can be achieved while maintaining the services that buildings provide.\n\n\nThe goal of this competition is to predict the energy consumption using building characteristics and climate and weather variables.\n\nThe WiDS Datathon 2022 focuses on a prediction task involving roughly 100k observations of building energy usage records collected over 7 years and a number of states within the United States. The dataset consists of building characteristics (e.g. floor area, facility type etc), weather data for the location of the building (e.g. annual average temperature, annual total precipitation etc) as well as the energy usage for the building and the given year, measured as Site Energy Usage Intensity (Site EUI). Each row in the data corresponds to the a single building observed in a given year. Your task is to predict the Site EUI for each row, given the characteristics of the building and the weather data for the location of the building.\n\n\n\nCodelibrary(gtExtras)\nlibrary(gtsummary)\nlibrary(tidyverse)\nlibrary(tidymodels)\n\ndata &lt;- read_csv(\"data/buildings-data/train.csv\")\n\n\n\n\nCodedata &lt;- data |&gt;\n  janitor::clean_names() |&gt;\n  mutate(\n    state_factor = factor(state_factor),\n    building_class = factor(building_class),\n    facility_type = factor(facility_type),\n    year_built = factor(year_built, ordered = TRUE),\n    energy_star_rating = factor(energy_star_rating, ordered = TRUE),\n    log_site_eui = log(site_eui)\n  )\n\ndata |&gt;\n  gt_preview() |&gt;\n  tab_header(title = \"Building Energy Dataset\")\n\n\n\n\n\n\nBuilding Energy Dataset\n\n\n\nyear_factor\nstate_factor\nbuilding_class\nfacility_type\nfloor_area\nyear_built\nenergy_star_rating\nelevation\njanuary_min_temp\njanuary_avg_temp\njanuary_max_temp\nfebruary_min_temp\nfebruary_avg_temp\nfebruary_max_temp\nmarch_min_temp\nmarch_avg_temp\nmarch_max_temp\napril_min_temp\napril_avg_temp\napril_max_temp\nmay_min_temp\nmay_avg_temp\nmay_max_temp\njune_min_temp\njune_avg_temp\njune_max_temp\njuly_min_temp\njuly_avg_temp\njuly_max_temp\naugust_min_temp\naugust_avg_temp\naugust_max_temp\nseptember_min_temp\nseptember_avg_temp\nseptember_max_temp\noctober_min_temp\noctober_avg_temp\noctober_max_temp\nnovember_min_temp\nnovember_avg_temp\nnovember_max_temp\ndecember_min_temp\ndecember_avg_temp\ndecember_max_temp\ncooling_degree_days\nheating_degree_days\nprecipitation_inches\nsnowfall_inches\nsnowdepth_inches\navg_temp\ndays_below_30f\ndays_below_20f\ndays_below_10f\ndays_below_0f\ndays_above_80f\ndays_above_90f\ndays_above_100f\ndays_above_110f\ndirection_max_wind_speed\ndirection_peak_wind_speed\nmax_wind_speed\ndays_with_fog\nsite_eui\nid\nlog_site_eui\n\n\n\n\n1\n1\nState_1\nCommercial\nGrocery_store_or_food_market\n61242\n1942\n11\n2.4\n36\n50.50000\n68\n35\n50.58929\n73\n40\n53.69355\n80\n41\n55.50000\n78\n46\n56.85484\n84\n50\n60.50000\n90\n52\n62.72581\n84\n52\n62.16129\n85\n52\n64.65000\n90\n47\n63.01613\n83\n43\n53.80\n72\n36\n49.27419\n71\n115\n2960\n16.59\n0.0\n0\n56.9726\n0\n0\n0\n0\n14\n0\n0\n0\n1\n1\n1\nNA\n248.682615\n0\n5.516177\n\n\n2\n1\nState_1\nCommercial\nWarehouse_Distribution_or_Shipping_center\n274000\n1955\n45\n1.8\n36\n50.50000\n68\n35\n50.58929\n73\n40\n53.69355\n80\n41\n55.50000\n78\n46\n56.85484\n84\n50\n60.50000\n90\n52\n62.72581\n84\n52\n62.16129\n85\n52\n64.65000\n90\n47\n63.01613\n83\n43\n53.80\n72\n36\n49.27419\n71\n115\n2960\n16.59\n0.0\n0\n56.9726\n0\n0\n0\n0\n14\n0\n0\n0\n1\nNA\n1\n12\n26.500150\n1\n3.277150\n\n\n3\n1\nState_1\nCommercial\nRetail_Enclosed_mall\n280025\n1951\n97\n1.8\n36\n50.50000\n68\n35\n50.58929\n73\n40\n53.69355\n80\n41\n55.50000\n78\n46\n56.85484\n84\n50\n60.50000\n90\n52\n62.72581\n84\n52\n62.16129\n85\n52\n64.65000\n90\n47\n63.01613\n83\n43\n53.80\n72\n36\n49.27419\n71\n115\n2960\n16.59\n0.0\n0\n56.9726\n0\n0\n0\n0\n14\n0\n0\n0\n1\nNA\n1\n12\n24.693619\n2\n3.206545\n\n\n4\n1\nState_1\nCommercial\nEducation_Other_classroom\n55325\n1980\n46\n1.8\n36\n50.50000\n68\n35\n50.58929\n73\n40\n53.69355\n80\n41\n55.50000\n78\n46\n56.85484\n84\n50\n60.50000\n90\n52\n62.72581\n84\n52\n62.16129\n85\n52\n64.65000\n90\n47\n63.01613\n83\n43\n53.80\n72\n36\n49.27419\n71\n115\n2960\n16.59\n0.0\n0\n56.9726\n0\n0\n0\n0\n14\n0\n0\n0\n1\nNA\n1\n12\n48.406926\n3\n3.879643\n\n\n5\n1\nState_1\nCommercial\nWarehouse_Nonrefrigerated\n66000\n1985\n100\n2.4\n36\n50.50000\n68\n35\n50.58929\n73\n40\n53.69355\n80\n41\n55.50000\n78\n46\n56.85484\n84\n50\n60.50000\n90\n52\n62.72581\n84\n52\n62.16129\n85\n52\n64.65000\n90\n47\n63.01613\n83\n43\n53.80\n72\n36\n49.27419\n71\n115\n2960\n16.59\n0.0\n0\n56.9726\n0\n0\n0\n0\n14\n0\n0\n0\n1\n1\n1\nNA\n3.899395\n4\n1.360821\n\n\n6..75756\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n75757\n6\nState_11\nResidential\n2to4_Unit_Building\n23888\n1974\n51\n36.6\n27\n36.93548\n51\n29\n42.17241\n60\n30\n41.40323\n66\n36\n51.53333\n85\n41\n53.88710\n80\n41\n58.43333\n90\n48\n60.53226\n83\n49\n64.33871\n90\n43\n55.93103\n75\n40\n48.53226\n60\n31\n45.15\n69\n18\n30.91935\n42\n148\n5853\n107.69\n28.8\n377\n49.1274\n17\n1\n0\n0\n16\n0\n0\n0\nNA\nNA\nNA\nNA\n29.154684\n75756\n3.372616\n\n\n\n\n\n\n\nCodedata |&gt;\n  ggplot(mapping = aes(x = log_site_eui)) +\n  geom_density()\n\n\n\n\n\n\n\n\nCoderes &lt;- lm(site_eui ~ facility_type, data = data)\nsummary(res)\n\n\nCall:\nlm(formula = site_eui ~ facility_type, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-317.99  -22.06   -5.52   13.75  909.55 \n\nCoefficients:\n                                                       Estimate Std. Error\n(Intercept)                                              31.877      1.182\nfacility_type5plus_Unit_Building                          4.861      1.864\nfacility_typeCommercial_Other                            60.765      1.707\nfacility_typeCommercial_Unknown                          81.275      5.406\nfacility_typeData_Center                                307.858      9.965\nfacility_typeEducation_College_or_university             76.752      1.975\nfacility_typeEducation_Other_classroom                   37.565      1.443\nfacility_typeEducation_Preschool_or_daycare              29.097      5.087\nfacility_typeEducation_Uncategorized                     14.296      2.264\nfacility_typeFood_Sales                                 104.916      6.258\nfacility_typeFood_Service_Other                          -2.640     12.526\nfacility_typeFood_Service_Restaurant_or_cafeteria       163.717      6.535\nfacility_typeFood_Service_Uncategorized                  96.925     12.176\nfacility_typeGrocery_store_or_food_market               209.258      2.701\nfacility_typeHealth_Care_Inpatient                      216.464      2.804\nfacility_typeHealth_Care_Outpatient_Clinic               71.736      6.972\nfacility_typeHealth_Care_Outpatient_Uncategorized       158.015      8.650\nfacility_typeHealth_Care_Uncategorized                  152.068      7.296\nfacility_typeIndustrial                                  93.468      2.878\nfacility_typeLaboratory                                 297.572      5.109\nfacility_typeLodging_Dormitory_or_fraternity_sorority    49.719      2.313\nfacility_typeLodging_Hotel                               73.058      1.630\nfacility_typeLodging_Other                               89.081      6.053\nfacility_typeLodging_Uncategorized                       34.719     23.024\nfacility_typeMixed_Use_Commercial_and_Residential        57.653      2.309\nfacility_typeMixed_Use_Predominantly_Commercial          37.256      3.424\nfacility_typeMixed_Use_Predominantly_Residential         49.921     17.179\nfacility_typeMultifamily_Uncategorized                   52.002      1.210\nfacility_typeNursing_Home                                99.437      2.196\nfacility_typeOffice_Bank_or_other_financial              58.019      4.084\nfacility_typeOffice_Medical_non_diagnostic               84.885      2.704\nfacility_typeOffice_Mixed_use                            50.233     12.176\nfacility_typeOffice_Uncategorized                        45.197      1.268\nfacility_typeParking_Garage                              35.474      3.454\nfacility_typePublic_Assembly_Drama_theater               49.040      6.258\nfacility_typePublic_Assembly_Entertainment_culture       87.023      5.043\nfacility_typePublic_Assembly_Library                     73.972      4.233\nfacility_typePublic_Assembly_Movie_Theater               71.218      8.317\nfacility_typePublic_Assembly_Other                       94.827      4.474\nfacility_typePublic_Assembly_Recreation                  83.301      6.174\nfacility_typePublic_Assembly_Social_meeting              47.045      5.607\nfacility_typePublic_Assembly_Stadium                    125.165     17.179\nfacility_typePublic_Assembly_Uncategorized               30.996     10.351\nfacility_typePublic_Safety_Courthouse                    71.305      8.424\nfacility_typePublic_Safety_Fire_or_police_station        99.246      4.270\nfacility_typePublic_Safety_Penitentiary                 139.028      8.535\nfacility_typePublic_Safety_Uncategorized                 51.832      7.929\nfacility_typeReligious_worship                           12.684      2.832\nfacility_typeRetail_Enclosed_mall                        69.089      4.840\nfacility_typeRetail_Strip_shopping_mall                  78.542      4.979\nfacility_typeRetail_Uncategorized                        49.026      1.933\nfacility_typeRetail_Vehicle_dealership_showroom          14.764      6.093\nfacility_typeService_Drycleaning_or_Laundry              10.236     17.179\nfacility_typeService_Uncategorized                       81.689      6.346\nfacility_typeService_Vehicle_service_repair_shop        105.719      4.533\nfacility_typeWarehouse_Distribution_or_Shipping_center    7.683      2.403\nfacility_typeWarehouse_Nonrefrigerated                    6.332      1.872\nfacility_typeWarehouse_Refrigerated                      64.648      4.979\nfacility_typeWarehouse_Selfstorage                      -10.288      2.445\nfacility_typeWarehouse_Uncategorized                      4.060      3.067\n                                                       t value Pr(&gt;|t|)    \n(Intercept)                                             26.975  &lt; 2e-16 ***\nfacility_type5plus_Unit_Building                         2.608 0.009101 ** \nfacility_typeCommercial_Other                           35.607  &lt; 2e-16 ***\nfacility_typeCommercial_Unknown                         15.035  &lt; 2e-16 ***\nfacility_typeData_Center                                30.893  &lt; 2e-16 ***\nfacility_typeEducation_College_or_university            38.866  &lt; 2e-16 ***\nfacility_typeEducation_Other_classroom                  26.038  &lt; 2e-16 ***\nfacility_typeEducation_Preschool_or_daycare              5.720 1.07e-08 ***\nfacility_typeEducation_Uncategorized                     6.315 2.72e-10 ***\nfacility_typeFood_Sales                                 16.765  &lt; 2e-16 ***\nfacility_typeFood_Service_Other                         -0.211 0.833054    \nfacility_typeFood_Service_Restaurant_or_cafeteria       25.054  &lt; 2e-16 ***\nfacility_typeFood_Service_Uncategorized                  7.960 1.74e-15 ***\nfacility_typeGrocery_store_or_food_market               77.465  &lt; 2e-16 ***\nfacility_typeHealth_Care_Inpatient                      77.211  &lt; 2e-16 ***\nfacility_typeHealth_Care_Outpatient_Clinic              10.290  &lt; 2e-16 ***\nfacility_typeHealth_Care_Outpatient_Uncategorized       18.267  &lt; 2e-16 ***\nfacility_typeHealth_Care_Uncategorized                  20.843  &lt; 2e-16 ***\nfacility_typeIndustrial                                 32.481  &lt; 2e-16 ***\nfacility_typeLaboratory                                 58.244  &lt; 2e-16 ***\nfacility_typeLodging_Dormitory_or_fraternity_sorority   21.499  &lt; 2e-16 ***\nfacility_typeLodging_Hotel                              44.824  &lt; 2e-16 ***\nfacility_typeLodging_Other                              14.716  &lt; 2e-16 ***\nfacility_typeLodging_Uncategorized                       1.508 0.131577    \nfacility_typeMixed_Use_Commercial_and_Residential       24.971  &lt; 2e-16 ***\nfacility_typeMixed_Use_Predominantly_Commercial         10.881  &lt; 2e-16 ***\nfacility_typeMixed_Use_Predominantly_Residential         2.906 0.003663 ** \nfacility_typeMultifamily_Uncategorized                  42.986  &lt; 2e-16 ***\nfacility_typeNursing_Home                               45.289  &lt; 2e-16 ***\nfacility_typeOffice_Bank_or_other_financial             14.207  &lt; 2e-16 ***\nfacility_typeOffice_Medical_non_diagnostic              31.395  &lt; 2e-16 ***\nfacility_typeOffice_Mixed_use                            4.126 3.70e-05 ***\nfacility_typeOffice_Uncategorized                       35.645  &lt; 2e-16 ***\nfacility_typeParking_Garage                             10.271  &lt; 2e-16 ***\nfacility_typePublic_Assembly_Drama_theater               7.836 4.69e-15 ***\nfacility_typePublic_Assembly_Entertainment_culture      17.257  &lt; 2e-16 ***\nfacility_typePublic_Assembly_Library                    17.475  &lt; 2e-16 ***\nfacility_typePublic_Assembly_Movie_Theater               8.563  &lt; 2e-16 ***\nfacility_typePublic_Assembly_Other                      21.197  &lt; 2e-16 ***\nfacility_typePublic_Assembly_Recreation                 13.493  &lt; 2e-16 ***\nfacility_typePublic_Assembly_Social_meeting              8.391  &lt; 2e-16 ***\nfacility_typePublic_Assembly_Stadium                     7.286 3.23e-13 ***\nfacility_typePublic_Assembly_Uncategorized               2.995 0.002750 ** \nfacility_typePublic_Safety_Courthouse                    8.465  &lt; 2e-16 ***\nfacility_typePublic_Safety_Fire_or_police_station       23.242  &lt; 2e-16 ***\nfacility_typePublic_Safety_Penitentiary                 16.289  &lt; 2e-16 ***\nfacility_typePublic_Safety_Uncategorized                 6.537 6.33e-11 ***\nfacility_typeReligious_worship                           4.478 7.54e-06 ***\nfacility_typeRetail_Enclosed_mall                       14.274  &lt; 2e-16 ***\nfacility_typeRetail_Strip_shopping_mall                 15.775  &lt; 2e-16 ***\nfacility_typeRetail_Uncategorized                       25.365  &lt; 2e-16 ***\nfacility_typeRetail_Vehicle_dealership_showroom          2.423 0.015384 *  \nfacility_typeService_Drycleaning_or_Laundry              0.596 0.551299    \nfacility_typeService_Uncategorized                      12.872  &lt; 2e-16 ***\nfacility_typeService_Vehicle_service_repair_shop        23.320  &lt; 2e-16 ***\nfacility_typeWarehouse_Distribution_or_Shipping_center   3.197 0.001387 ** \nfacility_typeWarehouse_Nonrefrigerated                   3.383 0.000716 ***\nfacility_typeWarehouse_Refrigerated                     12.984  &lt; 2e-16 ***\nfacility_typeWarehouse_Selfstorage                      -4.208 2.58e-05 ***\nfacility_typeWarehouse_Uncategorized                     1.324 0.185652    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 51.42 on 75697 degrees of freedom\nMultiple R-squared:  0.2217,    Adjusted R-squared:  0.221 \nF-statistic: 365.4 on 59 and 75697 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Week5/1-content.html#goal",
    "href": "Week5/1-content.html#goal",
    "title": "Energy Consumption Prediction",
    "section": "",
    "text": "The goal of this competition is to predict the energy consumption using building characteristics and climate and weather variables."
  },
  {
    "objectID": "Week5/1-content.html#data",
    "href": "Week5/1-content.html#data",
    "title": "Energy Consumption Prediction",
    "section": "",
    "text": "The WiDS Datathon 2022 focuses on a prediction task involving roughly 100k observations of building energy usage records collected over 7 years and a number of states within the United States. The dataset consists of building characteristics (e.g. floor area, facility type etc), weather data for the location of the building (e.g. annual average temperature, annual total precipitation etc) as well as the energy usage for the building and the given year, measured as Site Energy Usage Intensity (Site EUI). Each row in the data corresponds to the a single building observed in a given year. Your task is to predict the Site EUI for each row, given the characteristics of the building and the weather data for the location of the building."
  },
  {
    "objectID": "Week5/1-content.html#setup",
    "href": "Week5/1-content.html#setup",
    "title": "Energy Consumption Prediction",
    "section": "",
    "text": "Codelibrary(gtExtras)\nlibrary(gtsummary)\nlibrary(tidyverse)\nlibrary(tidymodels)\n\ndata &lt;- read_csv(\"data/buildings-data/train.csv\")\n\n\n\n\nCodedata &lt;- data |&gt;\n  janitor::clean_names() |&gt;\n  mutate(\n    state_factor = factor(state_factor),\n    building_class = factor(building_class),\n    facility_type = factor(facility_type),\n    year_built = factor(year_built, ordered = TRUE),\n    energy_star_rating = factor(energy_star_rating, ordered = TRUE),\n    log_site_eui = log(site_eui)\n  )\n\ndata |&gt;\n  gt_preview() |&gt;\n  tab_header(title = \"Building Energy Dataset\")\n\n\n\n\n\n\nBuilding Energy Dataset\n\n\n\nyear_factor\nstate_factor\nbuilding_class\nfacility_type\nfloor_area\nyear_built\nenergy_star_rating\nelevation\njanuary_min_temp\njanuary_avg_temp\njanuary_max_temp\nfebruary_min_temp\nfebruary_avg_temp\nfebruary_max_temp\nmarch_min_temp\nmarch_avg_temp\nmarch_max_temp\napril_min_temp\napril_avg_temp\napril_max_temp\nmay_min_temp\nmay_avg_temp\nmay_max_temp\njune_min_temp\njune_avg_temp\njune_max_temp\njuly_min_temp\njuly_avg_temp\njuly_max_temp\naugust_min_temp\naugust_avg_temp\naugust_max_temp\nseptember_min_temp\nseptember_avg_temp\nseptember_max_temp\noctober_min_temp\noctober_avg_temp\noctober_max_temp\nnovember_min_temp\nnovember_avg_temp\nnovember_max_temp\ndecember_min_temp\ndecember_avg_temp\ndecember_max_temp\ncooling_degree_days\nheating_degree_days\nprecipitation_inches\nsnowfall_inches\nsnowdepth_inches\navg_temp\ndays_below_30f\ndays_below_20f\ndays_below_10f\ndays_below_0f\ndays_above_80f\ndays_above_90f\ndays_above_100f\ndays_above_110f\ndirection_max_wind_speed\ndirection_peak_wind_speed\nmax_wind_speed\ndays_with_fog\nsite_eui\nid\nlog_site_eui\n\n\n\n\n1\n1\nState_1\nCommercial\nGrocery_store_or_food_market\n61242\n1942\n11\n2.4\n36\n50.50000\n68\n35\n50.58929\n73\n40\n53.69355\n80\n41\n55.50000\n78\n46\n56.85484\n84\n50\n60.50000\n90\n52\n62.72581\n84\n52\n62.16129\n85\n52\n64.65000\n90\n47\n63.01613\n83\n43\n53.80\n72\n36\n49.27419\n71\n115\n2960\n16.59\n0.0\n0\n56.9726\n0\n0\n0\n0\n14\n0\n0\n0\n1\n1\n1\nNA\n248.682615\n0\n5.516177\n\n\n2\n1\nState_1\nCommercial\nWarehouse_Distribution_or_Shipping_center\n274000\n1955\n45\n1.8\n36\n50.50000\n68\n35\n50.58929\n73\n40\n53.69355\n80\n41\n55.50000\n78\n46\n56.85484\n84\n50\n60.50000\n90\n52\n62.72581\n84\n52\n62.16129\n85\n52\n64.65000\n90\n47\n63.01613\n83\n43\n53.80\n72\n36\n49.27419\n71\n115\n2960\n16.59\n0.0\n0\n56.9726\n0\n0\n0\n0\n14\n0\n0\n0\n1\nNA\n1\n12\n26.500150\n1\n3.277150\n\n\n3\n1\nState_1\nCommercial\nRetail_Enclosed_mall\n280025\n1951\n97\n1.8\n36\n50.50000\n68\n35\n50.58929\n73\n40\n53.69355\n80\n41\n55.50000\n78\n46\n56.85484\n84\n50\n60.50000\n90\n52\n62.72581\n84\n52\n62.16129\n85\n52\n64.65000\n90\n47\n63.01613\n83\n43\n53.80\n72\n36\n49.27419\n71\n115\n2960\n16.59\n0.0\n0\n56.9726\n0\n0\n0\n0\n14\n0\n0\n0\n1\nNA\n1\n12\n24.693619\n2\n3.206545\n\n\n4\n1\nState_1\nCommercial\nEducation_Other_classroom\n55325\n1980\n46\n1.8\n36\n50.50000\n68\n35\n50.58929\n73\n40\n53.69355\n80\n41\n55.50000\n78\n46\n56.85484\n84\n50\n60.50000\n90\n52\n62.72581\n84\n52\n62.16129\n85\n52\n64.65000\n90\n47\n63.01613\n83\n43\n53.80\n72\n36\n49.27419\n71\n115\n2960\n16.59\n0.0\n0\n56.9726\n0\n0\n0\n0\n14\n0\n0\n0\n1\nNA\n1\n12\n48.406926\n3\n3.879643\n\n\n5\n1\nState_1\nCommercial\nWarehouse_Nonrefrigerated\n66000\n1985\n100\n2.4\n36\n50.50000\n68\n35\n50.58929\n73\n40\n53.69355\n80\n41\n55.50000\n78\n46\n56.85484\n84\n50\n60.50000\n90\n52\n62.72581\n84\n52\n62.16129\n85\n52\n64.65000\n90\n47\n63.01613\n83\n43\n53.80\n72\n36\n49.27419\n71\n115\n2960\n16.59\n0.0\n0\n56.9726\n0\n0\n0\n0\n14\n0\n0\n0\n1\n1\n1\nNA\n3.899395\n4\n1.360821\n\n\n6..75756\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n75757\n6\nState_11\nResidential\n2to4_Unit_Building\n23888\n1974\n51\n36.6\n27\n36.93548\n51\n29\n42.17241\n60\n30\n41.40323\n66\n36\n51.53333\n85\n41\n53.88710\n80\n41\n58.43333\n90\n48\n60.53226\n83\n49\n64.33871\n90\n43\n55.93103\n75\n40\n48.53226\n60\n31\n45.15\n69\n18\n30.91935\n42\n148\n5853\n107.69\n28.8\n377\n49.1274\n17\n1\n0\n0\n16\n0\n0\n0\nNA\nNA\nNA\nNA\n29.154684\n75756\n3.372616\n\n\n\n\n\n\n\nCodedata |&gt;\n  ggplot(mapping = aes(x = log_site_eui)) +\n  geom_density()\n\n\n\n\n\n\n\n\nCoderes &lt;- lm(site_eui ~ facility_type, data = data)\nsummary(res)\n\n\nCall:\nlm(formula = site_eui ~ facility_type, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-317.99  -22.06   -5.52   13.75  909.55 \n\nCoefficients:\n                                                       Estimate Std. Error\n(Intercept)                                              31.877      1.182\nfacility_type5plus_Unit_Building                          4.861      1.864\nfacility_typeCommercial_Other                            60.765      1.707\nfacility_typeCommercial_Unknown                          81.275      5.406\nfacility_typeData_Center                                307.858      9.965\nfacility_typeEducation_College_or_university             76.752      1.975\nfacility_typeEducation_Other_classroom                   37.565      1.443\nfacility_typeEducation_Preschool_or_daycare              29.097      5.087\nfacility_typeEducation_Uncategorized                     14.296      2.264\nfacility_typeFood_Sales                                 104.916      6.258\nfacility_typeFood_Service_Other                          -2.640     12.526\nfacility_typeFood_Service_Restaurant_or_cafeteria       163.717      6.535\nfacility_typeFood_Service_Uncategorized                  96.925     12.176\nfacility_typeGrocery_store_or_food_market               209.258      2.701\nfacility_typeHealth_Care_Inpatient                      216.464      2.804\nfacility_typeHealth_Care_Outpatient_Clinic               71.736      6.972\nfacility_typeHealth_Care_Outpatient_Uncategorized       158.015      8.650\nfacility_typeHealth_Care_Uncategorized                  152.068      7.296\nfacility_typeIndustrial                                  93.468      2.878\nfacility_typeLaboratory                                 297.572      5.109\nfacility_typeLodging_Dormitory_or_fraternity_sorority    49.719      2.313\nfacility_typeLodging_Hotel                               73.058      1.630\nfacility_typeLodging_Other                               89.081      6.053\nfacility_typeLodging_Uncategorized                       34.719     23.024\nfacility_typeMixed_Use_Commercial_and_Residential        57.653      2.309\nfacility_typeMixed_Use_Predominantly_Commercial          37.256      3.424\nfacility_typeMixed_Use_Predominantly_Residential         49.921     17.179\nfacility_typeMultifamily_Uncategorized                   52.002      1.210\nfacility_typeNursing_Home                                99.437      2.196\nfacility_typeOffice_Bank_or_other_financial              58.019      4.084\nfacility_typeOffice_Medical_non_diagnostic               84.885      2.704\nfacility_typeOffice_Mixed_use                            50.233     12.176\nfacility_typeOffice_Uncategorized                        45.197      1.268\nfacility_typeParking_Garage                              35.474      3.454\nfacility_typePublic_Assembly_Drama_theater               49.040      6.258\nfacility_typePublic_Assembly_Entertainment_culture       87.023      5.043\nfacility_typePublic_Assembly_Library                     73.972      4.233\nfacility_typePublic_Assembly_Movie_Theater               71.218      8.317\nfacility_typePublic_Assembly_Other                       94.827      4.474\nfacility_typePublic_Assembly_Recreation                  83.301      6.174\nfacility_typePublic_Assembly_Social_meeting              47.045      5.607\nfacility_typePublic_Assembly_Stadium                    125.165     17.179\nfacility_typePublic_Assembly_Uncategorized               30.996     10.351\nfacility_typePublic_Safety_Courthouse                    71.305      8.424\nfacility_typePublic_Safety_Fire_or_police_station        99.246      4.270\nfacility_typePublic_Safety_Penitentiary                 139.028      8.535\nfacility_typePublic_Safety_Uncategorized                 51.832      7.929\nfacility_typeReligious_worship                           12.684      2.832\nfacility_typeRetail_Enclosed_mall                        69.089      4.840\nfacility_typeRetail_Strip_shopping_mall                  78.542      4.979\nfacility_typeRetail_Uncategorized                        49.026      1.933\nfacility_typeRetail_Vehicle_dealership_showroom          14.764      6.093\nfacility_typeService_Drycleaning_or_Laundry              10.236     17.179\nfacility_typeService_Uncategorized                       81.689      6.346\nfacility_typeService_Vehicle_service_repair_shop        105.719      4.533\nfacility_typeWarehouse_Distribution_or_Shipping_center    7.683      2.403\nfacility_typeWarehouse_Nonrefrigerated                    6.332      1.872\nfacility_typeWarehouse_Refrigerated                      64.648      4.979\nfacility_typeWarehouse_Selfstorage                      -10.288      2.445\nfacility_typeWarehouse_Uncategorized                      4.060      3.067\n                                                       t value Pr(&gt;|t|)    \n(Intercept)                                             26.975  &lt; 2e-16 ***\nfacility_type5plus_Unit_Building                         2.608 0.009101 ** \nfacility_typeCommercial_Other                           35.607  &lt; 2e-16 ***\nfacility_typeCommercial_Unknown                         15.035  &lt; 2e-16 ***\nfacility_typeData_Center                                30.893  &lt; 2e-16 ***\nfacility_typeEducation_College_or_university            38.866  &lt; 2e-16 ***\nfacility_typeEducation_Other_classroom                  26.038  &lt; 2e-16 ***\nfacility_typeEducation_Preschool_or_daycare              5.720 1.07e-08 ***\nfacility_typeEducation_Uncategorized                     6.315 2.72e-10 ***\nfacility_typeFood_Sales                                 16.765  &lt; 2e-16 ***\nfacility_typeFood_Service_Other                         -0.211 0.833054    \nfacility_typeFood_Service_Restaurant_or_cafeteria       25.054  &lt; 2e-16 ***\nfacility_typeFood_Service_Uncategorized                  7.960 1.74e-15 ***\nfacility_typeGrocery_store_or_food_market               77.465  &lt; 2e-16 ***\nfacility_typeHealth_Care_Inpatient                      77.211  &lt; 2e-16 ***\nfacility_typeHealth_Care_Outpatient_Clinic              10.290  &lt; 2e-16 ***\nfacility_typeHealth_Care_Outpatient_Uncategorized       18.267  &lt; 2e-16 ***\nfacility_typeHealth_Care_Uncategorized                  20.843  &lt; 2e-16 ***\nfacility_typeIndustrial                                 32.481  &lt; 2e-16 ***\nfacility_typeLaboratory                                 58.244  &lt; 2e-16 ***\nfacility_typeLodging_Dormitory_or_fraternity_sorority   21.499  &lt; 2e-16 ***\nfacility_typeLodging_Hotel                              44.824  &lt; 2e-16 ***\nfacility_typeLodging_Other                              14.716  &lt; 2e-16 ***\nfacility_typeLodging_Uncategorized                       1.508 0.131577    \nfacility_typeMixed_Use_Commercial_and_Residential       24.971  &lt; 2e-16 ***\nfacility_typeMixed_Use_Predominantly_Commercial         10.881  &lt; 2e-16 ***\nfacility_typeMixed_Use_Predominantly_Residential         2.906 0.003663 ** \nfacility_typeMultifamily_Uncategorized                  42.986  &lt; 2e-16 ***\nfacility_typeNursing_Home                               45.289  &lt; 2e-16 ***\nfacility_typeOffice_Bank_or_other_financial             14.207  &lt; 2e-16 ***\nfacility_typeOffice_Medical_non_diagnostic              31.395  &lt; 2e-16 ***\nfacility_typeOffice_Mixed_use                            4.126 3.70e-05 ***\nfacility_typeOffice_Uncategorized                       35.645  &lt; 2e-16 ***\nfacility_typeParking_Garage                             10.271  &lt; 2e-16 ***\nfacility_typePublic_Assembly_Drama_theater               7.836 4.69e-15 ***\nfacility_typePublic_Assembly_Entertainment_culture      17.257  &lt; 2e-16 ***\nfacility_typePublic_Assembly_Library                    17.475  &lt; 2e-16 ***\nfacility_typePublic_Assembly_Movie_Theater               8.563  &lt; 2e-16 ***\nfacility_typePublic_Assembly_Other                      21.197  &lt; 2e-16 ***\nfacility_typePublic_Assembly_Recreation                 13.493  &lt; 2e-16 ***\nfacility_typePublic_Assembly_Social_meeting              8.391  &lt; 2e-16 ***\nfacility_typePublic_Assembly_Stadium                     7.286 3.23e-13 ***\nfacility_typePublic_Assembly_Uncategorized               2.995 0.002750 ** \nfacility_typePublic_Safety_Courthouse                    8.465  &lt; 2e-16 ***\nfacility_typePublic_Safety_Fire_or_police_station       23.242  &lt; 2e-16 ***\nfacility_typePublic_Safety_Penitentiary                 16.289  &lt; 2e-16 ***\nfacility_typePublic_Safety_Uncategorized                 6.537 6.33e-11 ***\nfacility_typeReligious_worship                           4.478 7.54e-06 ***\nfacility_typeRetail_Enclosed_mall                       14.274  &lt; 2e-16 ***\nfacility_typeRetail_Strip_shopping_mall                 15.775  &lt; 2e-16 ***\nfacility_typeRetail_Uncategorized                       25.365  &lt; 2e-16 ***\nfacility_typeRetail_Vehicle_dealership_showroom          2.423 0.015384 *  \nfacility_typeService_Drycleaning_or_Laundry              0.596 0.551299    \nfacility_typeService_Uncategorized                      12.872  &lt; 2e-16 ***\nfacility_typeService_Vehicle_service_repair_shop        23.320  &lt; 2e-16 ***\nfacility_typeWarehouse_Distribution_or_Shipping_center   3.197 0.001387 ** \nfacility_typeWarehouse_Nonrefrigerated                   3.383 0.000716 ***\nfacility_typeWarehouse_Refrigerated                     12.984  &lt; 2e-16 ***\nfacility_typeWarehouse_Selfstorage                      -4.208 2.58e-05 ***\nfacility_typeWarehouse_Uncategorized                     1.324 0.185652    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 51.42 on 75697 degrees of freedom\nMultiple R-squared:  0.2217,    Adjusted R-squared:  0.221 \nF-statistic: 365.4 on 59 and 75697 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Week5/3-analysis-workflow.html",
    "href": "Week5/3-analysis-workflow.html",
    "title": "The Data Analysis Workflow",
    "section": "",
    "text": "Source: @Wickham2023data, @Poldrack2023Statistical\n\nThere is a set of steps that we generally go through when we want to use our statistical model to test a scientific hypothesis:\n\nSpecify your question of interest\nIdentify or collect the appropriate data\nPrepare the data for analysis\nDetermine the appropriate model\nFit the model to the data\nCriticize the model to make sure it fits properly\nTest hypothesis and quantify effect size\nCommunicate your analysis\n\n\n\n\n\nThroughout, we have been using the tidyverse library of packages for data analysis.\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.\n\nCodelibrary(tidyverse)\n\n\n\n\n\n\n\n\nThere are tools for reading data from almost any source:\n\n\nread_csv(), read_excel(), read_rds(), …\n\n\nWhen we load a dataset with a tidyverse() function, it will return a tibble\n\n\n\nCodedata &lt;- read_csv(\"data/Apple_Emissions/greenhouse_gas_emissions.csv\")\n\n\n\nThe same data can be represented in multiple ways. Here’s the same data organized three different ways:\n\nEach dataset shows the same values of four variables: country, year, population, and number of documented cases of TB (tuberculosis), but each dataset organizes the values in a different way.\n\n\n\n\nCodetable1\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\nCodetable3\n\n# A tibble: 6 × 3\n  country      year rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n\n\n\nCodetable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n\n\n\n\n\nThere are three rules that make a dataset tidy:\n\nEach variable is a column; each column is a variable.\nEach observation is a row; each row is an observation.\nEach value is a cell; each cell is a single value.\n\n\n\n\n\n\n\nThere’s a general advantage to picking one consistent way of storing data. If you have a consistent data structure, it’s easier to learn the tools that work with it because they have an underlying uniformity.\nThere’s a specific advantage to placing variables in columns because it allows R’s vectorized nature to shine. That makes transforming tidy data feel particularly natural.\n\n\nSo, our first task after importing the data is to make sure it’s tidy. In addition to the rules above, this can also include things like:\n\nensure the data types are correct\nclean up the column names\nmake sure we know what the variables represent\n\nFor the .csv data we loaded, our column names can be a bit difficult to work with since they have spaces in them. We can use a function from the janitor package to clean these:\n\nCodedata &lt;- data |&gt;\n  janitor::clean_names()\ndata\n\n# A tibble: 127 × 6\n   fiscal_year category            type            scope   description emissions\n         &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;\n 1        2022 Corporate emissions Gross emissions Scope 1 Natural ga…     39700\n 2        2022 Corporate emissions Gross emissions Scope 1 Fleet vehi…     12600\n 3        2022 Corporate emissions Gross emissions Scope 1 Other (R&D…      2900\n 4        2022 Corporate emissions Gross emissions Scope … Electricity         0\n 5        2022 Corporate emissions Gross emissions Scope … Steam, hea…      3000\n 6        2022 Corporate emissions Gross emissions Scope 3 Business t…    113500\n 7        2022 Corporate emissions Gross emissions Scope 3 Employee c…    134200\n 8        2022 Corporate emissions Gross emissions Scope 3 Upstream f…     10600\n 9        2022 Corporate emissions Gross emissions Scope 3 Work from …      7500\n10        2022 Corporate emissions Gross emissions Scope 3 Transmissi…         0\n# ℹ 117 more rows\n\n\n\nWe’ve dealt with data transformations quite a bit already. This includes operations like calculating the mean for different groups, or for multiple groups:\n\nCodedata |&gt;\n  group_by(category) |&gt;\n  summarise(\n    mean_emissions = mean(emissions, na.rm = TRUE),\n  )\n\n# A tibble: 2 × 2\n  category                     mean_emissions\n  &lt;chr&gt;                                 &lt;dbl&gt;\n1 Corporate emissions                  35594.\n2 Product life cycle emissions       5630000 \n\n\n\n\nCodedata |&gt;\n  group_by(category, fiscal_year) |&gt;\n  summarise(emissions = sum(emissions, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = fiscal_year, y = emissions, color = category)) +\n  geom_line()\n\n\n\n\n\n\n\n\nThis is where we will dive into using Quarto. Start by downloading the Apple Emissions dataset from Moodle and open RStudio.\nWe’ll go through how to create and write a full analysis in a .qmd file using this dataset.\nRefer to our lecture notes specifically on using Quarto",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "The Data Analysis Workflow"
    ]
  },
  {
    "objectID": "Week5/3-analysis-workflow.html#the-process-of-statistical-modeling",
    "href": "Week5/3-analysis-workflow.html#the-process-of-statistical-modeling",
    "title": "The Data Analysis Workflow",
    "section": "",
    "text": "There is a set of steps that we generally go through when we want to use our statistical model to test a scientific hypothesis:\n\nSpecify your question of interest\nIdentify or collect the appropriate data\nPrepare the data for analysis\nDetermine the appropriate model\nFit the model to the data\nCriticize the model to make sure it fits properly\nTest hypothesis and quantify effect size\nCommunicate your analysis",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "The Data Analysis Workflow"
    ]
  },
  {
    "objectID": "Week5/3-analysis-workflow.html#import",
    "href": "Week5/3-analysis-workflow.html#import",
    "title": "The Data Analysis Workflow",
    "section": "",
    "text": "Throughout, we have been using the tidyverse library of packages for data analysis.\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.\n\nCodelibrary(tidyverse)",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "The Data Analysis Workflow"
    ]
  },
  {
    "objectID": "Week5/3-analysis-workflow.html#import-1",
    "href": "Week5/3-analysis-workflow.html#import-1",
    "title": "The Data Analysis Workflow",
    "section": "",
    "text": "There are tools for reading data from almost any source:\n\n\nread_csv(), read_excel(), read_rds(), …\n\n\nWhen we load a dataset with a tidyverse() function, it will return a tibble\n\n\n\nCodedata &lt;- read_csv(\"data/Apple_Emissions/greenhouse_gas_emissions.csv\")",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "The Data Analysis Workflow"
    ]
  },
  {
    "objectID": "Week5/3-analysis-workflow.html#tidy",
    "href": "Week5/3-analysis-workflow.html#tidy",
    "title": "The Data Analysis Workflow",
    "section": "",
    "text": "The same data can be represented in multiple ways. Here’s the same data organized three different ways:\n\nEach dataset shows the same values of four variables: country, year, population, and number of documented cases of TB (tuberculosis), but each dataset organizes the values in a different way.\n\n\n\n\nCodetable1\n\n# A tibble: 6 × 4\n  country      year  cases population\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\nCodetable3\n\n# A tibble: 6 × 3\n  country      year rate             \n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\n\n\n\nCodetable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n\n\n\n\n\nThere are three rules that make a dataset tidy:\n\nEach variable is a column; each column is a variable.\nEach observation is a row; each row is an observation.\nEach value is a cell; each cell is a single value.\n\n\n\n\n\n\n\nThere’s a general advantage to picking one consistent way of storing data. If you have a consistent data structure, it’s easier to learn the tools that work with it because they have an underlying uniformity.\nThere’s a specific advantage to placing variables in columns because it allows R’s vectorized nature to shine. That makes transforming tidy data feel particularly natural.\n\n\nSo, our first task after importing the data is to make sure it’s tidy. In addition to the rules above, this can also include things like:\n\nensure the data types are correct\nclean up the column names\nmake sure we know what the variables represent\n\nFor the .csv data we loaded, our column names can be a bit difficult to work with since they have spaces in them. We can use a function from the janitor package to clean these:\n\nCodedata &lt;- data |&gt;\n  janitor::clean_names()\ndata\n\n# A tibble: 127 × 6\n   fiscal_year category            type            scope   description emissions\n         &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;           &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;\n 1        2022 Corporate emissions Gross emissions Scope 1 Natural ga…     39700\n 2        2022 Corporate emissions Gross emissions Scope 1 Fleet vehi…     12600\n 3        2022 Corporate emissions Gross emissions Scope 1 Other (R&D…      2900\n 4        2022 Corporate emissions Gross emissions Scope … Electricity         0\n 5        2022 Corporate emissions Gross emissions Scope … Steam, hea…      3000\n 6        2022 Corporate emissions Gross emissions Scope 3 Business t…    113500\n 7        2022 Corporate emissions Gross emissions Scope 3 Employee c…    134200\n 8        2022 Corporate emissions Gross emissions Scope 3 Upstream f…     10600\n 9        2022 Corporate emissions Gross emissions Scope 3 Work from …      7500\n10        2022 Corporate emissions Gross emissions Scope 3 Transmissi…         0\n# ℹ 117 more rows",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "The Data Analysis Workflow"
    ]
  },
  {
    "objectID": "Week5/3-analysis-workflow.html#transform",
    "href": "Week5/3-analysis-workflow.html#transform",
    "title": "The Data Analysis Workflow",
    "section": "",
    "text": "We’ve dealt with data transformations quite a bit already. This includes operations like calculating the mean for different groups, or for multiple groups:\n\nCodedata |&gt;\n  group_by(category) |&gt;\n  summarise(\n    mean_emissions = mean(emissions, na.rm = TRUE),\n  )\n\n# A tibble: 2 × 2\n  category                     mean_emissions\n  &lt;chr&gt;                                 &lt;dbl&gt;\n1 Corporate emissions                  35594.\n2 Product life cycle emissions       5630000",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "The Data Analysis Workflow"
    ]
  },
  {
    "objectID": "Week5/3-analysis-workflow.html#visualize",
    "href": "Week5/3-analysis-workflow.html#visualize",
    "title": "The Data Analysis Workflow",
    "section": "",
    "text": "Codedata |&gt;\n  group_by(category, fiscal_year) |&gt;\n  summarise(emissions = sum(emissions, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = fiscal_year, y = emissions, color = category)) +\n  geom_line()",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "The Data Analysis Workflow"
    ]
  },
  {
    "objectID": "Week5/3-analysis-workflow.html#communicate",
    "href": "Week5/3-analysis-workflow.html#communicate",
    "title": "The Data Analysis Workflow",
    "section": "",
    "text": "This is where we will dive into using Quarto. Start by downloading the Apple Emissions dataset from Moodle and open RStudio.\nWe’ll go through how to create and write a full analysis in a .qmd file using this dataset.\nRefer to our lecture notes specifically on using Quarto",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "The Data Analysis Workflow"
    ]
  },
  {
    "objectID": "Week5/3-analysis-workflow.html#references",
    "href": "Week5/3-analysis-workflow.html#references",
    "title": "The Data Analysis Workflow",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "The Data Analysis Workflow"
    ]
  },
  {
    "objectID": "Week5/notes.html",
    "href": "Week5/notes.html",
    "title": "Communicating Statistics",
    "section": "",
    "text": "Slides\n\n Download PDF Slides",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Communicating Statistics"
    ]
  },
  {
    "objectID": "Week5/notes.html#this-weeks-lecture",
    "href": "Week5/notes.html#this-weeks-lecture",
    "title": "Communicating Statistics",
    "section": "",
    "text": "Slides\n\n Download PDF Slides",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Week 5",
      "Communicating Statistics"
    ]
  },
  {
    "objectID": "Week5/sampling-exercise-review.html",
    "href": "Week5/sampling-exercise-review.html",
    "title": "Sampling Exercise Review",
    "section": "",
    "text": "You can download the .R file and data file here:\nDownload Resources",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Sampling Exercise & Quarto",
      "Sampling Exercise Review"
    ]
  },
  {
    "objectID": "Week5/sampling-exercise-review.html#saving-and-running-code-in-a-.r-script",
    "href": "Week5/sampling-exercise-review.html#saving-and-running-code-in-a-.r-script",
    "title": "Sampling Exercise Review",
    "section": "Saving and running code in a .R script",
    "text": "Saving and running code in a .R script\nBefore diving into how a Quarto document works, let’s review how .R files work:\nA .R script works very much like running single lines of code in the console - it will run each line in order from top to bottom. If you run it from RStudio it will even echo the lines of code into the console so you can track exactly what is happening.\nBy saving your code in a .R file you can:\n\nSave your code rather than needing to type it or copy/paste it into the console line by line.\nBuild up a full script to perform several actions at once.\n\nFor instance, to prepare the AI Jobs dataset we looked at in class, I wrote a script to download the data, clean it, and make some adjustments, then save it to a .csv file. Rather than write this each time, I can just source the ai-jobs-data.R script to do it all at once.\nYou can see all the steps of that code in the ai-jobs-data.R file. Try to look through the code and identify the blocks of code that logically fit together - in other words, the multiple lines of code which are grouped together because they do the same thing or because they form one step of the process. I have added comments to outline these for you. Even if you don’t know what every line or function does, you should be able to follow the logical flow of what the code does.\n\n\nSo, in our sampling-exercise.R file we completed a few logical steps:\n\nFirst, we need to load the tidyverse library in order to access our standard data processing functions:\nlibrary(tidyverse)\nThen, we input the observations from each sample we took and saved them to a sample_x variable. These are all lists (using c()) of 10 numbers.\nsample_1 &lt;- c(83.2, 82.6, 82.6, 82.6, 93.2, 94, 94, 48.5, 33.6, 33.6)\nsample_2 &lt;- c(88, 48, 23, 23, 23, 23, 23, 23, 23, 23)\n# etc. ...\nNext, we calculated the means for each sample. I showed two ways of doing this, let’s look at just the first for now.\nWe calculated the mean of the list of numbers of each sample:\nmean_1 &lt;- mean(sample_1)\nmean_2 &lt;- mean(sample_2)\n# etc. ...\nThen we put these means together into a table:\nsample_means &lt;- tibble(mean_values = c(mean_1, mean_2, ...))\nThis gives us a table with one row for each sample we took and a column named mean_values which contains the means we calculated:\n\n\n\n\nmean_values\n\n\n\n\n1\n72.8\n\n\n2\n32\n\n\n…\n…\n\n\n\n\nThis is where we stopped in class, but compare the steps I just outlined with the code you wrote in your own .R file and you should be able to identify these logical blocks of code.\nBy ‘sourcing’ the file, we can run this all at once and either print out our sample_means table, or take a look at it within RStudio.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Sampling Exercise & Quarto",
      "Sampling Exercise Review"
    ]
  },
  {
    "objectID": "Week5/sampling-exercise-review.html#workflow-for-a-.r-file",
    "href": "Week5/sampling-exercise-review.html#workflow-for-a-.r-file",
    "title": "Sampling Exercise Review",
    "section": "Workflow for a .R file",
    "text": "Workflow for a .R file\n\n\n\n\n\nRStudio gives you some useful tools for when you are writing an .R file.\n\nTo run the whole file (what we call ‘source’-ing the file, you can press the ‘Source’ button at the top right.\n\nBy default, this will print out just the filepath to your console (e.g. source(\"~/Documents/UCL/Teaching/BSSC0021_25/Code/Week4/sampling-exercise.R\") ) and display any outputs like plots.\nIf you select the arrow next to it, you can choose ‘Source with Echo’. This will print out each row of code to the console (or ‘echo’ it) as the code runs. This is useful if you want to check exactly what is happening.\n\n\nBy selecting only certain rows and clicking the ‘Run’ button, RStudio will run only those lines of code. This is very useful as you are writing your code and building up a full script. You can check what each part does as you go without needing to run the whole file at once.\n\nA suggested workflow for writing a script is to move back and forth between the .R file editor and the console. Build your code up in the .R file and run each logical chunk of code as you write it to make sure it works the way you expect.\nAnything that you need to run which is temporary or a one-off, type this directly in the console (like if you need to look at the help page for a function such as mean, you would run ?mean in the console to bring it up).\n\n\nHint: To repeat a previously run line of code in the console, press the up arrow - this will cycle back through the history of commands you have run. Once you get to the one you want, just press enter to run it again.\nOnce you have a few chunks of code in the .R file, you can run ‘Source’ to check that the whole thing works top to bottom.",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Sampling Exercise & Quarto",
      "Sampling Exercise Review"
    ]
  },
  {
    "objectID": "Week5/sampling-exercise-review.html#appendix",
    "href": "Week5/sampling-exercise-review.html#appendix",
    "title": "Sampling Exercise Review",
    "section": "Appendix",
    "text": "Appendix\nsampling-exercise.R :\n# Load the tidyverse package\n# This will provide us with functions like `tibble`, `gather`, `group_by`, `summarise`, `ggplot`, etc.\nlibrary(tidyverse)\n\n# Input the values of the samples\n\nsample_1 &lt;- c(83.2, 82.6, 82.6, 82.6, 93.2, 94, 94, 48.5, 33.6, 33.6)\nsample_2 &lt;- c(48.8, 86.5, 67.5, 84.5, 97.6, 92, 60.7, 108, 84.3, 58.5)\nsample_3 &lt;- c(128, 53.7, 70.9, 75.2, 84.9, 91.2, 70.2, 82, 88.8, 82)\nsample_4 &lt;- c(122, 54, 101, 93.2, 89.4, 64.9, 68.3, 97.7, 77.7, 123)\nsample_5 &lt;- c(92.2, 82, 97.7, 48.5, 94, 70.6, 105, 60.7, 65.1, 82.3)\nsample_6 &lt;- c(91.2, 110, 57.7, 48.4, 122, 65.5, 86.5, 62.7, 62.7, 85.6)\nsample_7 &lt;- c(87.5, 72.8, 84.8, 56.5, 64.9, 42.2, 62.8, 54.1, 84.4, 89.7)\nsample_8 &lt;- c(78.8, 48.5, 54, 70.4, 43.8, 65.5, 78.8, 43.8, 113, 123)\nsample_9 &lt;- c(89.5, 125, 94, 92.4, 70, 99, 111, 96.9, 64.2, 63.7)\nsample_10 &lt;- c(102, 90.8, 110, 123, 79.4, 77.9, 82.3, 92.6, 90.8, 113)\nsample_11 &lt;- c(86.6, 83, 58.9, 101, 54.9, 96.8, 84.8, 60.4, 84, 83.3)\nsample_12 &lt;- c(63.7, 102, 84.3, 54.1, 71.6, 122, 40.8, 63.7, 84.4, 90.9)\nsample_13 &lt;- c(123, 93.5, 68.3, 97.3, 53.1, 50.2, 130, 48.5, 56.5, 65.1)\nsample_14 &lt;- c(109, 85.6, 54.9, 62.7, 30.3, 87.1, 94, 111, 54.1, 54.3)\nsample_15 &lt;- c(109, 44.2, 99.3, 58.5, 77, 86.2, 125, 128, 79.1, 98.2)\nsample_16 &lt;- c(41.8, 122, 70.6, 86.3, 83.7, 84.3, 82.9, 41.3, 72.7, 98.2)\nsample_17 &lt;- c(63.7, 89.8, 101, 70.2, 68.4, 77.9, 105, 53.1, 112, 55.8)\nsample_18 &lt;- c(48.8, 86.5, 67.5, 84.5, 97.6, 92, 60.7, 108, 84.3, 58.5)\nsample_19 &lt;- c(92.6, 82.3, 57.1, 57.1, 82, 62.7, 62.7, 82, 82, 77)\nsample_20 &lt;- c(67, 64.5, 87.8, 84.5, 43.8, 67.2, 30.3, 107, 85.2, 67.2)\nsample_21 &lt;- c(66.8, 85.6, 123, 109, 97.7, 89.7, 51.9, 48.5, 81.7, 51.9)\nsample_22 &lt;- c(97.1, 85.2, 118, 56.5, 91.2, 91.8, 59.3, 111, 93.2, 75.9)\nsample_23 &lt;- c(111, 64.4, 137, 82.3, 94, 64.9, 39, 75.2, 91.2, 63.8)\nsample_24 &lt;- c(97.7, 50.2, 78.8, 119, 109, 119, 40.8, 72.8, 97.1, 102)\nsample_25 &lt;- c(58.9, 79.8, 91.8, 56.5, 46.1, 84.4, 71.2, 71.2, 86.2, 70.6)\nsample_26 &lt;- c(75.8, 73.8, 57.7, 129, 101, 71.2, 137, 135, 60.3, 77)\nsample_27 &lt;- c(56.5, 52.3, 99.3, 107, 75.9, 89.2, 70, 84.3, 66.8, 93.5)\nsample_28 &lt;- c(28.3, 90.8, 39, 82.3, 89.5, 58.1, 120, 74.7, 28.8, 117)\nsample_29 &lt;- c(97.7, 85.4, 91.1, 54.1, 109, 102, 82, 90.8, 63.7, 92.3)\nsample_30 &lt;- c(101, 68.3, 62.8, 65.5, 75.8, 93.8, 81.7, 72.8, 63.4, 109)\n\n# Create a table with the samples and an id column\n\ndata &lt;- tibble(\n  sample_1, sample_2, sample_3, sample_4, sample_5,\n  sample_6, sample_7, sample_8, sample_9, sample_10,\n  sample_11, sample_12, sample_13, sample_14, sample_15,\n  sample_16, sample_17, sample_18, sample_19, sample_20,\n  sample_21, sample_22, sample_23, sample_24, sample_25,\n  sample_26, sample_27, sample_28, sample_29, sample_30\n) |&gt;\n  gather(key = \"sample\", value = \"value\")\n\ndata # Look at the table\n\n# Calculate the mean of each sample\n\nsample_means &lt;- data |&gt;\n  group_by(sample) |&gt;\n  summarise(mean = mean(value))\n\nsample_means # Look at the means\n#\n# # Plot the sampling distribution\n#\nggplot(sample_means, mapping = aes(x = mean)) +\n  geom_histogram() +\n  theme_minimal()",
    "crumbs": [
      "Weekly Lecture Notes",
      "Notes",
      "Sampling Exercise & Quarto",
      "Sampling Exercise Review"
    ]
  },
  {
    "objectID": "WeekNew/1-content.html",
    "href": "WeekNew/1-content.html",
    "title": "Section 1",
    "section": "",
    "text": "Section 1"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BSSC0021",
    "section": "",
    "text": "Lecture Notes Website\n\n\n\n\nThis is a website for hosting the lecture notes and slides for the 2024-25 Business Statistics and Data Analytics module.\nModule Leader: Dr. Andrew Mitchell\nYou can find the weekly lecture notes and slides by clicking on the links in the menu above. Each week includes a landing page for the week with links to the slides. Continue on to the next page for accompanying extended lecture notes."
  },
  {
    "objectID": "office-hours.html",
    "href": "office-hours.html",
    "title": "Office Hours and Programming Support",
    "section": "",
    "text": "Office Hours and Programming Support\nAndrew hosts office hours and programming support every Monday at 2pm. You can find the Teams link on the course Moodle page.\nPlease stop by and introduce yourself! Come in for any help you need with your programming, to ask questions about the course, or to just chat."
  }
]