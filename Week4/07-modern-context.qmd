---
title: "NHST in Modern Context"
subtitle: "Multiple Testing and Contemporary Challenges"
---

## The Scale of Modern Research

Contemporary Challenges:

1. Data Volume
   - Millions of variables per person
   - High-throughput methods
   - Genome-wide studies
   - Brain imaging research

2. Analysis Complexity
   - Multiple comparisons
   - Big data analytics
   - Complex relationships
   - Interdependencies

3. Statistical Implications
   - Multiple testing problems
   - Error rate inflation
   - Need for correction
   - Reproducibility issues

## Example: Genome-Wide Association Studies

Understanding GWAS:

1. The Science
   - Study genetic basis of traits
   - Schizophrenia example
   - 80% genetic contribution
   - Complex inheritance

2. The Method
   - Scan entire genome
   - Million+ locations
   - Compare between groups
   - Statistical tests at each site

3. The Challenge
   - Massive number of tests
   - Type I error inflation
   - Need for correction
   - Balance sensitivity/specificity

## The Multiple Testing Problem

What happens without correction:

1. Basic Math
   - α = 0.05 per test
   - 1,000,000 tests
   - Expected false positives
   - n * α = 50,000 errors!

2. Familywise Error
   - Error in any test
   - Almost guaranteed
   - Needs control
   - Critical for validity

## Visualizing the Problem

```{r mult-test-sim, echo=FALSE}
# Simulate studies with multiple tests
set.seed(123)
nRuns <- 1000
nTests <- 1000

# Uncorrected alpha
uncAlpha <- 0.05
uncOutcome <- replicate(nRuns, sum(rnorm(nTests) < qnorm(uncAlpha)))

# Create visualization
ggplot(data.frame(nsig=uncOutcome), aes(nsig)) +
  geom_histogram(bins=30) +
  xlab('Number of Significant Results') +
  ylab('Frequency') +
  ggtitle('False Positives Under Null (No Correction)')
```

## Understanding False Positives

What the simulation shows:

1. Distribution
   - Centered around 5%
   - Consistent pattern
   - Predictable errors

2. Implications
   - Many false positives
   - Cannot trust results
   - Need correction
   - Balance required

## The Bonferroni Solution

A Simple Approach:

1. The Method
   - Divide α by number of tests
   - Controls familywise error
   - Very conservative
   - Easy to implement

2. Example Calculation
   - Starting α = 0.05
   - 1,000,000 tests
   - New α = 0.05/1,000,000
   - = 0.00000005

## Effect of Bonferroni Correction

```{r correction-effect, echo=FALSE}
# Corrected alpha
corAlpha <- 0.05 / nTests
corOutcome <- replicate(nRuns, sum(rnorm(nTests) < qnorm(corAlpha)))

# Combine data
results <- data.frame(
  value = c(uncOutcome, corOutcome),
  type = rep(c("Uncorrected", "Bonferroni"), each=nRuns)
)

# Visualization
ggplot(results, aes(value, fill=type)) +
  geom_histogram(position="dodge", bins=30) +
  xlab('Number of Significant Results') +
  ylab('Frequency') +
  ggtitle('Impact of Bonferroni Correction')
```

## Understanding the Correction

Key Points:

1. Benefits
   - Controls false positives
   - Strong error control
   - Clear interpretation
   - Simple to apply

2. Limitations
   - Very conservative
   - Misses real effects
   - Power reduction
   - May be too strict

## Modern Alternative Approaches

Advanced Methods:

1. False Discovery Rate (FDR)
   - Controls proportion of errors
   - Less conservative
   - Better power
   - Widely used

2. Resampling Methods
   - Permutation tests
   - Bootstrap approaches
   - Data-driven
   - Computationally intensive

3. Hierarchical Testing
   - Structured approaches
   - Domain knowledge
   - Improved power
   - Complex implementation

## Choosing the Right Approach

Decision Factors:

1. Research Goals
   - Confirmatory vs exploratory
   - Required certainty
   - Acceptable error rates
   - Power needs

2. Practical Constraints
   - Computational resources
   - Time limitations
   - Field standards
   - Publication requirements

3. Data Characteristics
   - Number of tests
   - Dependencies
   - Effect sizes
   - Sample sizes

## Best Practices

Recommendations:

1. Study Planning
   - Anticipate multiple tests
   - Choose correction method
   - Power calculations
   - Pre-registration

2. Analysis Execution
   - Document all tests
   - Apply corrections consistently
   - Report all results
   - Transparent methods

3. Result Interpretation
   - Consider context
   - Acknowledge limitations
   - Multiple approaches
   - Clear communication

## Looking Forward

Future Directions:

1. Statistical Methods
   - New correction approaches
   - Better power/control balance
   - Computational advances
   - Integration of methods

2. Research Practice
   - Improved standards
   - Better reporting
   - More transparency
   - Reproducible science

::: {.notes}
So far we have discussed examples where we are interested in testing a single statistical hypothesis, and this is consistent with traditional science which often measured only a few variables at a time. However, in modern science we can often measure millions of variables per individual. For example, in genetic studies that quantify the entire genome, there may be many millions of measures per individual, and in the brain imaging research that my group does, we often collect data from more than 100,000 locations in the brain at once. When standard hypothesis testing is applied in these contexts, bad things can happen unless we take appropriate care.

The multiple testing problem arises because while we control for the error per test, we don't control the error rate across our entire *family* of tests (known as the *familywise error*), which is what we really want to control if we are going to be looking at the results from a large number of tests. Using p<.05, our familywise error rate in genomic studies would be one -- that is, we are pretty much guaranteed to make at least one error in any particular study.

A simple way to control for the familywise error is to divide the alpha level by the number of tests; this is known as the *Bonferroni* correction. This correction effectively controls the familywise error, such that the probability of making *any* errors in our study is controlled at right around .05. However, this correction is very conservative, especially with large numbers of tests, which means we might miss real effects (Type II errors).

Modern research has developed several alternative approaches to handle multiple testing, including False Discovery Rate control and various computational methods. The choice of method depends on research goals, practical constraints, and the specific characteristics of the data and analysis.
::::
