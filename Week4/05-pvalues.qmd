---
title: "Understanding P-values"
subtitle: "Interpretation and Common Misconceptions"
---

## Understanding P-values: A Simple Start

The Coin Flip Example:

1. The Setup
   - Flip a coin 100 times
   - Observe 70 heads
   - Question: Is coin biased?

2. The Hypotheses
   - H0: P(heads) ≤ 0.5
   - HA: P(heads) > 0.5
   - Test statistic: number of heads

3. The Question
   - How likely is 70+ heads?
   - If coin is fair (p = 0.5)?
   - Need probability calculation

## The Binomial Distribution

Mathematical approach to probability:

$$
P(X \le k) = \sum_{i=0}^k \binom{N}{k} p^i (1-p)^{(n-i)}
$$

Key Components:
- k: number of successes (heads)
- N: number of trials
- p: probability of success

For our question:
$$
P(X \ge 70) = 1 - P(X < 70)
$$

## Simulation Approach

```{r coin-sim, echo=FALSE}
# simulate coin flips
tossCoins <- function() {
  flips <- runif(100) > 0.5 
  return(sum(flips))
}

# use a large number of replications
coinFlips <- replicate(100000, tossCoins())

ggplot(data.frame(coinFlips), aes(coinFlips)) +
  geom_histogram(binwidth = 1) + 
  geom_vline(xintercept = 70, color='red', size=1) +
  xlab('Number of Heads') +
  ylab('Frequency') +
  ggtitle("Distribution of Heads in 100,000 Simulated Trials")
```

## Understanding the Simulation

What the plot shows:

1. Distribution Center
   - Centered at 50 heads
   - Expected for fair coin
   - Symmetric pattern

2. Our Observation
   - 70 heads (red line)
   - Far in right tail
   - Rare under H0

3. Implications
   - Unlikely if coin fair
   - Evidence for bias
   - Quantified by p-value

## The BMI Example: Computing p-values

Steps in calculating p-value:

1. Calculate Test Statistic
   - Compute t-statistic
   - Account for sample sizes
   - Consider variability

2. Determine Distribution
   - t distribution
   - Appropriate degrees of freedom
   - One or two-tailed test

3. Find Probability
   - Area in tail(s)
   - Compare to α level
   - Make decision

## Visualizing the P-value

```{r t-dist-example, echo=FALSE}
# Calculate t-statistic for BMI example
sampleSummary <-
  NHANES_sample %>%
  group_by(PhysActive) %>%
  summarize(
    N = length(BMI),
    mean = mean(BMI),
    sd = sd(BMI)
  )

meanDiff <- diff(sampleSummary$mean)
sumVariance <- sum(sampleSummary$sd^2 / sampleSummary$N)
tStat <- meanDiff / sqrt(sumVariance)

# Plot t distribution with observed value
x <- seq(-4, 4, length.out = 200)
df <- sum(sampleSummary$N) - 2
t_density <- dt(x, df)

ggplot(data.frame(x = x, density = t_density), aes(x, density)) +
  geom_line() +
  geom_vline(xintercept = tStat, color = "red") +
  geom_area(data = subset(data.frame(x = x, density = t_density), 
                         x >= abs(tStat) | x <= -abs(tStat)),
            aes(x = x, y = density), fill = "red", alpha = 0.3) +
  xlab("t-value") +
  ylab("Density") +
  ggtitle("T Distribution with Observed Value and P-value Region")
```

## One-Tailed vs Two-Tailed Tests

Important Considerations:

1. One-Tailed Test
   - Tests direction
   - Smaller p-values
   - Requires strong prior theory
   - Must specify before data

2. Two-Tailed Test
   - Tests any difference
   - More conservative
   - Generally preferred
   - Double the p-value

3. Decision Factors
   - Prior knowledge
   - Research question
   - Field standards
   - Pre-registration

## Common P-value Misinterpretations

::: {.columns}
::: {.column width="50%"}
### What p-values are NOT
- P(null hypothesis is true)
- P(making wrong decision)
- P(replication success)
- Measure of importance
- Effect size indicator
::::

::: {.column width="50%"}
### What p-values ARE
- P(data|H0 true)
- Measure of surprise
- Tool for decisions
- One piece of evidence
- Probability statement
::::
::::

## Understanding P-value Meaning

If p = .01, this does NOT mean:

1. H0 has 1% chance of being true
   - P-value ≠ P(H0|data)
   - Common misconception
   - Inverse probability error

2. 1% chance of wrong decision
   - P-value ≠ P(error)
   - Decision separate issue
   - Depends on many factors

3. 99% chance of replication
   - P-value about current data
   - Not about future studies
   - Replication complex issue

## Best Practices with P-values

1. Reporting
   - Report exact values
   - Include test details
   - Show effect sizes
   - Provide context

2. Interpretation
   - Understand limitations
   - Consider practical significance
   - Use with other evidence
   - Avoid overconfidence

3. Decision Making
   - Pre-specify criteria
   - Consider multiple factors
   - Document decisions
   - Be transparent

::: {.notes}
Key points about p-values:

1. They are about the probability of observing data at least as extreme as what we observed, assuming the null hypothesis is true.

2. They do NOT tell us:
   - The probability that the null hypothesis is true
   - The probability of making a wrong decision
   - The probability of replicating the result
   - Whether the effect is practically important

3. Best practices:
   - Always report exact p-values rather than just "significant" or "non-significant"
   - Consider practical significance alongside statistical significance
   - Use p-values as one piece of evidence among many
   - Never make decisions about how to perform a hypothesis test once you have looked at the data
::::
