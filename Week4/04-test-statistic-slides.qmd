---
title: "Hypothesis Testing"
subtitle: "Part 4: Test Statistics and the t-Distribution"
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(ggplot2)
library(cowplot)
library(knitr)
library(NHANES)

# drop duplicated IDs within the NHANES dataset
NHANES <- 
  NHANES %>% 
  dplyr::distinct(ID,.keep_all = TRUE)

NHANES_adult <- 
  NHANES %>%
  drop_na(PhysActive,BMI) %>%
  subset(Age >= 18)

# Sample data for use in examples
set.seed(123)
sampSize <- 250
NHANES_sample <- NHANES_adult %>%
  sample_n(sampSize)
```

## What is a Test Statistic?

A test statistic helps us make decisions by:

1. Measuring the size of an effect
   - How big is the difference between groups?
   - Is it in the direction we predicted?

2. Accounting for uncertainty
   - How much variation is in our data?
   - How large are our samples?

3. Providing a standardized measure
   - Allows comparison across studies
   - Has known probability distribution

::: {.notes}
We need a way to quantify the evidence in our data relative to the variability. The test statistic provides a standardized measure that accounts for both the size of the effect and the uncertainty in our measurements.
::::

## The t-statistic: An Example

For comparing two means (like BMI between active/inactive):

The t-statistic measures:

 - Difference between group means
 - Divided by uncertainty in that difference
 - Larger values = stronger evidence

```{r t-stat-example}
#| fig-width: 8
#| fig-height: 4
#| echo: false

# Compute means and standard errors
group_stats <- NHANES_sample %>%
  group_by(PhysActive) %>%
  summarise(
    mean = mean(BMI),
    se = sd(BMI)/sqrt(n())
  )

# Create visualization
ggplot(group_stats, aes(x = PhysActive, y = mean)) +
  geom_point(size = 3) +
  geom_errorbar(aes(ymin = mean - se, ymax = mean + se), 
                width = 0.2) +
  labs(title = "Mean BMI by Physical Activity Status",
       subtitle = "Points show means, bars show standard errors",
       x = "Physically Active?",
       y = "Body Mass Index (BMI)") +
  theme_minimal()
```

::: {.notes}
The t-statistic was developed by William Sealy Gossett (pen name "Student") while working at Guinness Brewery. It's particularly useful when working with smaller samples where we don't know the population standard deviation.
::::

## Understanding the t-Distribution {.smaller}

The t-distribution has:

 - Bell shape like normal distribution
 - Heavier tails (more extreme values)
 - Shape depends on sample size
 - Gets closer to normal with more data

```{r t-dist-viz}
#| fig-width: 10
#| fig-height: 5
#| echo: false

# Create visualization comparing t and normal distributions
x <- seq(-4, 4, length.out = 100)
normal_dist <- dnorm(x)
t_dist_4 <- dt(x, df = 4)
t_dist_1000 <- dt(x, df = 1000)

df <- data.frame(
  x = rep(x, 3),
  y = c(normal_dist, t_dist_4, t_dist_1000),
  Distribution = rep(c("Normal", "t (df=4)", "t (df=1000)"), each = 100)
)

p1 <- ggplot(df %>% filter(Distribution != "t (df=1000)"), 
             aes(x, y, color = Distribution, linetype = Distribution)) +
  geom_line(size = 1) +
  labs(title = "Small Sample (df = 4)",
       subtitle = "t-distribution has heavier tails",
       x = "Value",
       y = "Density") +
  theme_minimal()

p2 <- ggplot(df %>% filter(Distribution != "t (df=4)"), 
             aes(x, y, color = Distribution, linetype = Distribution)) +
  geom_line(size = 1) +
  labs(title = "Large Sample (df = 1000)",
       subtitle = "t-distribution nearly matches normal",
       x = "Value",
       y = "Density") +
  theme_minimal()

plot_grid(p1, p2)
```

::: {.notes}
The t-distribution accounts for the extra uncertainty we have when working with small samples. With larger samples, we become more certain about our estimates, and the distribution becomes more like the normal distribution.
::::

## Dealing with Unequal Groups {.smaller}

::: {.columns}
::: {.column width="50%"}

When comparing groups, we need to consider:

 - Different group sizes
 - Different amounts of variation
 - Need to adjust our calculations
::::
::: {.column width="50%"}

This is why we use Welch's t-test:

 - Accounts for unequal variances
 - Adjusts degrees of freedom 
 - More conservative than standard t-test
::::
::::

```{r welch-viz}
#| fig-width: 8
#| fig-height: 4
#| echo: false

# Visualize variance differences in our data
ggplot(NHANES_sample, aes(PhysActive, BMI)) +
  geom_violin(aes(fill = PhysActive), alpha = 0.3) +
  geom_boxplot(width = 0.2) +
  labs(title = "BMI Distribution by Physical Activity",
       subtitle = "Notice different spreads between groups",
       x = "Physically Active?",
       y = "Body Mass Index (BMI)") +
  theme_minimal() +
  theme(legend.position = "none")
```

::: {.notes}
When groups have different sizes or variances, we need to adjust our calculations. Welch's t-test modifies the standard t-test to account for these differences, making it more appropriate for real-world data where groups are often unequal.
::::

## Our BMI Example Results

Let's compute the t-test for our BMI data:

```{r t-stat}
#| fig-width: 8
#| fig-height: 4
#| echo: false

# Compute t-test
t_test_result <- t.test(BMI ~ PhysActive, data = NHANES_sample)

# Visualize the distributions with t-statistic
ggplot(NHANES_sample, aes(x = BMI, fill = PhysActive)) +
  geom_density(alpha = 0.5) +
  geom_vline(data = NHANES_sample %>% 
               group_by(PhysActive) %>% 
               summarize(mean = mean(BMI)),
             aes(xintercept = mean, color = PhysActive),
             linetype = "dashed") +
  annotate("text", x = 30, y = 0.06,
           label = sprintf("t-statistic = %.2f\nDegrees of freedom = %.1f\np-value = %.4f",
                          t_test_result$statistic,
                          t_test_result$parameter,
                          t_test_result$p.value)) +
  labs(title = "BMI Distribution by Physical Activity",
       subtitle = "Dashed lines show group means",
       x = "Body Mass Index (BMI)",
       y = "Density") +
  theme_minimal()
```

Key findings:

 - Clear difference between group means
 - Considerable overlap in distributions
 - Significant t-statistic (we'll learn about p-values next)

::: {.notes}
Here we see the actual results of our t-test comparing BMI between physically active and inactive groups. The visualization shows both the raw data distributions and the key test statistics. The overlap in distributions shows why we need formal statistical testing - the difference isn't immediately obvious just by looking at the data.
::::
