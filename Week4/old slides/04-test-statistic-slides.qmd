---
title: "Hypothesis Testing"
subtitle: "Part 4: Deep Dive into Test Statistics"
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(ggplot2)
library(cowplot)
library(knitr)
library(NHANES)

# drop duplicated IDs within the NHANES dataset
NHANES <- 
  NHANES %>% 
  dplyr::distinct(ID,.keep_all = TRUE)

NHANES_adult <- 
  NHANES %>%
  drop_na(PhysActive,BMI) %>%
  subset(Age >= 18)

# Sample data for use in examples
set.seed(123)
sampSize <- 250
NHANES_sample <- NHANES_adult %>%
  sample_n(sampSize)
```

## Historical Development {.smaller}

::: {.panel-tabset}

### The Student's Story

::: {.columns}
::: {.column width="60%"}
**William Sealy Gossett (1876-1937)**

- Worked at Guinness Brewery
- Needed to test small samples
- Published as "Student"
- Developed t-distribution

**Why "Student"?**

- Company policy
- Couldn't use real name
- Now known as "Student's t"
- Revolutionary for small samples
::::

::: {.column width="40%"}
![William Sealy Gossett](https://upload.wikimedia.org/wikipedia/commons/4/42/William_Sealy_Gosset.jpg){width=300}

*The man behind "Student's" t-test*
::::
::::

### The Brewery Problem

::: {.columns}
::: {.column width="50%"}
**Practical Need:**

- Small batch testing
- Quality control
- Cost constraints
- Quick decisions

**Challenge:**
- Unknown population SD
- Small samples
- Need reliability
- Industrial setting
::::

::: {.column width="50%"}
**Solution:**

- Account for uncertainty
- Use sample variance
- Adjust critical values
- Control errors
::::
::::
::::

::: {.notes}
Historical context:
1. Gossett's work at Guinness led to fundamental statistical innovation
2. Real-world problems drove theoretical development
3. Small sample statistics remain important today
4. Shows link between industry and statistics
::::

## Types of Test Statistics {.smaller}

::: {.panel-tabset}

### Common Tests

::: {.columns}
::: {.column width="33%"}
**t-statistic**

- Compare means
- Small samples
- Unknown variance
- Most common
::::

::: {.column width="33%"}
**z-statistic**

- Large samples
- Known variance
- Population parameters
- Normal distribution
::::

::: {.column width="33%"}
**F-statistic**

- Compare variances
- Multiple groups
- ANOVA
- Chi-square related
::::
::::

### Relationships

```{r test-relationships}
#| fig-width: 10
#| fig-height: 6
#| echo: false

# Create visualization of relationships between distributions
x <- seq(-4, 4, length.out = 200)
z_dist <- dnorm(x)
t_dist <- dt(x, df = 5)
f_dist <- df(x[x > 0], df1 = 5, df2 = 30)

# Plot distributions
p1 <- ggplot(data.frame(x = x, y = z_dist), aes(x, y)) +
  geom_line(color = "blue") +
  geom_line(data = data.frame(x = x, y = t_dist), 
            aes(x, y), color = "red", linetype = "dashed") +
  labs(title = "z vs t Distribution",
       x = "Value",
       y = "Density") +
  theme_minimal()

p2 <- ggplot(data.frame(x = x[x > 0], y = f_dist), aes(x, y)) +
  geom_line(color = "green") +
  labs(title = "F Distribution",
       x = "Value",
       y = "Density") +
  theme_minimal()

gridExtra::grid.arrange(p1, p2, ncol = 2)
```

### When to Use What

::: {.columns}
::: {.column width="50%"}
**Sample Size Matters:**

Small (n < 30):

- Use t-tests
- More conservative
- Accounts for uncertainty

Large (n ≥ 30):

- z-tests okay
- More powerful
- CLT applies
::::

::: {.column width="50%"}
**Other Considerations:**

1. Known Parameters?
   - Yes → z-test
   - No → t-test

2. Multiple Groups?
   - Two → t-test
   - More → F-test/ANOVA

3. Type of Data?
   - Continuous → t/F
   - Categorical → Chi-square
::::
::::
::::

::: {.notes}
Key points about test statistics:
1. Different tests for different situations
2. Sample size affects choice
3. Data type matters
4. Consider assumptions
::::

## The t-Distribution Family {.smaller}

::: {.panel-tabset}

### Degrees of Freedom

```{r df-effect}
#| fig-width: 10
#| fig-height: 6
#| echo: false

# Create visualization of t-distributions with different df
x <- seq(-4, 4, length.out = 200)
df_values <- c(1, 2, 5, 30)
t_dists <- map_dfc(df_values, ~dt(x, df = .x))
names(t_dists) <- paste0("df_", df_values)
t_dists$x <- x

t_dists_long <- t_dists %>%
  pivot_longer(-x, names_to = "df", values_to = "density")

ggplot(t_dists_long, aes(x = x, y = density, color = df)) +
  geom_line(size = 1) +
  geom_line(data = data.frame(x = x, y = dnorm(x)), 
            aes(x, y), color = "black", linetype = "dashed") +
  labs(title = "t-distributions with Different Degrees of Freedom",
       subtitle = "Dashed line shows normal distribution",
       x = "Value",
       y = "Density") +
  theme_minimal()
```

### Tail Behavior

::: {.columns}
::: {.column width="50%"}
**Small df:**

- Heavier tails
- More uncertainty
- More conservative
- Used for small samples
::::

::: {.column width="50%"}
**Large df:**

- Approaches normal
- Less uncertainty
- More powerful
- Used for large samples
::::
::::

### Interactive Example

```{r interactive-t}
#| fig-width: 10
#| fig-height: 6
#| echo: false

# Create interactive-style visualization
sample_sizes <- c(5, 10, 30, 100)
results <- map_dfr(sample_sizes, function(n) {
  samples <- replicate(1000, {
    x <- rnorm(n)
    t.test(x)$statistic
  })
  data.frame(
    t_value = samples,
    n = paste("n =", n)
  )
})

ggplot(results, aes(x = t_value)) +
  geom_density(aes(fill = n), alpha = 0.3) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  labs(title = "Sampling Distribution of t-statistic",
       subtitle = "Different sample sizes show convergence to normal",
       x = "t-value",
       y = "Density") +
  theme_minimal() +
  facet_wrap(~n)
```

::::

::: {.notes}
Key points about t-distribution:
1. Degrees of freedom affect shape
2. Converges to normal with large df
3. More conservative with small samples
4. Accounts for estimation uncertainty
::::

## Beyond the t-test {.smaller}

::: {.panel-tabset}

### Other Common Tests

::: {.columns}
::: {.column width="50%"}
**Parametric Tests:**

1. One-sample t-test
   - Compare to known value
   - Single group

2. Paired t-test
   - Before/after
   - Matched pairs

3. ANOVA
   - Multiple groups
   - Complex designs
::::

::: {.column width="50%"}
**Non-parametric Tests:**

1. Mann-Whitney U
   - Alternative to t-test
   - Ranks data

2. Wilcoxon signed-rank
   - Paired data
   - Ordinal data

3. Kruskal-Wallis
   - Multiple groups
   - No normality needed
::::
::::

### Choosing a Test

```{dot}
digraph G {
  node [shape=box];
  
  start [label="Start"];
  normal [label="Data\nNormal?"];
  groups [label="How Many\nGroups?"];
  paired [label="Paired\nData?"];
  
  start -> normal;
  normal -> groups [label="Yes"];
  normal -> nonparam [label="No"];
  groups -> ttest [label="Two"];
  groups -> anova [label="Three+"];
  ttest -> paired;
  paired -> pairedT [label="Yes"];
  paired -> indepT [label="No"];
  
  nonparam -> wilcox [label="Two Groups"];
  nonparam -> kw [label="Three+ Groups"];
}
```

### Modern Approaches

::: {.columns}
::: {.column width="50%"}
**Robust Statistics:**

- Resistant to outliers
- Fewer assumptions
- Modern computing
- Better reliability
::::

::: {.column width="50%"}
**Resampling Methods:**

- Bootstrapping
- Permutation tests
- Simulation-based
- Computer intensive
::::
::::
::::

::: {.notes}
Key points about statistical tests:
1. Many options available
2. Choice depends on:
   - Data type
   - Sample size
   - Research question
   - Assumptions met
3. Modern methods offer alternatives
4. Consider multiple approaches
::::
