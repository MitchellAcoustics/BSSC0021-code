---
r-fit-text: true
---

# Part 2: Statistical Sampling {background-color="#1E3D59"}

**Making inferences about populations from samples**

```{r}
#| echo: false
#| message: false
#| warning: false
library(tidyverse)
library(NHANES)
library(cowplot)
pdf.options(encoding = "CP1250")
set_null_device("png")

# create a NHANES dataset without duplicated IDs
NHANES <- NHANES %>%
  distinct(ID, .keep_all = TRUE)

# create a dataset of only adults
NHANES_adult <- NHANES %>%
  filter(!is.na(Height), Age >= 18)
```

## Why Study Sampling? {background-color="#f0f0f0"}

::: columns
::: {.column width="50%"}
**The Power of Sampling:**

Nate Silver's 2012 Election Prediction:

-   Correctly predicted all 50 states
-   Used only 21,000 people
-   To predict 125 million votes
-   Combined data from 21 polls
::::

::: {.column width="50%"}
**Key Insights:**

1.  Small samples can be powerful
2.  Proper methodology is crucial
3.  Combining data improves accuracy
4.  Statistical rigor matters
:::
::::

::: notes
One of the foundational ideas in statistics is that we can make inferences about
an entire population based on a relatively small sample of individuals from that
population.

Anyone living in the United States will be familiar with the concept of sampling
from the political polls that have become a central part of our electoral
process. In some cases, these polls can be incredibly accurate at predicting the
outcomes of elections. The best known example comes from the 2008 and 2012 US
Presidential elections, when the pollster Nate Silver correctly predicted
electoral outcomes for 49/50 states in 2008 and for all 50 states in 2012.

Silver did this by combining data from 21 different polls, which vary in the
degree to which they tend to lean towards either the Republican or Democratic
side. Each of these polls included data from about 1000 likely voters -- meaning
that Silver was able to almost perfectly predict the pattern of votes of more
than 125 million voters using data from only about 21,000 people, along with
other knowledge.
:::

## Sampling Fundamentals {.smaller}

::::::: columns
:::: {.column width="50%"}
::: fragment
1.  **Population vs Sample:**
    -   Population: Entire group of interest
    -   Sample: Subset used for measurement
    -   Goal: Infer population parameters from sample statistics
2.  **Representative Sampling:**
    -   Equal chance of selection
    -   Avoid systematic bias
    -   Random selection crucial
:::
::::

:::: {.column width="50%"}
::: fragment
3.  **Types of Sampling:**
    -   With replacement: Items can be selected multiple times
    -   Without replacement: Items selected only once
    -   Choice affects probability calculations
4.  **Key Terms:**
    -   Parameter: Population value (usually unknown)
    -   Statistic: Sample value (our estimate)
    -   Sampling Error: Difference between statistic and parameter
:::
::::
:::::::

::: notes
Our goal in sampling is to determine the value of a statistic for an entire
population of interest, using just a small subset of the population. We do this
primarily to save time and effort -- why go to the trouble of measuring every
individual in the population when just a small sample is sufficient to
accurately estimate the statistic of interest?

In the election example, the population is all registered voters in the region
being polled, and the sample is the set of 1000 individuals selected by the
polling organization. The way in which we select the sample is critical to
ensuring that the sample is representative of the entire population, which is a
main goal of statistical sampling.

It's important to also distinguish between two different ways of sampling: with
replacement versus without replacement. In sampling with replacement, after a
member of the population has been sampled, they are put back into the pool so
that they can potentially be sampled again. In sampling without replacement,
once a member has been sampled they are not eligible to be sampled again.
:::

## Sampling Error & Distribution {auto-animate="true"}

### Concept

**What is Sampling Error?**

-   Difference between sample and population
-   Varies across samples
-   Affects measurement quality
-   Can be quantified

## Sampling Error & Distribution {auto-animate="true"}

### Concept

::::: columns
::: {.column width="50%"}
```{r}
#| echo: true
#| code-fold: false

# Take 5 samples of 50 adults each
set.seed(123)
samples <- map_df(
  1:5,
  ~{
    NHANES_adult |>
      sample_n(50) |>
      summarise(
        mean_height = mean(Height),
        sd_height = sd(Height),
      )
  }
)
samples
```
:::

::: {.column width="50%"}
```{r}
#| echo: false
#| fig-width: 6
#| fig-height: 6
#| out-width: 100%

# Take 5000 samples and plot distribution
set.seed(123)
samples_large <- map_df(
  1:5000,
  ~{
    NHANES_adult |>
      sample_n(50) |>
      summarise(mean_height = mean(Height))
  }
)

ggplot(samples_large, aes(x = mean_height)) +
  geom_histogram(bins = 50, fill = "blue", alpha = 0.7) +
  geom_vline(xintercept = mean(NHANES_adult$Height), color = "red", linewidth = 1) +
  labs(
    title = "Sampling Distribution of Mean Height",
    x = "Sample Mean Height (cm)",
    y = "Count"
  )
```
:::
:::::

::: notes
Regardless of how representative our sample is, it's likely that the statistic
that we compute from the sample is going to differ at least slightly from the
population parameter. We refer to this as sampling error. If we take multiple
samples, the value of our statistical estimate will also vary from sample to
sample; we refer to this distribution of our statistic across samples as the
sampling distribution.

Sampling error is directly related to the quality of our measurement of the
population. Clearly we want the estimates obtained from our sample to be as
close as possible to the true value of the population parameter. However, even
if our statistic is unbiased (that is, we expect it to have the same value as
the population parameter), the value for any particular estimate will differ
from the population value, and those differences will be greater when the
sampling error is greater.

The visualization shows how sample means distribute around the true population
mean (red line) when we take many samples.
:::

## Standard Error of the Mean

::::: columns
::: {.column width="50%"}
**Definition:**

$SEM = \frac{\hat{\sigma}}{\sqrt{n}}$

Where:

-   $\hat{\sigma}$ is estimated standard deviation
-   $n$ is sample size

**Key Properties:**

-   Measures sampling distribution variability
-   Decreases with larger samples
-   Increases with population variability
:::

::: {.column width="50%"}
**Example with NHANES:**

```{r}
#| echo: true
# Population SEM
pop_sd <- sd(NHANES_adult$Height)
n <- 50
sem_theoretical <- pop_sd / sqrt(n)

# Observed SEM from samples
sem_observed <- sd(samples_large$mean_height)

cat("Theoretical SEM:", round(sem_theoretical, 2), "\n")
cat("Observed SEM:", round(sem_observed, 2))
```
:::
:::::

::: notes
Later in the course it will become essential to be able to characterize how
variable our samples are, in order to make inferences about the sample
statistics. For the mean, we do this using a quantity called the standard error
of the mean (SEM), which one can think of as the standard deviation of the
sampling distribution of the mean.

The formula for the standard error of the mean implies that the quality of our
measurement involves two quantities: the population variability, and the size of
our sample. Because the sample size is the denominator in the formula for SEM, a
larger sample size will yield a smaller SEM when holding the population
variability constant.

We have no control over the population variability, but we do have control over
the sample size. Thus, if we wish to improve our sample statistics (by reducing
their sampling variability) then we should use larger samples. However, the
formula also tells us something very fundamental about statistical sampling --
namely, that the utility of larger samples diminishes with the square root of
the sample size.
:::

## Sample Size Effects {auto-animate="true"}

::: panel-tabset
### Theory

**Impact of Sample Size:**

-   Larger n → Smaller SEM
-   Relationship is not linear
-   Diminishing returns
-   Square root relationship

### Visualization

```{r}
#| echo: false
#| fig-width: 8
#| fig-height: 6
#| out-width: 100%
# Create data for different sample sizes
sample_sizes <- c(10, 30, 50, 100, 200, 500)
results <- map_df(sample_sizes, function(n) {
  samples <- map_df(
    1:1000,
    ~{
      NHANES_adult %>%
        sample_n(n) %>%
        summarise(mean_height = mean(Height))
    }
  )
  data.frame(
    n = n,
    sem = sd(samples$mean_height)
  )
})

ggplot(results, aes(x = n, y = sem)) +
  geom_line(size = 1, color = "blue") +
  geom_point(size = 3, color = "red") +
  labs(
    title = "Standard Error vs Sample Size",
    x = "Sample Size",
    y = "Standard Error of Mean"
  ) +
  scale_x_continuous(breaks = sample_sizes)
```

### Code

```{r}
#| echo: true
#| eval: false

# Compare SEM for different sample sizes
n1 <- 50
n2 <- 200 # 4 times larger

sem1 <- pop_sd / sqrt(n1)
sem2 <- pop_sd / sqrt(n2)

# Improvement factor
improvement <- sem1 / sem2
cat("Improvement factor:", round(improvement, 2))
```
:::

::: notes
The relationship between sample size and standard error is not linear. Doubling
the sample size will not double the quality of the statistics; rather, it will
improve it by a factor of √2. This has important implications for study design
and resource allocation.

The visualization shows how the standard error decreases as sample size
increases, but with diminishing returns. This means that after a certain point,
increasing sample size may not be worth the additional cost and effort.

This relationship is fundamental to statistical power, which we will discuss in
later sections. Understanding this relationship helps researchers make informed
decisions about sample size requirements for their studies.
:::

## The Central Limit Theorem {background-color="#f0f0f0"}

**Key Points:**

1.  As sample size increases:
    -   Sampling distribution becomes normal
    -   Regardless of population distribution
    -   Mean approaches population mean
    -   Variance decreases
2.  Implications:
    -   Enables statistical inference
    -   Justifies normal approximation
    -   Explains real-world patterns

The Central Limit Theorem tells us that as sample sizes get larger, the sampling
distribution of the mean will become normally distributed, even if the data
within each sample are not normally distributed. **This is a powerful result
that allows us to make inferences about population parameters based on sample
statistics.**

## The Central Limit Theorem {background-color="#f0f0f0"}

### Normal Distribution:

::::: columns
::: {.column width="40%"}
-   Bell-shaped curve
-   Defined by mean ($\mu$) and SD ($\sigma$)
-   Symmetric around mean
:::

::: {.column width="60%"}
```{r}
#| echo: false
#| fig-width: 6
#| fig-height: 5
#| out-width: 95%
# Create simulated normal distribution plot
ggplot(data.frame(x = seq(-4, 4, length.out = 200)), aes(x)) +
  # Add shaded regions with stronger colors
  stat_function(fun = dnorm, geom = "area", fill = "#0066CC", alpha = 0.4,
                xlim = c(-1, 1)) +
  stat_function(fun = dnorm, geom = "area", fill = "#3399FF", alpha = 0.3,
                xlim = c(-2, -1)) +
  stat_function(fun = dnorm, geom = "area", fill = "#3399FF", alpha = 0.3,
                xlim = c(1, 2)) +
  stat_function(fun = dnorm, geom = "area", fill = "#66B2FF", alpha = 0.2,
                xlim = c(-3, -2)) +
  stat_function(fun = dnorm, geom = "area", fill = "#66B2FF", alpha = 0.2,
                xlim = c(2, 3)) +
  # Add the main curve
  stat_function(fun = dnorm, color = "#003366", size = 1.2) +
  # Add vertical lines for mean
  geom_vline(xintercept = 0, linetype = "dashed", color = "#CC0000", alpha = 0.7) +
  # Add annotations with better positioning
  annotate("text", x = 0, y = 0.275, label = "68%\n(±1\u03c3)", size = 4, fontface = "bold") +
  annotate("text", x = 1.5, y = 0.06, label = "95%\n(±2\u03c3)", size = 4, fontface = "bold") +
  annotate("text", x = 3, y = 0.06, label = "99.7% (±3\u03c3)", size = 4, fontface = "bold") +
  # Customize theme and labels
  labs(title = "Standard Normal Distribution",
       subtitle = "Showing empirical rule percentages and standard deviations (\u03c3)",
       x = "Standard Deviations from Mean (\u03c3)",
       y = "Probability Density") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
    plot.subtitle = element_text(hjust = 0.5, size = 12),
    axis.title = element_text(size = 11),
    panel.grid.minor = element_blank()
  ) +
  scale_x_continuous(breaks = -3:3)
```
:::
:::::

::: notes
The Central Limit Theorem tells us that as sample sizes get larger, the sampling
distribution of the mean will become normally distributed, even if the data
within each sample are not normally distributed.

The normal distribution is described in terms of two parameters: the mean (which
you can think of as the location of the peak), and the standard deviation (which
specifies the width of the distribution). The bell-like shape of the
distribution never changes, only its location and width.

The normal distribution is commonly observed in data collected in the real world
-- and the central limit theorem gives us some insight into why that occurs. For
example, the height of any adult depends on a complex mixture of their genetics
and experience; even if those individual contributions may not be normally
distributed, when we combine them the result is a normal distribution.
:::

## CLT in Action: NHANES Example

::: panel-tabset
### Original Distribution

```{r}
#| echo: false
#| fig-width: 8
#| fig-height: 4
#| out-width: 100%

# Clean alcohol data
NHANES_clean <- NHANES %>%
  drop_na(AlcoholYear)

# Plot original distribution
p1 <- ggplot(NHANES_clean, aes(AlcoholYear)) +
  geom_histogram(bins = 30, fill = "blue", alpha = 0.7) +
  labs(
    title = "Distribution of Alcohol Consumption",
    x = "Days of Alcohol Consumption per Year",
    y = "Count"
  )

# Get sampling distribution
set.seed(123)
samples_alc <- map_df(
  1:5000,
  ~{
    NHANES_clean %>%
      sample_n(50) %>%
      summarise(mean_alcohol = mean(AlcoholYear))
  }
)

# Plot sampling distribution
p2 <- ggplot(samples_alc, aes(mean_alcohol)) +
  geom_histogram(bins = 30, fill = "red", alpha = 0.7) +
  labs(
    title = "Sampling Distribution of Mean",
    x = "Sample Mean Alcohol Consumption",
    y = "Count"
  )

plot_grid(p1, p2)
```

### Code Example

```{r}
#| echo: true
#| eval: false

# Compare skewness
library(moments)
original_skew <- skewness(NHANES_clean$AlcoholYear)
sampling_skew <- skewness(samples_alc$mean_alcohol)

cat("Original Distribution Skewness:", round(original_skew, 2), "\n")
cat("Sampling Distribution Skewness:", round(sampling_skew, 2))
```

### Key Insights

1.  Original data is highly skewed
2.  Sampling distribution is nearly normal
3.  CLT works even with:
    -   Non-normal data
    -   Skewed distributions
    -   Discrete values
4.  Sample size of 50 is sufficient
:::

::: notes
Let's work with the variable AlcoholYear from the NHANES dataset, which is
highly skewed. This distribution is, for lack of a better word, funky -- and
definitely not normally distributed.

Now let's look at the sampling distribution of the mean for this variable.
Despite the clear non-normality of the original data, the sampling distribution
is remarkably close to the normal.

The Central Limit Theorem is important for statistics because it allows us to
safely assume that the sampling distribution of the mean will be normal in most
cases. This means that we can take advantage of statistical techniques that
assume a normal distribution.
:::

## Summary

::::::: columns
:::: {.column width="50%"}
::: fragment
1.  **Sampling Fundamentals:**
    -   Population vs Sample
    -   Representative sampling
    -   With/without replacement
    -   Sampling error
2.  **Standard Error:**
    -   Measures sampling variability
    -   Decreases with √n
    -   Guides sample size decisions
    -   Quantifies precision
:::
::::

:::: {.column width="50%"}
::: fragment
3.  **Central Limit Theorem:**
    -   Sampling distribution normality
    -   Independent of original distribution
    -   Enables statistical inference
    -   Foundation for hypothesis testing
4.  **Applications:**
    -   Political polling
    -   Clinical trials
    -   Quality control
    -   Research design
:::
::::
:::::::

::: notes
In this lecture, we covered: - The fundamentals of statistical sampling and why
it works - How to characterize sampling error and the sampling distribution -
The standard error of the mean and its relationship with sample size - The
Central Limit Theorem and its importance in statistical inference - Real-world
applications and examples using the NHANES dataset
:::
