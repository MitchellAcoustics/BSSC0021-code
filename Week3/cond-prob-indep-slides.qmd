---
title: "Computing Probabilities"
subtitle: "Conditional Probability, Multiplication Rule, and Independence"
draft: true
execute: 
  eval: false
---

```{r}
#| echo: false
#| message: false
#| warning: false

library(tidyverse)
library(stat20data)
library(infer)
library(patchwork)
```

::: {.content-visible unless-profile="book"}

# Part 2: Computing Probabilities {background-color="#40666e"}

::::

## The Cost of Statistical Misunderstanding {background-image="images/sally-clark-headline.jpeg" background-opacity="0.3" transition="zoom"}

::: {.column width="100%"}

A Tragic Case of Statistical Misunderstanding:

- Sally Clark: English solicitor convicted of murdering her two infant sons (1999)
- First son Christopher: died at 11 weeks (1996)
- Second son Harry: died at 8 weeks (1998)
- Initial assumption: First death was SIDS (Sudden Infant Death Syndrome)
- Key testimony: Sir Roy Meadow's statistical evidence
- Meadow's Law: "One sudden infant death is a tragedy, two is suspicious and three is murder until proved otherwise"

::::

::: {.notes}
[I]n November 1999, Sally Clark, an English solicitor, was convicted of murdering her infant sons. The first, Christopher, had been 11 weeks old when he died, in 1996, and the second, Harry, 8 weeks old, in January 1998, when he was found dead. Christopher was believed to have been a victim of "cot death", called SIDS (Sudden Infant Death Syndrome) in the US. After her second baby, Harry, also died in his crib, Sally Clark was arrested for murder. The star witness for the prosecution was a well known pediatrician and professor, Sir Roy Meadow, who authored the infamous Meadow's Law :"One sudden infant death is a tragedy, two is suspicious and three is murder until proved otherwise". Unfortunately it was easier to comprehend this "crude aphorism" than make the effort to understand the subtleties of conditional probability. The Royal Statistical Society protested the misuse of statistics in courts, but not early enough to prevent Sally Clark's conviction. She was eventually acquitted and released, only to die at the age of 42 through alcohol poisoning.
::::

## The Statistical Error {.smaller transition="fade"}

::: {.columns}

::: {.column width="60%"}
**Meadow's Flawed Calculation:**

- Individual SIDS probability: 1 in 8,543
- Assumed independent events
- Multiplied probabilities: 1/8,543 × 1/8,543
- Claimed probability: 1 in 73 million

**Reality:**

- Second SIDS death probability: 1 in 60 (given first death)
- Genetic and environmental factors ignored
- Prosecutor's Fallacy: Confused P(evidence|innocence) with P(innocence|evidence)
- Suppressed evidence: Second baby had severe infection
::::

::: {.column width="40%"}
![Sally Clark after her successful appeal](images/sally-clark.png)
::::

::::

::: {.callout-important appearance="minimal"}
This case demonstrates why understanding probability concepts is crucial in real-world applications
::::

::: {.notes}
The math presented by Meadow, in brief: Based on various studies, there is a probability of 1 in 8,543 of a baby dying of SIDS in a family such as the Clarks. As the Clarks suffered *two* deaths, Meadow multiplied 8,543 by 8,543 to arrive at 73 million. He told the jury that the chance or probability that the event of two "cot deaths" was 1 in 73 million. The defense did not employ a statistician to refute her claim, a choice that may have been disastrous for Sally Clark.

Two catastrophic errors were committed in her case by the prosecution, and not caught by the defense. The first error was in treating the deaths as independent, and the second was in looking at the wrong probability. It turns out that the probability of a second child dying of "cot death" or SIDS is 1/60 *given* that the first child died of SIDS. This was a massive error, and it turned out that the prosecution suppressed the pathology reports for the second baby, who had a very bad infection and might have died of that. It is also believed that male infants are more likely to suffer cot death.

The second error is an example of what is called the Prosecutor's Fallacy. What is needed is P(innocence|evidence), but it is often confused with (the *much* smaller) P(evidence|innocence). They should have actually compared the probability of innocence given the evidence with the probability of murder given the evidence.
::::

## Understanding Probability Through Games {auto-animate=true}

::: {.panel-tabset}

### De Méré's Games

De Méré's betting games:

1. At least one six in 4 rolls of a die
2. At least one double-six in 24 rolls of two dice

### Simulation Results

```{r}
#| echo: true
#| code-fold: false

set.seed(123)
die <- 1:6
num_simulations <- 1000

# Game 1: At least one six in 4 rolls
die_4 <- replicate(num_simulations, sample(die, 4, replace = TRUE))
prop_wins_game_1 <- mean(apply(die_4, 2, function(x) any(x == 6)))

# Game 2: At least one double six in 24 rolls
pair_dice <- c(2:12)
dice_24 <- replicate(num_simulations, sample(pair_dice, 24, replace = TRUE))
prop_wins_game_2 <- mean(apply(dice_24, 2, function(x) any(x == 12)))

cat("Game 1 probability:", round(prop_wins_game_1, 3), "\n")
cat("Game 2 probability:", round(prop_wins_game_2, 3))
```

### Long-run Frequency

- Probability as proportion of occurrences in many repetitions
- Simulation with 1000 trials shows:
  - Game 1: ~0.518 probability of winning
  - Game 2: ~0.491 probability of winning
::::

::: {.notes}
Recall that De Méré wanted to bet on at least one six in four rolls of a fair die, and also at least one double six in twenty-four rolls of a pair of dice.

In earlier notes, you have seen the result of simulating these two games to estimate the probability of De Méré winning his bet. The simulation used the idea of thinking of the probability of an event as the **long-run relative frequency**, or the proportion of times we observe that particular event (or the outcomes in the event) if we repeat the action that can result in the event over and over again. Let's do that again - that is, we estimate the probabilities using the long-run proportions. We will simulate playing the game over and over again and count the number of times we see at least one six in four rolls, and similarly for the second game. (De Méré did this by betting many times, and noticed that the number of times he won wasn't matching the probability he had computed. Long-run relative frequency in real life!)

We know that if two events are mutually exclusive, we can compute the probability of *at least one* of the events occurring (A∪B aka A or B) using the addition rule P(A∪B) = P(A) + P(B). We *cannot* use the addition rule to compute the probabilities that we have simulated above, since rolling a six on the first roll and rolling a six on the second roll (for example) are *not* mutually exclusive.
::::

## Drawing from a Box: With vs Without Replacement {.smaller}

::: {.columns}

::: {.column width="40%"}
![](images/box-red-blue.jpeg){width="300"}

**Initial Box Contents:**

- 2 red tickets
- 2 blue tickets
::::

::: {.column width="60%"}
**Drawing 3 Times:**

With Replacement:

- Put ticket back after each draw
- All sequences possible
- P(exactly 2 red) = 3/8

Without Replacement:

- Keep tickets out after drawing
- Fewer possible sequences
- P(exactly 2 red) = 3/6

**Key Difference:** Probabilities change when drawing without replacement
::::

::::

::: {.notes}
Consider a box with four tickets in it, two colored red and two blue. Except for their color, they are identical. Suppose we draw three times at random from this box, with replacement, that is, every time we draw a ticket from the box, we put it back before drawing the next ticket. 

Note that since each of the cards is equally likely to be drawn, therefore all the sequences of three cards are equally likely. We can count the number of possible outcomes that contain exactly 2 red cards, and divide that number by the number of total possible outcomes to get the probability of drawing exactly 2 red cards.

There are three outcomes that have exactly two cards, out of a total of 8 possible outcomes, so the probability of exactly two red cards in three draws at random with replacement is 3/8.

Now suppose we repeat the procedure, but draw without replacement (we don't put any tickets back). Notice that we have fewer possible outcomes (6 instead of 8), though they are still equally likely. Again, there are 3 outcomes that have exactly 2 red cards, and so the probability of 2 red cards in three draws is now 3/6.
::::

## Probability Distribution: With vs Without Replacement {.smaller}

::: {.panel-tabset}

### Distribution Table

| Number of reds in 3 draws |  probability, with replacement   | probability, without replacement |
|:---------------:|:--------------------------:|:--------------------------:|
|       0 red tickets       | $\displaystyle \frac{1}{8}$ |             $0$              |
|       1 red ticket        | $\displaystyle \frac{3}{8}$ | $\displaystyle \frac{3}{6}$ |
|       2 red tickets       | $\displaystyle \frac{3}{8}$ | $\displaystyle \frac{3}{6}$ |
|       3 red tickets       | $\displaystyle \frac{1}{8}$ |             $0$              |

### Visual Representation

![](images/without-repl-1.jpeg){fig-align="center" width="800"}

### Key Points

- Without replacement: Box composition changes after each draw
- With replacement: Box composition stays constant
- Affects possible outcomes and their probabilities
- Some outcomes become impossible without replacement (e.g., 3 reds)
::::

::: {.notes}
What about the probabilities for the number of red cards in three draws? Write down all the possible values for the number of red cards in three draws from a box with 2 red cards and 2 blue cards, while drawing with replacement, and their corresponding probabilities. Repeat this exercise for the same quantity (number of red cards in three draws from a box with 2 red cards and 2 blue cards), when you draw the tickets without replacement.

Below you see an illustration of what happens to the box when we draw without replacement, with the box at each stage being shown with one less ticket.

We see that the box reduces after each draw. After two draws, if the first 2 draws are red (as on the left most sequence) you can't get another red ticket, whereas if you are drawing with replacement, you can keep on drawing red tickets. (Note that the outcomes in the bottom row are not equally likely, since on the left branch of the tree, blue is twice as likely as red to be the second card, so the outcome RB is twice as likely as RR, and the outcome BR on the right branch of the tree is twice as likely as BB.)
::::

## Rules of Probability: A Recap {auto-animate=true}

::: {.incremental}

1. Sample Space ($\Omega$):
   - Contains all possible outcomes
   - P($\Omega$) = 1 (certain event)

2. Impossible Event ($\emptyset$ or {}):
   - Contains no outcomes
   - P($\emptyset$) = 0

3. Event Probability:
   - For any event A: 0 ≤ P(A) ≤ 1

4. Mutually Exclusive Events:
   - No outcomes in common
   - If A ∩ B = {}, then P(A ∪ B) = P(A) + P(B)

5. Complement Rule:
   - For event A, complement A^C is "not A"
   - P(A^C) = 1 - P(A)

::::

::: {.notes}
Rules of probability (recap)

1. Ω is the set of all possible outcomes. The probability of Ω is 1. It is called the certain event.

2. When an event has no outcomes in it, it is called the impossible event, and denoted by ∅ or {}. The probability of the impossible event is 0.

3. Let A be a collection of outcomes (for example, from the example above, A could be the event of two red tickets in 3 draws with replacement). Then the probability of A has to be between 0 and 1 (inclusive of 0 and 1).

4. If A and B are two events with no outcomes in common, then they are called mutually exclusive. If A and B have no outcomes in common, that is, A ∩ B = {}, then P(A ∪ B) = P(A) + P(B).

5. Consider an event A. The complement of A is not A, and denoted by A^C. The complement of A consists of all outcomes in Ω that are not in A and P(A^C) = 1-P(A).
::::

## Conditional Probability {background-color="#f0f0f0"}

::: {.columns}

::: {.column width="50%"}
**Definition:**

The probability of event B occurring, given that event A has occurred:

$$ P(B|A) = \frac{P(A \cap B)}{P(A)} $$

**Example with Cards:**

- First draw: Blue card
- P(Red on second | Blue on first) = 2/3
- P(Red on second) without condition = 1/2
::::

::: {.column width="50%"}
**Key Concepts:**

- Probability changes based on known information
- New sample space is restricted to outcomes where A occurred
- P(B|A) may be very different from P(A|B)
- Cannot divide by zero: P(A) must be > 0
::::

::::

::: {.notes}
In the first example above, we saw that the probability of a red ticket on a draw changes if we sample without replacement. If we get a blue ticket on the first draw, the probability of a red ticket on the second draw is 2/3 (since there are 3 tickets left, of which 2 are blue). 

If we get a red ticket on the first draw, the probability of a red ticket on the second draw is 1/3. These probabilities, that depend on what has happened on the first draw, are called conditional probabilities. If A is the event of a blue ticket on the first draw, and B is the event of a red ticket on the second draw, we say that the probability of B given A is 2/3, which is a conditional probability, because we put a condition on the first card, that it had to be blue. 

What about if we don't put a condition on the first card? What is the probability that the second card is red? The probability that the second card drawn is red is 1/2, if we don't have any information about the first card drawn. To see this, it is easier to imagine that we can shuffle all the cards in the box and they are put in some random order in which each of the 4 positions is equally likely. There are 2 red cards, so the probability that a red card will occupy any of the 4 positions, including the second, is 2/4.

This kind of probability, where we put no condition on the first card, is called an unconditional probability - we don't have any information about the first card.
::::

## The Multiplication Rule {background-color="#f8f8f8"}

::: {.columns}

::: {.column width="60%"}
**Computing Intersection Probabilities:**

$P(A \cap B) = P(A) \times P(B|A)$

**Key Points:**

- Probability of both A and B occurring
- Uses conditional probability
- Order can be reversed: $P(A \cap B) = P(B) \times P(A|B)$
- Intersection probability ≤ individual probabilities
::::

::: {.column width="40%"}
![](images/venn-diagram-intersection.png){width="400"}
::::

::::

::: {.notes}
We often want to know the probability that two (or more) events will *both* happen: What is the probability if we roll a pair of dice, that both will show six spots; or if we deal two cards from a standard 52 card deck, that both would be kings, or in a family with two babies, both would suffer SIDS. 

What do we know? We can draw a Venn diagram to represent intersecting events. This picture tells us that A∩B (the purple shaded part) is inside both A and B, so its probability should be less than each of P(A) and P(B): P(A∩B) ≤ P(A), P(B). In fact, we write the probability of the intersection as:

P(A ∩ B) = P(A) × P(B|A)

We read the second probability on the right-hand side of the equation as the conditional probability of B given A. Note that B|A is not an event, but we use P(B|A) as a shorthand for the conditional probability of B given A.
::::

## Example: Committee Selection {auto-animate=true}

::: {.columns}

::: {.column width="50%"}
**Setup:**

- 5 people: Alex, Emi, Fred, Max, Nan
- Select 2 people at random
- Order doesn't matter
- Drawing without replacement

**Question:**
What is P(Alex and Emi are selected)?
::::

::: {.column width="50%"}
![](images/committee.jpeg){width="500"}
::::

::::

::: {.notes}
We have a group of 5 people: Alex, Emi, Fred, Max, and Nan. Two of the five are to be selected at random to form a two person committee. Represent this situation using draws from a box of tickets.

We only care about who is picked, not the order in which they are picked. For instance, picking Alex first and then Emi results in the same committee as picking first Emi and then Alex. 

All the ten pairs are equally likely. On the first draw, there are 5 tickets to choose from, and on the second there are 4, making 5 × 4 = 20 possible draws of two tickets, drawn from this box, one at a time, without replacement. We have only 10 pairs here because of those 20 pairs, there are only 10 distinct ones. When we count 20 pairs, we are counting Alex + Emi as one pair, and Emi + Alex as another pair.
::::

## Solution: Committee Selection {auto-animate=true}

::: {.incremental}

**Using the Multiplication Rule:**

1. Path 1: Alex then Emi
   - P(Alex first) = 1/5
   - P(Emi second | Alex first) = 1/4
   - P(Alex then Emi) = 1/5 × 1/4 = 1/20

2. Path 2: Emi then Alex
   - P(Emi first) = 1/5
   - P(Alex second | Emi first) = 1/4
   - P(Emi then Alex) = 1/5 × 1/4 = 1/20

3. Total Probability:
   - P(Alex and Emi selected) = 1/20 + 1/20 = 1/10

::::

::: {.notes}
What is the probability that Alex and Emi will be selected? We could use the multiplication rule to compute this probability, which is much simpler than writing out all the possible outcomes. The committee can consist of Alex and Emi either if Alex is drawn first and Emi second, or Emi is drawn first and Alex second. 

The probability that Alex will be drawn first is 1/5. The conditional probability that Emi will be drawn second given that Alex was drawn is 1/4 since there are only 4 tickets left in the box. Using the multiplication rule, the probability that Alex will be drawn first and Emi second is (1/4) × (1/5) = 1/20. 

Similarly, the probability that Emi will be drawn first and Alex second is 1/20. This means that the probability that Alex and Emi will be selected for the committee is 1/20 + 1/20 = 1/10.
::::

## Independence {background-color="#f0f0f0"}

::: {.definition}
Two events A and B are **independent** if:

$P(B|A) = P(B)$

That is, knowing that A occurred doesn't change the probability of B occurring.
::::

::: {.callout-note}
## Computational Check for Independence

Check if $P(A \cap B) = P(A) \times P(B)$
::::

::: {.panel-tabset}

### With Replacement

Drawing cards with replacement:

- P(Red on second | Blue on first) = 1/2
- P(Red on second | Red on first) = 1/2
- P(Red on second) = 1/2
- **Conclusion:** Draws are independent

### Without Replacement

Drawing cards without replacement:

- P(Blue on second | Red on first) = 2/3
- P(Red on second | Red first) = 1/3
- P(Red on second) = 1/2
- **Conclusion:** Draws are dependent

::::

::: {.notes}
We say that two events are independent if the probabilities for the second event remain the same even if you know that the first event has happened, no matter how the first event turns out. Otherwise, the events are said to be dependent.

If A and B are independent, P(B|A) = P(B). Consequently, the multiplication rule reduces to:

P(A ∩ B) = P(A) × P(B|A) = P(A) × P(B)

Usually the fastest and most convenient way to check if two events are independent is to see if the product of their probabilities is the same as the probability of their intersection.

For example, consider our box of red and blue tickets. When we draw with replacement, the probability of a red ticket on the second draw given a blue ticket on the first draw remains at 1/2. If we had a red ticket on the first draw, the probability of the second ticket being red is still 1/2. The probability doesn't change because it does not depend on the outcome of the first draw, since we put the ticket back. 

If we draw the tickets without replacement, we have seen that the probabilities of draws change. The probability of a blue ticket on the second draw given a red ticket on the first draw is 2/3, but the probability of a red ticket on the second draw given a red ticket on the first is 1/3.
::::

## Example: Colored and Numbered Tickets {.smaller}

::: {.columns}

::: {.column width="40%"}
![](images/indep-box.jpeg){width="400"}
::::

::: {.column width="60%"}
**Box 1:**

- P(3 | Red) = 1/3
- P(3 | Blue) = 0
- Color and number are dependent

**Box 2:**

- Equal proportions maintained
- Color and number are independent
- P(number | color) = P(number)
::::

::::

::: {.notes}
I have two boxes that with numbered tickets colored red or blue as shown. Are color and number independent or dependent for box 1? What about box 2? 

For example, is the probability of a ticket marked 1 the same whether the ticket is red or blue?

For box 1, color and number are dependent, since the probability of 3 given that the ticket is red is 1/3, but the probability of 3 given that the ticket is blue is 0 (and similarly for the probability of 4).

Even though the probability for 1 or 2 given the ticket is red is the same as the probability for 1 or 2 given the ticket is blue, we say that color and number are dependent because of the tickets marked 3 or 4.
::::

## Example: Two Numbers on Tickets {.smaller}

::: {.columns}

::: {.column width="50%"}
![](images/box-2-number.jpeg){width="500"}
::::

::: {.column width="50%"}
**Box 1:**

- First and second numbers are independent
- P(second = 6 | first = 1) = P(second = 6)

**Box 2:**

- Numbers are dependent
- P(second = 6 | first = 1) = 2/3
- P(second = 6) = 1/2
::::

::::

::: {.notes}
Now I have two boxes that with numbered tickets, where each ticket has two numbers on them, as shown. For each box, are the two numbers independent or dependent? For example, if I know that the first number is 1 does it change the probability of the second number being 6 (or the other way around: if I know the second number is 6, does it change the probability of the first number being 1)?

For box 1, the first number and second number are independent, as shown below, using 1 and 6 as examples. If we know that the first number is 1, the box reduces as shown. The probability of the second number being 6 does not change for box 1. The probability does change for box 2, increasing from 1/2 to 2/3, since the second number is more likely to be 6 if the first number is 1.
::::

## Mutually Exclusive vs Independent {background-color="#f8f8f8"}

::: {.callout-important}
If two events A and B (both with positive probability) are mutually exclusive, they **cannot** be independent!
::::

**Proof:**

- If mutually exclusive: P(A ∩ B) = 0
- For independence: P(A ∩ B) = P(A) × P(B)
- If P(A) > 0 and P(B) > 0:
  - P(A) × P(B) > 0
  - Therefore: P(A ∩ B) ≠ P(A) × P(B)

::: {.notes}
Note that if two events A and B, both with positive probability, are mutually exclusive, they cannot be independent. If P(A ∩ B) = 0, but neither P(A) = 0 nor P(B) = 0, then P(A ∩ B) = 0 ≠ P(A)×P(B). However, if two events are not independent, that does not mean they are mutually exclusive.
::::

## Inclusion-Exclusion Formula {auto-animate=true}

::: {.columns}

::: {.column width="50%"}
For any two events A and B:

$P(A \cup B) = P(A) + P(B) - P(A \cap B)$

**Why subtract the intersection?**
- Avoid double counting
- Only count overlap once
::::

::: {.column width="50%"}
![](images/incl-excl.png){width="400"}
::::

::::

::: {.notes}
Now that we know how to compute the probability of the intersection of two events, we can compute the probability of the union of two events. You can see that if we just add the probabilities of A and B, we double count the overlap. By subtracting it once, we can get the correct probability, and we know how to compute the probability of A∩B. This is known as the inclusion-exclusion principle.
::::

## De Méré's Paradox: Solution {.smaller}

::: {.panel-tabset}

### Game 1: Four Rolls

**At least one six in 4 rolls:**

1. Use complement: P(no sixes in 4 rolls)
2. Each roll independent: P(no six) = 5/6
3. Multiply: $(5/6)^4$
4. Therefore:
   $P(\text{at least one six}) = 1 - (5/6)^4 \approx 0.518$

### Game 2: Twenty-four Rolls

**At least one double six in 24 rolls:**

1. Use complement: P(no double sixes)
2. Each roll independent: P(no double six) = 35/36
3. Multiply: $(35/36)^{24}$
4. Therefore:
   $P(\text{at least one double six}) = 1 - (35/36)^{24} \approx 0.491$

### Comparison

- Game 1: ~51.8% chance of winning
- Game 2: ~49.1% chance of winning
- Matches simulation results!
- De Méré was right to be suspicious
::::

::: {.notes}
Let's finally compute the probability of rolling at least one six in 4 rolls of a fair six-sided die. This is much easier to compute if we use the complement rule. The complement of at least one six is no sixes in 4 rolls. Each roll is independent of the other rolls because what you roll does not affect the values of future rolls. This means that we can use the multiplication rule to figure out the chance of no sixes in any of the rolls. The chance of no six in any particular roll is 5/6 (there are five outcomes that are not six). 

The chance of no sixes in any of the 4 rolls is therefore (5/6)^4 (because the rolls are independent). Using the complement rule, we get that:
P(at least one six in 4 rolls) = 1 - P(no sixes in any of the 4 rolls) = 1 - (5/6)^4 ≈ 0.518

Similarly, the probability of at least 1 double six in 24 rolls of a pair of dice is given by:
1- P(no double sixes in any of the 24 rolls) = 1 - (35/36)^24 ≈ 0.491

By the way, notice that the simulation was pretty accurate!
::::

## Summary

::: {.incremental}

1. **Real-World Impact:**
   - Sally Clark case shows importance of proper probability understanding
   - Statistical errors can have devastating consequences

2. **Key Concepts:**
   - Conditional probability: P(B|A) = P(A∩B)/P(A)
   - Independence: P(B|A) = P(B)
   - Multiplication rule: P(A∩B) = P(A) × P(B|A)
   - Inclusion-exclusion: P(A∪B) = P(A) + P(B) - P(A∩B)

3. **Critical Distinctions:**
   - With vs without replacement
   - Independent vs mutually exclusive events
   - Conditional vs unconditional probability

::::

::: {.notes}
In this lecture, we do a deep dive into computing probabilities. It is well known that people are just not good at estimating probabilities of events, and we saw the tragic example of Sally Clark (who, even more sadly, is not a unique case).

We defined conditional probability and independence, and the multiplication rule, considering draws at random with and without replacement. We finally computed the probabilities in the dice games from 17th century France by combining the multiplication rule and the complement rule. 

We noted that independent events are very different from mutually exclusive events, and finally we learned how to compute probabilities of unions of events that may not be mutually exclusive with the inclusion-exclusion or generalized addition rule.
::::
