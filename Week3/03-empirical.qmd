---
r-fit-text: true
---

```{r}
#| include: false
library(dplyr)
library(reshape2)
library(tidyr)
library(ggplot2)
library(knitr)
library(readr)
library(cowplot)
library(janitor)
library(NHANES)
```

# Determining Probabilities {background-color="#1E3D59"}

## Three Approaches

::: {.columns}
::: {.column width="50%"}

1. **Personal Belief**
   - Subjective assessment
   - Based on knowledge/experience
   - Limited scientific validity
   - Often only available approach

2. **Empirical Frequency**
   - Based on repeated experiments
   - Law of large numbers
   - Real-world data collection
::::

::: {.column width="50%"}

3. **Classical Probability**
   - Based on equally likely outcomes
   - Mathematical approach
   - Common in games of chance
   - No experiments needed
::::
::::

::: {.notes}
Now that we know what a probability is, how do we actually figure out what the probability is for any particular event? There are three main approaches, each with their own strengths and limitations.
::::

## Personal Belief

::: {.columns}
::: {.column width="60%"}
**Example Question:**

What was the probability that Bernie Sanders would have won the 2016 presidential election if he had been the democratic nominee?

**Key Points:**

 - Can't run this experiment
 - People can still estimate based on knowledge
 - Not scientifically satisfying
 - Often the only available approach

::::

::: {.column width="40%"}
**Other Examples:**

 - Weather forecasts
 - Sports predictions
 - Economic forecasts
 - Personal decisions

::::
::::

::: {.notes}
Let's say that I asked you what the probability was that Bernie Sanders would have won the 2016 presidential election if he had been the democratic nominee instead of Hilary Clinton? We can't actually do the experiment to find the outcome. However, most people with knowledge of American politics would be willing to at least offer a guess at the probability of this event. In many cases personal knowledge and/or opinion is the only guide we have determining the probability of an event, but this is not very scientifically satisfying.
::::

## Empirical Frequency

::: {.columns}
::: {.column width="40%"}

**San Francisco Rain Example:**

- Total days in 2017: 365
- Rainy days: 73
- P(rain in SF) = 73/365 = 0.2

**Key Steps:**

1. Define experiment clearly
2. Count occurrences
3. Divide by total trials
::::

::: {.column width="60%"}

```{r}
#| echo: false
#| warning: false
#| message: false

# load data on rain in San Francisco and compute probability
SFrain <- read_csv("data/SanFranciscoRain-1329219.csv")

# create a new variable indicating whether it rained on each day
SFrain <- SFrain %>%
  mutate(rainToday = as.integer(PRCP > 0))

# Calculate running probability of rain
SFrain <- SFrain %>%
  mutate(
    day_number = row_number(),
    cumulative_rain = cumsum(rainToday),
    running_prob = cumulative_rain / day_number
  )

# Plot the running probability
ggplot(SFrain, aes(x = day_number, y = running_prob)) +
  geom_line() +
  geom_hline(yintercept = 0.2, color = "blue", linetype = "dashed") +
  labs(
    x = "Number of days",
    y = "Running probability of rain"
  ) +
  theme_minimal()
```

::::
::::

::: {.notes}
Another way to determine the probability of an event is to do the experiment many times and count how often each event happens. From the relative frequency of the different outcomes, we can compute the probability of each outcome. For example, let's say that we are interested in knowing the probability of rain in San Francisco. We first have to define the experiment --- let's say that we will look at the National Weather Service data for each day in 2017 and determine whether there was any rain at the downtown San Francisco weather station. According to these data, in 2017 there were 73 rainy days. To compute the probability of rain in San Francisco, we simply divide the number of rainy days by the number of days counted (365), giving P(rain in SF in 2017) = 0.2.

The graph shows how the empirical probability of rain converges to 0.2 as we accumulate more days of data throughout the year.
::::

## Law of Large Numbers

::: {.columns}
::: {.column width="40%"}

**Coin Flip Example:**

 - True probability of heads = 0.5
 - Small samples vary widely
 - More flips = better estimate
 - Converges to true probability
 - "Law of small numbers" fallacy

::::

::: {.column width="60%"}

```{r}
#| echo: false
#| warning: false
#| message: false
set.seed(123)
nflips <- 100
flips <- rbinom(nflips, 1, 0.5)
df <- data.frame(
  flip_number = 1:nflips,
  running_prob = cumsum(flips) / seq_along(flips)
)
ggplot(df, aes(x = flip_number, y = running_prob)) +
  geom_line() +
  geom_hline(yintercept = 0.5, linetype = "dashed", color = "blue") +
  labs(
    title = "Probability of Heads Over Many Flips",
    x = "Number of Coin Flips",
    y = "Running Probability of Heads"
  ) +
  theme_minimal()
```

::::
::::

::: {.notes}
The graph shows how early results from coin flips can be highly variable and unrepresentative of the true value. Even though we know a fair coin has a probability of 0.5 for heads, small samples can give very different results. This demonstrates how small samples can give misleading results.

This was referred to as the "law of small numbers" by psychologists Danny Kahneman and Amos Tversky, who showed that people (even trained researchers) often behave as if the law of large numbers applies even to small samples, leading them to draw strong conclusions from insufficient data.
::::

## Real-World Example: Alabama Election

::: {.columns}
::: {.column width="40%"}

**2017 Senate Race:**

 - Roy Moore vs Doug Jones
 - Early results volatile
 - Final outcome different
 - Small sample warning

::::

::: {.column width="60%"}

```{r}
#| echo: false
#| warning: false
#| message: false
electionReturns <-
  read_csv("data/alabama_election_returns.csv") %>%
  gather(candidate, pctVotes, -pctResp)

p2 <- electionReturns %>%
  ggplot(aes(pctResp, pctVotes, color = candidate)) +
  geom_line(aes(linetype = candidate), size = 1) +
  scale_color_manual(values = c("#9999CC", "#CC6666")) +
  labs(
    x = "Percentage of precincts reporting",
    y = "Percentage of votes"
  ) +
  theme_minimal() +
  theme(legend.position = c(.7, 0.8))
p2
```

::::
::::

::: {.notes}
A real-world example of this was seen in the 2017 special election for the US Senate in Alabama. Early in the evening the vote counts were especially volatile, swinging from a large initial lead for Jones to a long period where Moore had the lead, until finally Jones took the lead to win the race.

This demonstrates how small samples can give misleading results. Unfortunately, many people forget this and overinterpret results from small samples.
::::
